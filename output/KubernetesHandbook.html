
<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">      
<meta name=generator content=gitbook2pdf><meta name=title content=KubernetesHandbook><meta name=author content=Pengfei Ni><meta name=dcterms.created content=2019-03-05T14:16:04><meta name=dcterms.modified content=2019-03-05T14:16:04><body><h1 class='level1'>基础入门</h1><section class="normal markdown-section">
                                
                                <h1 id="kubernetes-指南" class="level2">Introduction</h1>
<p><a href="https://github.com/feiskyer/kubernetes-handbook" target="_blank"><img src="https://githubbadges.com/star.svg?user=feiskyer&repo=kubernetes-handbook&style=default" alt="star this repo"/></a> <a href="https://github.com/feiskyer/kubernetes-handbook/fork" target="_blank"><img src="https://githubbadges.com/fork.svg?user=feiskyer&repo=kubernetes-handbook&style=default" alt="fork this repo"/></a> <a href="https://github.com/feiskyer/kubernetes-handbook/issues" target="_blank"><img src="https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat" alt="contributions welcome"/></a></p>
<p>Kubernetes 是谷歌开源的容器集群管理系统，是 Google 多年大规模容器管理技术 Borg 的开源版本，也是 CNCF 最重要的项目之一，主要功能包括：</p>
<ul>
<li>基于容器的应用部署、维护和滚动升级</li>
<li>负载均衡和服务发现</li>
<li>跨机器和跨地区的集群调度</li>
<li>自动伸缩</li>
<li>无状态服务和有状态服务</li>
<li>广泛的 Volume 支持</li>
<li>插件机制保证扩展性</li>
</ul>
<p>Kubernetes 发展非常迅速，已经成为容器编排领域的领导者。Kubernetes 的中文资料也非常丰富，但系统化和紧跟社区更新的则就比较少见了。《Kubernetes 指南》开源电子书旨在整理平时在开发和使用 Kubernetes 时的参考指南和实践总结，形成一个系统化的参考指南以方便查阅。欢迎大家关注和添加完善内容。</p>
<h2 id="在线阅读">在线阅读</h2>
<ul>
<li>中文：<ul>
<li>Gitbook: <a href="https://kubernetes.feisky.xyz/" target="_blank">https://kubernetes.feisky.xyz/</a>（或者 <a href="https://feisky.xyz/kubernetes-handbook/" target="_blank">这里</a>）</li>
<li><a href="https://github.com/feiskyer/kubernetes-handbook/blob/master/SUMMARY.md" target="_blank">Github</a></li>
<li><a href="http://www.infoq.com/cn/minibooks/Kubernetes-handbook" target="_blank">InfoQ</a></li>
</ul>
</li>
<li><a href="https://github.com/feiskyer/kubernetes-handbook/blob/master/en/SUMMARY.md" target="_blank">English</a></li>
<li>PDF 电子书：点击 <a href="https://legacy.gitbook.com/download/pdf/book/feisky/kubernetes" target="_blank">这里</a> 下载</li>
</ul>
<h2 id="项目源码">项目源码</h2>
<p>项目源码存放于 Github 上，<a href="https://github.com/feiskyer/kubernetes-handbook" target="_blank">https://github.com/feiskyer/kubernetes-handbook</a>。</p>
<h3 id="本书版本更新记录">本书版本更新记录</h3>
<p>如无特殊说明，本指南所有文档仅适用于 Kubernetes v1.6 及以上版本。详细更新记录见 <a href="https://github.com/feiskyer/kubernetes-handbook/blob/master/CHANGELOG.md" target="_blank">CHANGELOG</a>。</p>
<h2 id="微信公众号">微信公众号</h2>
<p>扫码关注微信公众号，回复关键字即可在微信中查看相关章节。</p>
<p align="center"> <img src="images/qrcode.jpg"/></p>

<h2 id="贡献者">贡献者</h2>
<p>欢迎参与贡献和完善内容，贡献方法参考 <a href="https://github.com/feiskyer/kubernetes-handbook/blob/master/CONTRIBUTING.md" target="_blank">CONTRIBUTING</a>。感谢所有的贡献者，贡献者列表见 <a href="https://github.com/feiskyer/kubernetes-handbook/graphs/contributors" target="_blank">contributors</a>。</p>
<h2 id="license">LICENSE</h2>
<p><img src="https://licensebuttons.net/l/by-nc-sa/4.0/88x31.png" alt=""/></p>
<p><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank">署名-非商业性使用-相同方式共享 4.0 (CC BY-NC-SA 4.0)</a>。</p>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="kubernetes-简介" class="level2">Kubernetes简介</h1>
<p>Kubernetes 是谷歌开源的容器集群管理系统，是 Google 多年大规模容器管理技术 Borg 的开源版本，主要功能包括：</p>
<ul>
<li>基于容器的应用部署、维护和滚动升级</li>
<li>负载均衡和服务发现</li>
<li>跨机器和跨地区的集群调度</li>
<li>自动伸缩</li>
<li>无状态服务和有状态服务</li>
<li>广泛的 Volume 支持</li>
<li>插件机制保证扩展性</li>
</ul>
<p>Kubernetes 发展非常迅速，已经成为容器编排领域的领导者。</p>
<h2 id="kubernetes-是一个平台">Kubernetes 是一个平台</h2>
<p>Kubernetes 提供了很多的功能，它可以简化应用程序的工作流，加快开发速度。通常，一个成功的应用编排系统需要有较强的自动化能力，这也是为什么 Kubernetes 被设计作为构建组件和工具的生态系统平台，以便更轻松地部署、扩展和管理应用程序。</p>
<p>用户可以使用 Label 以自己的方式组织管理资源，还可以使用 Annotation 来自定义资源的描述信息，比如为管理工具提供状态检查等。</p>
<p>此外，Kubernetes 控制器也是构建在跟开发人员和用户使用的相同的 API 之上。用户还可以编写自己的控制器和调度器，也可以通过各种插件机制扩展系统的功能。</p>
<p>这种设计使得可以方便地在 Kubernetes 之上构建各种应用系统。</p>
<h2 id="kubernetes-不是什么">Kubernetes 不是什么</h2>
<p>Kubernetes 不是一个传统意义上，包罗万象的 PaaS (平台即服务) 系统。它给用户预留了选择的自由。</p>
<ul>
<li>不限制支持的应用程序类型，它不插手应用程序框架, 也不限制支持的语言 (如 Java, Python, Ruby 等)，只要应用符合 <a href="http://12factor.net/" target="_blank">12 因素</a> 即可。Kubernetes 旨在支持极其多样化的工作负载，包括无状态、有状态和数据处理工作负载。只要应用可以在容器中运行，那么它就可以很好的在 Kubernetes 上运行。</li>
<li>不提供内置的中间件 (如消息中间件)、数据处理框架 (如 Spark)、数据库 (如 mysql) 或集群存储系统 (如 Ceph) 等。这些应用直接运行在 Kubernetes 之上。</li>
<li>不提供点击即部署的服务市场。</li>
<li>不直接部署代码，也不会构建您的应用程序，但您可以在 Kubernetes 之上构建需要的持续集成 (CI) 工作流。</li>
<li>允许用户选择自己的日志、监控和告警系统。</li>
<li>不提供应用程序配置语言或系统 (如 <a href="https://github.com/google/jsonnet" target="_blank">jsonnet</a>)。</li>
<li>不提供机器配置、维护、管理或自愈系统。</li>
</ul>
<p>另外，已经有很多 PaaS 系统运行在 Kubernetes 之上，如 <a href="https://github.com/openshift/origin" target="_blank">Openshift</a>, <a href="http://deis.io/" target="_blank">Deis</a> 和 <a href="http://eldarion.cloud/" target="_blank">Eldarion</a> 等。 您也可以构建自己的 PaaS 系统，或者只使用 Kubernetes 管理您的容器应用。</p>
<p>当然了，Kubernetes 不仅仅是一个 “编排系统”，它消除了编排的需要。Kubernetes 通过声明式的 API 和一系列独立、可组合的控制器保证了应用总是在期望的状态，而用户并不需要关心中间状态是如何转换的。这使得整个系统更容易使用，而且更强大、更可靠、更具弹性和可扩展性。</p>
<h2 id="核心组件">核心组件</h2>
<p>Kubernetes 主要由以下几个核心组件组成：</p>
<ul>
<li>etcd 保存了整个集群的状态；</li>
<li>apiserver 提供了资源操作的唯一入口，并提供认证、授权、访问控制、API 注册和发现等机制；</li>
<li>controller manager 负责维护集群的状态，比如故障检测、自动扩展、滚动更新等；</li>
<li>scheduler 负责资源的调度，按照预定的调度策略将 Pod 调度到相应的机器上；</li>
<li>kubelet 负责维护容器的生命周期，同时也负责 Volume（CVI）和网络（CNI）的管理；</li>
<li>Container runtime 负责镜像管理以及 Pod 和容器的真正运行（CRI）；</li>
<li>kube-proxy 负责为 Service 提供 cluster 内部的服务发现和负载均衡</li>
</ul>
<p><img src="architecture.png" alt=""/></p>
<h2 id="kubernetes-版本">Kubernetes 版本</h2>
<p>Kubernetes 的稳定版本在发布后会继续支持 9 个月。每个版本的支持周期为：</p>
<table>
<thead>
<tr>
<th>Kubernetes version</th>
<th>Release month</th>
<th>End-of-life-month</th>
</tr>
</thead>
<tbody>
<tr>
<td>v1.6.x</td>
<td>March 2017</td>
<td>December 2017</td>
</tr>
<tr>
<td>v1.7.x</td>
<td>June 2017</td>
<td>March 2018</td>
</tr>
<tr>
<td>v1.8.x</td>
<td>September 2017</td>
<td>June 2018</td>
</tr>
<tr>
<td>v1.9.x</td>
<td>December 2017</td>
<td>September 2018</td>
</tr>
<tr>
<td>v1.10.x</td>
<td>March 2018</td>
<td>December 2018</td>
</tr>
<tr>
<td>v1.11.x</td>
<td>June 2018</td>
<td>March 2019</td>
</tr>
</tbody>
</table>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/" target="_blank">What is Kubernetes?</a></li>
<li><a href="https://apprenda.com/blog/customers-really-using-kubernetes/" target="_blank">HOW CUSTOMERS ARE REALLY USING KUBERNETES</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="kubernetes-基本概念" class="level2">Kubernetes基本概念</h1>
<h2 id="container">Container</h2>
<p>Container（容器）是一种便携式、轻量级的操作系统级虚拟化技术。它使用 namespace 隔离不同的软件运行环境，并通过镜像自包含软件的运行环境，从而使得容器可以很方便的在任何地方运行。</p>
<p>由于容器体积小且启动快，因此可以在每个容器镜像中打包一个应用程序。这种一对一的应用镜像关系拥有很多好处。使用容器，不需要与外部的基础架构环境绑定, 因为每一个应用程序都不需要外部依赖，更不需要与外部的基础架构环境依赖。完美解决了从开发到生产环境的一致性问题。</p>
<p>容器同样比虚拟机更加透明，这有助于监测和管理。尤其是容器进程的生命周期由基础设施管理，而不是被进程管理器隐藏在容器内部。最后，每个应用程序用容器封装，管理容器部署就等同于管理应用程序部署。</p>
<p>其他容器的优点还包括</p>
<ul>
<li>敏捷的应用程序创建和部署: 与虚拟机镜像相比，容器镜像更易用、更高效。</li>
<li>持续开发、集成和部署: 提供可靠与频繁的容器镜像构建、部署和快速简便的回滚（镜像是不可变的）。</li>
<li>开发与运维的关注分离: 在构建/发布时即创建容器镜像，从而将应用与基础架构分离。</li>
<li>开发、测试与生产环境的一致性: 在笔记本电脑上运行和云中一样。</li>
<li>可观测：不仅显示操作系统的信息和度量，还显示应用自身的信息和度量。</li>
<li>云和操作系统的分发可移植性: 可运行在 Ubuntu, RHEL, CoreOS, 物理机, GKE 以及其他任何地方。</li>
<li>以应用为中心的管理: 从传统的硬件上部署操作系统提升到操作系统中部署应用程序。</li>
<li>松耦合、分布式、弹性伸缩、微服务: 应用程序被分成更小，更独立的模块，并可以动态管理和部署 - 而不是运行在专用设备上的大型单体程序。</li>
<li>资源隔离：可预测的应用程序性能。</li>
<li>资源利用：高效率和高密度。</li>
</ul>
<h2 id="pod">Pod</h2>
<p>Kubernetes 使用 Pod 来管理容器，每个 Pod 可以包含一个或多个紧密关联的容器。</p>
<p>Pod 是一组紧密关联的容器集合，它们共享 PID、IPC、Network 和 UTS namespace，是 Kubernetes 调度的基本单位。Pod 内的多个容器共享网络和文件系统，可以通过进程间通信和文件共享这种简单高效的方式组合完成服务。</p>
<p><img src="media/pod.png" alt="pod"/></p>
<p>在 Kubernetes 中，所有对象都使用 manifest（yaml 或 json）来定义，比如一个简单的 nginx 服务可以定义为 nginx.yaml，它包含一个镜像为 nginx 的容器：</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> nginx
<span class="hljs-attr">  labels:</span>
<span class="hljs-attr">    app:</span> nginx
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">  - name:</span> nginx
<span class="hljs-attr">    image:</span> nginx
<span class="hljs-attr">    ports:</span>
<span class="hljs-attr">    - containerPort:</span> <span class="hljs-number">80</span>
</code></pre>
<h2 id="node">Node</h2>
<p>Node 是 Pod 真正运行的主机，可以是物理机，也可以是虚拟机。为了管理 Pod，每个 Node 节点上至少要运行 container runtime（比如 docker 或者 rkt）、<code>kubelet</code> 和 <code>kube-proxy</code> 服务。</p>
<p><img src="media/node.png" alt="node"/></p>
<h2 id="namespace">Namespace</h2>
<p>Namespace 是对一组资源和对象的抽象集合，比如可以用来将系统内部的对象划分为不同的项目组或用户组。常见的 pods, services, replication controllers 和 deployments 等都是属于某一个 namespace 的（默认是 default），而 node, persistentVolumes 等则不属于任何 namespace。</p>
<h2 id="service">Service</h2>
<p>Service 是应用服务的抽象，通过 labels 为应用提供负载均衡和服务发现。匹配 labels 的 Pod IP 和端口列表组成 endpoints，由 kube-proxy 负责将服务 IP 负载均衡到这些 endpoints 上。</p>
<p>每个 Service 都会自动分配一个 cluster IP（仅在集群内部可访问的虚拟地址）和 DNS 名，其他容器可以通过该地址或 DNS 来访问服务，而不需要了解后端容器的运行。</p>
<p><img src="media/14731220608865.png" alt=""/></p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Service
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> nginx
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  ports:</span>
<span class="hljs-attr">  - port:</span> <span class="hljs-number">8078</span> <span class="hljs-comment"># the port that this service should serve on</span>
<span class="hljs-attr">    name:</span> http
    <span class="hljs-comment"># the container on each pod to connect to, can be a name</span>
    <span class="hljs-comment"># (e.g. 'www') or a number (e.g. 80)</span>
<span class="hljs-attr">    targetPort:</span> <span class="hljs-number">80</span>
<span class="hljs-attr">    protocol:</span> TCP
<span class="hljs-attr">  selector:</span>
<span class="hljs-attr">    app:</span> nginx
</code></pre>
<h2 id="label">Label</h2>
<p>Label 是识别 Kubernetes 对象的标签，以 key/value 的方式附加到对象上（key 最长不能超过 63 字节，value 可以为空，也可以是不超过 253 字节的字符串）。</p>
<p>Label 不提供唯一性，并且实际上经常是很多对象（如 Pods）都使用相同的 label 来标志具体的应用。</p>
<p>Label 定义好后其他对象可以使用 Label Selector 来选择一组相同 label 的对象（比如 ReplicaSet 和 Service 用 label 来选择一组 Pod）。Label Selector 支持以下几种方式：</p>
<ul>
<li>等式，如 <code>app=nginx</code> 和 <code>env!=production</code></li>
<li>集合，如 <code>env in (production, qa)</code></li>
<li>多个 label（它们之间是 AND 关系），如 <code>app=nginx,env=test</code></li>
</ul>
<h2 id="annotations">Annotations</h2>
<p>Annotations 是 key/value 形式附加于对象的注解。不同于 Labels 用于标志和选择对象，Annotations 则是用来记录一些附加信息，用来辅助应用部署、安全策略以及调度策略等。比如 deployment 使用 annotations 来记录 rolling update 的状态。</p>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="kubernetes-101" class="level2">Kubernetes101</h1>
<p>体验 Kubernetes 最简单的方法是跑一个 nginx 容器，然后使用 kubectl 操作该容器。Kubernetes 提供了一个类似于 <code>docker run</code> 的命令 <code>kubectl run</code>，可以方便的创建一个容器（实际上创建的是一个由 deployment 来管理的 Pod）：</p>
<pre><code class="lang-sh">$ kubectl run --image=nginx:alpine nginx-app --port=80
deployment <span class="hljs-string">"nginx-app"</span> created
$ kubectl get pods
NAME                         READY     STATUS    RESTARTS   AGE
nginx-app-4028413181-cnt1i   1/1       Running   0          52s
</code></pre>
<p>等到容器变成 Running 后，就可以用 <code>kubectl</code> 命令来操作它了，比如</p>
<ul>
<li><code>kubectl get</code> - 类似于 <code>docker ps</code>，查询资源列表</li>
<li><code>kubectl describe</code> - 类似于 <code>docker inspect</code>，获取资源的详细信息</li>
<li><code>kubectl logs</code> - 类似于 <code>docker logs</code>，获取容器的日志</li>
<li><code>kubectl exec</code> - 类似于 <code>docker exec</code>，在容器内执行一个命令</li>
</ul>
<pre><code class="lang-sh">$ kubectl get pods
NAME                         READY     STATUS    RESTARTS   AGE
nginx-app-4028413181-cnt1i   1/1       Running   0          6m

$ kubectl <span class="hljs-built_in">exec</span> nginx-app-4028413181-cnt1i ps aux
USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root         1  0.0  0.5  31736  5108 ?        Ss   00:19   0:00 nginx: master process nginx -g daemon off;
nginx        5  0.0  0.2  32124  2844 ?        S    00:19   0:00 nginx: worker process
root        18  0.0  0.2  17500  2112 ?        Rs   00:25   0:00 ps aux

$ kubectl describe pod nginx-app-4028413181-cnt1i
Name:          nginx-app-4028413181-cnt1i
Namespace:         default
Node:          boot2docker/192.168.64.12
Start Time:        Tue, 06 Sep 2016 08:18:41 +0800
Labels:        pod-template-hash=4028413181
               run=nginx-app
Status:        Running
IP:            172.17.0.3
Controllers:       ReplicaSet/nginx-app-4028413181
Containers:
  nginx-app:
    Container ID:              docker://4ef989b57d0a7638ad9c5bbc22e16d5ea5b459281c77074<span class="hljs-built_in">fc</span>982eba50973107f
    Image:                 nginx
    Image ID:              docker://sha256:4efb2fcdb1ab05fb03c9435234343c1cc65289eeb016be86193e88d3a5d84f6b
    Port:                  80/TCP
    State:                 Running
      Started:             Tue, 06 Sep 2016 08:19:30 +0800
    Ready:                 True
    Restart Count:             0
    Environment Variables:         <none>
Conditions:
  Type         Status
  Initialized      True
  Ready            True
  PodScheduled     True
Volumes:
  default-token-9o8ks:
    Type:          Secret (a volume populated by a Secret)
    SecretName:    default-token-9o8ks
QoS Tier:          BestEffort
Events:
  FirstSeen        LastSeen           Count      From               SubobjectPath              Type           Reason         Message
  ---------        --------           -----      ----               -------------              --------           ------         -------
  8m           8m             1          {default-scheduler}                       Normal         Scheduled          Successfully assigned nginx-app-4028413181-cnt1i to boot2docker
  8m           8m             1          {kubelet boot2docker}      spec.containers{nginx-app}         Normal         Pulling        pulling image <span class="hljs-string">"nginx"</span>
  7m           7m             1          {kubelet boot2docker}      spec.containers{nginx-app}         Normal         Pulled         Successfully pulled image <span class="hljs-string">"nginx"</span>
  7m           7m             1          {kubelet boot2docker}      spec.containers{nginx-app}         Normal         Created        Created container with docker id 4ef989b57d0a
  7m           7m             1          {kubelet boot2docker}      spec.containers{nginx-app}         Normal         Started        Started container with docker id 4ef989b57d0a

$ curl http://172.17.0.3
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>
<p>For online documentation and support please refer to
<a href=<span class="hljs-string">"http://nginx.org/"</span>>nginx.org</a>.<br/>
Commercial support is available at
<a href=<span class="hljs-string">"http://nginx.com/"</span>>nginx.com</a>.</p>
<p><em>Thank you <span class="hljs-keyword">for</span> using nginx.</em></p>
</body>
</html>

$ kubectl logs nginx-app-4028413181-cnt1i
127.0.0.1 - - [06/Sep/2016:00:27:13 +0000] <span class="hljs-string">"GET / HTTP/1.0"</span> 200 612 <span class="hljs-string">"-"</span> <span class="hljs-string">"-"</span> <span class="hljs-string">"-"</span>
</code></pre>
<h2 id="使用-yaml-定义-pod">使用 yaml 定义 Pod</h2>
<p>上面是通过 <code>kubectl run</code> 来启动了第一个 Pod，但是 <code>kubectl run</code> 并不支持所有的功能。在 Kubernetes 中，更经常使用 yaml 文件来定义资源，并通过 <code>kubectl create -f file.yaml</code> 来创建资源。比如，一个简单的 nginx Pod 可以定义为：</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> nginx
<span class="hljs-attr">  labels:</span>
<span class="hljs-attr">    app:</span> nginx
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">  - name:</span> nginx
<span class="hljs-attr">    image:</span> nginx
<span class="hljs-attr">    ports:</span>
<span class="hljs-attr">    - containerPort:</span> <span class="hljs-number">80</span>
</code></pre>
<p>前面提到，<code>kubectl run</code> 并不是直接创建一个 Pod，而是先创建一个 Deployment 资源（replicas=1），再由与 Deployment 关联的 ReplicaSet 来自动创建 Pod，这等价于这样一个配置：</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> extensions/v1beta1
<span class="hljs-attr">kind:</span> Deployment
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  labels:</span>
<span class="hljs-attr">    run:</span> nginx-app
<span class="hljs-attr">  name:</span> nginx-app
<span class="hljs-attr">  namespace:</span> default
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  replicas:</span> <span class="hljs-number">1</span>
<span class="hljs-attr">  selector:</span>
<span class="hljs-attr">    matchLabels:</span>
<span class="hljs-attr">      run:</span> nginx-app
<span class="hljs-attr">  strategy:</span>
<span class="hljs-attr">    rollingUpdate:</span>
<span class="hljs-attr">      maxSurge:</span> <span class="hljs-number">1</span>
<span class="hljs-attr">      maxUnavailable:</span> <span class="hljs-number">1</span>
<span class="hljs-attr">    type:</span> RollingUpdate
<span class="hljs-attr">  template:</span>
<span class="hljs-attr">    metadata:</span>
<span class="hljs-attr">      labels:</span>
<span class="hljs-attr">        run:</span> nginx-app
<span class="hljs-attr">    spec:</span>
<span class="hljs-attr">      containers:</span>
<span class="hljs-attr">      - image:</span> nginx
<span class="hljs-attr">        name:</span> nginx-app
<span class="hljs-attr">        ports:</span>
<span class="hljs-attr">        - containerPort:</span> <span class="hljs-number">80</span>
<span class="hljs-attr">          protocol:</span> TCP
<span class="hljs-attr">      dnsPolicy:</span> ClusterFirst
<span class="hljs-attr">      restartPolicy:</span> Always
</code></pre>
<h2 id="使用-volume">使用 Volume</h2>
<p>Pod 的生命周期通常比较短，只要出现了异常，就会创建一个新的 Pod 来代替它。那容器产生的数据呢？容器内的数据会随着 Pod 消亡而自动消失。Volume 就是为了持久化容器数据而生，比如可以为 redis 容器指定一个 hostPath 来存储 redis 数据：</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> redis
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">  - name:</span> redis
<span class="hljs-attr">    image:</span> redis
<span class="hljs-attr">    volumeMounts:</span>
<span class="hljs-attr">    - name:</span> redis-persistent-storage
<span class="hljs-attr">      mountPath:</span> /data/redis
<span class="hljs-attr">  volumes:</span>
<span class="hljs-attr">  - name:</span> redis-persistent-storage
<span class="hljs-attr">    hostPath:</span>
<span class="hljs-attr">      path:</span> /data/
</code></pre>
<p>Kubernetes volume 支持非常多的插件，可以根据实际需要来选择：</p>
<ul>
<li>emptyDir</li>
<li>hostPath</li>
<li>gcePersistentDisk</li>
<li>awsElasticBlockStore</li>
<li>nfs</li>
<li>iscsi</li>
<li>flocker</li>
<li>glusterfs</li>
<li>rbd</li>
<li>cephfs</li>
<li>gitRepo</li>
<li>secret</li>
<li>persistentVolumeClaim</li>
<li>downwardAPI</li>
<li>azureFileVolume</li>
<li>vsphereVolume</li>
</ul>
<h2 id="使用-service">使用 Service</h2>
<p>前面虽然创建了 Pod，但是在 kubernetes 中，Pod 的 IP 地址会随着 Pod 的重启而变化，并不建议直接拿 Pod 的 IP 来交互。那如何来访问这些 Pod 提供的服务呢？使用 Service。Service 为一组 Pod（通过 labels 来选择）提供一个统一的入口，并为它们提供负载均衡和自动服务发现。比如，可以为前面的 <code>nginx-app</code> 创建一个 service：</p>
<pre><code class="lang-yaml">$ kubectl expose deployment nginx-app --port=<span class="hljs-number">80</span> --target-port=<span class="hljs-number">80</span> --type=NodePort
service <span class="hljs-string">"nginx-app"</span> exposed
$ kubectl describe service nginx-app
<span class="hljs-attr">Name:</span>              nginx-app
<span class="hljs-attr">Namespace:</span>             default
<span class="hljs-attr">Labels:</span>            run=nginx-app
<span class="hljs-attr">Selector:</span>              run=nginx-app
<span class="hljs-attr">Type:</span>              ClusterIP
<span class="hljs-attr">IP:</span>                <span class="hljs-number">10.0</span><span class="hljs-number">.0</span><span class="hljs-number">.66</span>
<span class="hljs-attr">Port:</span>              <unset>    <span class="hljs-number">80</span>/TCP
<span class="hljs-attr">NodePort:</span>              <unset>    <span class="hljs-number">30772</span>/TCP
<span class="hljs-attr">Endpoints:</span>             <span class="hljs-number">172.17</span><span class="hljs-number">.0</span><span class="hljs-number">.3</span>:<span class="hljs-number">80</span>
Session Affinity:          None
<span class="hljs-literal">No</span> events.
</code></pre>
<p>这样，在 cluster 内部就可以通过 <code>http://10.0.0.66</code> 和 <code>http://node-ip:30772</code> 来访问 nginx-app。而在 cluster 外面，则只能通过 <code>http://node-ip:30772</code> 来访问。</p>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="kubernetes-201" class="level2">Kubernetes201</h1>
<h2 id="扩展应用">扩展应用</h2>
<p>通过修改 Deployment 中副本的数量（replicas），可以动态扩展或收缩应用：</p>
<p><img src="media/scale.png" alt="scale"/></p>
<p>这些自动扩展的容器会自动加入到 service 中，而收缩回收的容器也会自动从 service 中删除。</p>
<pre><code class="lang-sh">$ kubectl scale --replicas=3 deployment/nginx-app
$ kubectl get deploy
NAME        DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-app   3         3         3            3           10m
</code></pre>
<h2 id="滚动升级">滚动升级</h2>
<p>滚动升级（Rolling Update）通过逐个容器替代升级的方式来实现无中断的服务升级：</p>
<pre><code>kubectl rolling-update frontend-v1 frontend-v2 --image=image:v2
</code></pre><p><img src="media/update1.png" alt="update1"/></p>
<p><img src="media/update2.png" alt="update2"/></p>
<p><img src="media/update3.png" alt="update3"/></p>
<p><img src="media/update4.png" alt="update4"/></p>
<p>在滚动升级的过程中，如果发现了失败或者配置错误，还可以随时回滚：</p>
<pre><code>kubectl rolling-update frontend-v1 frontend-v2 --rollback
</code></pre><p>需要注意的是，<code>kubectl rolling-update</code> 只针对 ReplicationController。对于更新策略是 RollingUpdate 的 Deployment（Deployment 可以在 spec 中设置更新策略为 RollingUpdate，默认就是 RollingUpdate），更新应用后会自动滚动升级：</p>
<pre><code class="lang-yaml"><span class="hljs-attr">  spec:</span>
<span class="hljs-attr">    replicas:</span> <span class="hljs-number">3</span>
<span class="hljs-attr">    selector:</span>
<span class="hljs-attr">      matchLabels:</span>
<span class="hljs-attr">        run:</span> nginx-app
<span class="hljs-attr">    strategy:</span>
<span class="hljs-attr">      rollingUpdate:</span>
<span class="hljs-attr">        maxSurge:</span> <span class="hljs-number">1</span>
<span class="hljs-attr">        maxUnavailable:</span> <span class="hljs-number">1</span>
<span class="hljs-attr">      type:</span> RollingUpdate
</code></pre>
<p>而更新应用的话，就可以直接用 <code>kubectl set</code> 命令：</p>
<pre><code class="lang-sh">kubectl <span class="hljs-built_in">set</span> image deployment/nginx-app nginx-app=nginx:1.9.1
</code></pre>
<p>滚动升级的过程可以用 <code>rollout</code> 命令查看:</p>
<pre><code class="lang-sh">$ kubectl rollout status deployment/nginx-app
Waiting <span class="hljs-keyword">for</span> rollout to finish: 2 out of 3 new replicas have been updated...
Waiting <span class="hljs-keyword">for</span> rollout to finish: 2 of 3 updated replicas are available...
Waiting <span class="hljs-keyword">for</span> rollout to finish: 2 of 3 updated replicas are available...
Waiting <span class="hljs-keyword">for</span> rollout to finish: 2 of 3 updated replicas are available...
Waiting <span class="hljs-keyword">for</span> rollout to finish: 2 of 3 updated replicas are available...
Waiting <span class="hljs-keyword">for</span> rollout to finish: 2 of 3 updated replicas are available...
deployment <span class="hljs-string">"nginx-app"</span> successfully rolled out
</code></pre>
<p>Deployment 也支持回滚：</p>
<pre><code class="lang-sh">$ kubectl rollout <span class="hljs-built_in">history</span> deployment/nginx-app
deployments <span class="hljs-string">"nginx-app"</span>
REVISION    CHANGE-CAUSE
1        <none>
2        <none>

$ kubectl rollout undo deployment/nginx-app
deployment <span class="hljs-string">"nginx-app"</span> rolled back
</code></pre>
<h2 id="资源限制">资源限制</h2>
<p>Kubernetes 通过 cgroups 提供容器资源管理的功能，可以限制每个容器的 CPU 和内存使用，比如对于刚才创建的 deployment，可以通过下面的命令限制 nginx 容器最多只用 50% 的 CPU 和 128MB 的内存：</p>
<pre><code class="lang-sh">$ kubectl <span class="hljs-built_in">set</span> resources deployment nginx-app -c=nginx --limits=cpu=500m,memory=128Mi
deployment <span class="hljs-string">"nginx"</span> resource requirements updated
</code></pre>
<p>这等同于在每个 Pod 中设置 resources limits：</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  labels:</span>
<span class="hljs-attr">    app:</span> nginx
<span class="hljs-attr">  name:</span> nginx
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">    - image:</span> nginx
<span class="hljs-attr">      name:</span> nginx
<span class="hljs-attr">      resources:</span>
<span class="hljs-attr">        limits:</span>
<span class="hljs-attr">          cpu:</span> <span class="hljs-string">"500m"</span>
<span class="hljs-attr">          memory:</span> <span class="hljs-string">"128Mi"</span>
</code></pre>
<h2 id="健康检查">健康检查</h2>
<p>Kubernetes 作为一个面向应用的集群管理工具，需要确保容器在部署后确实处在正常的运行状态。Kubernetes 提供了两种探针（Probe，支持 exec、tcpSocket 和 http 方式）来探测容器的状态：</p>
<ul>
<li>LivenessProbe：探测应用是否处于健康状态，如果不健康则删除并重新创建容器</li>
<li>ReadinessProbe：探测应用是否启动完成并且处于正常服务状态，如果不正常则不会接收来自 Kubernetes Service 的流量</li>
</ul>
<p>对于已经部署的 deployment，可以通过 <code>kubectl edit deployment/nginx-app</code> 来更新 manifest，增加健康检查部分：</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> extensions/v1beta1
<span class="hljs-attr">kind:</span> Deployment
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  labels:</span>
<span class="hljs-attr">    app:</span> nginx
<span class="hljs-attr">  name:</span> nginx-default
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  replicas:</span> <span class="hljs-number">3</span>
<span class="hljs-attr">  selector:</span>
<span class="hljs-attr">    matchLabels:</span>
<span class="hljs-attr">      app:</span> nginx
<span class="hljs-attr">  template:</span>
<span class="hljs-attr">    metadata:</span>
<span class="hljs-attr">      labels:</span>
<span class="hljs-attr">        app:</span> nginx
<span class="hljs-attr">    spec:</span>
<span class="hljs-attr">      containers:</span>
<span class="hljs-attr">      - image:</span> nginx
<span class="hljs-attr">        imagePullPolicy:</span> Always
<span class="hljs-attr">        name:</span> http
<span class="hljs-attr">        resources:</span> {}
<span class="hljs-attr">        terminationMessagePath:</span> /dev/termination-log
<span class="hljs-attr">        terminationMessagePolicy:</span> File
<span class="hljs-attr">        resources:</span>
<span class="hljs-attr">          limits:</span>
<span class="hljs-attr">            cpu:</span> <span class="hljs-string">"500m"</span>
<span class="hljs-attr">            memory:</span> <span class="hljs-string">"128Mi"</span>
<span class="hljs-attr">        livenessProbe:</span>
<span class="hljs-attr">          httpGet:</span>
<span class="hljs-attr">            path:</span> /
<span class="hljs-attr">            port:</span> <span class="hljs-number">80</span>
<span class="hljs-attr">          initialDelaySeconds:</span> <span class="hljs-number">15</span>
<span class="hljs-attr">          timeoutSeconds:</span> <span class="hljs-number">1</span>
<span class="hljs-attr">        readinessProbe:</span>
<span class="hljs-attr">          httpGet:</span>
<span class="hljs-attr">            path:</span> /
<span class="hljs-attr">            port:</span> <span class="hljs-number">80</span>
<span class="hljs-attr">          initialDelaySeconds:</span> <span class="hljs-number">5</span>
<span class="hljs-attr">          timeoutSeconds:</span> <span class="hljs-number">1</span>
</code></pre>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="kubernetes-集群" class="level2">Kubernetes集群</h1>
<p><img src="architecture.png" alt=""/></p>
<p>一个 Kubernetes 集群由分布式存储 etcd、控制节点 controller 以及服务节点 Node 组成。</p>
<ul>
<li>控制节点主要负责整个集群的管理，比如容器的调度、维护资源的状态、自动扩展以及滚动更新等</li>
<li>服务节点是真正运行容器的主机，负责管理镜像和容器以及 cluster 内的服务发现和负载均衡</li>
<li>etcd 集群保存了整个集群的状态</li>
</ul>
<p>详细的介绍请参考 <a href="../architecture/architecture.html">Kubernetes 架构</a>。</p>
<h2 id="集群联邦">集群联邦</h2>
<p>集群联邦（Federation）用于跨可用区的 Kubernetes 集群，需要配合云服务商（如 GCE、AWS）一起实现。</p>
<p><img src="federation.png" alt=""/></p>
<p>详细的介绍请参考 <a href="../components/federation.html">Federation</a>。</p>
<h2 id="创建-kubernetes-集群">创建 Kubernetes 集群</h2>
<p>可以参考 <a href="../deploy/">Kubernetes 部署指南</a> 来部署一套 Kubernetes 集群。而对于初学者或者简单验证测试的用户，则可以使用以下几种更简单的方法。</p>
<h3 id="minikube">minikube</h3>
<p>创建 Kubernetes cluster（单机版）最简单的方法是 <a href="https://github.com/kubernetes/minikube" target="_blank">minikube</a>:</p>
<pre><code class="lang-sh">$ minikube start
Starting <span class="hljs-built_in">local</span> Kubernetes cluster...
Kubectl is now configured to use the cluster.
$ kubectl cluster-info
Kubernetes master is running at https://192.168.64.12:8443
kubernetes-dashboard is running at https://192.168.64.12:8443/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard

To further debug and diagnose cluster problems, use <span class="hljs-string">'kubectl cluster-info dump'</span>.
</code></pre>
<h3 id="play-with-k8s">play-with-k8s</h3>
<p><a href="http://play-with-k8s.com" target="_blank">Play with Kubernetes</a> 提供了一个免费的 Kubernetes 体验环境，直接访问 < <a href="http://play-with-k8s.com" target="_blank">http://play-with-k8s.com</a> > 就可以使用 kubeadm 来创建 Kubernetes 集群。注意，每次创建的集群最长可以使用 4 小时。</p>
<p>Play with Kubernetes 有个非常方便的功能：自动在页面上显示所有 NodePort 类型服务的端口，点击该端口即可访问对应的服务。</p>
<p>详细使用方法可以参考 <a href="https://github.com/feiskyer/kubernetes-handbook/blob/master/zh/appendix/play-with-k8s.md" target="_blank">Play-With-Kubernetes</a>。</p>
<h3 id="katacoda-playground">Katacoda playground</h3>
<p><a href="https://www.katacoda.com/courses/kubernetes/playground" target="_blank">Katacoda playground</a>也提供了一个免费的 2 节点 Kubernetes 体验环境，网络基于 WeaveNet，并且会自动部署整个集群。但要注意，刚打开 <a href="https://www.katacoda.com/courses/kubernetes/playground" target="_blank">Katacoda playground</a> 页面时集群有可能还没初始化完成，可以在 master 节点上运行 <code>launch.sh</code> 等待集群初始化完成。</p>
<p>部署并访问 kubernetes dashboard 的方法：</p>
<pre><code class="lang-sh"><span class="hljs-comment"># 在 master node 上面运行</span>
kubectl create <span class="hljs-_">-f</span> https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml
kubectl proxy --address=<span class="hljs-string">'0.0.0.0'</span> --port=8080 --accept-hosts=<span class="hljs-string">'^*$'</span>&
</code></pre>
<p>然后点击 Terminal Host 1 右边的➕，从弹出的菜单里选择 View HTTP port 8080 on Host 1，即可打开 Kubernetes 的 API 页面。在该网址后面增加 <code>/ui</code> 即可访问 dashboard。</p>
</section>
                            
    <h1 class='level1'>核心原理</h1><section class="normal markdown-section">
                                
                                <h1 id="核心原理" class="level2">核心原理</h1>
<p>介绍 Kubernetes 架构以及核心组件，包括</p>
<ul>
<li><a href="architecture.html">架构原理</a></li>
<li><a href="concepts.html">设计理念</a></li>
<li><a href="../components/">核心组件</a><ul>
<li><a href="../components/etcd.html">etcd</a></li>
<li><a href="../components/apiserver.html">kube-apiserver</a></li>
<li><a href="../components/scheduler.html">kube-scheduler</a></li>
<li><a href="../components/controller-manager.html">kube-controller-manager</a></li>
<li><a href="../components/kubelet.html">kubelet</a></li>
<li><a href="../components/kube-proxy.html">kube-proxy</a></li>
<li><a href="../components/kube-dns.html">kube-dns</a></li>
<li><a href="../components/federation.html">Federation</a></li>
<li><a href="../components/kubeadm.html">kubeadm</a></li>
<li><a href="../components/hyperkube.html">hyperkube</a></li>
<li><a href="../components/kubectl.html">kubectl</a> </li>
</ul>
</li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="kubernetes-架构" class="level2">架构原理</h1>
<p>Kubernetes 最初源于谷歌内部的 Borg，提供了面向应用的容器集群部署和管理系统。Kubernetes 的目标旨在消除编排物理 / 虚拟计算，网络和存储基础设施的负担，并使应用程序运营商和开发人员完全将重点放在以容器为中心的原语上进行自助运营。Kubernetes 也提供稳定、兼容的基础（平台），用于构建定制化的 workflows 和更高级的自动化任务。
Kubernetes 具备完善的集群管理能力，包括多层次的安全防护和准入机制、多租户应用支撑能力、透明的服务注册和服务发现机制、内建负载均衡器、故障发现和自我修复能力、服务滚动升级和在线扩容、可扩展的资源自动调度机制、多粒度的资源配额管理能力。
Kubernetes 还提供完善的管理工具，涵盖开发、部署测试、运维监控等各个环节。</p>
<h2 id="borg-简介">Borg 简介</h2>
<p>Borg 是谷歌内部的大规模集群管理系统，负责对谷歌内部很多核心服务的调度和管理。Borg 的目的是让用户能够不必操心资源管理的问题，让他们专注于自己的核心业务，并且做到跨多个数据中心的资源利用率最大化。</p>
<p>Borg 主要由 BorgMaster、Borglet、borgcfg 和 Scheduler 组成，如下图所示</p>
<p><img src="images/borg.png" alt="borg"/></p>
<ul>
<li>BorgMaster 是整个集群的大脑，负责维护整个集群的状态，并将数据持久化到 Paxos 存储中；</li>
<li>Scheduer 负责任务的调度，根据应用的特点将其调度到具体的机器上去；</li>
<li>Borglet 负责真正运行任务（在容器中）；</li>
<li>borgcfg 是 Borg 的命令行工具，用于跟 Borg 系统交互，一般通过一个配置文件来提交任务。</li>
</ul>
<h2 id="kubernetes-架构">Kubernetes 架构</h2>
<p>Kubernetes 借鉴了 Borg 的设计理念，比如 Pod、Service、Labels 和单 Pod 单 IP 等。Kubernetes 的整体架构跟 Borg 非常像，如下图所示</p>
<p><img src="images/architecture.png" alt="architecture"/></p>
<p>Kubernetes 主要由以下几个核心组件组成：</p>
<ul>
<li>etcd 保存了整个集群的状态；</li>
<li>kube-apiserver 提供了资源操作的唯一入口，并提供认证、授权、访问控制、API 注册和发现等机制；</li>
<li>kube-controller-manager 负责维护集群的状态，比如故障检测、自动扩展、滚动更新等；</li>
<li>kube-scheduler 负责资源的调度，按照预定的调度策略将 Pod 调度到相应的机器上；</li>
<li>kubelet 负责维持容器的生命周期，同时也负责 Volume（CVI）和网络（CNI）的管理；</li>
<li>Container runtime 负责镜像管理以及 Pod 和容器的真正运行（CRI），默认的容器运行时为 Docker；</li>
<li>kube-proxy 负责为 Service 提供 cluster 内部的服务发现和负载均衡；</li>
</ul>
<p><img src="images/components.png" alt=""/></p>
<p>除了核心组件，还有一些推荐的 Add-ons：</p>
<ul>
<li>kube-dns 负责为整个集群提供 DNS 服务</li>
<li>Ingress Controller 为服务提供外网入口</li>
<li>Heapster 提供资源监控</li>
<li>Dashboard 提供 GUI</li>
<li>Federation 提供跨可用区的集群</li>
<li>Fluentd-elasticsearch 提供集群日志采集、存储与查询</li>
</ul>
<h3 id="分层架构">分层架构</h3>
<p>Kubernetes 设计理念和功能其实就是一个类似 Linux 的分层架构，如下图所示</p>
<p><img src="images/14937095836427.jpg" alt=""/></p>
<ul>
<li>核心层：Kubernetes 最核心的功能，对外提供 API 构建高层的应用，对内提供插件式应用执行环境</li>
<li>应用层：部署（无状态应用、有状态应用、批处理任务、集群应用等）和路由（服务发现、DNS 解析等）</li>
<li>管理层：系统度量（如基础设施、容器和网络的度量），自动化（如自动扩展、动态 Provision 等）以及策略管理（RBAC、Quota、PSP、NetworkPolicy 等）</li>
<li>接口层：kubectl 命令行工具、客户端 SDK 以及集群联邦</li>
<li>生态系统：在接口层之上的庞大容器集群管理调度的生态系统，可以划分为两个范畴<ul>
<li>Kubernetes 外部：日志、监控、配置管理、CI、CD、Workflow、FaaS、OTS 应用、ChatOps 等</li>
<li>Kubernetes 内部：CRI、CNI、CVI、镜像仓库、Cloud Provider、集群自身的配置和管理等</li>
</ul>
</li>
</ul>
<h3 id="核心组件">核心组件</h3>
<p><img src="images/core-packages.png" alt=""/></p>
<h3 id="核心-api">核心 API</h3>
<p><img src="images/core-apis.png" alt=""/></p>
<h3 id="生态系统">生态系统</h3>
<p><img src="images/core-ecosystem.png" alt=""/></p>
<p>关于分层架构，可以关注下 Kubernetes 社区正在推进的 <a href="https://github.com/kubernetes/community/tree/master/sig-architecture" target="_blank">Kubernetes architectual roadmap</a>。</p>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/architecture/architecture.md" target="_blank">Kubernetes design and architecture</a></li>
<li><a href="http://queue.acm.org/detail.cfm?id=2898444" target="_blank">http://queue.acm.org/detail.cfm?id=2898444</a></li>
<li><a href="http://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/43438.pdf" target="_blank">http://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/43438.pdf</a></li>
<li><a href="http://thenewstack.io/kubernetes-an-overview" target="_blank">http://thenewstack.io/kubernetes-an-overview</a></li>
<li><a href="https://github.com/kubernetes/community/tree/master/sig-architecture" target="_blank">Kubernetes Architecture SIG</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="kubernetes设计理念" class="level2">设计理念</h1>
<h3 id="设计理念与分布式系统">设计理念与分布式系统</h3>
<p>分析和理解Kubernetes的设计理念可以使我们更深入地了解Kubernetes系统，更好地利用它管理分布式部署的云原生应用，另一方面也可以让我们借鉴其在分布式系统设计方面的经验。</p>
<h3 id="api设计原则">API设计原则</h3>
<p>对于云计算系统，系统API实际上处于系统设计的统领地位。Kubernetes集群系统每支持一项新功能，引入一项新技术，一定会新引入对应的API对象，支持对该功能的管理操作。理解掌握的API，就好比抓住了K8s系统的牛鼻子。Kubernetes系统API的设计有以下几条原则：</p>
<ol>
<li><strong>所有API应该是声明式的</strong>。声明式的操作，相对于命令式操作，对于重复操作的效果是稳定的，这对于容易出现数据丢失或重复的分布式环境来说是很重要的。另外，声明式操作更容易被用户使用，可以使系统向用户隐藏实现的细节，同时也保留了系统未来持续优化的可能性。此外，声明式的API还隐含了所有的API对象都是名词性质的，例如Service、Volume这些API都是名词，这些名词描述了用户所期望得到的一个目标对象。 </li>
<li><strong>API对象是彼此互补而且可组合的</strong>。这实际上鼓励API对象尽量实现面向对象设计时的要求，即“高内聚，松耦合”，对业务相关的概念有一个合适的分解，提高分解出来的对象的可重用性。 </li>
<li><strong>高层API以操作意图为基础设计</strong>。如何能够设计好API，跟如何能用面向对象的方法设计好应用系统有相通的地方，高层设计一定是从业务出发，而不是过早的从技术实现出发。因此，针对Kubernetes的高层API设计，一定是以K8s的业务为基础出发，也就是以系统调度管理容器的操作意图为基础设计。 </li>
<li><strong>低层API根据高层API的控制需要设计</strong>。设计实现低层API的目的，是为了被高层API使用，考虑减少冗余、提高重用性的目的，低层API的设计也要以需求为基础，要尽量抵抗受技术实现影响的诱惑。 </li>
<li><strong>尽量避免简单封装，不要有在外部API无法显式知道的内部隐藏的机制</strong>。简单的封装，实际没有提供新的功能，反而增加了对所封装API的依赖性。内部隐藏的机制也是非常不利于系统维护的设计方式，例如StatefulSet和ReplicaSet，本来就是两种Pod集合，那么Kubernetes就用不同API对象来定义它们，而不会说只用同一个ReplicaSet，内部通过特殊的算法再来区分这个ReplicaSet是有状态的还是无状态。 </li>
<li><strong>API操作复杂度与对象数量成正比</strong>。这一条主要是从系统性能角度考虑，要保证整个系统随着系统规模的扩大，性能不会迅速变慢到无法使用，那么最低的限定就是API的操作复杂度不能超过O(N)，N是对象的数量，否则系统就不具备水平伸缩性了。 </li>
<li><strong>API对象状态不能依赖于网络连接状态</strong>。由于众所周知，在分布式环境下，网络连接断开是经常发生的事情，因此要保证API对象状态能应对网络的不稳定，API对象的状态就不能依赖于网络连接状态。 </li>
<li><strong>尽量避免让操作机制依赖于全局状态，因为在分布式系统中要保证全局状态的同步是非常困难的</strong>。</li>
</ol>
<h3 id="控制机制设计原则">控制机制设计原则</h3>
<ul>
<li><strong>控制逻辑应该只依赖于当前状态</strong>。这是为了保证分布式系统的稳定可靠，对于经常出现局部错误的分布式系统，如果控制逻辑只依赖当前状态，那么就非常容易将一个暂时出现故障的系统恢复到正常状态，因为你只要将该系统重置到某个稳定状态，就可以自信的知道系统的所有控制逻辑会开始按照正常方式运行。 </li>
<li><strong>假设任何错误的可能，并做容错处理</strong>。在一个分布式系统中出现局部和临时错误是大概率事件。错误可能来自于物理系统故障，外部系统故障也可能来自于系统自身的代码错误，依靠自己实现的代码不会出错来保证系统稳定其实也是难以实现的，因此要设计对任何可能错误的容错处理。 </li>
<li><strong>尽量避免复杂状态机，控制逻辑不要依赖无法监控的内部状态</strong>。因为分布式系统各个子系统都是不能严格通过程序内部保持同步的，所以如果两个子系统的控制逻辑如果互相有影响，那么子系统就一定要能互相访问到影响控制逻辑的状态，否则，就等同于系统里存在不确定的控制逻辑。 </li>
<li><strong>假设任何操作都可能被任何操作对象拒绝，甚至被错误解析</strong>。由于分布式系统的复杂性以及各子系统的相对独立性，不同子系统经常来自不同的开发团队，所以不能奢望任何操作被另一个子系统以正确的方式处理，要保证出现错误的时候，操作级别的错误不会影响到系统稳定性。 </li>
<li><strong>每个模块都可以在出错后自动恢复</strong>。由于分布式系统中无法保证系统各个模块是始终连接的，因此每个模块要有自我修复的能力，保证不会因为连接不到其他模块而自我崩溃。 </li>
<li><strong>每个模块都可以在必要时优雅地降级服务</strong>。所谓优雅地降级服务，是对系统鲁棒性的要求，即要求在设计实现模块时划分清楚基本功能和高级功能，保证基本功能不会依赖高级功能，这样同时就保证了不会因为高级功能出现故障而导致整个模块崩溃。根据这种理念实现的系统，也更容易快速地增加新的高级功能，因为不必担心引入高级功能影响原有的基本功能。</li>
</ul>
<h3 id="架构设计原则">架构设计原则</h3>
<ul>
<li>只有apiserver可以直接访问etcd存储，其他服务必须通过Kubernetes API来访问集群状态</li>
<li>单节点故障不应该影响集群的状态</li>
<li>在没有新请求的情况下，所有组件应该在故障恢复后继续执行上次最后收到的请求（比如网络分区或服务重启等）</li>
<li>所有组件都应该在内存中保持所需要的状态，apiserver将状态写入etcd存储，而其他组件则通过apiserver更新并监听所有的变化</li>
<li>优先使用事件监听而不是轮询</li>
</ul>
<h3 id="引导（bootstrapping）原则">引导（Bootstrapping）原则</h3>
<ul>
<li><a href="http://issue.k8s.io/246" target="_blank">Self-hosting</a> 是目标</li>
<li>减少依赖，特别是稳态运行的依赖</li>
<li>通过分层的原则管理依赖</li>
<li>循环依赖问题的原则<ul>
<li>同时还接受其他方式的数据输入（比如本地文件等），这样在其他服务不可用时还可以手动配置引导服务</li>
<li>状态应该是可恢复或可重新发现的</li>
<li>支持简单的启动临时实例来创建稳态运行所需要的状态；使用分布式锁或文件锁等来协调不同状态的切换（通常称为<code>pivoting</code>技术）</li>
<li>自动重启异常退出的服务，比如副本或者进程管理器等</li>
</ul>
</li>
</ul>
<h2 id="核心技术概念和api对象">核心技术概念和API对象</h2>
<p>API对象是K8s集群中的管理操作单元。K8s集群系统每支持一项新功能，引入一项新技术，一定会新引入对应的API对象，支持对该功能的管理操作。例如副本集Replica Set对应的API对象是RS。</p>
<p>每个API对象都有3大类属性：元数据metadata、规范spec和状态status。元数据是用来标识API对象的，每个对象都至少有3个元数据：namespace，name和uid；除此以外还有各种各样的标签labels用来标识和匹配不同的对象，例如用户可以用标签env来标识区分不同的服务部署环境，分别用env=dev、env=testing、env=production来标识开发、测试、生产的不同服务。规范描述了用户期望K8s集群中的分布式系统达到的理想状态（Desired State），例如用户可以通过复制控制器Replication Controller设置期望的Pod副本数为3；status描述了系统实际当前达到的状态（Status），例如系统当前实际的Pod副本数为2；那么复本控制器当前的程序逻辑就是自动启动新的Pod，争取达到副本数为3。</p>
<p>K8s中所有的配置都是通过API对象的spec去设置的，也就是用户通过配置系统的理想状态来改变系统，这是k8s重要设计理念之一，即所有的操作都是声明式（Declarative）的而不是命令式（Imperative）的。声明式操作在分布式系统中的好处是稳定，不怕丢操作或运行多次，例如设置副本数为3的操作运行多次也还是一个结果，而给副本数加1的操作就不是声明式的，运行多次结果就错了。</p>
<h3 id="pod">Pod</h3>
<p>K8s有很多技术概念，同时对应很多API对象，最重要的也是最基础的是微服务Pod。Pod是在K8s集群中运行部署应用或服务的最小单元，它是可以支持多容器的。Pod的设计理念是支持多个容器在一个Pod中共享网络地址和文件系统，可以通过进程间通信和文件共享这种简单高效的方式组合完成服务。Pod对多容器的支持是K8s最基础的设计理念。比如你运行一个操作系统发行版的软件仓库，一个Nginx容器用来发布软件，另一个容器专门用来从源仓库做同步，这两个容器的镜像不太可能是一个团队开发的，但是他们一块儿工作才能提供一个微服务；这种情况下，不同的团队各自开发构建自己的容器镜像，在部署的时候组合成一个微服务对外提供服务。</p>
<p>Pod是K8s集群中所有业务类型的基础，可以看作运行在K8s集群中的小机器人，不同类型的业务就需要不同类型的小机器人去执行。目前K8s中的业务主要可以分为长期伺服型（long-running）、批处理型（batch）、节点后台支撑型（node-daemon）和有状态应用型（stateful application）；分别对应的小机器人控制器为Deployment、Job、DaemonSet和StatefulSet，本文后面会一一介绍。</p>
<h3 id="复制控制器（replication-controller，rc）">复制控制器（Replication Controller，RC）</h3>
<p>RC是K8s集群中最早的保证Pod高可用的API对象。通过监控运行中的Pod来保证集群中运行指定数目的Pod副本。指定的数目可以是多个也可以是1个；少于指定数目，RC就会启动运行新的Pod副本；多于指定数目，RC就会杀死多余的Pod副本。即使在指定数目为1的情况下，通过RC运行Pod也比直接运行Pod更明智，因为RC也可以发挥它高可用的能力，保证永远有1个Pod在运行。RC是K8s较早期的技术概念，只适用于长期伺服型的业务类型，比如控制小机器人提供高可用的Web服务。</p>
<h3 id="副本集（replica-set，rs）">副本集（Replica Set，RS）</h3>
<p>RS是新一代RC，提供同样的高可用能力，区别主要在于RS后来居上，能支持更多种类的匹配模式。副本集对象一般不单独使用，而是作为Deployment的理想状态参数使用。</p>
<h3 id="部署deployment">部署(Deployment)</h3>
<p>部署表示用户对K8s集群的一次更新操作。部署是一个比RS应用模式更广的API对象，可以是创建一个新的服务，更新一个新的服务，也可以是滚动升级一个服务。滚动升级一个服务，实际是创建一个新的RS，然后逐渐将新RS中副本数增加到理想状态，将旧RS中的副本数减小到0的复合操作；这样一个复合操作用一个RS是不太好描述的，所以用一个更通用的Deployment来描述。以K8s的发展方向，未来对所有长期伺服型的的业务的管理，都会通过Deployment来管理。</p>
<h3 id="服务（service）">服务（Service）</h3>
<p>RC、RS和Deployment只是保证了支撑服务的微服务Pod的数量，但是没有解决如何访问这些服务的问题。一个Pod只是一个运行服务的实例，随时可能在一个节点上停止，在另一个节点以一个新的IP启动一个新的Pod，因此不能以确定的IP和端口号提供服务。要稳定地提供服务需要服务发现和负载均衡能力。服务发现完成的工作，是针对客户端访问的服务，找到对应的的后端服务实例。在K8s集群中，客户端需要访问的服务就是Service对象。每个Service会对应一个集群内部有效的虚拟IP，集群内部通过虚拟IP访问一个服务。在K8s集群中微服务的负载均衡是由Kube-proxy实现的。Kube-proxy是K8s集群内部的负载均衡器。它是一个分布式代理服务器，在K8s的每个节点上都有一个；这一设计体现了它的伸缩性优势，需要访问服务的节点越多，提供负载均衡能力的Kube-proxy就越多，高可用节点也随之增多。与之相比，我们平时在服务器端使用反向代理作负载均衡，还要进一步解决反向代理的高可用问题。</p>
<h3 id="任务（job）">任务（Job）</h3>
<p>Job是K8s用来控制批处理型任务的API对象。批处理业务与长期伺服业务的主要区别是批处理业务的运行有头有尾，而长期伺服业务在用户不停止的情况下永远运行。Job管理的Pod根据用户的设置把任务成功完成就自动退出了。成功完成的标志根据不同的spec.completions策略而不同：单Pod型任务有一个Pod成功就标志完成；定数成功型任务保证有N个任务全部成功；工作队列型任务根据应用确认的全局成功而标志成功。</p>
<h3 id="后台支撑服务集（daemonset）">后台支撑服务集（DaemonSet）</h3>
<p>长期伺服型和批处理型服务的核心在业务应用，可能有些节点运行多个同类业务的Pod，有些节点上又没有这类Pod运行；而后台支撑型服务的核心关注点在K8s集群中的节点（物理机或虚拟机），要保证每个节点上都有一个此类Pod运行。节点可能是所有集群节点也可能是通过nodeSelector选定的一些特定节点。典型的后台支撑型服务包括，存储，日志和监控等在每个节点上支撑K8s集群运行的服务。</p>
<h3 id="有状态服务集（statefulset）">有状态服务集（StatefulSet）</h3>
<p>K8s在1.3版本里发布了Alpha版的PetSet以支持有状态服务，并从1.5版本开始重命名为StatefulSet。在云原生应用的体系里，有下面两组近义词；第一组是无状态（stateless）、牲畜（cattle）、无名（nameless）、可丢弃（disposable）；第二组是有状态（stateful）、宠物（pet）、有名（having name）、不可丢弃（non-disposable）。RC和RS主要是控制提供无状态服务的，其所控制的Pod的名字是随机设置的，一个Pod出故障了就被丢弃掉，在另一个地方重启一个新的Pod，名字变了、名字和启动在哪儿都不重要，重要的只是Pod总数；而StatefulSet是用来控制有状态服务，StatefulSet中的每个Pod的名字都是事先确定的，不能更改。StatefulSet中Pod的名字的作用，并不是《千与千寻》的人性原因，而是关联与该Pod对应的状态。</p>
<p>对于RC和RS中的Pod，一般不挂载存储或者挂载共享存储，保存的是所有Pod共享的状态，Pod像牲畜一样没有分别（这似乎也确实意味着失去了人性特征）；对于StatefulSet中的Pod，每个Pod挂载自己独立的存储，如果一个Pod出现故障，从其他节点启动一个同样名字的Pod，要挂载上原来Pod的存储继续以它的状态提供服务。</p>
<p>适合于StatefulSet的业务包括数据库服务MySQL和PostgreSQL，集群化管理服务Zookeeper、etcd等有状态服务。StatefulSet的另一种典型应用场景是作为一种比普通容器更稳定可靠的模拟虚拟机的机制。传统的虚拟机正是一种有状态的宠物，运维人员需要不断地维护它，容器刚开始流行时，我们用容器来模拟虚拟机使用，所有状态都保存在容器里，而这已被证明是非常不安全、不可靠的。使用StatefulSet，Pod仍然可以通过漂移到不同节点提供高可用，而存储也可以通过外挂的存储来提供高可靠性，StatefulSet做的只是将确定的Pod与确定的存储关联起来保证状态的连续性。StatefulSet还只在Alpha阶段，后面的设计如何演变，我们还要继续观察。</p>
<h3 id="集群联邦（federation）">集群联邦（Federation）</h3>
<p>K8s在1.3版本里发布了beta版的Federation功能。在云计算环境中，服务的作用距离范围从近到远一般可以有：同主机（Host，Node）、跨主机同可用区（Available Zone）、跨可用区同地区（Region）、跨地区同服务商（Cloud Service Provider）、跨云平台。K8s的设计定位是单一集群在同一个地域内，因为同一个地区的网络性能才能满足K8s的调度和计算存储连接要求。而联合集群服务就是为提供跨Region跨服务商K8s集群服务而设计的。</p>
<p>每个K8s Federation有自己的分布式存储、API Server和Controller Manager。用户可以通过Federation的API Server注册该Federation的成员K8s Cluster。当用户通过Federation的API Server创建、更改API对象时，Federation API Server会在自己所有注册的子K8s Cluster都创建一份对应的API对象。在提供业务请求服务时，K8s Federation会先在自己的各个子Cluster之间做负载均衡，而对于发送到某个具体K8s Cluster的业务请求，会依照这个K8s Cluster独立提供服务时一样的调度模式去做K8s Cluster内部的负载均衡。而Cluster之间的负载均衡是通过域名服务的负载均衡来实现的。</p>
<p>所有的设计都尽量不影响K8s Cluster现有的工作机制，这样对于每个子K8s集群来说，并不需要更外层的有一个K8s Federation，也就是意味着所有现有的K8s代码和机制不需要因为Federation功能有任何变化。</p>
<h3 id="存储卷（volume）">存储卷（Volume）</h3>
<p>K8s集群中的存储卷跟Docker的存储卷有些类似，只不过Docker的存储卷作用范围为一个容器，而K8s的存储卷的生命周期和作用范围是一个Pod。每个Pod中声明的存储卷由Pod中的所有容器共享。K8s支持非常多的存储卷类型，特别的，支持多种公有云平台的存储，包括AWS，Google和Azure云；支持多种分布式存储包括GlusterFS和Ceph；也支持较容易使用的主机本地目录hostPath和NFS。K8s还支持使用Persistent Volume Claim即PVC这种逻辑存储，使用这种存储，使得存储的使用者可以忽略后台的实际存储技术（例如AWS，Google或GlusterFS和Ceph），而将有关存储实际技术的配置交给存储管理员通过Persistent Volume来配置。</p>
<h3 id="持久存储卷（persistent-volume，pv）和持久存储卷声明（persistent-volume-claim，pvc）">持久存储卷（Persistent Volume，PV）和持久存储卷声明（Persistent Volume Claim，PVC）</h3>
<p>PV和PVC使得K8s集群具备了存储的逻辑抽象能力，使得在配置Pod的逻辑里可以忽略对实际后台存储技术的配置，而把这项配置的工作交给PV的配置者，即集群的管理者。存储的PV和PVC的这种关系，跟计算的Node和Pod的关系是非常类似的；PV和Node是资源的提供者，根据集群的基础设施变化而变化，由K8s集群管理员配置；而PVC和Pod是资源的使用者，根据业务服务的需求变化而变化，由K8s集群的使用者即服务的管理员来配置。</p>
<h3 id="节点（node）">节点（Node）</h3>
<p>K8s集群中的计算能力由Node提供，最初Node称为服务节点Minion，后来改名为Node。K8s集群中的Node也就等同于Mesos集群中的Slave节点，是所有Pod运行所在的工作主机，可以是物理机也可以是虚拟机。不论是物理机还是虚拟机，工作主机的统一特征是上面要运行kubelet管理节点上运行的容器。</p>
<h3 id="密钥对象（secret）">密钥对象（Secret）</h3>
<p>Secret是用来保存和传递密码、密钥、认证凭证这些敏感信息的对象。使用Secret的好处是可以避免把敏感信息明文写在配置文件里。在K8s集群中配置和使用服务不可避免的要用到各种敏感信息实现登录、认证等功能，例如访问AWS存储的用户名密码。为了避免将类似的敏感信息明文写在所有需要使用的配置文件中，可以将这些信息存入一个Secret对象，而在配置文件中通过Secret对象引用这些敏感信息。这种方式的好处包括：意图明确，避免重复，减少暴露机会。</p>
<h3 id="用户帐户（user-account）和服务帐户（service-account）">用户帐户（User Account）和服务帐户（Service Account）</h3>
<p>顾名思义，用户帐户为人提供账户标识，而服务账户为计算机进程和K8s集群中运行的Pod提供账户标识。用户帐户和服务帐户的一个区别是作用范围；用户帐户对应的是人的身份，人的身份与服务的namespace无关，所以用户账户是跨namespace的；而服务帐户对应的是一个运行中程序的身份，与特定namespace是相关的。</p>
<h3 id="名字空间（namespace）">名字空间（Namespace）</h3>
<p>名字空间为K8s集群提供虚拟的隔离作用，K8s集群初始有两个名字空间，分别是默认名字空间default和系统名字空间kube-system，除此以外，管理员可以创建新的名字空间满足需要。</p>
<h3 id="rbac访问授权">RBAC访问授权</h3>
<p>K8s在1.3版本中发布了alpha版的基于角色的访问控制（Role-based Access Control，RBAC）的授权模式。相对于基于属性的访问控制（Attribute-based Access Control，ABAC），RBAC主要是引入了角色（Role）和角色绑定（RoleBinding）的抽象概念。在ABAC中，K8s集群中的访问策略只能跟用户直接关联；而在RBAC中，访问策略可以跟某个角色关联，具体的用户在跟一个或多个角色相关联。显然，RBAC像其他新功能一样，每次引入新功能，都会引入新的API对象，从而引入新的概念抽象，而这一新的概念抽象一定会使集群服务管理和使用更容易扩展和重用。</p>
<h2 id="总结">总结</h2>
<p>从K8s的系统架构、技术概念和设计理念，我们可以看到K8s系统最核心的两个设计理念：一个是<strong>容错性</strong>，一个是<strong>易扩展性</strong>。容错性实际是保证K8s系统稳定性和安全性的基础，易扩展性是保证K8s对变更友好，可以快速迭代增加新功能的基础。</p>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/architecture/principles.md" target="_blank">Kubernetes Design Principles</a></li>
<li><a href="http://www.infoq.com/cn/articles/kubernetes-and-cloud-native-applications-part01" target="_blank">Kubernetes与云原生应用</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="核心组件" class="level2">核心组件</h1>
<p><img src="images/components.png" alt="components"/></p>
<p>Kubernetes 主要由以下几个核心组件组成:</p>
<ul>
<li>etcd 保存了整个集群的状态；</li>
<li>apiserver 提供了资源操作的唯一入口，并提供认证、授权、访问控制、API 注册和发现等机制；</li>
<li>controller manager 负责维护集群的状态，比如故障检测、自动扩展、滚动更新等；</li>
<li>scheduler 负责资源的调度，按照预定的调度策略将 Pod 调度到相应的机器上；</li>
<li>kubelet 负责维护容器的生命周期，同时也负责 Volume（CVI）和网络（CNI）的管理；</li>
<li>Container runtime 负责镜像管理以及 Pod 和容器的真正运行（CRI）；</li>
<li>kube-proxy 负责为 Service 提供 cluster 内部的服务发现和负载均衡；</li>
</ul>
<h2 id="组件通信">组件通信</h2>
<p>Kubernetes 多组件之间的通信原理为</p>
<ul>
<li>apiserver 负责 etcd 存储的所有操作，且只有 apiserver 才直接操作 etcd 集群</li>
<li>apiserver 对内（集群中的其他组件）和对外（用户）提供统一的 REST API，其他组件均通过 apiserver 进行通信<ul>
<li>controller manager、scheduler、kube-proxy 和 kubelet 等均通过 apiserver watch API 监测资源变化情况，并对资源作相应的操作</li>
<li>所有需要更新资源状态的操作均通过 apiserver 的 REST API 进行</li>
</ul>
</li>
<li>apiserver 也会直接调用 kubelet API（如 logs, exec, attach 等），默认不校验 kubelet 证书，但可以通过 <code>--kubelet-certificate-authority</code> 开启（而 GKE 通过 SSH 隧道保护它们之间的通信）</li>
</ul>
<p>比如典型的创建 Pod 的流程为</p>
<p><img src="images/workflow.png" alt=""/></p>
<ol>
<li>用户通过 REST API 创建一个 Pod</li>
<li>apiserver 将其写入 etcd</li>
<li>scheduluer 检测到未绑定 Node 的 Pod，开始调度并更新 Pod 的 Node 绑定</li>
<li>kubelet 检测到有新的 Pod 调度过来，通过 container runtime 运行该 Pod</li>
<li>kubelet 通过 container runtime 取到 Pod 状态，并更新到 apiserver 中</li>
</ol>
<h2 id="端口号">端口号</h2>
<p><img src="images/ports.png" alt="ports"/></p>
<h3 id="master-nodes">Master node(s)</h3>
<table>
<thead>
<tr>
<th>Protocol</th>
<th>Direction</th>
<th>Port Range</th>
<th>Purpose</th>
</tr>
</thead>
<tbody>
<tr>
<td>TCP</td>
<td>Inbound</td>
<td>6443*</td>
<td>Kubernetes API server</td>
</tr>
<tr>
<td>TCP</td>
<td>Inbound</td>
<td>8080</td>
<td>Kubernetes API insecure server</td>
</tr>
<tr>
<td>TCP</td>
<td>Inbound</td>
<td>2379-2380</td>
<td>etcd server client API</td>
</tr>
<tr>
<td>TCP</td>
<td>Inbound</td>
<td>10250</td>
<td>Kubelet API</td>
</tr>
<tr>
<td>TCP</td>
<td>Inbound</td>
<td>10251</td>
<td>kube-scheduler healthz</td>
</tr>
<tr>
<td>TCP</td>
<td>Inbound</td>
<td>10252</td>
<td>kube-controller-manager healthz</td>
</tr>
<tr>
<td>TCP</td>
<td>Inbound</td>
<td>10253</td>
<td>cloud-controller-manager healthz</td>
</tr>
<tr>
<td>TCP</td>
<td>Inbound</td>
<td>10255</td>
<td>Read-only Kubelet API</td>
</tr>
<tr>
<td>TCP</td>
<td>Inbound</td>
<td>10256</td>
<td>kube-proxy healthz</td>
</tr>
</tbody>
</table>
<h3 id="worker-nodes">Worker node(s)</h3>
<table>
<thead>
<tr>
<th>Protocol</th>
<th>Direction</th>
<th>Port Range</th>
<th>Purpose</th>
</tr>
</thead>
<tbody>
<tr>
<td>TCP</td>
<td>Inbound</td>
<td>4194</td>
<td>Kubelet cAdvisor</td>
</tr>
<tr>
<td>TCP</td>
<td>Inbound</td>
<td>10248</td>
<td>Kubelet healthz</td>
</tr>
<tr>
<td>TCP</td>
<td>Inbound</td>
<td>10249</td>
<td>kube-proxy metrics</td>
</tr>
<tr>
<td>TCP</td>
<td>Inbound</td>
<td>10250</td>
<td>Kubelet API</td>
</tr>
<tr>
<td>TCP</td>
<td>Inbound</td>
<td>10255</td>
<td>Read-only Kubelet API</td>
</tr>
<tr>
<td>TCP</td>
<td>Inbound</td>
<td>10256</td>
<td>kube-proxy healthz</td>
</tr>
<tr>
<td>TCP</td>
<td>Inbound</td>
<td>30000-32767</td>
<td>NodePort Services**</td>
</tr>
</tbody>
</table>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://kubernetes.io/docs/concepts/architecture/master-node-communication/" target="_blank">Master-Node communication</a></li>
<li><a href="https://blog.heptio.com/core-kubernetes-jazz-improv-over-orchestration-a7903ea92ca" target="_blank">Core Kubernetes: Jazz Improv over Orchestration</a></li>
<li><a href="https://kubernetes.io/docs/setup/independent/install-kubeadm/#check-required-ports" target="_blank">Installing kubeadm</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="etcd" class="level3">etcd</h1>
<p>Etcd 是 CoreOS 基于 Raft 开发的分布式 key-value 存储，可用于服务发现、共享配置以及一致性保障（如数据库选主、分布式锁等）。</p>
<h2 id="etcd-主要功能">Etcd 主要功能</h2>
<ul>
<li>基本的 key-value 存储</li>
<li>监听机制</li>
<li>key 的过期及续约机制，用于监控和服务发现</li>
<li>原子 CAS 和 CAD，用于分布式锁和 leader 选举</li>
</ul>
<h2 id="etcd-基于-raft-的一致性">Etcd 基于 RAFT 的一致性</h2>
<p>选举方法</p>
<ul>
<li>1) 初始启动时，节点处于 follower 状态并被设定一个 election timeout，如果在这一时间周期内没有收到来自 leader 的 heartbeat，节点将发起选举：将自己切换为 candidate 之后，向集群中其它 follower 节点发送请求，询问其是否选举自己成为 leader。</li>
<li>2) 当收到来自集群中过半数节点的接受投票后，节点即成为 leader，开始接收保存 client 的数据并向其它的 follower 节点同步日志。如果没有达成一致，则 candidate 随机选择一个等待间隔（150ms ~ 300ms）再次发起投票，得到集群中半数以上 follower 接受的 candidate 将成为 leader</li>
<li>3) leader 节点依靠定时向 follower 发送 heartbeat 来保持其地位。</li>
<li>4) 任何时候如果其它 follower 在 election timeout 期间都没有收到来自 leader 的 heartbeat，同样会将自己的状态切换为 candidate 并发起选举。每成功选举一次，新 leader 的任期（Term）都会比之前 leader 的任期大 1。</li>
</ul>
<p>日志复制</p>
<p>当前 Leader 收到客户端的日志（事务请求）后先把该日志追加到本地的 Log 中，然后通过 heartbeat 把该 Entry 同步给其他 Follower，Follower 接收到日志后记录日志然后向 Leader 发送 ACK，当 Leader 收到大多数（n/2+1）Follower 的 ACK 信息后将该日志设置为已提交并追加到本地磁盘中，通知客户端并在下个 heartbeat 中 Leader 将通知所有的 Follower 将该日志存储在自己的本地磁盘中。</p>
<p>安全性</p>
<p>安全性是用于保证每个节点都执行相同序列的安全机制，如当某个 Follower 在当前 Leader commit Log 时变得不可用了，稍后可能该 Follower 又会被选举为 Leader，这时新 Leader 可能会用新的 Log 覆盖先前已 committed 的 Log，这就是导致节点执行不同序列；Safety 就是用于保证选举出来的 Leader 一定包含先前 committed Log 的机制；</p>
<ul>
<li>选举安全性（Election Safety）：每个任期（Term）只能选举出一个 Leader</li>
<li>Leader 完整性（Leader Completeness）：指 Leader 日志的完整性，当 Log 在任期 Term1 被 Commit 后，那么以后任期 Term2、Term3… 等的 Leader 必须包含该 Log；Raft 在选举阶段就使用 Term 的判断用于保证完整性：当请求投票的该 Candidate 的 Term 较大或 Term 相同 Index 更大则投票，否则拒绝该请求。</li>
</ul>
<p>失效处理</p>
<ul>
<li>1) Leader 失效：其他没有收到 heartbeat 的节点会发起新的选举，而当 Leader 恢复后由于步进数小会自动成为 follower（日志也会被新 leader 的日志覆盖）</li>
<li>2）follower 节点不可用：follower 节点不可用的情况相对容易解决。因为集群中的日志内容始终是从 leader 节点同步的，只要这一节点再次加入集群时重新从 leader 节点处复制日志即可。</li>
<li>3）多个 candidate：冲突后 candidate 将随机选择一个等待间隔（150ms ~ 300ms）再次发起投票，得到集群中半数以上 follower 接受的 candidate 将成为 leader</li>
</ul>
<h3 id="wal-日志">wal 日志</h3>
<p>Etcd 实现 raft 的时候，充分利用了 go 语言 CSP 并发模型和 chan 的魔法，想更进行一步了解的可以去看源码，这里只简单分析下它的 wal 日志。</p>
<p><img src="images/etcd-log.png" alt="etcdv3"/></p>
<p>wal 日志是二进制的，解析出来后是以上数据结构 LogEntry。其中第一个字段 type，只有两种，一种是 0 表示 Normal，1 表示 ConfChange（ConfChange 表示 Etcd 本身的配置变更同步，比如有新的节点加入等）。第二个字段是 term，每个 term 代表一个主节点的任期，每次主节点变更 term 就会变化。第三个字段是 index，这个序号是严格有序递增的，代表变更序号。第四个字段是二进制的 data，将 raft request 对象的 pb 结构整个保存下。Etcd 源码下有个 tools/etcd-dump-logs，可以将 wal 日志 dump 成文本查看，可以协助分析 raft 协议。</p>
<p>raft 协议本身不关心应用数据，也就是 data 中的部分，一致性都通过同步 wal 日志来实现，每个节点将从主节点收到的 data apply 到本地的存储，raft 只关心日志的同步状态，如果本地存储实现的有 bug，比如没有正确的将 data apply 到本地，也可能会导致数据不一致。</p>
<h2 id="etcd-v2-与-v3">Etcd v2 与 v3</h2>
<p>Etcd v2 和 v3 本质上是共享同一套 raft 协议代码的两个独立的应用，接口不一样，存储不一样，数据互相隔离。也就是说如果从 Etcd v2 升级到 Etcd v3，原来 v2 的数据还是只能用 v2 的接口访问，v3 的接口创建的数据也只能访问通过 v3 的接口访问。所以我们按照 v2 和 v3 分别分析。</p>
<p>推荐在 Kubernetes 集群中使用 <strong>Etcd v3</strong>，<strong>v2 版本已在 Kubernetes v1.11 中弃用</strong>。</p>
<h2 id="etcd-v2-存储，watch-以及过期机制">Etcd v2 存储，Watch 以及过期机制</h2>
<p><img src="images/etcd-v2.png" alt="etcdv2"/></p>
<p>Etcd v2 是个纯内存的实现，并未实时将数据写入到磁盘，持久化机制很简单，就是将 store 整合序列化成 json 写入文件。数据在内存中是一个简单的树结构。比如以下数据存储到 Etcd 中的结构就如图所示。</p>
<pre><code>/nodes/1/name  node1
/nodes/1/ip    192.168.1.1
</code></pre><p>store 中有一个全局的 currentIndex，每次变更，index 会加 1. 然后每个 event 都会关联到 currentIndex.</p>
<p>当客户端调用 watch 接口（参数中增加 wait 参数）时，如果请求参数中有 waitIndex，并且 waitIndex 小于 currentIndex，则从 EventHistroy 表中查询 index 大于等于 waitIndex，并且和 watch key 匹配的 event，如果有数据，则直接返回。如果历史表中没有或者请求没有带 waitIndex，则放入 WatchHub 中，每个 key 会关联一个 watcher 列表。 当有变更操作时，变更生成的 event 会放入 EventHistroy 表中，同时通知和该 key 相关的 watcher。</p>
<p>这里有几个影响使用的细节问题：</p>
<ol>
<li>EventHistroy 是有长度限制的，最长 1000。也就是说，如果你的客户端停了许久，然后重新 watch 的时候，可能和该 waitIndex 相关的 event 已经被淘汰了，这种情况下会丢失变更。</li>
<li>如果通知 watcher 的时候，出现了阻塞（每个 watcher 的 channel 有 100 个缓冲空间），Etcd 会直接把 watcher 删除，也就是会导致 wait 请求的连接中断，客户端需要重新连接。</li>
<li>Etcd store 的每个 node 中都保存了过期时间，通过定时机制进行清理。</li>
</ol>
<p>从而可以看出，Etcd v2 的一些限制：</p>
<ol>
<li>过期时间只能设置到每个 key 上，如果多个 key 要保证生命周期一致则比较困难。</li>
<li>watcher 只能 watch 某一个 key 以及其子节点（通过参数 recursive)，不能进行多个 watch。</li>
<li>很难通过 watch 机制来实现完整的数据同步（有丢失变更的风险），所以当前的大多数使用方式是通过 watch 得知变更，然后通过 get 重新获取数据，并不完全依赖于 watch 的变更 event。</li>
</ol>
<h2 id="etcd-v3-存储，watch-以及过期机制">Etcd v3 存储，Watch 以及过期机制</h2>
<p><img src="images/etcd-v3.png" alt="etcdv3"/></p>
<p>Etcd v3 将 watch 和 store 拆开实现，我们先分析下 store 的实现。</p>
<p>Etcd v3 store 分为两部分，一部分是内存中的索引，kvindex，是基于 google 开源的一个 golang 的 btree 实现的，另外一部分是后端存储。按照它的设计，backend 可以对接多种存储，当前使用的 boltdb。boltdb 是一个单机的支持事务的 kv 存储，Etcd 的事务是基于 boltdb 的事务实现的。Etcd 在 boltdb 中存储的 key 是 revision，value 是 Etcd 自己的 key-value 组合，也就是说 Etcd 会在 boltdb 中把每个版本都保存下，从而实现了多版本机制。</p>
<p>举个例子：
用 etcdctl 通过批量接口写入两条记录：</p>
<pre><code>etcdctl txn <<<'
put key1 "v1"
put key2 "v2"

'
</code></pre><p>再通过批量接口更新这两条记录：</p>
<pre><code>etcdctl txn <<<'
put key1 "v12"
put key2 "v22"

'
</code></pre><p>boltdb 中其实有了 4 条数据：</p>
<pre><code>rev={3 0}, key=key1, value="v1"
rev={3 1}, key=key2, value="v2"
rev={4 0}, key=key1, value="v12"
rev={4 1}, key=key2, value="v22"
</code></pre><p>revision 主要由两部分组成，第一部分 main rev，每次事务进行加一，第二部分 sub rev，同一个事务中的每次操作加一。如上示例，第一次操作的 main rev 是 3，第二次是 4。当然这种机制大家想到的第一个问题就是空间问题，所以 Etcd 提供了命令和设置选项来控制 compact，同时支持 put 操作的参数来精确控制某个 key 的历史版本数。</p>
<p>了解了 Etcd 的磁盘存储，可以看出如果要从 boltdb 中查询数据，必须通过 revision，但客户端都是通过 key 来查询 value，所以 Etcd 的内存 kvindex 保存的就是 key 和 revision 之前的映射关系，用来加速查询。</p>
<p>然后我们再分析下 watch 机制的实现。Etcd v3 的 watch 机制支持 watch 某个固定的 key，也支持 watch 一个范围（可以用于模拟目录的结构的 watch），所以 watchGroup 包含两种 watcher，一种是 key watchers，数据结构是每个 key 对应一组 watcher，另外一种是 range watchers, 数据结构是一个 IntervalTree（不熟悉的参看文文末链接），方便通过区间查找到对应的 watcher。</p>
<p>同时，每个 WatchableStore 包含两种 watcherGroup，一种是 synced，一种是 unsynced，前者表示该 group 的 watcher 数据都已经同步完毕，在等待新的变更，后者表示该 group 的 watcher 数据同步落后于当前最新变更，还在追赶。</p>
<p>当 Etcd 收到客户端的 watch 请求，如果请求携带了 revision 参数，则比较请求的 revision 和 store 当前的 revision，如果大于当前 revision，则放入 synced 组中，否则放入 unsynced 组。同时 Etcd 会启动一个后台的 goroutine 持续同步 unsynced 的 watcher，然后将其迁移到 synced 组。也就是这种机制下，Etcd v3 支持从任意版本开始 watch，没有 v2 的 1000 条历史 event 表限制的问题（当然这是指没有 compact 的情况下）。</p>
<p>另外我们前面提到的，Etcd v2 在通知客户端时，如果网络不好或者客户端读取比较慢，发生了阻塞，则会直接关闭当前连接，客户端需要重新发起请求。Etcd v3 为了解决这个问题，专门维护了一个推送时阻塞的 watcher 队列，在另外的 goroutine 里进行重试。</p>
<p>Etcd v3 对过期机制也做了改进，过期时间设置在 lease 上，然后 key 和 lease 关联。这样可以实现多个 key 关联同一个 lease id，方便设置统一的过期时间，以及实现批量续约。</p>
<p>相比 Etcd v2, Etcd v3 的一些主要变化：</p>
<ol>
<li>接口通过 grpc 提供 rpc 接口，放弃了 v2 的 http 接口。优势是长连接效率提升明显，缺点是使用不如以前方便，尤其对不方便维护长连接的场景。</li>
<li>废弃了原来的目录结构，变成了纯粹的 kv，用户可以通过前缀匹配模式模拟目录。</li>
<li>内存中不再保存 value，同样的内存可以支持存储更多的 key。</li>
<li>watch 机制更稳定，基本上可以通过 watch 机制实现数据的完全同步。</li>
<li>提供了批量操作以及事务机制，用户可以通过批量事务请求来实现 Etcd v2 的 CAS 机制（批量事务支持 if 条件判断）。</li>
</ol>
<h2 id="etcd，zookeeper，consul-比较">Etcd，Zookeeper，Consul 比较</h2>
<ul>
<li>Etcd 和 Zookeeper 提供的能力非常相似，都是通用的一致性元信息存储，都提供 watch 机制用于变更通知和分发，也都被分布式系统用来作为共享信息存储，在软件生态中所处的位置也几乎是一样的，可以互相替代的。二者除了实现细节，语言，一致性协议上的区别，最大的区别在周边生态圈。Zookeeper 是 apache 下的，用 java 写的，提供 rpc 接口，最早从 hadoop 项目中孵化出来，在分布式系统中得到广泛使用（hadoop, solr, kafka, mesos 等）。Etcd 是 coreos 公司旗下的开源产品，比较新，以其简单好用的 rest 接口以及活跃的社区俘获了一批用户，在新的一些集群中得到使用（比如 kubernetes）。虽然 v3 为了性能也改成二进制 rpc 接口了，但其易用性上比 Zookeeper 还是好一些。</li>
<li>而 Consul 的目标则更为具体一些，Etcd 和 Zookeeper 提供的是分布式一致性存储能力，具体的业务场景需要用户自己实现，比如服务发现，比如配置变更。而 Consul 则以服务发现和配置变更为主要目标，同时附带了 kv 存储。</li>
</ul>
<h2 id="etcd-的周边工具">Etcd 的周边工具</h2>
<ol>
<li><p><strong>Confd</strong></p>
<p> 在分布式系统中，理想情况下是应用程序直接和 Etcd 这样的服务发现 / 配置中心交互，通过监听 Etcd 进行服务发现以及配置变更。但我们还有许多历史遗留的程序，服务发现以及配置大多都是通过变更配置文件进行的。Etcd 自己的定位是通用的 kv 存储，所以并没有像 Consul 那样提供实现配置变更的机制和工具，而 Confd 就是用来实现这个目标的工具。</p>
<p> Confd 通过 watch 机制监听 Etcd 的变更，然后将数据同步到自己的一个本地存储。用户可以通过配置定义自己关注哪些 key 的变更，同时提供一个配置文件模板。Confd 一旦发现数据变更就使用最新数据渲染模板生成配置文件，如果新旧配置文件有变化，则进行替换，同时触发用户提供的 reload 脚本，让应用程序重新加载配置。</p>
<p> Confd 相当于实现了部分 Consul 的 agent 以及 consul-template 的功能，作者是 kubernetes 的 Kelsey Hightower，但大神貌似很忙，没太多时间关注这个项目了，很久没有发布版本，我们着急用，所以 fork 了一份自己更新维护，主要增加了一些新的模板函数以及对 metad 后端的支持。<a href="https://github.com/yunify/confd" target="_blank">confd</a></p>
</li>
<li><p><strong>Metad</strong></p>
<p> 服务注册的实现模式一般分为两种，一种是调度系统代为注册，一种是应用程序自己注册。调度系统代为注册的情况下，应用程序启动后需要有一种机制让应用程序知道『我是谁』，然后发现自己所在的集群以及自己的配置。Metad 提供这样一种机制，客户端请求 Metad 的一个固定的接口 /self，由 Metad 告知应用程序其所属的元信息，简化了客户端的服务发现和配置变更逻辑。</p>
<p> Metad 通过保存一个 ip 到元信息路径的映射关系来做到这一点，当前后端支持 Etcd v3，提供简单好用的 http rest 接口。 它会把 Etcd 的数据通过 watch 机制同步到本地内存中，相当于 Etcd 的一个代理。所以也可以把它当做 Etcd 的代理来使用，适用于不方便使用 Etcd v3 的 rpc 接口或者想降低 Etcd 压力的场景。  <a href="https://github.com/yunify/metad" target="_blank">metad</a></p>
</li>
</ol>
<h2 id="etcd-使用注意事项">Etcd 使用注意事项</h2>
<ol>
<li><p>Etcd cluster 初始化的问题</p>
<p> 如果集群第一次初始化启动的时候，有一台节点未启动，通过 v3 的接口访问的时候，会报告 Error:  Etcdserver: not capable 错误。这是为兼容性考虑，集群启动时默认的 API 版本是 2.3，只有当集群中的所有节点都加入了，确认所有节点都支持 v3 接口时，才提升集群版本到 v3。这个只有第一次初始化集群的时候会遇到，如果集群已经初始化完毕，再挂掉节点，或者集群关闭重启（关闭重启的时候会从持久化数据中加载集群 API 版本），都不会有影响。</p>
</li>
<li><p>Etcd 读请求的机制</p>
<p> v2  quorum=true 的时候，读取是通过 raft 进行的，通过 cli 请求，该参数默认为 true。</p>
<p> v3  --consistency=“l” 的时候（默认）通过 raft 读取，否则读取本地数据。sdk 代码里则是通过是否打开：WithSerializable option 来控制。</p>
<p> 一致性读取的情况下，每次读取也需要走一次 raft 协议，能保证一致性，但性能有损失，如果出现网络分区，集群的少数节点是不能提供一致性读取的。但如果不设置该参数，则是直接从本地的 store 里读取，这样就损失了一致性。使用的时候需要注意根据应用场景设置这个参数，在一致性和可用性之间进行取舍。</p>
</li>
<li><p>Etcd 的 compact 机制</p>
<p> Etcd 默认不会自动 compact，需要设置启动参数，或者通过命令进行 compact，如果变更频繁建议设置，否则会导致空间和内存的浪费以及错误。Etcd v3 的默认的 backend quota 2GB，如果不 compact，boltdb 文件大小超过这个限制后，就会报错：”Error:  etcdserver: mvcc: database space exceeded”，导致数据无法写入。</p>
</li>
</ol>
<h2 id="etcd-的问题">etcd 的问题</h2>
<pre><code>当前 Etcd 的 raft 实现保证了多个节点数据之间的同步，但明显的一个问题就是扩充节点不能解决容量问题。要想解决容量问题，只能进行分片，但分片后如何使用 raft 同步数据？只能实现一个 multiple group raft，每个分片的多个副本组成一个虚拟的 raft group，通过 raft 实现数据同步。当前实现了 multiple group raft 的有 TiKV 和 Cockroachdb，但尚未一个独立通用的。理论上来说，如果有了这套 multiple group raft，后面挂个持久化的 kv 就是一个分布式 kv 存储，挂个内存 kv 就是分布式缓存，挂个 lucene 就是分布式搜索引擎。当然这只是理论上，要真实现复杂度还是不小。
</code></pre><p>注： 部分转自 <a href="http://jolestar.com/etcd-architecture/" target="_blank">jolestar</a> 和<a href="http://www.infoq.com/cn/articles/etcd-interpretation-application-scenario-implement-principle" target="_blank">infoq</a>.</p>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://coreos.com/etcd/" target="_blank">Etcd website</a></li>
<li><a href="https://github.com/coreos/etcd/" target="_blank">Etcd github</a></li>
<li><a href="https://github.com/coreos/etcd/blob/master/Documentation/production-users.md" target="_blank">Projects using etcd</a></li>
<li><a href="http://jolestar.com/etcd-architecture/" target="_blank">http://jolestar.com/etcd-architecture/</a></li>
<li><a href="http://www.infoq.com/cn/articles/etcd-interpretation-application-scenario-implement-principle" target="_blank">etcd 从应用场景到实现原理的全方位解读</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="api-server" class="level3">kube-apiserver</h1>
<p>kube-apiserver 是 Kubernetes 最重要的核心组件之一，主要提供以下的功能</p>
<ul>
<li>提供集群管理的 REST API 接口，包括认证授权、数据校验以及集群状态变更等</li>
<li>提供其他模块之间的数据交互和通信的枢纽（其他模块通过 API Server 查询或修改数据，只有 API Server 才直接操作 etcd）</li>
</ul>
<h2 id="rest-api">REST API</h2>
<p>kube-apiserver 支持同时提供 https（默认监听在 6443 端口）和 http API（默认监听在 127.0.0.1 的 8080 端口），其中 http API 是非安全接口，不做任何认证授权机制，不建议生产环境启用。两个接口提供的 REST API 格式相同，参考 <a href="https://kubernetes.io/docs/api-reference/v1.8/" target="_blank">Kubernetes API Reference</a> 查看所有 API 的调用格式。</p>
<p>在实际使用中，通常通过 <a href="https://kubernetes.io/docs/user-guide/kubectl-overview/" target="_blank">kubectl</a> 来访问 apiserver，也可以通过 Kubernetes 各个语言的 client 库来访问 apiserver。在使用 kubectl 时，打开调试日志也可以看到每个 API 调用的格式，比如</p>
<pre><code class="lang-sh">$ kubectl --v=8 get pods
</code></pre>
<p>可通过 <code>kubectl api-versions</code> 和 <code>kubectl api-resources</code> 查询 Kubernetes API 支持的 API 版本以及资源对象。</p>
<pre><code class="lang-sh">$ kubectl api-versions
admissionregistration.k8s.io/v1beta1
apiextensions.k8s.io/v1beta1
apiregistration.k8s.io/v1
apiregistration.k8s.io/v1beta1
apps/v1
apps/v1beta1
apps/v1beta2
authentication.k8s.io/v1
authentication.k8s.io/v1beta1
authorization.k8s.io/v1
authorization.k8s.io/v1beta1
autoscaling/v1
autoscaling/v2beta1
batch/v1
batch/v1beta1
certificates.k8s.io/v1beta1
events.k8s.io/v1beta1
extensions/v1beta1
metrics.k8s.io/v1beta1
networking.k8s.io/v1
policy/v1beta1
rbac.authorization.k8s.io/v1
rbac.authorization.k8s.io/v1beta1
scheduling.k8s.io/v1beta1
storage.k8s.io/v1
storage.k8s.io/v1beta1
v1

$ kubectl api-resources --api-group=storage.k8s.io
NAME                SHORTNAMES   APIGROUP         NAMESPACED   KIND
storageclasses      sc           storage.k8s.io   <span class="hljs-literal">false</span>        StorageClass
volumeattachments                storage.k8s.io   <span class="hljs-literal">false</span>        VolumeAttachment
</code></pre>
<h2 id="openapi-和-swagger">OpenAPI 和 Swagger</h2>
<p>通过 <code>/swaggerapi</code> 可以查看 Swagger API，<code>/openapi/v2</code> 查看 OpenAPI。</p>
<p>开启 <code>--enable-swagger-ui=true</code> 后还可以通过 <code>/swagger-ui</code> 访问 Swagger UI。</p>
<p>根据 OpenAPI 也可以生成各种语言的客户端，比如可以用下面的命令生成 Go 语言的客户端：</p>
<pre><code class="lang-sh">git <span class="hljs-built_in">clone</span> https://github.com/kubernetes-client/gen /tmp/gen
cat >go.settings <<EOF
<span class="hljs-comment"># Kubernetes branch name</span>
<span class="hljs-built_in">export</span> KUBERNETES_BRANCH=<span class="hljs-string">"release-1.11"</span>

<span class="hljs-comment"># client version for packaging and releasing.</span>
<span class="hljs-built_in">export</span> CLIENT_VERSION=<span class="hljs-string">"1.0"</span>

<span class="hljs-comment"># Name of the release package</span>
<span class="hljs-built_in">export</span> PACKAGE_NAME=<span class="hljs-string">"client-go"</span>
EOF

/tmp/gen/openapi/go.sh ./client-go ./go.settings
</code></pre>
<h2 id="访问控制">访问控制</h2>
<p>Kubernetes API 的每个请求都会经过多阶段的访问控制之后才会被接受，这包括认证、授权以及准入控制（Admission Control）等。</p>
<p><img src="images/access_control.png" alt=""/></p>
<h3 id="认证">认证</h3>
<p>开启 TLS 时，所有的请求都需要首先认证。Kubernetes 支持多种认证机制，并支持同时开启多个认证插件（只要有一个认证通过即可）。如果认证成功，则用户的 <code>username</code> 会传入授权模块做进一步授权验证；而对于认证失败的请求则返回 HTTP 401。</p>
<blockquote>
<p><strong>Kubernetes 不直接管理用户</strong></p>
<p>虽然 Kubernetes 认证和授权用到了 username，但 Kubernetes 并不直接管理用户，不能创建 <code>user</code> 对象，也不存储 username。</p>
</blockquote>
<p>更多认证模块的使用方法可以参考 <a href="../plugins/auth.html#%20认证">Kubernetes 认证插件</a>。</p>
<h3 id="授权">授权</h3>
<p>认证之后的请求就到了授权模块。跟认证类似，Kubernetes 也支持多种授权机制，并支持同时开启多个授权插件（只要有一个验证通过即可）。如果授权成功，则用户的请求会发送到准入控制模块做进一步的请求验证；而对于授权失败的请求则返回 HTTP 403.</p>
<p>更多授权模块的使用方法可以参考 <a href="../plugins/auth.html#%20授权">Kubernetes 授权插件</a>。</p>
<h3 id="准入控制">准入控制</h3>
<p>准入控制（Admission Control）用来对请求做进一步的验证或添加默认参数。不同于授权和认证只关心请求的用户和操作，准入控制还处理请求的内容，并且仅对创建、更新、删除或连接（如代理）等有效，而对读操作无效。准入控制也支持同时开启多个插件，它们依次调用，只有全部插件都通过的请求才可以放过进入系统。</p>
<p>更多准入控制模块的使用方法可以参考 <a href="../plugins/admission.html">Kubernetes 准入控制</a>。</p>
<h2 id="启动-apiserver-示例">启动 apiserver 示例</h2>
<pre><code class="lang-sh">kube-apiserver --feature-gates=AllAlpha=<span class="hljs-literal">true</span> --runtime-config=api/all=<span class="hljs-literal">true</span> \
    --requestheader-allowed-names=front-proxy-client \
    --client-ca-file=/etc/kubernetes/pki/ca.crt \
    --allow-privileged=<span class="hljs-literal">true</span> \
    --experimental-bootstrap-token-auth=<span class="hljs-literal">true</span> \
    --storage-backend=etcd3 \
    --requestheader-username-headers=X-Remote-User \
    --requestheader-extra-headers-prefix=X-Remote-Extra- \
    --service-account-key-file=/etc/kubernetes/pki/sa.pub \
    --tls-cert-file=/etc/kubernetes/pki/apiserver.crt \
    --tls-private-key-file=/etc/kubernetes/pki/apiserver.key \
    --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt \
    --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt \
    --insecure-port=8080 \
    --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeLabel,DefaultStorageClass,ResourceQuota,DefaultTolerationSeconds \
    --requestheader-group-headers=X-Remote-Group \
    --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key \
    --secure-port=6443 \
    --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname \
    --service-cluster-ip-range=10.96.0.0/12 \
    --authorization-mode=RBAC \
    --advertise-address=192.168.0.20 --etcd-servers=http://127.0.0.1:2379
</code></pre>
<h2 id="工作原理">工作原理</h2>
<p>kube-apiserver 提供了 Kubernetes 的 REST API，实现了认证、授权、准入控制等安全校验功能，同时也负责集群状态的存储操作（通过 etcd）。</p>
<p><img src="images/kube-apiserver.png" alt=""/></p>
<h2 id="api-访问">API 访问</h2>
<p>有多种方式可以访问 Kubernetes 提供的 REST API：</p>
<ul>
<li><a href="kubectl.html">kubectl</a> 命令行工具</li>
<li>SDK，支持多种语言<ul>
<li><a href="https://github.com/kubernetes/client-go" target="_blank">Go</a></li>
<li><a href="https://github.com/kubernetes-incubator/client-python" target="_blank">Python</a></li>
<li><a href="https://github.com/kubernetes-client/javascript" target="_blank">Javascript</a></li>
<li><a href="https://github.com/kubernetes-client/java" target="_blank">Java</a></li>
<li><a href="https://github.com/kubernetes-client/csharp" target="_blank">CSharp</a></li>
<li>其他 <a href="https://www.openapis.org/" target="_blank">OpenAPI</a> 支持的语言，可以通过 <a href="https://github.com/kubernetes-client/gen" target="_blank">gen</a> 工具生成相应的 client</li>
</ul>
</li>
</ul>
<h3 id="kubectl">kubectl</h3>
<pre><code class="lang-sh">kubectl get --raw /api/v1/namespaces
kubectl get --raw /apis/metrics.k8s.io/v1beta1/nodes
kubectl get --raw /apis/metrics.k8s.io/v1beta1/pods
</code></pre>
<h3 id="kubectl-proxy">kubectl proxy</h3>
<pre><code class="lang-sh">$ kubectl proxy --port=8080 &

$ curl http://localhost:8080/api/
{
  <span class="hljs-string">"versions"</span>: [
    <span class="hljs-string">"v1"</span>
  ]
}
</code></pre>
<h3 id="curl">curl</h3>
<pre><code class="lang-sh"><span class="hljs-comment"># In Pods with service account.</span>
$ TOKEN=$(cat /run/secrets/kubernetes.io/serviceaccount/token)
$ CACERT=/run/secrets/kubernetes.io/serviceaccount/ca.crt
$ curl --cacert <span class="hljs-variable">$CACERT</span> --header <span class="hljs-string">"Authorization: Bearer <span class="hljs-variable">$TOKEN</span>"</span>  https://<span class="hljs-variable">$KUBERNETES_SERVICE_HOST</span>:<span class="hljs-variable">$KUBERNETES_SERVICE_PORT</span>/api
{
  <span class="hljs-string">"kind"</span>: <span class="hljs-string">"APIVersions"</span>,
  <span class="hljs-string">"versions"</span>: [
    <span class="hljs-string">"v1"</span>
  ],
  <span class="hljs-string">"serverAddressByClientCIDRs"</span>: [
    {
      <span class="hljs-string">"clientCIDR"</span>: <span class="hljs-string">"0.0.0.0/0"</span>,
      <span class="hljs-string">"serverAddress"</span>: <span class="hljs-string">"10.0.1.149:443"</span>
    }
  ]
}
</code></pre>
<pre><code class="lang-sh"><span class="hljs-comment"># Outside of Pods.</span>
$ APISERVER=$(kubectl config view | grep server | cut <span class="hljs-_">-f</span> 2- <span class="hljs-_">-d</span> <span class="hljs-string">":"</span> | tr <span class="hljs-_">-d</span> <span class="hljs-string">" "</span>)
$ TOKEN=$(kubectl describe secret $(kubectl get secrets | grep default | cut <span class="hljs-_">-f</span>1 <span class="hljs-_">-d</span> <span class="hljs-string">' '</span>) | grep -E <span class="hljs-string">'^token'</span>| cut <span class="hljs-_">-f</span>2 <span class="hljs-_">-d</span><span class="hljs-string">':'</span>| tr <span class="hljs-_">-d</span> <span class="hljs-string">'\t'</span>)
$ curl <span class="hljs-variable">$APISERVER</span>/api --header <span class="hljs-string">"Authorization: Bearer <span class="hljs-variable">$TOKEN</span>"</span> --insecure
{
  <span class="hljs-string">"kind"</span>: <span class="hljs-string">"APIVersions"</span>,
  <span class="hljs-string">"versions"</span>: [
    <span class="hljs-string">"v1"</span>
  ],
  <span class="hljs-string">"serverAddressByClientCIDRs"</span>: [
    {
      <span class="hljs-string">"clientCIDR"</span>: <span class="hljs-string">"0.0.0.0/0"</span>,
      <span class="hljs-string">"serverAddress"</span>: <span class="hljs-string">"10.0.1.149:443"</span>
    }
  ]
}
</code></pre>
<h2 id="api-参考文档">API 参考文档</h2>
<p>最近 4 个稳定版本的 API 参考文档为：</p>
<ul>
<li><a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.11/" target="_blank">v1.11 API Reference</a></li>
<li><a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/" target="_blank">v1.10 API Reference</a></li>
<li><a href="https://kubernetes.io/docs/api-reference/v1.9/" target="_blank">v1.9 API Reference</a></li>
<li><a href="https://kubernetes.io/docs/api-reference/v1.8/" target="_blank">v1.8 API Reference</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="kube-scheduler" class="level3">kube-scheduler</h1>
<p>kube-scheduler 负责分配调度 Pod 到集群内的节点上，它监听 kube-apiserver，查询还未分配 Node 的 Pod，然后根据调度策略为这些 Pod 分配节点（更新 Pod 的 <code>NodeName</code> 字段）。</p>
<p>调度器需要充分考虑诸多的因素：</p>
<ul>
<li>公平调度</li>
<li>资源高效利用</li>
<li>QoS</li>
<li>affinity 和 anti-affinity</li>
<li>数据本地化（data locality）</li>
<li>内部负载干扰（inter-workload interference）</li>
<li>deadlines</li>
</ul>
<h2 id="指定-node-节点调度">指定 Node 节点调度</h2>
<p>有三种方式指定 Pod 只运行在指定的 Node 节点上</p>
<ul>
<li>nodeSelector：只调度到匹配指定 label 的 Node 上</li>
<li>nodeAffinity：功能更丰富的 Node 选择器，比如支持集合操作</li>
<li>podAffinity：调度到满足条件的 Pod 所在的 Node 上</li>
</ul>
<h3 id="nodeselector-示例">nodeSelector 示例</h3>
<p>首先给 Node 打上标签</p>
<pre><code class="lang-sh">kubectl label nodes node-01 disktype=ssd
</code></pre>
<p>然后在 daemonset 中指定 nodeSelector 为 <code>disktype=ssd</code>：</p>
<pre><code class="lang-yaml"><span class="hljs-attr">spec:</span>
<span class="hljs-attr">  nodeSelector:</span>
<span class="hljs-attr">    disktype:</span> ssd
</code></pre>
<h3 id="nodeaffinity-示例">nodeAffinity 示例</h3>
<p>nodeAffinity 目前支持两种：requiredDuringSchedulingIgnoredDuringExecution 和 preferredDuringSchedulingIgnoredDuringExecution，分别代表必须满足条件和优选条件。比如下面的例子代表调度到包含标签 <code>kubernetes.io/e2e-az-name</code> 并且值为 e2e-az1 或 e2e-az2 的 Node 上，并且优选还带有标签 <code>another-node-label-key=another-node-label-value</code> 的 Node。</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> with-node-affinity
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  affinity:</span>
<span class="hljs-attr">    nodeAffinity:</span>
<span class="hljs-attr">      requiredDuringSchedulingIgnoredDuringExecution:</span>
<span class="hljs-attr">        nodeSelectorTerms:</span>
<span class="hljs-attr">        - matchExpressions:</span>
<span class="hljs-attr">          - key:</span> kubernetes.io/e2e-az-name
<span class="hljs-attr">            operator:</span> In
<span class="hljs-attr">            values:</span>
<span class="hljs-bullet">            -</span> e2e-az1
<span class="hljs-bullet">            -</span> e2e-az2
<span class="hljs-attr">      preferredDuringSchedulingIgnoredDuringExecution:</span>
<span class="hljs-attr">      - weight:</span> <span class="hljs-number">1</span>
<span class="hljs-attr">        preference:</span>
<span class="hljs-attr">          matchExpressions:</span>
<span class="hljs-attr">          - key:</span> another-node-label-key
<span class="hljs-attr">            operator:</span> In
<span class="hljs-attr">            values:</span>
<span class="hljs-bullet">            -</span> another-node-label-value
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">  - name:</span> with-node-affinity
<span class="hljs-attr">    image:</span> gcr.io/google_containers/pause:<span class="hljs-number">2.0</span>
</code></pre>
<h3 id="podaffinity-示例">podAffinity 示例</h3>
<p>podAffinity 基于 Pod 的标签来选择 Node，仅调度到满足条件 Pod 所在的 Node 上，支持 podAffinity 和 podAntiAffinity。这个功能比较绕，以下面的例子为例：</p>
<ul>
<li>如果一个 “Node 所在 Zone 中包含至少一个带有 <code>security=S1</code> 标签且运行中的 Pod”，那么可以调度到该 Node</li>
<li>不调度到 “包含至少一个带有 <code>security=S2</code> 标签且运行中 Pod” 的 Node 上</li>
</ul>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> with-pod-affinity
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  affinity:</span>
<span class="hljs-attr">    podAffinity:</span>
<span class="hljs-attr">      requiredDuringSchedulingIgnoredDuringExecution:</span>
<span class="hljs-attr">      - labelSelector:</span>
<span class="hljs-attr">          matchExpressions:</span>
<span class="hljs-attr">          - key:</span> security
<span class="hljs-attr">            operator:</span> In
<span class="hljs-attr">            values:</span>
<span class="hljs-bullet">            -</span> S1
<span class="hljs-attr">        topologyKey:</span> failure-domain.beta.kubernetes.io/zone
<span class="hljs-attr">    podAntiAffinity:</span>
<span class="hljs-attr">      preferredDuringSchedulingIgnoredDuringExecution:</span>
<span class="hljs-attr">      - weight:</span> <span class="hljs-number">100</span>
<span class="hljs-attr">        podAffinityTerm:</span>
<span class="hljs-attr">          labelSelector:</span>
<span class="hljs-attr">            matchExpressions:</span>
<span class="hljs-attr">            - key:</span> security
<span class="hljs-attr">              operator:</span> In
<span class="hljs-attr">              values:</span>
<span class="hljs-bullet">              -</span> S2
<span class="hljs-attr">          topologyKey:</span> kubernetes.io/hostname
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">  - name:</span> with-pod-affinity
<span class="hljs-attr">    image:</span> gcr.io/google_containers/pause:<span class="hljs-number">2.0</span>
</code></pre>
<h2 id="taints-和-tolerations">Taints 和 tolerations</h2>
<p>Taints 和 tolerations 用于保证 Pod 不被调度到不合适的 Node 上，其中 Taint 应用于 Node 上，而 toleration 则应用于 Pod 上。</p>
<p>目前支持的 taint 类型</p>
<ul>
<li>NoSchedule：新的 Pod 不调度到该 Node 上，不影响正在运行的 Pod</li>
<li>PreferNoSchedule：soft 版的 NoSchedule，尽量不调度到该 Node 上</li>
<li>NoExecute：新的 Pod 不调度到该 Node 上，并且删除（evict）已在运行的 Pod。Pod 可以增加一个时间（tolerationSeconds），</li>
</ul>
<p>然而，当 Pod 的 Tolerations 匹配 Node 的所有 Taints 的时候可以调度到该 Node 上；当 Pod 是已经运行的时候，也不会被删除（evicted）。另外对于 NoExecute，如果 Pod 增加了一个 tolerationSeconds，则会在该时间之后才删除 Pod。</p>
<p>比如，假设 node1 上应用以下几个 taint</p>
<pre><code class="lang-sh">kubectl taint nodes node1 key1=value1:NoSchedule
kubectl taint nodes node1 key1=value1:NoExecute
kubectl taint nodes node1 key2=value2:NoSchedule
</code></pre>
<p>下面的这个 Pod 由于没有 tolerate<code>key2=value2:NoSchedule</code> 无法调度到 node1 上</p>
<pre><code class="lang-yaml"><span class="hljs-attr">tolerations:</span>
<span class="hljs-attr">- key:</span> <span class="hljs-string">"key1"</span>
<span class="hljs-attr">  operator:</span> <span class="hljs-string">"Equal"</span>
<span class="hljs-attr">  value:</span> <span class="hljs-string">"value1"</span>
<span class="hljs-attr">  effect:</span> <span class="hljs-string">"NoSchedule"</span>
<span class="hljs-attr">- key:</span> <span class="hljs-string">"key1"</span>
<span class="hljs-attr">  operator:</span> <span class="hljs-string">"Equal"</span>
<span class="hljs-attr">  value:</span> <span class="hljs-string">"value1"</span>
<span class="hljs-attr">  effect:</span> <span class="hljs-string">"NoExecute"</span>
</code></pre>
<p>而正在运行且带有 tolerationSeconds 的 Pod 则会在 600s 之后删除</p>
<pre><code class="lang-yaml"><span class="hljs-attr">tolerations:</span>
<span class="hljs-attr">- key:</span> <span class="hljs-string">"key1"</span>
<span class="hljs-attr">  operator:</span> <span class="hljs-string">"Equal"</span>
<span class="hljs-attr">  value:</span> <span class="hljs-string">"value1"</span>
<span class="hljs-attr">  effect:</span> <span class="hljs-string">"NoSchedule"</span>
<span class="hljs-attr">- key:</span> <span class="hljs-string">"key1"</span>
<span class="hljs-attr">  operator:</span> <span class="hljs-string">"Equal"</span>
<span class="hljs-attr">  value:</span> <span class="hljs-string">"value1"</span>
<span class="hljs-attr">  effect:</span> <span class="hljs-string">"NoExecute"</span>
<span class="hljs-attr">  tolerationSeconds:</span> <span class="hljs-number">600</span>
<span class="hljs-attr">- key:</span> <span class="hljs-string">"key2"</span>
<span class="hljs-attr">  operator:</span> <span class="hljs-string">"Equal"</span>
<span class="hljs-attr">  value:</span> <span class="hljs-string">"value2"</span>
<span class="hljs-attr">  effect:</span> <span class="hljs-string">"NoSchedule"</span>
</code></pre>
<p>注意，DaemonSet 创建的 Pod 会自动加上对 <code>node.alpha.kubernetes.io/unreachable</code> 和 <code>node.alpha.kubernetes.io/notReady</code> 的 NoExecute Toleration，以避免它们因此被删除。</p>
<h2 id="优先级调度">优先级调度</h2>
<p>从 v1.8 开始，kube-scheduler 支持定义 Pod 的优先级，从而保证高优先级的 Pod 优先调度。并从 v1.11 开始默认开启。</p>
<blockquote>
<p>注：在 v1.8-v1.10 版本中的开启方法为</p>
<ul>
<li>apiserver 配置 <code>--feature-gates=PodPriority=true</code> 和 <code>--runtime-config=scheduling.k8s.io/v1alpha1=true</code></li>
<li>kube-scheduler 配置 <code>--feature-gates=PodPriority=true</code></li>
</ul>
</blockquote>
<p>在指定 Pod 的优先级之前需要先定义一个 PriorityClass（非 namespace 资源），如</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> PriorityClass
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> high-priority
<span class="hljs-attr">value:</span> <span class="hljs-number">1000000</span>
<span class="hljs-attr">globalDefault:</span> <span class="hljs-literal">false</span>
<span class="hljs-attr">description:</span> <span class="hljs-string">"This priority class should be used for XYZ service pods only."</span>
</code></pre>
<p>其中</p>
<ul>
<li><code>value</code> 为 32 位整数的优先级，该值越大，优先级越高</li>
<li><code>globalDefault</code> 用于未配置 PriorityClassName 的 Pod，整个集群中应该只有一个 PriorityClass 将其设置为 true</li>
</ul>
<p>然后，在 PodSpec 中通过 PriorityClassName 设置 Pod 的优先级：</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> nginx
<span class="hljs-attr">  labels:</span>
<span class="hljs-attr">    env:</span> test
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">  - name:</span> nginx
<span class="hljs-attr">    image:</span> nginx
<span class="hljs-attr">    imagePullPolicy:</span> IfNotPresent
<span class="hljs-attr">  priorityClassName:</span> high-priority
</code></pre>
<h2 id="多调度器">多调度器</h2>
<p>如果默认的调度器不满足要求，还可以部署自定义的调度器。并且，在整个集群中还可以同时运行多个调度器实例，通过 <code>podSpec.schedulerName</code> 来选择使用哪一个调度器（默认使用内置的调度器）。</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> nginx
<span class="hljs-attr">  labels:</span>
<span class="hljs-attr">    app:</span> nginx
<span class="hljs-attr">spec:</span>
  <span class="hljs-comment"># 选择使用自定义调度器 my-scheduler</span>
<span class="hljs-attr">  schedulerName:</span> my-scheduler
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">  - name:</span> nginx
<span class="hljs-attr">    image:</span> nginx:<span class="hljs-number">1.10</span>
</code></pre>
<p>调度器的示例参见 <a href="../plugins/scheduler.html">这里</a>。</p>
<h2 id="调度器扩展">调度器扩展</h2>
<p>kube-scheduler 还支持使用 <code>--policy-config-file</code> 指定一个调度策略文件来自定义调度策略，比如</p>
<pre><code class="lang-json">{
<span class="hljs-string">"kind"</span> : <span class="hljs-string">"Policy"</span>,
<span class="hljs-string">"apiVersion"</span> : <span class="hljs-string">"v1"</span>,
<span class="hljs-string">"predicates"</span> : [
    {<span class="hljs-string">"name"</span> : <span class="hljs-string">"PodFitsHostPorts"</span>},
    {<span class="hljs-string">"name"</span> : <span class="hljs-string">"PodFitsResources"</span>},
    {<span class="hljs-string">"name"</span> : <span class="hljs-string">"NoDiskConflict"</span>},
    {<span class="hljs-string">"name"</span> : <span class="hljs-string">"MatchNodeSelector"</span>},
    {<span class="hljs-string">"name"</span> : <span class="hljs-string">"HostName"</span>}
    ],
<span class="hljs-string">"priorities"</span> : [
    {<span class="hljs-string">"name"</span> : <span class="hljs-string">"LeastRequestedPriority"</span>, <span class="hljs-string">"weight"</span> : <span class="hljs-number">1</span>},
    {<span class="hljs-string">"name"</span> : <span class="hljs-string">"BalancedResourceAllocation"</span>, <span class="hljs-string">"weight"</span> : <span class="hljs-number">1</span>},
    {<span class="hljs-string">"name"</span> : <span class="hljs-string">"ServiceSpreadingPriority"</span>, <span class="hljs-string">"weight"</span> : <span class="hljs-number">1</span>},
    {<span class="hljs-string">"name"</span> : <span class="hljs-string">"EqualPriority"</span>, <span class="hljs-string">"weight"</span> : <span class="hljs-number">1</span>}
    ],
<span class="hljs-string">"extenders"</span>:[
    {
        <span class="hljs-string">"urlPrefix"</span>: <span class="hljs-string">"http://127.0.0.1:12346/scheduler"</span>,
        <span class="hljs-string">"apiVersion"</span>: <span class="hljs-string">"v1beta1"</span>,
        <span class="hljs-string">"filterVerb"</span>: <span class="hljs-string">"filter"</span>,
        <span class="hljs-string">"prioritizeVerb"</span>: <span class="hljs-string">"prioritize"</span>,
        <span class="hljs-string">"weight"</span>: <span class="hljs-number">5</span>,
        <span class="hljs-string">"enableHttps"</span>: <span class="hljs-literal">false</span>,
        <span class="hljs-string">"nodeCacheCapable"</span>: <span class="hljs-literal">false</span>
    }
    ]
}
</code></pre>
<h2 id="其他影响调度的因素">其他影响调度的因素</h2>
<ul>
<li>如果 Node Condition 处于 MemoryPressure，则所有 BestEffort 的新 Pod（未指定 resources limits 和 requests）不会调度到该 Node 上</li>
<li>如果 Node Condition 处于 DiskPressure，则所有新 Pod 都不会调度到该 Node 上</li>
<li>为了保证 Critical Pods 的正常运行，当它们处于异常状态时会自动重新调度。Critical Pods 是指<ul>
<li>annotation 包括 <code>scheduler.alpha.kubernetes.io/critical-pod=''</code></li>
<li>tolerations 包括 <code>[{"key":"CriticalAddonsOnly", "operator":"Exists"}]</code></li>
<li>priorityClass 为 <code>system-cluster-critical</code> 或者 <code>system-node-critical</code></li>
</ul>
</li>
</ul>
<h2 id="启动-kube-scheduler-示例">启动 kube-scheduler 示例</h2>
<pre><code class="lang-sh">kube-scheduler --address=127.0.0.1 --leader-elect=<span class="hljs-literal">true</span> --kubeconfig=/etc/kubernetes/scheduler.conf
</code></pre>
<h2 id="kube-scheduler-工作原理">kube-scheduler 工作原理</h2>
<p>kube-scheduler 调度原理：</p>
<pre><code>For given pod:

    +---------------------------------------------+
    |               Schedulable nodes:            |
    |                                             |
    | +--------+    +--------+      +--------+    |
    | | node 1 |    | node 2 |      | node 3 |    |
    | +--------+    +--------+      +--------+    |
    |                                             |
    +-------------------+-------------------------+
                        |
                        |
                        v
    +-------------------+-------------------------+

    Pred. filters: node 3 doesn't have enough resource

    +-------------------+-------------------------+
                        |
                        |
                        v
    +-------------------+-------------------------+
    |             remaining nodes:                |
    |   +--------+                 +--------+     |
    |   | node 1 |                 | node 2 |     |
    |   +--------+                 +--------+     |
    |                                             |
    +-------------------+-------------------------+
                        |
                        |
                        v
    +-------------------+-------------------------+

    Priority function:    node 1: p=2
                          node 2: p=5

    +-------------------+-------------------------+
                        |
                        |
                        v
            select max{node priority} = node 2
</code></pre><p>kube-scheduler 调度分为两个阶段，predicate 和 priority</p>
<ul>
<li>predicate：过滤不符合条件的节点</li>
<li>priority：优先级排序，选择优先级最高的节点</li>
</ul>
<p>predicates 策略</p>
<ul>
<li>PodFitsPorts：同 PodFitsHostPorts</li>
<li>PodFitsHostPorts：检查是否有 Host Ports 冲突</li>
<li>PodFitsResources：检查 Node 的资源是否充足，包括允许的 Pod 数量、CPU、内存、GPU 个数以及其他的 OpaqueIntResources</li>
<li>HostName：检查 <code>pod.Spec.NodeName</code> 是否与候选节点一致</li>
<li>MatchNodeSelector：检查候选节点的 <code>pod.Spec.NodeSelector</code> 是否匹配</li>
<li>NoVolumeZoneConflict：检查 volume zone 是否冲突</li>
<li>MaxEBSVolumeCount：检查 AWS EBS Volume 数量是否过多（默认不超过 39）</li>
<li>MaxGCEPDVolumeCount：检查 GCE PD Volume 数量是否过多（默认不超过 16）</li>
<li>MaxAzureDiskVolumeCount：检查 Azure Disk Volume 数量是否过多（默认不超过 16）</li>
<li>MatchInterPodAffinity：检查是否匹配 Pod 的亲和性要求</li>
<li>NoDiskConflict：检查是否存在 Volume 冲突，仅限于 GCE PD、AWS EBS、Ceph RBD 以及 ISCSI</li>
<li>GeneralPredicates：分为 noncriticalPredicates 和 EssentialPredicates。noncriticalPredicates 中包含 PodFitsResources，EssentialPredicates 中包含 PodFitsHost，PodFitsHostPorts 和 PodSelectorMatches。</li>
<li>PodToleratesNodeTaints：检查 Pod 是否容忍 Node Taints</li>
<li>CheckNodeMemoryPressure：检查 Pod 是否可以调度到 MemoryPressure 的节点上</li>
<li>CheckNodeDiskPressure：检查 Pod 是否可以调度到 DiskPressure 的节点上</li>
<li>NoVolumeNodeConflict：检查节点是否满足 Pod 所引用的 Volume 的条件</li>
</ul>
<p>priorities 策略</p>
<ul>
<li>SelectorSpreadPriority：优先减少节点上属于同一个 Service 或 Replication Controller 的 Pod 数量</li>
<li>InterPodAffinityPriority：优先将 Pod 调度到相同的拓扑上（如同一个节点、Rack、Zone 等）</li>
<li>LeastRequestedPriority：优先调度到请求资源少的节点上</li>
<li>BalancedResourceAllocation：优先平衡各节点的资源使用</li>
<li>NodePreferAvoidPodsPriority：alpha.kubernetes.io/preferAvoidPods 字段判断, 权重为 10000，避免其他优先级策略的影响</li>
<li>NodeAffinityPriority：优先调度到匹配 NodeAffinity 的节点上</li>
<li>TaintTolerationPriority：优先调度到匹配 TaintToleration 的节点上</li>
<li>ServiceSpreadingPriority：尽量将同一个 service 的 Pod 分布到不同节点上，已经被 SelectorSpreadPriority 替代 [默认未使用]</li>
<li>EqualPriority：将所有节点的优先级设置为 1[默认未使用]</li>
<li>ImageLocalityPriority：尽量将使用大镜像的容器调度到已经下拉了该镜像的节点上 [默认未使用]</li>
<li>MostRequestedPriority：尽量调度到已经使用过的 Node 上，特别适用于 cluster-autoscaler[默认未使用]</li>
</ul>
<blockquote>
<p><strong> 代码入口路径 </strong></p>
<p>与 Kubernetes 其他组件的入口不同 (其他都是位于 <code>cmd/</code> 目录)，kube-schedular 的入口在 <code>plugin/cmd/kube-scheduler</code>。</p>
</blockquote>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/" target="_blank">Pod Priority and Preemption</a></li>
<li><a href="https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/" target="_blank">Configure Multiple Schedulers</a></li>
<li><a href="https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/" target="_blank">Taints and Tolerations</a></li>
<li><a href="https://kubernetes.io/blog/2017/03/advanced-scheduling-in-kubernetes/" target="_blank">Advanced Scheduling in Kubernetes</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="controller-manager" class="level3">kube-controller-manager</h1>
<p>Controller Manager 由 kube-controller-manager 和 cloud-controller-manager 组成，是 Kubernetes 的大脑，它通过 apiserver 监控整个集群的状态，并确保集群处于预期的工作状态。</p>
<p><img src="images/post-ccm-arch.png" alt=""/></p>
<p>kube-controller-manager 由一系列的控制器组成</p>
<ul>
<li>Replication Controller</li>
<li>Node Controller</li>
<li>CronJob Controller</li>
<li>Daemon Controller</li>
<li>Deployment Controller</li>
<li>Endpoint Controller</li>
<li>Garbage Collector</li>
<li>Namespace Controller</li>
<li>Job Controller</li>
<li>Pod AutoScaler</li>
<li>RelicaSet</li>
<li>Service Controller</li>
<li>ServiceAccount Controller</li>
<li>StatefulSet Controller</li>
<li>Volume Controller</li>
<li>Resource quota Controller</li>
</ul>
<p>cloud-controller-manager 在 Kubernetes 启用 Cloud Provider 的时候才需要，用来配合云服务提供商的控制，也包括一系列的控制器，如</p>
<ul>
<li>Node Controller</li>
<li>Route Controller</li>
<li>Service Controller</li>
</ul>
<p>从 v1.6 开始，cloud provider 已经经历了几次重大重构，以便在不修改 Kubernetes 核心代码的同时构建自定义的云服务商支持。参考 <a href="../plugins/cloud-provider.html">这里</a> 查看如何为云提供商构建新的 Cloud Provider。</p>
<h2 id="metrics">Metrics</h2>
<p>Controller manager metrics 提供了控制器内部逻辑的性能度量，如 Go 语言运行时度量、etcd 请求延时、云服务商 API 请求延时、云存储请求延时等。Controller manager metrics 默认监听在 <code>kube-controller-manager</code> 的 10252 端口，提供 Prometheus 格式的性能度量数据，可以通过 <code>http://localhost:10252/metrics</code> 来访问。</p>
<pre><code>$ curl http://localhost:10252/metrics
...
# HELP etcd_request_cache_add_latencies_summary Latency in microseconds of adding an object to etcd cache
# TYPE etcd_request_cache_add_latencies_summary summary
etcd_request_cache_add_latencies_summary{quantile="0.5"} NaN
etcd_request_cache_add_latencies_summary{quantile="0.9"} NaN
etcd_request_cache_add_latencies_summary{quantile="0.99"} NaN
etcd_request_cache_add_latencies_summary_sum 0
etcd_request_cache_add_latencies_summary_count 0
# HELP etcd_request_cache_get_latencies_summary Latency in microseconds of getting an object from etcd cache
# TYPE etcd_request_cache_get_latencies_summary summary
etcd_request_cache_get_latencies_summary{quantile="0.5"} NaN
etcd_request_cache_get_latencies_summary{quantile="0.9"} NaN
etcd_request_cache_get_latencies_summary{quantile="0.99"} NaN
etcd_request_cache_get_latencies_summary_sum 0
etcd_request_cache_get_latencies_summary_count 0
...
</code></pre><h2 id="kube-controller-manager-启动示例">kube-controller-manager 启动示例</h2>
<pre><code class="lang-sh">ube-controller-manager \
  --enable-dynamic-provisioning=<span class="hljs-literal">true</span> \
  --feature-gates=AllAlpha=<span class="hljs-literal">true</span> \
  --horizontal-pod-autoscaler-sync-period=10s \
  --horizontal-pod-autoscaler-use-rest-clients=<span class="hljs-literal">true</span> \
  --node-monitor-grace-period=10s \
  --address=127.0.0.1 \
  --leader-elect=<span class="hljs-literal">true</span> \
  --kubeconfig=/etc/kubernetes/controller-manager.conf \
  --cluster-signing-key-file=/etc/kubernetes/pki/ca.key \
  --use-service-account-credentials=<span class="hljs-literal">true</span> \
  --controllers=*,bootstrapsigner,tokencleaner \
  --root-ca-file=/etc/kubernetes/pki/ca.crt \
  --service-account-private-key-file=/etc/kubernetes/pki/sa.key \
  --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt \
  --allocate-node-cidrs=<span class="hljs-literal">true</span> \
  --cluster-cidr=10.244.0.0/16 \
  --node-cidr-mask-size=24
</code></pre>
<h2 id="控制器">控制器</h2>
<h3 id="kube-controller-manager">kube-controller-manager</h3>
<p>kube-controller-manager 由一系列的控制器组成，这些控制器可以划分为三组</p>
<ol>
<li>必须启动的控制器<ul>
<li>EndpointController</li>
<li>ReplicationController：</li>
<li>PodGCController</li>
<li>ResourceQuotaController</li>
<li>NamespaceController</li>
<li>ServiceAccountController</li>
<li>GarbageCollectorController</li>
<li>DaemonSetController</li>
<li>JobController</li>
<li>DeploymentController</li>
<li>ReplicaSetController</li>
<li>HPAController</li>
<li>DisruptionController</li>
<li>StatefulSetController</li>
<li>CronJobController</li>
<li>CSRSigningController</li>
<li>CSRApprovingController</li>
<li>TTLController</li>
</ul>
</li>
<li>默认启动的可选控制器，可通过选项设置是否开启<ul>
<li>TokenController</li>
<li>NodeController</li>
<li>ServiceController</li>
<li>RouteController</li>
<li>PVBinderController</li>
<li>AttachDetachController</li>
</ul>
</li>
<li>默认禁止的可选控制器，可通过选项设置是否开启<ul>
<li>BootstrapSignerController</li>
<li>TokenCleanerController</li>
</ul>
</li>
</ol>
<h3 id="cloud-controller-manager">cloud-controller-manager</h3>
<p>cloud-controller-manager 在 Kubernetes 启用 Cloud Provider 的时候才需要，用来配合云服务提供商的控制，也包括一系列的控制器</p>
<ul>
<li>CloudNodeController</li>
<li>RouteController</li>
<li>ServiceController</li>
</ul>
<h2 id="高可用">高可用</h2>
<p>在启动时设置 <code>--leader-elect=true</code> 后，controller manager 会使用多节点选主的方式选择主节点。只有主节点才会调用 <code>StartControllers()</code> 启动所有控制器，而其他从节点则仅执行选主算法。</p>
<p>多节点选主的实现方法见 <a href="https://github.com/kubernetes/client-go/blob/master/tools/leaderelection/leaderelection.go" target="_blank">leaderelection.go</a>。它实现了两种资源锁（Endpoint 或 ConfigMap，kube-controller-manager 和 cloud-controller-manager 都使用 Endpoint 锁），通过更新资源的 Annotation（<code>control-plane.alpha.kubernetes.io/leader</code>），来确定主从关系。</p>
<h2 id="高性能">高性能</h2>
<p>从 Kubernetes 1.7 开始，所有需要监控资源变化情况的调用均推荐使用 <a href="https://github.com/kubernetes/client-go/blob/master/tools/cache/shared_informer.go" target="_blank">Informer</a>。Informer 提供了基于事件通知的只读缓存机制，可以注册资源变化的回调函数，并可以极大减少 API 的调用。</p>
<p>Informer 的使用方法可以参考 <a href="https://github.com/feiskyer/kubernetes-handbook/tree/master/zh/examples/client/informer" target="_blank">这里</a>。</p>
<h2 id="node-eviction">Node Eviction</h2>
<p>Node 控制器在节点异常后，会按照默认的速率（<code>--node-eviction-rate=0.1</code>，即每10秒一个节点的速率）进行 Node 的驱逐。Node 控制器按照 Zone 将节点划分为不同的组，再跟进 Zone 的状态进行速率调整：</p>
<ul>
<li>Normal：所有节点都 Ready，默认速率驱逐。</li>
<li>PartialDisruption：即超过33% 的节点 NotReady 的状态。当异常节点比例大于 <code>--unhealthy-zone-threshold=0.55</code> 时开始减慢速率：<ul>
<li>小集群（即节点数量小于 <code>--large-cluster-size-threshold=50</code>）：停止驱逐</li>
<li>大集群，减慢速率为 <code>--secondary-node-eviction-rate=0.01</code></li>
</ul>
</li>
<li>FullDisruption：所有节点都 NotReady，返回使用默认速率驱逐。但当所有 Zone 都处在 FullDisruption 时，停止驱逐。</li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="kubelet" class="level3">kubelet</h1>
<p>每个节点上都运行一个 kubelet 服务进程，默认监听 10250 端口，接收并执行 master 发来的指令，管理 Pod 及 Pod 中的容器。每个 kubelet 进程会在 API Server 上注册节点自身信息，定期向 master 节点汇报节点的资源使用情况，并通过 cAdvisor 监控节点和容器的资源。</p>
<h2 id="节点管理">节点管理</h2>
<p>节点管理主要是节点自注册和节点状态更新：</p>
<ul>
<li>Kubelet 可以通过设置启动参数 --register-node 来确定是否向 API Server 注册自己；</li>
<li>如果 Kubelet 没有选择自注册模式，则需要用户自己配置 Node 资源信息，同时需要告知 Kubelet 集群上的 API Server 的位置；</li>
<li>Kubelet 在启动时通过 API Server 注册节点信息，并定时向 API Server 发送节点新消息，API Server 在接收到新消息后，将信息写入 etcd</li>
</ul>
<h2 id="pod-管理">Pod 管理</h2>
<h3 id="获取-pod-清单">获取 Pod 清单</h3>
<p>Kubelet 以 PodSpec 的方式工作。PodSpec 是描述一个 Pod 的 YAML 或 JSON 对象。 kubelet 采用一组通过各种机制提供的 PodSpecs（主要通过 apiserver），并确保这些 PodSpecs 中描述的 Pod 正常健康运行。</p>
<p>向 Kubelet 提供节点上需要运行的 Pod 清单的方法：</p>
<ul>
<li>文件：启动参数 --config 指定的配置目录下的文件 (默认 / etc/kubernetes/manifests/)。该文件每 20 秒重新检查一次（可配置）。</li>
<li>HTTP endpoint (URL)：启动参数 --manifest-url 设置。每 20 秒检查一次这个端点（可配置）。</li>
<li>API Server：通过 API Server 监听 etcd 目录，同步 Pod 清单。</li>
<li>HTTP server：kubelet 侦听 HTTP 请求，并响应简单的 API 以提交新的 Pod 清单。</li>
</ul>
<h3 id="通过-api-server-获取-pod-清单及创建-pod-的过程">通过 API Server 获取 Pod 清单及创建 Pod 的过程</h3>
<p>Kubelet 通过 API Server Client(Kubelet 启动时创建)使用 Watch 加 List 的方式监听 "/registry/nodes/$ 当前节点名" 和 “/registry/pods” 目录，将获取的信息同步到本地缓存中。</p>
<p>Kubelet 监听 etcd，所有针对 Pod 的操作都将会被 Kubelet 监听到。如果发现有新的绑定到本节点的 Pod，则按照 Pod 清单的要求创建该 Pod。</p>
<p>如果发现本地的 Pod 被修改，则 Kubelet 会做出相应的修改，比如删除 Pod 中某个容器时，则通过 Docker Client 删除该容器。
如果发现删除本节点的 Pod，则删除相应的 Pod，并通过 Docker Client 删除 Pod 中的容器。</p>
<p>Kubelet 读取监听到的信息，如果是创建和修改 Pod 任务，则执行如下处理：</p>
<ul>
<li>为该 Pod 创建一个数据目录；</li>
<li>从 API Server 读取该 Pod 清单；</li>
<li>为该 Pod 挂载外部卷；</li>
<li>下载 Pod 用到的 Secret；</li>
<li>检查已经在节点上运行的 Pod，如果该 Pod 没有容器或 Pause 容器没有启动，则先停止 Pod 里所有容器的进程。如果在 Pod 中有需要删除的容器，则删除这些容器；</li>
<li>用 “kubernetes/pause” 镜像为每个 Pod 创建一个容器。Pause 容器用于接管 Pod 中所有其他容器的网络。每创建一个新的 Pod，Kubelet 都会先创建一个 Pause 容器，然后创建其他容器。</li>
<li>为 Pod 中的每个容器做如下处理：<ol>
<li>为容器计算一个 hash 值，然后用容器的名字去 Docker 查询对应容器的 hash 值。若查找到容器，且两者 hash 值不同，则停止 Docker 中容器的进程，并停止与之关联的 Pause 容器的进程；若两者相同，则不做任何处理；</li>
<li>如果容器被终止了，且容器没有指定的 restartPolicy，则不做任何处理；</li>
<li>调用 Docker Client 下载容器镜像，调用 Docker Client 运行容器。</li>
</ol>
</li>
</ul>
<h3 id="static-pod">Static Pod</h3>
<p>所有以非 API Server 方式创建的 Pod 都叫 Static Pod。Kubelet 将 Static Pod 的状态汇报给 API Server，API Server 为该 Static Pod 创建一个 Mirror Pod 和其相匹配。Mirror Pod 的状态将真实反映 Static Pod 的状态。当 Static Pod 被删除时，与之相对应的 Mirror Pod 也会被删除。</p>
<h2 id="容器健康检查">容器健康检查</h2>
<p>Pod 通过两类探针检查容器的健康状态:</p>
<ul>
<li>(1) LivenessProbe 探针：用于判断容器是否健康，告诉 Kubelet 一个容器什么时候处于不健康的状态。如果 LivenessProbe 探针探测到容器不健康，则 Kubelet 将删除该容器，并根据容器的重启策略做相应的处理。如果一个容器不包含 LivenessProbe 探针，那么 Kubelet 认为该容器的 LivenessProbe 探针返回的值永远是 “Success”；</li>
<li>(2)ReadinessProbe：用于判断容器是否启动完成且准备接收请求。如果 ReadinessProbe 探针探测到失败，则 Pod 的状态将被修改。Endpoint Controller 将从 Service 的 Endpoint 中删除包含该容器所在 Pod 的 IP 地址的 Endpoint 条目。</li>
</ul>
<p>Kubelet 定期调用容器中的 LivenessProbe 探针来诊断容器的健康状况。LivenessProbe 包含如下三种实现方式：</p>
<ul>
<li>ExecAction：在容器内部执行一个命令，如果该命令的退出状态码为 0，则表明容器健康；</li>
<li>TCPSocketAction：通过容器的 IP 地址和端口号执行 TCP 检查，如果端口能被访问，则表明容器健康；</li>
<li>HTTPGetAction：通过容器的 IP 地址和端口号及路径调用 HTTP GET 方法，如果响应的状态码大于等于 200 且小于 400，则认为容器状态健康。</li>
</ul>
<p>LivenessProbe 探针包含在 Pod 定义的 spec.containers.{某个容器} 中。</p>
<h2 id="cadvisor-资源监控">cAdvisor 资源监控</h2>
<p>Kubernetes 集群中，应用程序的执行情况可以在不同的级别上监测到，这些级别包括：容器、Pod、Service 和整个集群。Heapster 项目为 Kubernetes 提供了一个基本的监控平台，它是集群级别的监控和事件数据集成器 (Aggregator)。Heapster 以 Pod 的方式运行在集群中，Heapster 通过 Kubelet 发现所有运行在集群中的节点，并查看来自这些节点的资源使用情况。Kubelet 通过 cAdvisor 获取其所在节点及容器的数据。Heapster 通过带着关联标签的 Pod 分组这些信息，这些数据将被推到一个可配置的后端，用于存储和可视化展示。支持的后端包括 InfluxDB(使用 Grafana 实现可视化) 和 Google Cloud Monitoring。</p>
<p>cAdvisor 是一个开源的分析容器资源使用率和性能特性的代理工具，已集成到 Kubernetes 代码中。cAdvisor 自动查找所有在其所在节点上的容器，自动采集 CPU、内存、文件系统和网络使用的统计信息。cAdvisor 通过它所在节点机的 Root 容器，采集并分析该节点机的全面使用情况。</p>
<p>cAdvisor 通过其所在节点机的 4194 端口暴露一个简单的 UI。</p>
<h2 id="kubelet-eviction（驱逐）">Kubelet Eviction（驱逐）</h2>
<p>Kubelet 会监控资源的使用情况，并使用驱逐机制防止计算和存储资源耗尽。在驱逐时，Kubelet 将 Pod 的所有容器停止，并将 PodPhase 设置为 Failed。</p>
<p>Kubelet 定期（<code>housekeeping-interval</code>）检查系统的资源是否达到了预先配置的驱逐阈值，包括</p>
<table>
<thead>
<tr>
<th>Eviction Signal</th>
<th>Condition</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>memory.available</code></td>
<td>MemoryPressue</td>
<td><code>memory.available</code> := <code>node.status.capacity[memory]</code> - <code>node.stats.memory.workingSet</code> （计算方法参考<a href="https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/memory-available.sh" target="_blank">这里</a>）</td>
</tr>
<tr>
<td><code>nodefs.available</code></td>
<td>DiskPressure</td>
<td><code>nodefs.available</code> := <code>node.stats.fs.available</code>（Kubelet Volume以及日志等）</td>
</tr>
<tr>
<td><code>nodefs.inodesFree</code></td>
<td>DiskPressure</td>
<td><code>nodefs.inodesFree</code> := <code>node.stats.fs.inodesFree</code></td>
</tr>
<tr>
<td><code>imagefs.available</code></td>
<td>DiskPressure</td>
<td><code>imagefs.available</code> := <code>node.stats.runtime.imagefs.available</code>（镜像以及容器可写层等）</td>
</tr>
<tr>
<td><code>imagefs.inodesFree</code></td>
<td>DiskPressure</td>
<td><code>imagefs.inodesFree</code> := <code>node.stats.runtime.imagefs.inodesFree</code></td>
</tr>
</tbody>
</table>
<p>这些驱逐阈值可以使用百分比，也可以使用绝对值，如</p>
<pre><code class="lang-sh">--eviction-hard=memory.available<500Mi,nodefs.available<1Gi,imagefs.available<100Gi
--eviction-minimum-reclaim=<span class="hljs-string">"memory.available=0Mi,nodefs.available=500Mi,imagefs.available=2Gi"</span>`
--system-reserved=memory=1.5Gi
</code></pre>
<p>这些驱逐信号可以分为软驱逐和硬驱逐</p>
<ul>
<li>软驱逐（Soft Eviction）：配合驱逐宽限期（eviction-soft-grace-period和eviction-max-pod-grace-period）一起使用。系统资源达到软驱逐阈值并在超过宽限期之后才会执行驱逐动作。</li>
<li>硬驱逐（Hard Eviction ）：系统资源达到硬驱逐阈值时立即执行驱逐动作。</li>
</ul>
<p>驱逐动作包括回收节点资源和驱逐用户 Pod 两种：</p>
<ul>
<li>回收节点资源<ul>
<li>配置了 imagefs 阈值时<ul>
<li>达到 nodefs 阈值：删除已停止的 Pod</li>
<li>达到 imagefs 阈值：删除未使用的镜像</li>
</ul>
</li>
<li>未配置 imagefs 阈值时<ul>
<li>达到 nodefs阈值时，按照删除已停止的 Pod 和删除未使用镜像的顺序清理资源</li>
</ul>
</li>
</ul>
</li>
<li>驱逐用户 Pod<ul>
<li>驱逐顺序为：BestEffort、Burstable、Guaranteed</li>
<li>配置了 imagefs 阈值时<ul>
<li>达到 nodefs 阈值，基于 nodefs 用量驱逐（local volume + logs）</li>
<li>达到 imagefs 阈值，基于 imagefs 用量驱逐（容器可写层）</li>
</ul>
</li>
<li>未配置 imagefs 阈值时<ul>
<li>达到 nodefs阈值时，按照总磁盘使用驱逐（local volume + logs + 容器可写层）</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="容器运行时">容器运行时</h2>
<p>容器运行时（Container Runtime）是 Kubernetes 最重要的组件之一，负责真正管理镜像和容器的生命周期。Kubelet 通过 <a href="../plugins/CRI.html">Container Runtime Interface (CRI)</a> 与容器运行时交互，以管理镜像和容器。</p>
<p>Container Runtime Interface（CRI）是 Kubernetes v1.5 引入的容器运行时接口，它将 Kubelet 与容器运行时解耦，将原来完全面向 Pod 级别的内部接口拆分成面向 Sandbox 和 Container 的 gRPC 接口，并将镜像管理和容器管理分离到不同的服务。</p>
<p><img src="../plugins/images/cri.png" alt=""/></p>
<p>CRI 最早从从 1.4 版就开始设计讨论和开发，在 v1.5 中发布第一个测试版。在 v1.6 时已经有了很多外部容器运行时，如 frakti 和 cri-o 等。v1.7 中又新增了 cri-containerd 支持用 Containerd 来管理容器。</p>
<p>CRI 基于 gRPC 定义了 RuntimeService 和 ImageService 等两个 gRPC 服务，分别用于容器运行时和镜像的管理。其定义在</p>
<ul>
<li>v1.10-v1.11: <a href="https://github.com/kubernetes/kubernetes/tree/master/pkg/kubelet/apis/cri/runtime/v1alpha2" target="_blank">pkg/kubelet/apis/cri/runtime/v1alpha2</a></li>
<li>v1.7-v1.9: <a href="https://github.com/kubernetes/kubernetes/tree/release-1.9/pkg/kubelet/apis/cri/v1alpha1/runtime" target="_blank">pkg/kubelet/apis/cri/v1alpha1/runtime</a></li>
<li>v1.6: <a href="https://github.com/kubernetes/kubernetes/tree/release-1.6/pkg/kubelet/api/v1alpha1/runtime" target="_blank">pkg/kubelet/api/v1alpha1/runtime</a></li>
</ul>
<p>Kubelet 作为 CRI 的客户端，而容器运行时则需要实现 CRI 的服务端（即 gRPC server，通常称为 CRI shim）。容器运行时在启动 gRPC server 时需要监听在本地的 Unix Socket （Windows 使用 tcp 格式）。</p>
<p><img src="images/cri.png" alt=""/></p>
<p>目前基于 CRI 容器引擎已经比较丰富了，包括</p>
<ul>
<li>Docker: 核心代码依然保留在 kubelet 内部（<a href="https://github.com/kubernetes/kubernetes/tree/master/pkg/kubelet/dockershim" target="_blank">pkg/kubelet/dockershim</a>），是最稳定和特性支持最好的运行时</li>
<li>OCI 容器运行时：<ul>
<li>社区有两个实现<ul>
<li><a href="https://github.com/containerd/cri" target="_blank">Containerd</a>，支持 kubernetes v1.7+</li>
<li><a href="https://github.com/kubernetes-incubator/cri-o" target="_blank">CRI-O</a>，支持 Kubernetes v1.6+</li>
</ul>
</li>
<li>支持的 OCI 容器引擎包括<ul>
<li><a href="https://github.com/opencontainers/runc" target="_blank">runc</a>：OCI 标准容器引擎</li>
<li><a href="https://github.com/google/gvisor" target="_blank">gVisor</a>：谷歌开源的基于用户空间内核的沙箱容器引擎</li>
<li><a href="https://github.com/clearcontainers/runtime" target="_blank">Clear Containers</a>：Intel 开源的基于虚拟化的容器引擎</li>
<li><a href="https://github.com/kata-containers/runtime" target="_blank">Kata Containers</a>：基于虚拟化的容器引擎，由 Clear Containers 和 runV 合并而来</li>
</ul>
</li>
</ul>
</li>
<li><a href="https://github.com/alibaba/pouch" target="_blank">PouchContainer</a>：阿里巴巴开源的胖容器引擎</li>
<li><a href="https://github.com/kubernetes/frakti" target="_blank">Frakti</a>：支持 Kubernetes v1.6+，提供基于 hypervisor 和 docker 的混合运行时，适用于运行非可信应用，如多租户和 NFV 等场景</li>
<li><a href="https://github.com/kubernetes-incubator/rktlet" target="_blank">Rktlet</a>：支持 <a href="https://github.com/rkt/rkt" target="_blank">rkt</a> 容器引擎</li>
<li><a href="https://github.com/Mirantis/virtlet" target="_blank">Virtlet</a>：Mirantis 开源的虚拟机容器引擎，直接管理 libvirt 虚拟机，镜像须是 qcow2 格式</li>
<li><a href="https://github.com/apporbit/infranetes" target="_blank">Infranetes</a>：直接管理 IaaS 平台虚拟机，如 GCE、AWS 等</li>
</ul>
<h2 id="启动-kubelet-示例">启动 kubelet 示例</h2>
<pre><code class="lang-sh">/usr/bin/kubelet \
  --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf \
  --kubeconfig=/etc/kubernetes/kubelet.conf \
  --pod-manifest-path=/etc/kubernetes/manifests \
  --allow-privileged=<span class="hljs-literal">true</span> \
  --network-plugin=cni \
  --cni-conf-dir=/etc/cni/net.d \
  --cni-bin-dir=/opt/cni/bin \
  --cluster-dns=10.96.0.10 \
  --cluster-domain=cluster.local \
  --authorization-mode=Webhook \
  --client-ca-file=/etc/kubernetes/pki/ca.crt \
  --cadvisor-port=0 \
  --rotate-certificates=<span class="hljs-literal">true</span> \
  --cert-dir=/var/lib/kubelet/pki
</code></pre>
<h2 id="kubelet-工作原理">kubelet 工作原理</h2>
<p>如下 kubelet 内部组件结构图所示，Kubelet 由许多内部组件构成</p>
<ul>
<li>Kubelet API，包括 10250 端口的认证 API、4194 端口的 cAdvisor API、10255 端口的只读 API 以及 10248 端口的健康检查 API</li>
<li>syncLoop：从 API 或者 manifest 目录接收 Pod 更新，发送到 podWorkers 处理，大量使用 channel 处理来处理异步请求</li>
<li>辅助的 manager，如 cAdvisor、PLEG、Volume Manager 等，处理 syncLoop 以外的其他工作</li>
<li>CRI：容器执行引擎接口，负责与 container runtime shim 通信</li>
<li>容器执行引擎，如 dockershim、rkt 等（注：rkt 暂未完成 CRI 的迁移）</li>
<li>网络插件，目前支持 CNI 和 kubenet</li>
</ul>
<p><img src="images/kubelet.png" alt=""/></p>
<h3 id="pod-启动流程">Pod 启动流程</h3>
<p><img src="images/pod-start.png" alt="Pod Start"/></p>
<h3 id="查询-node-汇总指标">查询 Node 汇总指标</h3>
<p>通过 Kubelet 的 10255 端口可以查询 Node 的汇总指标。有两种访问方式</p>
<ul>
<li>在集群内部可以直接访问 kubelet 的 10255 端口，比如 <code>http://<node-name>:10255/stats/summary</code></li>
<li>在集群外部可以借助 <code>kubectl proxy</code> 来访问，比如</li>
</ul>
<pre><code class="lang-sh">kubectl proxy&
curl http://localhost:8001/api/v1/proxy/nodes/<node-name>:10255/stats/summary
</code></pre>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="kube-proxy" class="level3">kube-proxy</h1>
<p>每台机器上都运行一个 kube-proxy 服务，它监听 API server 中 service 和 endpoint 的变化情况，并通过 iptables 等来为服务配置负载均衡（仅支持 TCP 和 UDP）。</p>
<p>kube-proxy 可以直接运行在物理机上，也可以以 static pod 或者 daemonset 的方式运行。</p>
<p>kube-proxy 当前支持一下几种实现</p>
<ul>
<li>userspace：最早的负载均衡方案，它在用户空间监听一个端口，所有服务通过 iptables 转发到这个端口，然后在其内部负载均衡到实际的 Pod。该方式最主要的问题是效率低，有明显的性能瓶颈。</li>
<li>iptables：目前推荐的方案，完全以 iptables 规则的方式来实现 service 负载均衡。该方式最主要的问题是在服务多的时候产生太多的 iptables 规则，非增量式更新会引入一定的时延，大规模情况下有明显的性能问题</li>
<li>ipvs：为解决 iptables 模式的性能问题，v1.11 新增了 ipvs 模式（v1.8 开始支持测试版，并在 v1.11 GA），采用增量式更新，并可以保证 service 更新期间连接保持不断开</li>
<li>winuserspace：同 userspace，但仅工作在 windows 节点上</li>
</ul>
<p>注意：使用 ipvs 模式时，需要预先在每台 Node 上加载内核模块 <code>nf_conntrack_ipv4</code>, <code>ip_vs</code>, <code>ip_vs_rr</code>, <code>ip_vs_wrr</code>, <code>ip_vs_sh</code> 等。</p>
<pre><code class="lang-sh"><span class="hljs-comment"># load module <module_name></span>
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack_ipv4

<span class="hljs-comment"># to check loaded modules, use</span>
lsmod | grep <span class="hljs-_">-e</span> ipvs <span class="hljs-_">-e</span> nf_conntrack_ipv4
<span class="hljs-comment"># or</span>
cut <span class="hljs-_">-f</span>1 <span class="hljs-_">-d</span> <span class="hljs-string">" "</span>  /proc/modules | grep <span class="hljs-_">-e</span> ip_vs <span class="hljs-_">-e</span> nf_conntrack_ipv4
</code></pre>
<h2 id="iptables-示例">Iptables 示例</h2>
<p><img src="images/iptables-mode.png" alt=""/></p>
<p>(图片来自<a href="https://github.com/cilium/k8s-iptables-diagram" target="_blank">cilium/k8s-iptables-diagram</a>)</p>
<pre><code class="lang-sh"><span class="hljs-comment"># Masquerade</span>
-A KUBE-MARK-DROP -j MARK --set-xmark 0x8000/0x8000
-A KUBE-MARK-MASQ -j MARK --set-xmark 0x4000/0x4000
-A KUBE-POSTROUTING -m comment --comment <span class="hljs-string">"kubernetes service traffic requiring SNAT"</span> -m mark --mark 0x4000/0x4000 -j MASQUERADE

<span class="hljs-comment"># clusterIP and publicIP</span>
-A KUBE-SERVICES ! <span class="hljs-_">-s</span> 10.244.0.0/16 <span class="hljs-_">-d</span> 10.98.154.163/32 -p tcp -m comment --comment <span class="hljs-string">"default/nginx: cluster IP"</span> -m tcp --dport 80 -j KUBE-MARK-MASQ
-A KUBE-SERVICES <span class="hljs-_">-d</span> 10.98.154.163/32 -p tcp -m comment --comment <span class="hljs-string">"default/nginx: cluster IP"</span> -m tcp --dport 80 -j KUBE-SVC-4N57TFCL4MD7ZTDA
-A KUBE-SERVICES <span class="hljs-_">-d</span> 12.12.12.12/32 -p tcp -m comment --comment <span class="hljs-string">"default/nginx: loadbalancer IP"</span> -m tcp --dport 80 -j KUBE-FW-4N57TFCL4MD7ZTDA

<span class="hljs-comment"># Masq for publicIP</span>
-A KUBE-FW-4N57TFCL4MD7ZTDA -m comment --comment <span class="hljs-string">"default/nginx: loadbalancer IP"</span> -j KUBE-MARK-MASQ
-A KUBE-FW-4N57TFCL4MD7ZTDA -m comment --comment <span class="hljs-string">"default/nginx: loadbalancer IP"</span> -j KUBE-SVC-4N57TFCL4MD7ZTDA
-A KUBE-FW-4N57TFCL4MD7ZTDA -m comment --comment <span class="hljs-string">"default/nginx: loadbalancer IP"</span> -j KUBE-MARK-DROP

<span class="hljs-comment"># Masq for nodePort</span>
-A KUBE-NODEPORTS -p tcp -m comment --comment <span class="hljs-string">"default/nginx:"</span> -m tcp --dport 30938 -j KUBE-MARK-MASQ
-A KUBE-NODEPORTS -p tcp -m comment --comment <span class="hljs-string">"default/nginx:"</span> -m tcp --dport 30938 -j KUBE-SVC-4N57TFCL4MD7ZTDA

<span class="hljs-comment"># load balance for each endpoints</span>
-A KUBE-SVC-4N57TFCL4MD7ZTDA -m comment --comment <span class="hljs-string">"default/nginx:"</span> -m statistic --mode random --probability 0.33332999982 -j KUBE-SEP-UXHBWR5XIMVGXW3H
-A KUBE-SVC-4N57TFCL4MD7ZTDA -m comment --comment <span class="hljs-string">"default/nginx:"</span> -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-TOYRWPNILILHH3OR
-A KUBE-SVC-4N57TFCL4MD7ZTDA -m comment --comment <span class="hljs-string">"default/nginx:"</span> -j KUBE-SEP-6QCC2MHJZP35QQAR

<span class="hljs-comment"># endpoint #1</span>
-A KUBE-SEP-6QCC2MHJZP35QQAR <span class="hljs-_">-s</span> 10.244.3.4/32 -m comment --comment <span class="hljs-string">"default/nginx:"</span> -j KUBE-MARK-MASQ
-A KUBE-SEP-6QCC2MHJZP35QQAR -p tcp -m comment --comment <span class="hljs-string">"default/nginx:"</span> -m tcp -j DNAT --to-destination 10.244.3.4:80

<span class="hljs-comment"># endpoint #2</span>
-A KUBE-SEP-TOYRWPNILILHH3OR <span class="hljs-_">-s</span> 10.244.2.4/32 -m comment --comment <span class="hljs-string">"default/nginx:"</span> -j KUBE-MARK-MASQ
-A KUBE-SEP-TOYRWPNILILHH3OR -p tcp -m comment --comment <span class="hljs-string">"default/nginx:"</span> -m tcp -j DNAT --to-destination 10.244.2.4:80

<span class="hljs-comment"># endpoint #3</span>
-A KUBE-SEP-UXHBWR5XIMVGXW3H <span class="hljs-_">-s</span> 10.244.1.2/32 -m comment --comment <span class="hljs-string">"default/nginx:"</span> -j KUBE-MARK-MASQ
-A KUBE-SEP-UXHBWR5XIMVGXW3H -p tcp -m comment --comment <span class="hljs-string">"default/nginx:"</span> -m tcp -j DNAT --to-destination 10.244.1.2:80
</code></pre>
<p>如果服务设置了 <code>externalTrafficPolicy: Local</code> 并且当前 Node 上面没有任何属于该服务的 Pod，那么在 <code>KUBE-XLB-4N57TFCL4MD7ZTDA</code> 中会直接丢掉从公网 IP 请求的包：</p>
<pre><code class="lang-sh">-A KUBE-XLB-4N57TFCL4MD7ZTDA -m comment --comment <span class="hljs-string">"default/nginx: has no local endpoints"</span> -j KUBE-MARK-DROP
</code></pre>
<h2 id="ipvs-示例">ipvs 示例</h2>
<p><a href="https://github.com/kubernetes/kubernetes/blob/master/pkg/proxy/ipvs/README.md" target="_blank">Kube-proxy IPVS mode</a> 列出了各种服务在 IPVS 模式下的工作原理。</p>
<p><img src="images/ipvs-mode.png" alt=""/></p>
<pre><code class="lang-sh">$ ipvsadm -ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  10.0.0.1:443 rr persistent 10800
  -> 192.168.0.1:6443             Masq    1      1          0
TCP  10.0.0.10:53 rr
  -> 172.17.0.2:53                Masq    1      0          0
UDP  10.0.0.10:53 rr
  -> 172.17.0.2:53                Masq    1      0          0
</code></pre>
<p>注意，IPVS 模式也会使用 iptables 来执行 SNAT 和 IP 伪装（MASQUERADE），并使用 ipset 来简化 iptables 规则的管理：</p>
<table>
<thead>
<tr>
<th>ipset 名</th>
<th>成员</th>
<th>用途</th>
</tr>
</thead>
<tbody>
<tr>
<td>KUBE-CLUSTER-IP</td>
<td>All service IP + port</td>
<td>Mark-Masq for cases that <code>masquerade-all=true</code> or <code>clusterCIDR</code> specified</td>
</tr>
<tr>
<td>KUBE-LOOP-BACK</td>
<td>All service IP + port + IP</td>
<td>masquerade for solving hairpin purpose</td>
</tr>
<tr>
<td>KUBE-EXTERNAL-IP</td>
<td>service external IP + port</td>
<td>masquerade for packages to external IPs</td>
</tr>
<tr>
<td>KUBE-LOAD-BALANCER</td>
<td>load balancer ingress IP + port</td>
<td>masquerade for packages to load balancer type service</td>
</tr>
<tr>
<td>KUBE-LOAD-BALANCER-LOCAL</td>
<td>LB ingress IP + port with <code>externalTrafficPolicy=local</code></td>
<td>accept packages to load balancer with <code>externalTrafficPolicy=local</code></td>
</tr>
<tr>
<td>KUBE-LOAD-BALANCER-FW</td>
<td>load balancer ingress IP + port with <code>loadBalancerSourceRanges</code></td>
<td>package filter for load balancer with <code>loadBalancerSourceRanges</code> specified</td>
</tr>
<tr>
<td>KUBE-LOAD-BALANCER-SOURCE-CIDR</td>
<td>load balancer ingress IP + port + source CIDR</td>
<td>package filter for load balancer with <code>loadBalancerSourceRanges</code> specified</td>
</tr>
<tr>
<td>KUBE-NODE-PORT-TCP</td>
<td>nodeport type service TCP port</td>
<td>masquerade for packets to nodePort(TCP)</td>
</tr>
<tr>
<td>KUBE-NODE-PORT-LOCAL-TCP</td>
<td>nodeport type service TCP port with <code>externalTrafficPolicy=local</code></td>
<td>accept packages to nodeport service with <code>externalTrafficPolicy=local</code></td>
</tr>
<tr>
<td>KUBE-NODE-PORT-UDP</td>
<td>nodeport type service UDP port</td>
<td>masquerade for packets to nodePort(UDP)</td>
</tr>
<tr>
<td>KUBE-NODE-PORT-LOCAL-UDP</td>
<td>nodeport type service UDP port with<code>externalTrafficPolicy=local</code></td>
<td>accept packages to nodeport service with<code>externalTrafficPolicy=local</code></td>
</tr>
</tbody>
</table>
<h2 id="启动-kube-proxy-示例">启动 kube-proxy 示例</h2>
<pre><code class="lang-sh">kube-proxy --kubeconfig=/var/lib/kubelet/kubeconfig --cluster-cidr=10.240.0.0/12 --feature-gates=ExperimentalCriticalPodAnnotation=<span class="hljs-literal">true</span> --proxy-mode=iptables
</code></pre>
<h2 id="kube-proxy-工作原理">kube-proxy 工作原理</h2>
<p>kube-proxy 监听 API server 中 service 和 endpoint 的变化情况，并通过 userspace、iptables、ipvs 或 winuserspace 等 proxier 来为服务配置负载均衡（仅支持 TCP 和 UDP）。</p>
<p><img src="images/kube-proxy.png" alt=""/></p>
<h2 id="kube-proxy-不足">kube-proxy 不足</h2>
<p>kube-proxy 目前仅支持 TCP 和 UDP，不支持 HTTP 路由，并且也没有健康检查机制。这些可以通过自定义 <a href="../plugins/ingress.html">Ingress Controller</a> 的方法来解决。</p>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="dns" class="level3">kube-dns</h1>
<p>DNS 是 Kubernetes 的核心功能之一，通过 kube-dns 或 CoreDNS 作为集群的必备扩展来提供命名服务。</p>
<h2 id="coredns">CoreDNS</h2>
<p>从 v1.11 开始可以使用 <a href="https://coredns.io/" target="_blank">CoreDNS</a> 来提供命名服务，并从 v1.13 开始成为默认 DNS 服务。CoreDNS 的特点是效率更高，资源占用率更小，推荐使用 CoreDNS 替代 kube-dns 为集群提供 DNS 服务。</p>
<p>从 kube-dns 升级为 CoreDNS 的步骤为：</p>
<pre><code class="lang-sh">$ git <span class="hljs-built_in">clone</span> https://github.com/coredns/deployment
$ <span class="hljs-built_in">cd</span> deployment/kubernetes
$ ./deploy.sh | kubectl apply <span class="hljs-_">-f</span> -
$ kubectl delete --namespace=kube-system deployment kube-dns
</code></pre>
<p>全新部署的话，可以点击<a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dns" target="_blank">这里</a> 查看 CoreDNS 扩展的配置方法。</p>
<h2 id="支持的-dns-格式">支持的 DNS 格式</h2>
<ul>
<li>Service<ul>
<li>A record：生成 <code>my-svc.my-namespace.svc.cluster.local</code>，解析 IP 分为两种情况<ul>
<li>普通 Service 解析为 Cluster IP</li>
<li>Headless Service 解析为指定的 Pod IP 列表</li>
</ul>
</li>
<li>SRV record：生成 <code>_my-port-name._my-port-protocol.my-svc.my-namespace.svc.cluster.local</code></li>
</ul>
</li>
<li>Pod<ul>
<li>A record：<code>pod-ip-address.my-namespace.pod.cluster.local</code></li>
<li>指定 hostname 和 subdomain：<code>hostname.custom-subdomain.default.svc.cluster.local</code>，如下所示</li>
</ul>
</li>
</ul>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> busybox2
<span class="hljs-attr">  labels:</span>
<span class="hljs-attr">    name:</span> busybox
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  hostname:</span> busybox<span class="hljs-bullet">-2</span>
<span class="hljs-attr">  subdomain:</span> default-subdomain
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">  - image:</span> busybox
<span class="hljs-attr">    command:</span>
<span class="hljs-bullet">      -</span> sleep
<span class="hljs-bullet">      -</span> <span class="hljs-string">"3600"</span>
<span class="hljs-attr">    name:</span> busybox
</code></pre>
<p><img src="images/dns-demo.png" alt=""/></p>
<h2 id="支持配置私有-dns-服务器和上游-dns-服务器">支持配置私有 DNS 服务器和上游 DNS 服务器</h2>
<p>从 Kubernetes 1.6 开始，可以通过为 kube-dns 提供 ConfigMap 来实现对存根域以及上游名称服务器的自定义指定。例如，下面的配置插入了一个单独的私有根 DNS 服务器和两个上游 DNS 服务器。</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> ConfigMap
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> kube-dns
<span class="hljs-attr">  namespace:</span> kube-system
<span class="hljs-attr">data:</span>
<span class="hljs-attr">  stubDomains:</span> <span class="hljs-string">|
    {“acme.local”: [“1.2.3.4”]}
</span><span class="hljs-attr">  upstreamNameservers:</span> <span class="hljs-string">|
    [“8.8.8.8”, “8.8.4.4”]
</span></code></pre>
<p>使用上述特定配置，查询请求首先会被发送到 kube-dns 的 DNS 缓存层 (Dnsmasq 服务器)。Dnsmasq 服务器会先检查请求的后缀，带有集群后缀（例如：”.cluster.local”）的请求会被发往 kube-dns，拥有存根域后缀的名称（例如：”.acme.local”）将会被发送到配置的私有 DNS 服务器 [“1.2.3.4”]。最后，不满足任何这些后缀的请求将会被发送到上游 DNS [“8.8.8.8”, “8.8.4.4”] 里。</p>
<p><img src="images/kube-dns-upstream.png" alt=""/></p>
<h2 id="kube-dns">kube-dns</h2>
<h3 id="启动-kube-dns-示例">启动 kube-dns 示例</h3>
<p>一般通过扩展的方式部署 DNS 服务，如把 <a href="https://kubernetes.feisky.xyz/manifests/kubedns/kube-dns.yaml" target="_blank">kube-dns.yaml</a> 放到 Master 节点的 <code>/etc/kubernetes/addons</code> 目录中。当然也可以手动部署：</p>
<pre><code class="lang-sh">kubectl apply <span class="hljs-_">-f</span> https://kubernetes.feisky.xyz/manifests/kubedns/kube-dns.yaml
</code></pre>
<p>这会在 Kubernetes 中启动一个包含三个容器的 Pod，运行着 DNS 相关的三个服务：</p>
<pre><code class="lang-sh"><span class="hljs-comment"># kube-dns container</span>
kube-dns --domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2

<span class="hljs-comment"># dnsmasq container</span>
dnsmasq-nanny -v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=<span class="hljs-literal">true</span> -- -k --cache-size=1000 --log-facility=- --server=127.0.0.1<span class="hljs-comment">#10053</span>

<span class="hljs-comment"># sidecar container</span>
sidecar --v=2 --logtostderr --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local.,5,A --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local.,5,A
</code></pre>
<p>Kubernetes v1.10 也支持 Beta 版的 CoreDNS，其性能较 kube-dns 更好。可以以扩展方式部署，如把 <a href="https://kubernetes.feisky.xyz/manifests/kubedns/coredns.yaml" target="_blank">coredns.yaml</a> 放到 Master 节点的 <code>/etc/kubernetes/addons</code> 目录中。当然也可以手动部署：</p>
<pre><code class="lang-sh">kubectl apply <span class="hljs-_">-f</span> https://kubernetes.feisky.xyz/manifests/kubedns/coredns.yaml
</code></pre>
<h3 id="kube-dns-工作原理">kube-dns 工作原理</h3>
<p>如下图所示，kube-dns 由三个容器构成：</p>
<ul>
<li>kube-dns：DNS 服务的核心组件，主要由 KubeDNS 和 SkyDNS 组成<ul>
<li>KubeDNS 负责监听 Service 和 Endpoint 的变化情况，并将相关的信息更新到 SkyDNS 中</li>
<li>SkyDNS 负责 DNS 解析，监听在 10053 端口 (tcp/udp)，同时也监听在 10055 端口提供 metrics</li>
<li>kube-dns 还监听了 8081 端口，以供健康检查使用</li>
</ul>
</li>
<li>dnsmasq-nanny：负责启动 dnsmasq，并在配置发生变化时重启 dnsmasq<ul>
<li>dnsmasq 的 upstream 为 SkyDNS，即集群内部的 DNS 解析由 SkyDNS 负责</li>
</ul>
</li>
<li>sidecar：负责健康检查和提供 DNS metrics（监听在 10054 端口）</li>
</ul>
<p><img src="images/kube-dns.png" alt=""/></p>
<h3 id="源码简介">源码简介</h3>
<p>kube-dns 的代码已经从 kubernetes 里面分离出来，放到了 <a href="https://github.com/kubernetes/dns" target="_blank">https://github.com/kubernetes/dns</a>。</p>
<p>kube-dns、dnsmasq-nanny 和 sidecar 的代码均是从 <code>cmd/<cmd-name>/main.go</code> 开始，并分别调用 <code>pkg/dns</code>、<code>pkg/dnsmasq</code> 和 <code>pkg/sidecar</code> 完成相应的功能。而最核心的 DNS 解析则是直接引用了 <code>github.com/skynetservices/skydns/server</code> 的代码，具体实现见 <a href="https://github.com/skynetservices/skydns/tree/master/server" target="_blank">skynetservices/skydns</a>。</p>
<h2 id="常见问题">常见问题</h2>
<p><strong>Ubuntu 18.04 中 DNS 无法解析的问题 </strong></p>
<p>Ubuntu 18.04 中默认开启了 systemd-resolved，它会在系统的 /etc/resolv.conf 中写入 <code>nameserver 127.0.0.53</code>。由于这是一个本地地址，从而会导致 CoreDNS 或者 kube-dns 无法解析外网地址。</p>
<p>解决方法是替换掉 systemd-resolved 生成的 resolv.conf 文件：</p>
<pre><code class="lang-sh">sudo rm /etc/resolv.conf
sudo ln <span class="hljs-_">-s</span> /run/systemd/resolve/resolv.conf /etc/resolv.conf
</code></pre>
<p>或者为 DNS 服务手动指定 resolv.conf 的路径：</p>
<pre><code class="lang-sh">--resolv-conf=/run/systemd/resolve/resolv.conf
</code></pre>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/" target="_blank">dns-pod-service 介绍</a></li>
<li><a href="https://github.com/coredns/coredns" target="_blank">coredns/coredns</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="federation" class="level3">Federation</h1>
<p>在云计算环境中，服务的作用距离范围从近到远一般可以有：同主机（Host，Node）、跨主机同可用区（Available Zone）、跨可用区同地区（Region）、跨地区同服务商（Cloud Service Provider）、跨云平台。K8s 的设计定位是单一集群在同一个地域内，因为同一个地区的网络性能才能满足 K8s 的调度和计算存储连接要求。而集群联邦（Federation）就是为提供跨 Region 跨服务商 K8s 集群服务而设计的。</p>
<p>每个 Federation 有自己的分布式存储、API Server 和 Controller Manager。用户可以通过 Federation 的 API Server 注册该 Federation 的成员 K8s Cluster。当用户通过 Federation 的 API Server 创建、更改 API 对象时，Federation API Server 会在自己所有注册的子 K8s Cluster 都创建一份对应的 API 对象。在提供业务请求服务时，K8s Federation 会先在自己的各个子 Cluster 之间做负载均衡，而对于发送到某个具体 K8s Cluster 的业务请求，会依照这个 K8s Cluster 独立提供服务时一样的调度模式去做 K8s Cluster 内部的负载均衡。而 Cluster 之间的负载均衡是通过域名服务的负载均衡来实现的。</p>
<p><img src="images/federation-service.png" alt=""/></p>
<p>所有的设计都尽量不影响 K8s Cluster 现有的工作机制，这样对于每个子 K8s 集群来说，并不需要更外层的有一个 K8s Federation，也就是意味着所有现有的 K8s 代码和机制不需要因为 Federation 功能有任何变化。</p>
<p><img src="images/federation-api-4x.png" alt=""/></p>
<p>Federation 主要包括三个组件</p>
<ul>
<li>federation-apiserver：类似 kube-apiserver，但提供的是跨集群的 REST API</li>
<li>federation-controller-manager：类似 kube-controller-manager，但提供多集群状态的同步机制</li>
<li>kubefed：Federation 管理命令行工具</li>
</ul>
<p>Federation 的代码维护在 <a href="https://github.com/kubernetes/federation" target="_blank">https://github.com/kubernetes/federation</a>。</p>
<h2 id="federation-部署方法">Federation 部署方法</h2>
<h3 id="下载-kubefed-和-kubectl">下载 kubefed 和 kubectl</h3>
<p>kubefed 下载</p>
<pre><code class="lang-sh"><span class="hljs-comment"># Linux</span>
curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl <span class="hljs-_">-s</span> https://storage.googleapis.com/kubernetes-release/release/stable.txt)/kubernetes-client-linux-amd64.tar.gz
tar -xzvf kubernetes-client-linux-amd64.tar.gz

<span class="hljs-comment"># OS X</span>
curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl <span class="hljs-_">-s</span> https://storage.googleapis.com/kubernetes-release/release/stable.txt)/kubernetes-client-darwin-amd64.tar.gz
tar -xzvf kubernetes-client-darwin-amd64.tar.gz

<span class="hljs-comment"># Windows</span>
curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl <span class="hljs-_">-s</span> https://storage.googleapis.com/kubernetes-release/release/stable.txt)/kubernetes-client-windows-amd64.tar.gz
tar -xzvf kubernetes-client-windows-amd64.tar.gz
</code></pre>
<p>kubectl 的下载可以参考 <a href="kubectl.html#%20附录">这里</a>。</p>
<h3 id="初始化主集群">初始化主集群</h3>
<p>选择一个已部署好的 Kubernetes 集群作为主集群，作为集群联邦的控制平面，并配置好本地的 kubeconfig。然后运行 <code>kubefed init</code> 命令来初始化主集群：</p>
<pre><code class="lang-sh">$ kubefed init fellowship \
    --host-cluster-context=rivendell \   <span class="hljs-comment"># 部署集群的 kubeconfig 配置名称</span>
    --dns-provider=<span class="hljs-string">"google-clouddns"</span> \   <span class="hljs-comment"># DNS 服务提供商，还支持 aws-route53 或 coredns</span>
    --dns-zone-name=<span class="hljs-string">"example.com."</span> \     <span class="hljs-comment"># 域名后缀，必须以. 结束</span>
    --apiserver-enable-basic-auth=<span class="hljs-literal">true</span> \ <span class="hljs-comment"># 开启 basic 认证</span>
    --apiserver-enable-token-auth=<span class="hljs-literal">true</span> \ <span class="hljs-comment"># 开启 token 认证</span>
    --apiserver-arg-overrides=<span class="hljs-string">"--anonymous-auth=false,--v=4"</span> <span class="hljs-comment"># federation API server 自定义参数</span>
$ kubectl config use-context fellowship
</code></pre>
<h3 id="自定义-dns">自定义 DNS</h3>
<p>coredns 需要先部署一套 etcd 集群，可以用 helm 来部署：</p>
<pre><code class="lang-sh">$ helm install --namespace my-namespace --name etcd-operator stable/etcd-operator
$ helm upgrade --namespace my-namespace --set cluster.enabled=<span class="hljs-literal">true</span> etcd-operator stable/etcd-operator
</code></pre>
<p>然后部署 coredns</p>
<pre><code class="lang-sh">$ cat Values.yaml
isClusterService: <span class="hljs-literal">false</span>
serviceType: <span class="hljs-string">"LoadBalancer"</span>
middleware:
  kubernetes:
    enabled: <span class="hljs-literal">false</span>
  etcd:
    enabled: <span class="hljs-literal">true</span>
    zones:
    - <span class="hljs-string">"example.com."</span>
    endpoint: <span class="hljs-string">"http://etcd-cluster.my-namespace:2379"</span>

$ helm install --namespace my-namespace --name coredns <span class="hljs-_">-f</span> Values.yaml stable/coredns
</code></pre>
<p>使用 coredns 时，还需要传入 coredns 的配置</p>
<pre><code class="lang-sh">$ cat <span class="hljs-variable">$HOME</span>/coredns-provider.conf
[Global]
etcd-endpoints = http://etcd-cluster.my-namespace:2379
zones = example.com.

$ kubefed init fellowship \
    --host-cluster-context=rivendell \   <span class="hljs-comment"># 部署集群的 kubeconfig 配置名称</span>
    --dns-provider=<span class="hljs-string">"coredns"</span> \           <span class="hljs-comment"># DNS 服务提供商，还支持 aws-route53 或 google-clouddns</span>
    --dns-zone-name=<span class="hljs-string">"example.com."</span> \     <span class="hljs-comment"># 域名后缀，必须以. 结束</span>
    --apiserver-enable-basic-auth=<span class="hljs-literal">true</span> \ <span class="hljs-comment"># 开启 basic 认证</span>
    --apiserver-enable-token-auth=<span class="hljs-literal">true</span> \ <span class="hljs-comment"># 开启 token 认证</span>
    --dns-provider-config=<span class="hljs-string">"<span class="hljs-variable">$HOME</span>/coredns-provider.conf"</span> \ <span class="hljs-comment"># coredns 配置</span>
    --apiserver-arg-overrides=<span class="hljs-string">"--anonymous-auth=false,--v=4"</span> <span class="hljs-comment"># federation API server 自定义参数</span>
</code></pre>
<h3 id="物理机部署">物理机部署</h3>
<p>默认情况下，<code>kubefed init</code> 会创建一个 LoadBalancer 类型的 federation API server 服务，这需要 Cloud Provider 的支持。在物理机部署时，可以通过 <code>--api-server-service-type</code> 选项将其改成 NodePort：</p>
<pre><code class="lang-sh">$ kubefed init fellowship \
    --host-cluster-context=rivendell \   <span class="hljs-comment"># 部署集群的 kubeconfig 配置名称</span>
    --dns-provider=<span class="hljs-string">"coredns"</span> \           <span class="hljs-comment"># DNS 服务提供商，还支持 aws-route53 或 google-clouddns</span>
    --dns-zone-name=<span class="hljs-string">"example.com."</span> \     <span class="hljs-comment"># 域名后缀，必须以. 结束</span>
    --apiserver-enable-basic-auth=<span class="hljs-literal">true</span> \ <span class="hljs-comment"># 开启 basic 认证</span>
    --apiserver-enable-token-auth=<span class="hljs-literal">true</span> \ <span class="hljs-comment"># 开启 token 认证</span>
    --dns-provider-config=<span class="hljs-string">"<span class="hljs-variable">$HOME</span>/coredns-provider.conf"</span> \ <span class="hljs-comment"># coredns 配置</span>
    --apiserver-arg-overrides=<span class="hljs-string">"--anonymous-auth=false,--v=4"</span> \ <span class="hljs-comment"># federation API server 自定义参数</span>
    --api-server-service-type=<span class="hljs-string">"NodePort"</span> \
    --api-server-advertise-address=<span class="hljs-string">"10.0.10.20"</span>
</code></pre>
<h3 id="自定义-etcd-存储">自定义 etcd 存储</h3>
<p>默认情况下，<code>kubefed init</code> 通过动态创建 PV 的方式为 etcd 创建持久化存储。如果 kubernetes 集群不支持动态创建 PV，则可以预先创建 PV，注意 PV 要匹配 <code>kubefed</code> 的 PVC:</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> PersistentVolumeClaim
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  annotations:</span>
    volume.alpha.kubernetes.io/storage-class: <span class="hljs-string">"yes"</span>
<span class="hljs-attr">  labels:</span>
<span class="hljs-attr">    app:</span> federated-cluster
<span class="hljs-attr">  name:</span> fellowship-federation-apiserver-etcd-claim
<span class="hljs-attr">  namespace:</span> federation-system
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  accessModes:</span>
<span class="hljs-bullet">  -</span> ReadWriteOnce
<span class="hljs-attr">  resources:</span>
<span class="hljs-attr">    requests:</span>
<span class="hljs-attr">      storage:</span> <span class="hljs-number">10</span>Gi
</code></pre>
<h2 id="注册集群">注册集群</h2>
<p>除主集群外，其他 kubernetes 集群可以通过 <code>kubefed join</code> 命令加入集群联邦：</p>
<pre><code class="lang-sh">$ kubefed join gondor --host-cluster-context=rivendell --cluster-context=gondor_needs-no_king
</code></pre>
<h2 id="集群查询">集群查询</h2>
<p>查询注册到 Federation 的 kubernetes 集群列表</p>
<pre><code class="lang-sh">$ kubectl --context=federation get clusters
</code></pre>
<h2 id="clusterselector">ClusterSelector</h2>
<p>v1.7 + 支持使用 annotation <code>federation.alpha.kubernetes.io/cluster-selector</code> 为新对象选择 kubernetes 集群。该 annotation 的值是一个 json 数组，比如</p>
<pre><code class="lang-yaml"><span class="hljs-attr">  metadata:</span>
<span class="hljs-attr">    annotations:</span>
      federation.alpha.kubernetes.io/cluster-selector: <span class="hljs-string">'[{"key":"pci","operator":
        "In", "values": ["true"]}, {"key": "environment", "operator": "NotIn", "values":
        ["test"]}]'</span>
</code></pre>
<p>每条记录包含三个键值</p>
<ul>
<li>key：集群的 label 名字</li>
<li>operator：包括 In, NotIn, Exists, DoesNotExist, Gt, Lt</li>
<li>values：集群的 label 值</li>
</ul>
<h2 id="策略调度">策略调度</h2>
<blockquote>
<p>注：仅 v1.7 + 支持策略调度。</p>
</blockquote>
<p>开启策略调度的方法</p>
<p>（1）创建 ConfigMap</p>
<pre><code class="lang-sh">kubectl create <span class="hljs-_">-f</span> https://raw.githubusercontent.com/kubernetes/kubernetes.github.io/master/docs/tutorials/federation/scheduling-policy-admission.yaml
</code></pre>
<p>（2） 编辑 federation-apiserver</p>
<pre><code>kubectl -n federation-system edit deployment federation-apiserver
</code></pre><p>增加选项：</p>
<pre><code>--admission-control=SchedulingPolicy
--admission-control-config-file=/etc/kubernetes/admission/config.yml
</code></pre><p>增加 volume：</p>
<pre><code>- name: admission-config
  configMap:
    name: admission
</code></pre><p>增加 volumeMounts:</p>
<pre><code>volumeMounts:
- name: admission-config
  mountPath: /etc/kubernetes/admission
</code></pre><p>（3）部署外部策略引擎，如 <a href="http://www.openpolicyagent.org" target="_blank">Open Policy Agent (OPA)</a></p>
<pre><code>kubectl create -f https://raw.githubusercontent.com/kubernetes/kubernetes.github.io/master/docs/tutorials/federation/policy-engine-service.yaml
kubectl create -f https://raw.githubusercontent.com/kubernetes/kubernetes.github.io/master/docs/tutorials/federation/policy-engine-deployment.yaml
</code></pre><p>（4）创建 namespace <code>kube-federation-scheduling-policy</code> 以供外部策略引擎使用</p>
<pre><code>kubectl --context=federation create namespace kube-federation-scheduling-policy
</code></pre><p>（5）创建策略</p>
<pre><code>wget https://raw.githubusercontent.com/kubernetes/kubernetes.github.io/master/docs/tutorials/federation/policy.rego
kubectl --context=federation -n kube-federation-scheduling-policy create configmap scheduling-policy --from-file=policy.rego
</code></pre><p>（6）验证策略</p>
<pre><code>kubectl --context=federation annotate clusters cluster-name-1 pci-certified=true
kubectl --context=federation create -f https://raw.githubusercontent.com/kubernetes/kubernetes.github.io/master/docs/tutorials/federation/replicaset-example-policy.yaml
kubectl --context=federation get rs nginx-pci -o jsonpath='{.metadata.annotations}'
</code></pre><h2 id="集群联邦使用">集群联邦使用</h2>
<p>集群联邦支持以下联邦资源，这些资源会自动在所有注册的 kubernetes 集群中创建：</p>
<ul>
<li>Federated ConfigMap</li>
<li>Federated Service</li>
<li>Federated DaemonSet</li>
<li>Federated Deployment</li>
<li>Federated Ingress</li>
<li>Federated Namespaces</li>
<li>Federated ReplicaSets</li>
<li>Federated Secrets</li>
<li>Federated Events（仅存在 federation 控制平面）</li>
<li>Federated Jobs（v1.8+）</li>
<li>Federated Horizontal Pod Autoscaling (HPA，v1.8+)</li>
</ul>
<p>比如使用 Federated Service 的方法如下：</p>
<pre><code class="lang-sh"><span class="hljs-comment"># 这会在所有注册到联邦的 kubernetes 集群中创建服务</span>
$ kubectl --context=federation-cluster create <span class="hljs-_">-f</span> services/nginx.yaml

<span class="hljs-comment"># 添加后端 Pod</span>
$ <span class="hljs-keyword">for</span> CLUSTER <span class="hljs-keyword">in</span> asia-east1-c asia-east1<span class="hljs-_">-a</span> asia-east1-b \
                        europe-west1<span class="hljs-_">-d</span> europe-west1-c europe-west1-b \
                        us-central1<span class="hljs-_">-f</span> us-central1<span class="hljs-_">-a</span> us-central1-b us-central1-c \
                        us-east1<span class="hljs-_">-d</span> us-east1-c us-east1-b
<span class="hljs-keyword">do</span>
  kubectl --context=<span class="hljs-variable">$CLUSTER</span> run nginx --image=nginx:1.11.1-alpine --port=80
<span class="hljs-keyword">done</span>

<span class="hljs-comment"># 查看服务状态</span>
$ kubectl --context=federation-cluster describe services nginx
</code></pre>
<p>可以通过 DNS 来访问联邦服务，访问格式包括以下几种</p>
<ul>
<li><code>nginx.mynamespace.myfederation.</code></li>
<li><code>nginx.mynamespace.myfederation.svc.example.com.</code></li>
<li><code>nginx.mynamespace.myfederation.svc.us-central1.example.com.</code></li>
</ul>
<h2 id="删除集群">删除集群</h2>
<pre><code class="lang-sh">$ kubefed unjoin gondor --host-cluster-context=rivendell
</code></pre>
<h2 id="删除集群联邦">删除集群联邦</h2>
<p>集群联邦控制平面的删除功能还在开发中，目前可以通过删除 namespace <code>federation-system</code> 的方法来清理（注意 pv 不会删除）：</p>
<pre><code class="lang-sh">$ kubectl delete ns federation-system
</code></pre>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://kubernetes.io/docs/concepts/cluster-administration/federation/" target="_blank">Kubernetes federation</a></li>
<li><a href="https://kubernetes.io/docs/tasks/federation/set-up-cluster-federation-kubefed/" target="_blank">kubefed</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="kubeadm-工作原理" class="level3">kubeadm</h1>
<p>kubeadm 是 Kubernetes 主推的部署工具之一，正在快速迭代开发中。</p>
<h2 id="初始化系统">初始化系统</h2>
<p>所有机器都需要初始化容器执行引擎（如 docker 或 frakti 等）和 kubelet。这是因为 kubeadm 依赖 kubelet 来启动 Master 组件，比如 kube-apiserver、kube-manager-controller、kube-scheduler、kube-proxy 等。</p>
<h2 id="安装-master">安装 master</h2>
<p>在初始化 master 时，只需要执行 kubeadm init 命令即可，比如</p>
<pre><code class="lang-sh">kubeadm init --pod-network-cidr 10.244.0.0/16 --kubernetes-version stable
</code></pre>
<p>这个命令会自动</p>
<ul>
<li>系统状态检查</li>
<li>生成 token</li>
<li>生成自签名 CA 和 client 端证书</li>
<li>生成 kubeconfig 用于 kubelet 连接 API server</li>
<li>为 Master 组件生成 Static Pod manifests，并放到 <code>/etc/kubernetes/manifests</code> 目录中</li>
<li>配置 RBAC 并设置 Master node 只运行控制平面组件</li>
<li>创建附加服务，比如 kube-proxy 和 kube-dns</li>
</ul>
<h2 id="配置-network-plugin">配置 Network plugin</h2>
<p>kubeadm 在初始化时并不关心网络插件，默认情况下，kubelet 配置使用 CNI 插件，这样就需要用户来额外初始化网络插件。</p>
<h3 id="cni-bridge">CNI bridge</h3>
<pre><code class="lang-sh">mkdir -p /etc/cni/net.d
cat >/etc/cni/net.d/10-mynet.conf <<-EOF
{
    <span class="hljs-string">"cniVersion"</span>: <span class="hljs-string">"0.3.0"</span>,
    <span class="hljs-string">"name"</span>: <span class="hljs-string">"mynet"</span>,
    <span class="hljs-string">"type"</span>: <span class="hljs-string">"bridge"</span>,
    <span class="hljs-string">"bridge"</span>: <span class="hljs-string">"cni0"</span>,
    <span class="hljs-string">"isGateway"</span>: <span class="hljs-literal">true</span>,
    <span class="hljs-string">"ipMasq"</span>: <span class="hljs-literal">true</span>,
    <span class="hljs-string">"ipam"</span>: {
        <span class="hljs-string">"type"</span>: <span class="hljs-string">"host-local"</span>,
        <span class="hljs-string">"subnet"</span>: <span class="hljs-string">"10.244.1.0/24"</span>,
        <span class="hljs-string">"routes"</span>: [
            {<span class="hljs-string">"dst"</span>: <span class="hljs-string">"0.0.0.0/0"</span>}
        ]
    }
}
EOF
cat >/etc/cni/net.d/99-loopback.conf <<-EOF
{
    <span class="hljs-string">"cniVersion"</span>: <span class="hljs-string">"0.3.0"</span>,
    <span class="hljs-string">"type"</span>: <span class="hljs-string">"loopback"</span>
}
EOF
</code></pre>
<h3 id="flannel">flannel</h3>
<pre><code class="lang-sh">kubectl create <span class="hljs-_">-f</span> https://github.com/coreos/flannel/raw/master/Documentation/kube-flannel-rbac.yml
kubectl create <span class="hljs-_">-f</span> https://github.com/coreos/flannel/raw/master/Documentation/kube-flannel.yml
</code></pre>
<h3 id="weave">weave</h3>
<pre><code class="lang-sh">kubectl apply <span class="hljs-_">-f</span> <span class="hljs-string">"https://cloud.weave.works/k8s/net?k8s-version=<span class="hljs-variable">$(kubectl version | base64 | tr -d'\n')</span>"</span>
</code></pre>
<h3 id="calico">calico</h3>
<pre><code class="lang-sh">kubectl apply <span class="hljs-_">-f</span> https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/rbac-kdd.yaml
kubectl apply <span class="hljs-_">-f</span> https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calico-networking/1.7/calico.yaml
</code></pre>
<h2 id="添加-node">添加 Node</h2>
<pre><code class="lang-sh">token=$(kubeadm token list | grep authentication,signing | awk <span class="hljs-string">'{print $1}'</span>)
kubeadm join --token <span class="hljs-variable">$token</span> <span class="hljs-variable">${master_ip}</span>
</code></pre>
<p>这包括以下几个步骤</p>
<ul>
<li>从 API server 下载 CA</li>
<li>创建本地证书，并请求 API Server 签名</li>
<li>最后配置 kubelet 连接到 API Server</li>
</ul>
<h2 id="删除安装">删除安装</h2>
<pre><code class="lang-sh">kubeadm reset
</code></pre>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://kubernetes.io/docs/admin/kubeadm/" target="_blank">kubeadm Setup Tool</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="hyperkube" class="level3">hyperkube</h1>
<p>hyperkube是Kubernetes的allinone binary，可以用来启动多种kubernetes服务，常用在Docker镜像中。每个Kubernetes发布都会同时发布一个包含hyperkube的docker镜像，如<code>gcr.io/google_containers/hyperkube:v1.6.4</code>。</p>
<p>hyperkube支持的子命令包括</p>
<ul>
<li>kubelet</li>
<li>apiserver</li>
<li>controller-manager</li>
<li>federation-apiserver</li>
<li>federation-controller-manager</li>
<li>kubectl</li>
<li>proxy</li>
<li>scheduler</li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="kubectl" class="level3">kubectl</h1>
<p>kubectl 是 Kubernetes 的命令行工具（CLI），是 Kubernetes 用户和管理员必备的管理工具。</p>
<p>kubectl 提供了大量的子命令，方便管理 Kubernetes 集群中的各种功能。这里不再罗列各种子命令的格式，而是介绍下如何查询命令的帮助</p>
<ul>
<li><code>kubectl -h</code> 查看子命令列表</li>
<li><code>kubectl options</code> 查看全局选项</li>
<li><code>kubectl <command> --help</code> 查看子命令的帮助</li>
<li><code>kubectl [command] [PARAMS] -o=<format></code> 设置输出格式（如 json、yaml、jsonpath 等）</li>
<li><code>kubectl explain [RESOURCE]</code> 查看资源的定义</li>
</ul>
<h2 id="配置">配置</h2>
<p>使用 kubectl 的第一步是配置 Kubernetes 集群以及认证方式，包括</p>
<ul>
<li>cluster 信息：Kubernetes server 地址</li>
<li>用户信息：用户名、密码或密钥</li>
<li>Context：cluster、用户信息以及 Namespace 的组合</li>
</ul>
<p>示例</p>
<pre><code class="lang-sh">kubectl config <span class="hljs-built_in">set</span>-credentials myself --username=admin --password=secret
kubectl config <span class="hljs-built_in">set</span>-cluster <span class="hljs-built_in">local</span>-server --server=http://localhost:8080
kubectl config <span class="hljs-built_in">set</span>-context default-context --cluster=<span class="hljs-built_in">local</span>-server --user=myself --namespace=default
kubectl config use-context default-context
kubectl config view
</code></pre>
<h2 id="常用命令格式">常用命令格式</h2>
<ul>
<li>创建：<code>kubectl run <name> --image=<image></code> 或者 <code>kubectl create -f manifest.yaml</code></li>
<li>查询：<code>kubectl get <resource></code></li>
<li>更新 <code>kubectl set</code> 或者 <code>kubectl patch</code></li>
<li>删除：<code>kubectl delete <resource> <name></code> 或者 <code>kubectl delete -f manifest.yaml</code></li>
<li>查询 Pod IP：<code>kubectl get pod <pod-name> -o jsonpath='{.status.podIP}'</code></li>
<li>容器内执行命令：<code>kubectl exec -ti <pod-name> sh</code></li>
<li>容器日志：<code>kubectl logs [-f] <pod-name></code></li>
<li>导出服务：<code>kubectl expose deploy <name> --port=80</code></li>
<li>Base64 解码：</li>
</ul>
<pre><code class="lang-sh">kubectl get secret SECRET -o go-template=<span class="hljs-string">'{{ .data.KEY | base64decode }}'</span>
</code></pre>
<p>注意，<code>kubectl run</code> 仅支持 Pod、Replication Controller、Deployment、Job 和 CronJob 等几种资源。具体的资源类型是由参数决定的，默认为 Deployment：</p>
<table>
<thead>
<tr>
<th>创建的资源类型</th>
<th>参数</th>
</tr>
</thead>
<tbody>
<tr>
<td>Pod</td>
<td><code>--restart=Never</code></td>
</tr>
<tr>
<td>Replication Controller</td>
<td><code>--generator=run/v1</code></td>
</tr>
<tr>
<td>Deployment</td>
<td><code>--restart=Always</code></td>
</tr>
<tr>
<td>Job</td>
<td><code>--restart=OnFailure</code></td>
</tr>
<tr>
<td>CronJob</td>
<td><code>--schedule=<cron></code></td>
</tr>
</tbody>
</table>
<h2 id="命令行自动补全">命令行自动补全</h2>
<p>Linux 系统 Bash：</p>
<pre><code class="lang-sh"><span class="hljs-built_in">source</span> /usr/share/bash-completion/bash_completion
<span class="hljs-built_in">source</span> <(kubectl completion bash)
</code></pre>
<p>MacOS zsh</p>
<pre><code class="lang-sh"><span class="hljs-built_in">source</span> <(kubectl completion zsh)
</code></pre>
<h2 id="日志查看">日志查看</h2>
<p><code>kubectl logs</code> 用于显示 pod 运行中，容器内程序输出到标准输出的内容。跟 docker 的 logs 命令类似。</p>
<pre><code class="lang-sh">  <span class="hljs-comment"># Return snapshot logs from pod nginx with only one container</span>
  kubectl logs nginx

  <span class="hljs-comment"># Return snapshot of previous terminated ruby container logs from pod web-1</span>
  kubectl logs -p -c ruby web-1

  <span class="hljs-comment"># Begin streaming the logs of the ruby container in pod web-1</span>
  kubectl logs <span class="hljs-_">-f</span> -c ruby web-1
</code></pre>
<h2 id="连接到一个正在运行的容器">连接到一个正在运行的容器</h2>
<p><code>kubectl attach</code> 用于连接到一个正在运行的容器。跟 docker 的 attach 命令类似。</p>
<pre><code class="lang-sh">  <span class="hljs-comment"># Get output from running pod 123456-7890, using the first container by default</span>
  kubectl attach 123456-7890

  <span class="hljs-comment"># Get output from ruby-container from pod 123456-7890</span>
  kubectl attach 123456-7890 -c ruby-container

  <span class="hljs-comment"># Switch to raw terminal mode, sends stdin to 'bash' in ruby-container from pod 123456-7890</span>
  <span class="hljs-comment"># and sends stdout/stderr from 'bash' back to the client</span>
  kubectl attach 123456-7890 -c ruby-container -i -t

Options:
  -c, --container=<span class="hljs-string">''</span>: Container name. If omitted, the first container <span class="hljs-keyword">in</span> the pod will be chosen
  -i, --stdin=<span class="hljs-literal">false</span>: Pass stdin to the container
  -t, --tty=<span class="hljs-literal">false</span>: Stdin is a TTY
</code></pre>
<h2 id="在容器内部执行命令">在容器内部执行命令</h2>
<p><code>kubectl exec</code> 用于在一个正在运行的容器执行命令。跟 docker 的 exec 命令类似。</p>
<pre><code class="lang-sh">  <span class="hljs-comment"># Get output from running 'date' from pod 123456-7890, using the first container by default</span>
  kubectl <span class="hljs-built_in">exec</span> 123456-7890 date

  <span class="hljs-comment"># Get output from running 'date' in ruby-container from pod 123456-7890</span>
  kubectl <span class="hljs-built_in">exec</span> 123456-7890 -c ruby-container date

  <span class="hljs-comment"># Switch to raw terminal mode, sends stdin to 'bash' in ruby-container from pod 123456-7890</span>
  <span class="hljs-comment"># and sends stdout/stderr from 'bash' back to the client</span>
  kubectl <span class="hljs-built_in">exec</span> 123456-7890 -c ruby-container -i -t -- bash -il

Options:
  -c, --container=<span class="hljs-string">''</span>: Container name. If omitted, the first container <span class="hljs-keyword">in</span> the pod will be chosen
  -p, --pod=<span class="hljs-string">''</span>: Pod name
  -i, --stdin=<span class="hljs-literal">false</span>: Pass stdin to the container
  -t, --tty=<span class="hljs-literal">false</span>: Stdin is a TT
</code></pre>
<h2 id="端口转发">端口转发</h2>
<p><code>kubectl port-forward</code> 用于将本地端口转发到指定的 Pod。</p>
<pre><code class="lang-sh"><span class="hljs-comment"># Listen on ports 5000 and 6000 locally, forwarding data to/from ports 5000 and 6000 in the pod</span>
kubectl port-forward mypod 5000 6000

<span class="hljs-comment"># Listen on port 8888 locally, forwarding to 5000 in the pod</span>
kubectl port-forward mypod 8888:5000

<span class="hljs-comment"># Listen on a random port locally, forwarding to 5000 in the pod</span>
kubectl port-forward mypod :5000

<span class="hljs-comment"># Listen on a random port locally, forwarding to 5000 in the pod</span>
kubectl port-forward mypod 0:5000
</code></pre>
<p>也可以将本地端口转发到服务、复制控制器或者部署的端口。</p>
<pre><code class="lang-sh"><span class="hljs-comment"># Forward to deployment</span>
kubectl port-forward deployment/redis-master 6379:6379

<span class="hljs-comment"># Forward to replicaSet</span>
kubectl port-forward rs/redis-master 6379:6379

<span class="hljs-comment"># Forward to service</span>
kubectl port-forward svc/redis-master 6379:6379
</code></pre>
<h2 id="api-server-代理">API Server 代理</h2>
<p><code>kubectl proxy</code> 命令提供了一个 Kubernetes API 服务的 HTTP 代理。</p>
<pre><code class="lang-sh">$ kubectl proxy --port=8080
Starting to serve on 127.0.0.1:8080
</code></pre>
<p>可以通过代理地址 <code>http://localhost:8080/api/</code> 来直接访问 Kubernetes API，比如查询 Pod 列表</p>
<pre><code class="lang-sh">curl http://localhost:8080/api/v1/namespaces/default/pods
</code></pre>
<p>注意，如果通过 <code>--address</code> 指定了非 localhost 的地址，则访问 8080 端口时会报未授权的错误，可以设置 <code>--accept-hosts</code> 来避免这个问题（<strong> 不推荐生产环境这么设置 </strong>）：</p>
<pre><code class="lang-sh">kubectl proxy --address=<span class="hljs-string">'0.0.0.0'</span> --port=8080 --accept-hosts=<span class="hljs-string">'^*$'</span>
</code></pre>
<h2 id="文件拷贝">文件拷贝</h2>
<p><code>kubectl cp</code> 支持从容器中拷贝，或者拷贝文件到容器中</p>
<pre><code class="lang-sh">  <span class="hljs-comment"># Copy /tmp/foo_dir local directory to /tmp/bar_dir in a remote pod in the default namespace</span>
  kubectl cp /tmp/foo_dir <some-pod>:/tmp/bar_dir

  <span class="hljs-comment"># Copy /tmp/foo local file to /tmp/bar in a remote pod in a specific container</span>
  kubectl cp /tmp/foo <some-pod>:/tmp/bar -c <specific-container>

  <span class="hljs-comment"># Copy /tmp/foo local file to /tmp/bar in a remote pod in namespace <some-namespace></span>
  kubectl cp /tmp/foo <some-namespace>/<some-pod>:/tmp/bar

  <span class="hljs-comment"># Copy /tmp/foo from a remote pod to /tmp/bar locally</span>
  kubectl cp <some-namespace>/<some-pod>:/tmp/foo /tmp/bar

Options:
  -c, --container=<span class="hljs-string">''</span>: Container name. If omitted, the first container <span class="hljs-keyword">in</span> the pod will be chosen
</code></pre>
<p>注意：文件拷贝依赖于 tar 命令，所以容器中需要能够执行 tar 命令</p>
<h2 id="kubectl-drain">kubectl drain</h2>
<pre><code class="lang-sh">kubectl drain NODE [Options]
</code></pre>
<ul>
<li>它会删除该 NODE 上由 ReplicationController, ReplicaSet, DaemonSet, StatefulSet or Job 创建的 Pod</li>
<li>不删除 mirror pods（因为不可通过 API 删除 mirror pods）</li>
<li>如果还有其它类型的 Pod（比如不通过 RC 而直接通过 kubectl create 的 Pod）并且没有 --force 选项，该命令会直接失败</li>
<li>如果命令中增加了 --force 选项，则会强制删除这些不是通过 ReplicationController, Job 或者 DaemonSet 创建的 Pod</li>
</ul>
<p>有的时候不需要 evict pod，只需要标记 Node 不可调用，可以用 <code>kubectl cordon</code> 命令。</p>
<p>恢复的话只需要运行 <code>kubectl uncordon NODE</code> 将 NODE 重新改成可调度状态。</p>
<h2 id="权限检查">权限检查</h2>
<p><code>kubectl auth</code> 提供了两个子命令用于检查用户的鉴权情况：</p>
<ul>
<li><code>kubectl auth can-i</code> 检查用户是否有权限进行某个操作，比如</li>
</ul>
<pre><code class="lang-sh">  <span class="hljs-comment"># Check to see if I can create pods in any namespace</span>
  kubectl auth can-i create pods --all-namespaces

  <span class="hljs-comment"># Check to see if I can list deployments in my current namespace</span>
  kubectl auth can-i list deployments.extensions

  <span class="hljs-comment"># Check to see if I can do everything in my current namespace ("*" means all)</span>
  kubectl auth can-i <span class="hljs-string">'*'</span> <span class="hljs-string">'*'</span>

  <span class="hljs-comment"># Check to see if I can get the job named "bar" in namespace "foo"</span>
  kubectl auth can-i list jobs.batch/bar -n foo
</code></pre>
<ul>
<li><code>kubectl auth reconcile</code> 自动修复有问题的 RBAC 策略，如</li>
</ul>
<pre><code class="lang-sh">  <span class="hljs-comment"># Reconcile rbac resources from a file</span>
  kubectl auth reconcile <span class="hljs-_">-f</span> my-rbac-rules.yaml
</code></pre>
<h2 id="kubectl-插件">kubectl 插件</h2>
<p>kubectl 插件提供了一种扩展 kubectl 的机制，比如添加新的子命令。插件可以以任何语言编写，只需要满足以下条件即可</p>
<ul>
<li>插件放在 <code>~/.kube/plugins</code> 或环境变量 <code>KUBECTL_PLUGINS_PATH</code> 指定的目录中</li>
<li>插件的格式为 <code>子目录 / 可执行文件或脚本</code> 且子目录中要包括 <code>plugin.yaml</code> 配置文件</li>
</ul>
<p>比如</p>
<pre><code class="lang-sh">$ tree
.
└── hello
    └── plugin.yaml

1 directory, 1 file

$ cat hello/plugin.yaml
name: <span class="hljs-string">"hello"</span>
shortDesc: <span class="hljs-string">"Hello kubectl plugin!"</span>
<span class="hljs-built_in">command</span>: <span class="hljs-string">"echo Hello plugins!"</span>

$ kubectl plugin hello
Hello plugins!
</code></pre>
<p>你也可以使用 <a href="../deploy/kubectl.html">krew</a> 来管理 kubectl 插件。</p>
<h2 id="原始-uri">原始 URI</h2>
<p>kubectl 也可以用来直接访问原始 URI，比如要访问 <a href="https://github.com/kubernetes-incubator/metrics-server" target="_blank">Metrics API</a> 可以</p>
<ul>
<li><code>kubectl get --raw /apis/metrics.k8s.io/v1beta1/nodes</code></li>
<li><code>kubectl get --raw /apis/metrics.k8s.io/v1beta1/pods</code></li>
<li><code>kubectl get --raw /apis/metrics.k8s.io/v1beta1/nodes/<node-name></code></li>
<li><code>kubectl get --raw /apis/metrics.k8s.io/v1beta1/namespace/<namespace-name>/pods/<pod-name></code></li>
</ul>
<h2 id="附录">附录</h2>
<p>kubectl 的安装方法</p>
<pre><code class="lang-sh"><span class="hljs-comment"># OS X</span>
curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl <span class="hljs-_">-s</span> https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/darwin/amd64/kubectl

<span class="hljs-comment"># Linux</span>
curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl <span class="hljs-_">-s</span> https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl

<span class="hljs-comment"># Windows</span>
curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl <span class="hljs-_">-s</span> https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/windows/amd64/kubectl.exe
</code></pre>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="资源对象" class="level2">资源对象</h1>
<p>Kubernetes 主要概念和对象介绍。</p>
<ul>
<li><a href="autoscaling.html">Autoscaling (HPA)</a></li>
<li><a href="configmap.html">ConfigMap</a></li>
<li><a href="cronjob.html">CronJob</a></li>
<li><a href="customresourcedefinition.html">CustomResourceDefinition</a></li>
<li><a href="daemonset.html">DaemonSet</a></li>
<li><a href="deployment.html">Deployment</a></li>
<li><a href="ingress.html">Ingress</a></li>
<li><a href="job.html">Job</a></li>
<li><a href="local-volume.html">LocalVolume</a></li>
<li><a href="namespace.html">Namespace</a></li>
<li><a href="network-policy.html">NetworkPolicy</a></li>
<li><a href="node.html">Node</a></li>
<li><a href="persistent-volume.html">PersistentVolume</a></li>
<li><a href="pod.html">Pod</a></li>
<li><a href="podpreset.html">PodPreset</a></li>
<li><a href="replicaset.html">ReplicaSet</a></li>
<li><a href="quota.html">Resource Quota</a></li>
<li><a href="secret.html">Secret</a></li>
<li><a href="security-context.html">SecurityContext</a></li>
<li><a href="service.html">Service</a></li>
<li><a href="serviceaccount.html">ServiceAccount</a></li>
<li><a href="statefulset.html">StatefulSet</a></li>
<li><a href="thirdpartyresources.html">ThirdPartyResources</a></li>
<li><a href="volume.html">Volume</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="horizontal-pod-autoscaling-hpa" class="level3">Autoscaling</h1>
<p>Horizontal Pod Autoscaling (HPA) 可以根据 CPU 使用率或应用自定义 metrics 自动扩展 Pod 数量（支持 replication controller、deployment 和 replica set ）。</p>
<ul>
<li>控制管理器每隔 30s（可以通过 <code>--horizontal-pod-autoscaler-sync-period</code> 修改）查询 metrics 的资源使用情况</li>
<li>支持三种 metrics 类型<ul>
<li>预定义 metrics（比如 Pod 的 CPU）以利用率的方式计算</li>
<li>自定义的 Pod metrics，以原始值（raw value）的方式计算</li>
<li>自定义的 object metrics</li>
</ul>
</li>
<li>支持两种 metrics 查询方式：Heapster 和自定义的 REST API</li>
<li>支持多 metrics</li>
</ul>
<p>注意：</p>
<ul>
<li>本章是关于 Pod 的自动扩展，而 Node 的自动扩展请参考 <a href="../addons/cluster-autoscaler.html">Cluster AutoScaler</a>。</li>
<li>在使用 HPA 之前需要 <strong>确保已部署好 <a href="../addons/metrics.html">metrics-server</a></strong>。</li>
</ul>
<h2 id="api-版本对照表">API 版本对照表</h2>
<table>
<thead>
<tr>
<th>Kubernetes 版本</th>
<th>autoscaling API 版本</th>
<th>支持的 metrics</th>
</tr>
</thead>
<tbody>
<tr>
<td>v1.5+</td>
<td>autoscaling/v1</td>
<td>CPU</td>
</tr>
<tr>
<td>v1.6+</td>
<td>autoscaling/v2beta1</td>
<td>Memory及自定义</td>
</tr>
</tbody>
</table>
<h2 id="示例">示例</h2>
<pre><code class="lang-sh"><span class="hljs-comment"># 创建 pod 和 service</span>
$ kubectl run php-apache --image=k8s.gcr.io/hpa-example --requests=cpu=200m --expose --port=80
service <span class="hljs-string">"php-apache"</span> created
deployment <span class="hljs-string">"php-apache"</span> created

<span class="hljs-comment"># 创建 autoscaler</span>
$ kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10
deployment <span class="hljs-string">"php-apache"</span> autoscaled

$ kubectl get hpa
NAME         REFERENCE                     TARGET    MINPODS   MAXPODS   REPLICAS   AGE
php-apache   Deployment/php-apache/scale   0% / 50%  1         10        1          18s

<span class="hljs-comment"># 增加负载</span>
$ kubectl run -i --tty load-generator --image=busybox /bin/sh
Hit enter <span class="hljs-keyword">for</span> <span class="hljs-built_in">command</span> prompt
$ <span class="hljs-keyword">while</span> <span class="hljs-literal">true</span>; <span class="hljs-keyword">do</span> wget -q -O- http://php-apache.default.svc.cluster.local; <span class="hljs-keyword">done</span>

<span class="hljs-comment"># 过一会就可以看到负载升高了</span>
$ kubectl get hpa
NAME         REFERENCE                     TARGET      CURRENT   MINPODS   MAXPODS   REPLICAS   AGE
php-apache   Deployment/php-apache/scale   305% / 50%  305%      1         10        1          3m

<span class="hljs-comment"># autoscaler 将这个 deployment 扩展为 7 个 pod</span>
$ kubectl get deployment php-apache
NAME         DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
php-apache   7         7         7            7           19m

<span class="hljs-comment"># 删除刚才创建的负载增加 pod 后会发现负载降低，并且 pod 数量也自动降回 1 个</span>
$ kubectl get hpa
NAME         REFERENCE                     TARGET       MINPODS   MAXPODS   REPLICAS   AGE
php-apache   Deployment/php-apache/scale   0% / 50%     1         10        1          11m

$ kubectl get deployment php-apache
NAME         DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
php-apache   1         1         1            1           27m
</code></pre>
<h2 id="自定义-metrics">自定义 metrics</h2>
<p>使用方法</p>
<ul>
<li>控制管理器开启 <code>--horizontal-pod-autoscaler-use-rest-clients</code></li>
<li>控制管理器配置的 <code>--master</code> 或者 <code>--kubeconfig</code></li>
<li>在 API Server Aggregator 中注册自定义的 metrics API，如 <a href="https://github.com/kubernetes-incubator/custom-metrics-apiserver" target="_blank">https://github.com/kubernetes-incubator/custom-metrics-apiserver</a> 和 <a href="https://github.com/kubernetes/metrics" target="_blank">https://github.com/kubernetes/metrics</a></li>
</ul>
<blockquote>
<p>注：可以参考 <a href="https://github.com/kubernetes/metrics" target="_blank">k8s.io/metics</a> 开发自定义的 metrics API server。</p>
</blockquote>
<p>比如 HorizontalPodAutoscaler 保证每个 Pod 占用 50% CPU、1000pps 以及 10000 请求 / s：</p>
<p>HPA 示例</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> autoscaling/v1
<span class="hljs-attr">kind:</span> HorizontalPodAutoscaler
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> php-apache
<span class="hljs-attr">  namespace:</span> default
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  scaleTargetRef:</span>
<span class="hljs-attr">    apiVersion:</span> apps/v1beta1
<span class="hljs-attr">    kind:</span> Deployment
<span class="hljs-attr">    name:</span> php-apache
<span class="hljs-attr">  minReplicas:</span> <span class="hljs-number">1</span>
<span class="hljs-attr">  maxReplicas:</span> <span class="hljs-number">10</span>
<span class="hljs-attr">  metrics:</span>
<span class="hljs-attr">  - type:</span> Resource
<span class="hljs-attr">    resource:</span>
<span class="hljs-attr">      name:</span> cpu
<span class="hljs-attr">      targetAverageUtilization:</span> <span class="hljs-number">50</span>
<span class="hljs-attr">  - type:</span> Pods
<span class="hljs-attr">    pods:</span>
<span class="hljs-attr">      metricName:</span> packets-per-second
<span class="hljs-attr">      targetAverageValue:</span> <span class="hljs-number">1</span>k
<span class="hljs-attr">  - type:</span> Object
<span class="hljs-attr">    object:</span>
<span class="hljs-attr">      metricName:</span> requests-per-second
<span class="hljs-attr">      target:</span>
<span class="hljs-attr">        apiVersion:</span> extensions/v1beta1
<span class="hljs-attr">        kind:</span> Ingress
<span class="hljs-attr">        name:</span> main-route
<span class="hljs-attr">      targetValue:</span> <span class="hljs-number">10</span>k
<span class="hljs-attr">status:</span>
<span class="hljs-attr">  observedGeneration:</span> <span class="hljs-number">1</span>
<span class="hljs-attr">  lastScaleTime:</span> <some-time<span class="hljs-string">>
</span><span class="hljs-attr">  currentReplicas:</span> <span class="hljs-number">1</span>
<span class="hljs-attr">  desiredReplicas:</span> <span class="hljs-number">1</span>
<span class="hljs-attr">  currentMetrics:</span>
<span class="hljs-attr">  - type:</span> Resource
<span class="hljs-attr">    resource:</span>
<span class="hljs-attr">      name:</span> cpu
<span class="hljs-attr">      currentAverageUtilization:</span> <span class="hljs-number">0</span>
<span class="hljs-attr">      currentAverageValue:</span> <span class="hljs-number">0</span>
</code></pre>
<h2 id="状态条件">状态条件</h2>
<p>v1.7+ 可以在客户端中看到 Kubernetes 为 HorizontalPodAutoscaler 设置的状态条件 <code>status.conditions</code>，用来判断 HorizontalPodAutoscaler 是否可以扩展（AbleToScale）、是否开启扩展（ScalingActive）以及是否受到限制（ScalingLimitted）。</p>
<pre><code class="lang-sh">$ kubectl describe hpa cm-test
Name:                           cm-test
Namespace:                      prom
Labels:                         <none>
Annotations:                    <none>
CreationTimestamp:              Fri, 16 Jun 2017 18:09:22 +0000
Reference:                      ReplicationController/cm-test
Metrics:                        (current / target)
  <span class="hljs-string">"http_requests"</span> on pods:      66m / 500m
M<span class="hljs-keyword">in</span> replicas:                   1
Max replicas:                   4
ReplicationController pods:     1 current / 1 desired
Conditions:
  Type                  Status  Reason                  Message
  ----                  ------  ------                  -------
  AbleToScale           True    ReadyForNewScale        the last scale time was sufficiently old as to warrant a new scale
  ScalingActive         True    ValidMetricFound        the HPA was able to successfully calculate a replica count from pods metric http_requests
  ScalingLimited        False   DesiredWithinRange      the desired replica count is within the acceptible range
Events:
</code></pre>
<h2 id="hpa-最佳实践">HPA 最佳实践</h2>
<ul>
<li>为容器配置 CPU Requests</li>
<li>HPA 目标设置恰当，如设置 70% 给容器和应用预留 30% 的余量</li>
<li>保持 Pods 和 Nodes 健康（避免 Pod 频繁重建）</li>
<li>保证用户请求的负载均衡</li>
<li>使用 <code>kubectl top node</code> 和 <code>kubectl top pod</code> 查看资源使用情况</li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="configmap" class="level3">ConfigMap</h1>
<p>在执行应用程式或是生产环境等等, 会有许多的情况需要做变更, 而我们不希望因应每一种需求就要准备一个镜像档, 这时就可以透过 ConfigMap 来帮我们做一个配置档或是命令参数的映射, 更加弹性化使用我们的服务或是应用程式。</p>
<p>ConfigMap 用于保存配置数据的键值对，可以用来保存单个属性，也可以用来保存配置文件。ConfigMap 跟 secret 很类似，但它可以更方便地处理不包含敏感信息的字符串。</p>
<h2 id="api-版本对照表">API 版本对照表</h2>
<table>
<thead>
<tr>
<th>Kubernetes 版本</th>
<th>Core API 版本</th>
</tr>
</thead>
<tbody>
<tr>
<td>v1.5+</td>
<td>core/v1</td>
</tr>
</tbody>
</table>
<h2 id="configmap-创建">ConfigMap 创建</h2>
<p>可以使用 <code>kubectl create configmap</code> 从文件、目录或者 key-value 字符串创建等创建 ConfigMap。也可以通过 <code>kubectl create -f file</code> 创建。</p>
<h3 id="从-key-value-字符串创建">从 key-value 字符串创建</h3>
<pre><code class="lang-sh">$ kubectl create configmap special-config --from-literal=special.how=very
configmap <span class="hljs-string">"special-config"</span> created
$ kubectl get configmap special-config -o go-template=<span class="hljs-string">'{{.data}}'</span>
map[special.how:very]
</code></pre>
<h3 id="从-env-文件创建">从 env 文件创建</h3>
<pre><code class="lang-sh">$ <span class="hljs-built_in">echo</span> <span class="hljs-_">-e</span> <span class="hljs-string">"a=b\nc=d"</span> | tee config.env
a=b
c=d
$ kubectl create configmap special-config --from-env-file=config.env
configmap <span class="hljs-string">"special-config"</span> created
$ kubectl get configmap special-config -o go-template=<span class="hljs-string">'{{.data}}'</span>
map[a:b c:d]
</code></pre>
<h3 id="从目录创建">从目录创建</h3>
<pre><code class="lang-sh">$ mkdir config
$ <span class="hljs-built_in">echo</span> a>config/a
$ <span class="hljs-built_in">echo</span> b>config/b
$ kubectl create configmap special-config --from-file=config/
configmap <span class="hljs-string">"special-config"</span> created
$ kubectl get configmap special-config -o go-template=<span class="hljs-string">'{{.data}}'</span>
map[a:a
 b:b
]
</code></pre>
<h3 id="从文件-yamljson-文件创建">从文件 Yaml/Json 文件创建</h3>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> ConfigMap
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> special-config
<span class="hljs-attr">  namespace:</span> default
<span class="hljs-attr">data:</span>
  special.how: very
  special.type: charm
</code></pre>
<pre><code class="lang-sh">$ kubectl create  <span class="hljs-_">-f</span>  config.yaml
configmap <span class="hljs-string">"special-config"</span> created
</code></pre>
<h2 id="configmap-使用">ConfigMap 使用</h2>
<p>ConfigMap 可以通过三种方式在 Pod 中使用，三种分别方式为：设置环境变量、设置容器命令行参数以及在 Volume 中直接挂载文件或目录。</p>
<blockquote>
<p><strong>注意</strong></p>
<ul>
<li>ConfigMap 必须在 Pod 引用它之前创建</li>
<li>使用 <code>envFrom</code> 时，将会自动忽略无效的键</li>
<li>Pod 只能使用同一个命名空间内的 ConfigMap</li>
</ul>
</blockquote>
<p>首先创建 ConfigMap：</p>
<pre><code class="lang-sh">$ kubectl create configmap special-config --from-literal=special.how=very --from-literal=special.type=charm
$ kubectl create configmap env-config --from-literal=<span class="hljs-built_in">log</span>_level=INFO
</code></pre>
<h3 id="用作环境变量">用作环境变量</h3>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> test-pod
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">    - name:</span> test-container
<span class="hljs-attr">      image:</span> gcr.io/google_containers/busybox
<span class="hljs-attr">      command:</span> [<span class="hljs-string">"/bin/sh"</span>, <span class="hljs-string">"-c"</span>, <span class="hljs-string">"env"</span>]
<span class="hljs-attr">      env:</span>
<span class="hljs-attr">        - name:</span> SPECIAL_LEVEL_KEY
<span class="hljs-attr">          valueFrom:</span>
<span class="hljs-attr">            configMapKeyRef:</span>
<span class="hljs-attr">              name:</span> special-config
<span class="hljs-attr">              key:</span> special.how
<span class="hljs-attr">        - name:</span> SPECIAL_TYPE_KEY
<span class="hljs-attr">          valueFrom:</span>
<span class="hljs-attr">            configMapKeyRef:</span>
<span class="hljs-attr">              name:</span> special-config
<span class="hljs-attr">              key:</span> special.type
<span class="hljs-attr">      envFrom:</span>
<span class="hljs-attr">        - configMapRef:</span>
<span class="hljs-attr">            name:</span> env-config
<span class="hljs-attr">  restartPolicy:</span> Never
</code></pre>
<p>当 Pod 结束后会输出</p>
<pre><code>SPECIAL_LEVEL_KEY=very
SPECIAL_TYPE_KEY=charm
log_level=INFO
</code></pre><h3 id="用作命令行参数">用作命令行参数</h3>
<p>将 ConfigMap 用作命令行参数时，需要先把 ConfigMap 的数据保存在环境变量中，然后通过 <code>$(VAR_NAME)</code> 的方式引用环境变量.</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> dapi-test-pod
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">    - name:</span> test-container
<span class="hljs-attr">      image:</span> gcr.io/google_containers/busybox
<span class="hljs-attr">      command:</span> [<span class="hljs-string">"/bin/sh"</span>, <span class="hljs-string">"-c"</span>, <span class="hljs-string">"echo $(SPECIAL_LEVEL_KEY) $(SPECIAL_TYPE_KEY)"</span> ]
<span class="hljs-attr">      env:</span>
<span class="hljs-attr">        - name:</span> SPECIAL_LEVEL_KEY
<span class="hljs-attr">          valueFrom:</span>
<span class="hljs-attr">            configMapKeyRef:</span>
<span class="hljs-attr">              name:</span> special-config
<span class="hljs-attr">              key:</span> special.how
<span class="hljs-attr">        - name:</span> SPECIAL_TYPE_KEY
<span class="hljs-attr">          valueFrom:</span>
<span class="hljs-attr">            configMapKeyRef:</span>
<span class="hljs-attr">              name:</span> special-config
<span class="hljs-attr">              key:</span> special.type
<span class="hljs-attr">  restartPolicy:</span> Never
</code></pre>
<p>当 Pod 结束后会输出</p>
<pre><code>very charm
</code></pre><h3 id="使用-volume-将-configmap-作为文件或目录直接挂载">使用 volume 将 ConfigMap 作为文件或目录直接挂载</h3>
<p>将创建的 ConfigMap 直接挂载至 Pod 的 / etc/config 目录下，其中每一个 key-value 键值对都会生成一个文件，key 为文件名，value 为内容</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> vol-test-pod
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">    - name:</span> test-container
<span class="hljs-attr">      image:</span> gcr.io/google_containers/busybox
<span class="hljs-attr">      command:</span> [<span class="hljs-string">"/bin/sh"</span>, <span class="hljs-string">"-c"</span>, <span class="hljs-string">"cat /etc/config/special.how"</span>]
<span class="hljs-attr">      volumeMounts:</span>
<span class="hljs-attr">      - name:</span> config-volume
<span class="hljs-attr">        mountPath:</span> /etc/config
<span class="hljs-attr">  volumes:</span>
<span class="hljs-attr">    - name:</span> config-volume
<span class="hljs-attr">      configMap:</span>
<span class="hljs-attr">        name:</span> special-config
<span class="hljs-attr">  restartPolicy:</span> Never
</code></pre>
<p>当 Pod 结束后会输出</p>
<pre><code>very
</code></pre><p>将创建的 ConfigMap 中 special.how 这个 key 挂载到 / etc/config 目录下的一个相对路径 / keys/special.level。如果存在同名文件，直接覆盖。其他的 key 不挂载</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> dapi-test-pod
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">    - name:</span> test-container
<span class="hljs-attr">      image:</span> gcr.io/google_containers/busybox
<span class="hljs-attr">      command:</span> [<span class="hljs-string">"/bin/sh"</span>,<span class="hljs-string">"-c"</span>,<span class="hljs-string">"cat /etc/config/keys/special.level"</span>]
<span class="hljs-attr">      volumeMounts:</span>
<span class="hljs-attr">      - name:</span> config-volume
<span class="hljs-attr">        mountPath:</span> /etc/config
<span class="hljs-attr">  volumes:</span>
<span class="hljs-attr">    - name:</span> config-volume
<span class="hljs-attr">      configMap:</span>
<span class="hljs-attr">        name:</span> special-config
<span class="hljs-attr">        items:</span>
<span class="hljs-attr">        - key:</span> special.how
<span class="hljs-attr">          path:</span> keys/special.level
<span class="hljs-attr">  restartPolicy:</span> Never
</code></pre>
<p>当 Pod 结束后会输出</p>
<pre><code>very
</code></pre><p>ConfigMap 支持同一个目录下挂载多个 key 和多个目录。例如下面将 special.how 和 special.type 通过挂载到 / etc/config 下。并且还将 special.how 同时挂载到 / etc/config2 下。</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> dapi-test-pod
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">    - name:</span> test-container
<span class="hljs-attr">      image:</span> gcr.io/google_containers/busybox
<span class="hljs-attr">      command:</span> [<span class="hljs-string">"/bin/sh"</span>,<span class="hljs-string">"-c"</span>,<span class="hljs-string">"sleep 36000"</span>]
<span class="hljs-attr">      volumeMounts:</span>
<span class="hljs-attr">      - name:</span> config-volume
<span class="hljs-attr">        mountPath:</span> /etc/config
<span class="hljs-attr">      - name:</span> config-volume2
<span class="hljs-attr">        mountPath:</span> /etc/config2
<span class="hljs-attr">  volumes:</span>
<span class="hljs-attr">    - name:</span> config-volume
<span class="hljs-attr">      configMap:</span>
<span class="hljs-attr">        name:</span> special-config
<span class="hljs-attr">        items:</span>
<span class="hljs-attr">        - key:</span> special.how
<span class="hljs-attr">          path:</span> keys/special.level
<span class="hljs-attr">        - key:</span> special.type
<span class="hljs-attr">          path:</span> keys/special.type
<span class="hljs-attr">    - name:</span> config-volume2
<span class="hljs-attr">      configMap:</span>
<span class="hljs-attr">        name:</span> special-config
<span class="hljs-attr">        items:</span>
<span class="hljs-attr">        - key:</span> special.how
<span class="hljs-attr">          path:</span> keys/special.level
<span class="hljs-attr">  restartPolicy:</span> Never
</code></pre>
<pre><code class="lang-sh"><span class="hljs-comment"># ls  /etc/config/keys/</span>
special.level  special.type
<span class="hljs-comment"># ls  /etc/config2/keys/</span>
special.level
<span class="hljs-comment"># cat  /etc/config/keys/special.level</span>
very
<span class="hljs-comment"># cat  /etc/config/keys/special.type</span>
charm
</code></pre>
<h3 id="使用-subpath-将-configmap-作为单独的文件挂载到目录">使用 subpath 将 ConfigMap 作为单独的文件挂载到目录</h3>
<p>在一般情况下 configmap 挂载文件时，会先覆盖掉挂载目录，然后再将 congfigmap 中的内容作为文件挂载进行。如果想不对原来的文件夹下的文件造成覆盖，只是将 configmap 中的每个 key，按照文件的方式挂载到目录下，可以使用 subpath 参数。</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> dapi-test-pod
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">    - name:</span> test-container
<span class="hljs-attr">      image:</span> nginx
<span class="hljs-attr">      command:</span> [<span class="hljs-string">"/bin/sh"</span>,<span class="hljs-string">"-c"</span>,<span class="hljs-string">"sleep 36000"</span>]
<span class="hljs-attr">      volumeMounts:</span>
<span class="hljs-attr">      - name:</span> config-volume
<span class="hljs-attr">        mountPath:</span> /etc/nginx/special.how
<span class="hljs-attr">        subPath:</span> special.how
<span class="hljs-attr">  volumes:</span>
<span class="hljs-attr">    - name:</span> config-volume
<span class="hljs-attr">      configMap:</span>
<span class="hljs-attr">        name:</span> special-config
<span class="hljs-attr">        items:</span>
<span class="hljs-attr">        - key:</span> special.how
<span class="hljs-attr">          path:</span> special.how
<span class="hljs-attr">  restartPolicy:</span> Never
</code></pre>
<pre><code class="lang-sh">root@dapi-test-pod:/<span class="hljs-comment"># ls /etc/nginx/</span>
conf.d    fastcgi_params    koi-utf  koi-win  mime.types  modules  nginx.conf  scgi_params    special.how  uwsgi_params  win-utf
root@dapi-test-pod:/<span class="hljs-comment"># cat /etc/nginx/special.how</span>
very
root@dapi-test-pod:/<span class="hljs-comment">#</span>
</code></pre>
<p>参考文档：</p>
<ul>
<li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/" target="_blank">ConfigMap</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="cronjob" class="level3">CronJob</h1>
<p>CronJob 即定时任务，就类似于 Linux 系统的 crontab，在指定的时间周期运行指定的任务。</p>
<h2 id="api-版本对照表">API 版本对照表</h2>
<table>
<thead>
<tr>
<th>Kubernetes 版本</th>
<th>Batch API 版本</th>
<th>默认开启</th>
</tr>
</thead>
<tbody>
<tr>
<td>v1.5-v1.7</td>
<td>batch/v2alpha1</td>
<td>否</td>
</tr>
<tr>
<td>v1.8-v1.9</td>
<td>batch/v1beta1</td>
<td>是</td>
</tr>
</tbody>
</table>
<p>注意：使用默认未开启的 API 时需要在 kube-apiserver 中配置 <code>--runtime-config=batch/v2alpha1</code>。</p>
<h2 id="cronjob-spec">CronJob Spec</h2>
<ul>
<li><code>.spec.schedule</code> 指定任务运行周期，格式同 <a href="https://en.wikipedia.org/wiki/Cron" target="_blank">Cron</a></li>
<li><code>.spec.jobTemplate</code> 指定需要运行的任务，格式同 <a href="job.html">Job</a></li>
<li><code>.spec.startingDeadlineSeconds</code> 指定任务开始的截止期限</li>
<li><code>.spec.concurrencyPolicy</code> 指定任务的并发策略，支持 Allow、Forbid 和 Replace 三个选项</li>
</ul>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> batch/v1beta1
<span class="hljs-attr">kind:</span> CronJob
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> hello
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  schedule:</span> <span class="hljs-string">"*/1 * * * *"</span>
<span class="hljs-attr">  jobTemplate:</span>
<span class="hljs-attr">    spec:</span>
<span class="hljs-attr">      template:</span>
<span class="hljs-attr">        spec:</span>
<span class="hljs-attr">          containers:</span>
<span class="hljs-attr">          - name:</span> hello
<span class="hljs-attr">            image:</span> busybox
<span class="hljs-attr">            args:</span>
<span class="hljs-bullet">            -</span> /bin/sh
<span class="hljs-bullet">            -</span> -c
<span class="hljs-bullet">            -</span> date; echo Hello from the Kubernetes cluster
<span class="hljs-attr">          restartPolicy:</span> OnFailure
</code></pre>
<pre><code>$ kubectl create -f cronjob.yaml
cronjob "hello" created
</code></pre><p>当然，也可以用 <code>kubectl run</code> 来创建一个 CronJob：</p>
<pre><code>kubectl run hello --schedule="*/1 * * * *" --restart=OnFailure --image=busybox -- /bin/sh -c "date; echo Hello from the Kubernetes cluster"
</code></pre><pre><code>$ kubectl get cronjob
NAME      SCHEDULE      SUSPEND   ACTIVE    LAST-SCHEDULE
hello     */1 * * * *   False     0         <none>
$ kubectl get jobs
NAME               DESIRED   SUCCESSFUL   AGE
hello-1202039034   1         1            49s
$ pods=$(kubectl get pods --selector=job-name=hello-1202039034 --output=jsonpath={.items..metadata.name} -a)
$ kubectl logs $pods
Mon Aug 29 21:34:09 UTC 2016
Hello from the Kubernetes cluster

# 注意，删除 cronjob 的时候不会自动删除 job，这些 job 可以用 kubectl delete job 来删除
$ kubectl delete cronjob hello
cronjob "hello" deleted
</code></pre><h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/" target="_blank">Cron Jobs</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="customresourcedefinition" class="level3">CustomResourceDefinition</h1>
<p>CustomResourceDefinition（CRD）是 v1.7 新增的无需改变代码就可以扩展 Kubernetes API 的机制，用来管理自定义对象。它实际上是 <a href="thirdpartyresources.html">ThirdPartyResources（TPR）</a> 的升级版本，而 TPR 已经在 v1.8 中删除。</p>
<h2 id="api-版本对照表">API 版本对照表</h2>
<table>
<thead>
<tr>
<th>Kubernetes 版本</th>
<th>CRD API 版本</th>
</tr>
</thead>
<tbody>
<tr>
<td>v1.8+</td>
<td>apiextensions.k8s.io/v1beta1</td>
</tr>
</tbody>
</table>
<h2 id="crd-示例">CRD 示例</h2>
<p>下面的例子会创建一个 <code>/apis/stable.example.com/v1/namespaces/<namespace>/crontabs/…</code> 的自定义 API：</p>
<pre><code class="lang-sh">apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  <span class="hljs-comment"># name must match the spec fields below, and be in the form: <plural>.<group></span>
  name: crontabs.stable.example.com
spec:
  <span class="hljs-comment"># group name to use for REST API: /apis/<group>/<version></span>
  group: stable.example.com
  <span class="hljs-comment"># versions to use for REST API: /apis/<group>/<version></span>
  versions:
  - name: v1beta1
    <span class="hljs-comment"># Each version can be enabled/disabled by Served flag.</span>
    served: <span class="hljs-literal">true</span>
    <span class="hljs-comment"># One and only one version must be marked as the storage version.</span>
    storage: <span class="hljs-literal">true</span>
  - name: v1
    served: <span class="hljs-literal">true</span>
    storage: <span class="hljs-literal">false</span>
  <span class="hljs-comment"># either Namespaced or Cluster</span>
  scope: Namespaced
  names:
    <span class="hljs-comment"># plural name to be used in the URL: /apis/<group>/<version>/<plural></span>
    plural: crontabs
    <span class="hljs-comment"># singular name to be used as an alias on the CLI and for display</span>
    singular: crontab
    <span class="hljs-comment"># kind is normally the CamelCased singular type. Your resource manifests use this.</span>
    kind: CronTab
    <span class="hljs-comment"># shortNames allow shorter string to match your resource on the CLI</span>
    shortNames:
    - ct
</code></pre>
<p>API 创建好后，就可以创建具体的 CronTab 对象了</p>
<pre><code class="lang-sh">$ cat my-cronjob.yaml
apiVersion: <span class="hljs-string">"stable.example.com/v1"</span>
kind: CronTab
metadata:
  name: my-new-cron-object
spec:
  cronSpec: <span class="hljs-string">"* * * * /5"</span>
  image: my-awesome-cron-image

$ kubectl create <span class="hljs-_">-f</span> my-crontab.yaml
crontab <span class="hljs-string">"my-new-cron-object"</span> created

$ kubectl get crontab
NAME                 KIND
my-new-cron-object   CronTab.v1.stable.example.com
$ kubectl get crontab my-new-cron-object -o yaml
apiVersion: stable.example.com/v1
kind: CronTab
metadata:
  creationTimestamp: 2017-07-03T19:00:56Z
  name: my-new-cron-object
  namespace: default
  resourceVersion: <span class="hljs-string">"20630"</span>
  selfLink: /apis/stable.example.com/v1/namespaces/default/crontabs/my-new-cron-object
  uid: 5c82083e-5fbd-11e7<span class="hljs-_">-a</span>204-42010a8c0002
spec:
  cronSpec: <span class="hljs-string">'* * * * /5'</span>
  image: my-awesome-cron-image
</code></pre>
<h2 id="finalizer">Finalizer</h2>
<p>Finalizer 用于实现控制器的异步预删除钩子，可以通过 <code>metadata.finalizers</code> 来指定 Finalizer。</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">"stable.example.com/v1"</span>
<span class="hljs-attr">kind:</span> CronTab
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  finalizers:</span>
<span class="hljs-bullet">  -</span> finalizer.stable.example.com
</code></pre>
<p>Finalizer 指定后，客户端删除对象的操作只会设置 <code>metadata.deletionTimestamp</code> 而不是直接删除。这会触发正在监听 CRD 的控制器，控制器执行一些删除前的清理操作，从列表中删除自己的 finalizer，然后再重新发起一个删除操作。此时，被删除的对象才会真正删除。</p>
<h2 id="validation">Validation</h2>
<p>v1.8 开始新增了实验性的基于 <a href="https://github.com/OAI/OpenAPI-Specification/blob/master/versions/3.0.0.md#schemaObject" target="_blank">OpenAPI v3 schema</a> 的验证（Validation）机制，可以用来提前验证用户提交的资源是否符合规范。使用该功能需要配置 kube-apiserver 的 <code>--feature-gates=CustomResourceValidation=true</code>。</p>
<p>比如下面的 CRD 要求</p>
<ul>
<li><code>spec.cronSpec</code> 必须是匹配正则表达式的字符串</li>
<li><code>spec.replicas</code> 必须是从 1 到 10 的整数</li>
</ul>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> apiextensions.k8s.io/v1beta1
<span class="hljs-attr">kind:</span> CustomResourceDefinition
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> crontabs.stable.example.com
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  group:</span> stable.example.com
<span class="hljs-attr">  version:</span> v1
<span class="hljs-attr">  scope:</span> Namespaced
<span class="hljs-attr">  names:</span>
<span class="hljs-attr">    plural:</span> crontabs
<span class="hljs-attr">    singular:</span> crontab
<span class="hljs-attr">    kind:</span> CronTab
<span class="hljs-attr">    shortNames:</span>
<span class="hljs-bullet">    -</span> ct
<span class="hljs-attr">  validation:</span>
   <span class="hljs-comment"># openAPIV3Schema is the schema for validating custom objects.</span>
<span class="hljs-attr">    openAPIV3Schema:</span>
<span class="hljs-attr">      properties:</span>
<span class="hljs-attr">        spec:</span>
<span class="hljs-attr">          properties:</span>
<span class="hljs-attr">            cronSpec:</span>
<span class="hljs-attr">              type:</span> string
<span class="hljs-attr">              pattern:</span> <span class="hljs-string">'^(\d+|\*)(/\d+)?(\s+(\d+|\*)(/\d+)?){4}$'</span>
<span class="hljs-attr">            replicas:</span>
<span class="hljs-attr">              type:</span> integer
<span class="hljs-attr">              minimum:</span> <span class="hljs-number">1</span>
<span class="hljs-attr">              maximum:</span> <span class="hljs-number">10</span>
</code></pre>
<p>这样，在创建下面的 CronTab 时</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">"stable.example.com/v1"</span>
<span class="hljs-attr">kind:</span> CronTab
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> my-new-cron-object
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  cronSpec:</span> <span class="hljs-string">"* * * *"</span>
<span class="hljs-attr">  image:</span> my-awesome-cron-image
<span class="hljs-attr">  replicas:</span> <span class="hljs-number">15</span>
</code></pre>
<p>会报验证失败的错误：</p>
<pre><code class="lang-sh">The CronTab <span class="hljs-string">"my-new-cron-object"</span> is invalid: []: Invalid value: map[string]interface {}{<span class="hljs-string">"apiVersion"</span>:<span class="hljs-string">"stable.example.com/v1"</span>, <span class="hljs-string">"kind"</span>:<span class="hljs-string">"CronTab"</span>, <span class="hljs-string">"metadata"</span>:map[string]interface {}{<span class="hljs-string">"name"</span>:<span class="hljs-string">"my-new-cron-object"</span>, <span class="hljs-string">"namespace"</span>:<span class="hljs-string">"default"</span>, <span class="hljs-string">"deletionTimestamp"</span>:interface {}(nil), <span class="hljs-string">"deletionGracePeriodSeconds"</span>:(*int64)(nil), <span class="hljs-string">"creationTimestamp"</span>:<span class="hljs-string">"2017-09-05T05:20:07Z"</span>, <span class="hljs-string">"uid"</span>:<span class="hljs-string">"e14d79e7-91f9-11e7-a598-f0761cb232d1"</span>, <span class="hljs-string">"selfLink"</span>:<span class="hljs-string">""</span>,<span class="hljs-string">"clusterName"</span>:<span class="hljs-string">""</span>}, <span class="hljs-string">"spec"</span>:map[string]interface {}{<span class="hljs-string">"cronSpec"</span>:<span class="hljs-string">"* * * *"</span>, <span class="hljs-string">"image"</span>:<span class="hljs-string">"my-awesome-cron-image"</span>, <span class="hljs-string">"replicas"</span>:15}}:
validation failure list:
spec.cronSpec <span class="hljs-keyword">in</span> body should match <span class="hljs-string">'^(\d+|\*)(/\d+)?(\s+(\d+|\*)(/\d+)?){4}$'</span>
spec.replicas <span class="hljs-keyword">in</span> body should be less than or equal to 10
</code></pre>
<h2 id="subresources">Subresources</h2>
<p>v1.10 开始 CRD 还支持 <code>/status</code> 和 <code>/scale</code> 等两个子资源（Beta），并且从 v1.11 开始默认开启。</p>
<blockquote>
<p>v1.10 版本使用前需要在 <code>kube-apiserver</code> 开启 <code>--feature-gates=CustomResourceSubresources=true</code>。</p>
</blockquote>
<pre><code class="lang-yaml"><span class="hljs-comment"># resourcedefinition.yaml</span>
<span class="hljs-attr">apiVersion:</span> apiextensions.k8s.io/v1beta1
<span class="hljs-attr">kind:</span> CustomResourceDefinition
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> crontabs.stable.example.com
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  group:</span> stable.example.com
<span class="hljs-attr">  version:</span> v1
<span class="hljs-attr">  scope:</span> Namespaced
<span class="hljs-attr">  names:</span>
<span class="hljs-attr">    plural:</span> crontabs
<span class="hljs-attr">    singular:</span> crontab
<span class="hljs-attr">    kind:</span> CronTab
<span class="hljs-attr">    shortNames:</span>
<span class="hljs-bullet">    -</span> ct
  <span class="hljs-comment"># subresources describes the subresources for custom resources.</span>
<span class="hljs-attr">  subresources:</span>
    <span class="hljs-comment"># status enables the status subresource.</span>
<span class="hljs-attr">    status:</span> {}
    <span class="hljs-comment"># scale enables the scale subresource.</span>
<span class="hljs-attr">    scale:</span>
      <span class="hljs-comment"># specReplicasPath defines the JSONPath inside of a custom resource that corresponds to Scale.Spec.Replicas.</span>
<span class="hljs-attr">      specReplicasPath:</span> .spec.replicas
      <span class="hljs-comment"># statusReplicasPath defines the JSONPath inside of a custom resource that corresponds to Scale.Status.Replicas.</span>
<span class="hljs-attr">      statusReplicasPath:</span> .status.replicas
      <span class="hljs-comment"># labelSelectorPath defines the JSONPath inside of a custom resource that corresponds to Scale.Status.Selector.</span>
<span class="hljs-attr">      labelSelectorPath:</span> .status.labelSelector
</code></pre>
<pre><code class="lang-sh">$ kubectl create <span class="hljs-_">-f</span> resourcedefinition.yaml
$ kubectl create <span class="hljs-_">-f</span>- <<EOF
apiVersion: <span class="hljs-string">"stable.example.com/v1"</span>
kind: CronTab
metadata:
  name: my-new-cron-object
spec:
  cronSpec: <span class="hljs-string">"* * * * */5"</span>
  image: my-awesome-cron-image
  replicas: 3
EOF

$ kubectl scale --replicas=5 crontabs/my-new-cron-object
crontabs <span class="hljs-string">"my-new-cron-object"</span> scaled

$ kubectl get crontabs my-new-cron-object -o jsonpath=<span class="hljs-string">'{.spec.replicas}'</span>
5
</code></pre>
<h2 id="categories">Categories</h2>
<p>Categories 用来将 CRD 对象分组，这样就可以使用 <code>kubectl get <category-name></code> 来查询属于该组的所有对象。</p>
<pre><code class="lang-yaml"><span class="hljs-comment"># resourcedefinition.yaml</span>
<span class="hljs-attr">apiVersion:</span> apiextensions.k8s.io/v1beta1
<span class="hljs-attr">kind:</span> CustomResourceDefinition
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> crontabs.stable.example.com
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  group:</span> stable.example.com
<span class="hljs-attr">  version:</span> v1
<span class="hljs-attr">  scope:</span> Namespaced
<span class="hljs-attr">  names:</span>
<span class="hljs-attr">    plural:</span> crontabs
<span class="hljs-attr">    singular:</span> crontab
<span class="hljs-attr">    kind:</span> CronTab
<span class="hljs-attr">    shortNames:</span>
<span class="hljs-bullet">    -</span> ct
    <span class="hljs-comment"># categories is a list of grouped resources the custom resource belongs to.</span>
<span class="hljs-attr">    categories:</span>
<span class="hljs-bullet">    -</span> all
</code></pre>
<pre><code class="lang-yaml"><span class="hljs-comment"># my-crontab.yaml</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">"stable.example.com/v1"</span>
<span class="hljs-attr">kind:</span> CronTab
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> my-new-cron-object
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  cronSpec:</span> <span class="hljs-string">"* * * * */5"</span>
<span class="hljs-attr">  image:</span> my-awesome-cron-image
</code></pre>
<pre><code class="lang-sh">$ kubectl create <span class="hljs-_">-f</span> resourcedefinition.yaml
$ kubectl create <span class="hljs-_">-f</span> my-crontab.yaml
$ kubectl get all
NAME                          AGE
crontabs/my-new-cron-object   3s
</code></pre>
<h2 id="crd-控制器">CRD 控制器</h2>
<p>在使用 CRD 扩展 Kubernetes API 时，通常还需要实现一个新建资源的控制器，监听新资源的变化情况，并作进一步的处理。</p>
<p><a href="https://github.com/kubernetes/sample-controller" target="_blank">https://github.com/kubernetes/sample-controller</a> 提供了一个 CRD 控制器的示例，包括</p>
<ul>
<li>如何注册资源 <code>Foo</code></li>
<li>如何创建、删除和查询 <code>Foo</code> 对象</li>
<li>如何监听 <code>Foo</code> 资源对象的变化情况</li>
</ul>
<h2 id="kubebuilder">Kubebuilder</h2>
<p>从上面的实例中可以看到从头构建一个 CRD 控制器并不容易，需要对 Kubernetes 的 API 有深入了解，并且RBAC 集成、镜像构建、持续集成和部署等都需要很大工作量。</p>
<p><a href="https://github.com/kubernetes-sigs/kubebuilder" target="_blank">kubebuilder</a> 正是为解决这个问题而生，为 CRD 控制器提供了一个简单易用的框架，并可直接生成镜像构建、持续集成、持续部署等所需的资源文件。</p>
<h3 id="安装">安装</h3>
<pre><code class="lang-sh"><span class="hljs-comment"># Install kubebuilder</span>
VERSION=1.0.1
wget https://github.com/kubernetes-sigs/kubebuilder/releases/download/v<span class="hljs-variable">${VERSION}</span>/kubebuilder_<span class="hljs-variable">${VERSION}</span>_linux_amd64.tar.gz
tar zxvf kubebuilder_<span class="hljs-variable">${VERSION}</span>_linux_amd64.tar.gz
sudo mv kubebuilder_<span class="hljs-variable">${VERSION}</span>_linux_amd64 /usr/<span class="hljs-built_in">local</span>/kubebuilder
<span class="hljs-built_in">export</span> PATH=<span class="hljs-variable">$PATH</span>:/usr/<span class="hljs-built_in">local</span>/kubebuilder/bin

<span class="hljs-comment"># Install dep kustomize</span>
go get -u github.com/golang/dep/cmd/dep
go get github.com/kubernetes-sigs/kustomize
</code></pre>
<h3 id="使用方法">使用方法</h3>
<h4 id="初始化项目">初始化项目</h4>
<pre><code class="lang-sh">mkdir -p <span class="hljs-variable">$GOPATH</span>/src/demo
<span class="hljs-built_in">cd</span> <span class="hljs-variable">$GOPATH</span>/src/demo
kubebuilder init --domain k8s.io --license apache2 --owner <span class="hljs-string">"The Kubernetes Authors"</span>
</code></pre>
<h4 id="创建-api">创建 API</h4>
<pre><code class="lang-sh">kubebuilder create api --group ships --version v1beta1 --kind Sloop
</code></pre>
<p>然后按照实际需要修改 <code>pkg/apis/ship/v1beta1/sloop_types.go</code> 和 <code>pkg/controller/sloop/sloop_controller.go</code> 增加业务逻辑。</p>
<h4 id="本地运行测试">本地运行测试</h4>
<pre><code class="lang-sh">make install
make run
</code></pre>
<blockquote>
<p>如果碰到错误 <code>ValidationError(CustomResourceDefinition.status): missing required field "storedVersions" in io.k8s.apiextensions-apiserver.pkg.apis.apiextensions.v1beta1.CustomResourceDefinitionStatus]</code>，可以手动修改 <code>config/crds/ships_v1beta1_sloop.yaml</code>:
```yaml
status:
  acceptedNames:
    kind: ""
    plural: ""
  conditions: []
  storedVersions: []</p>
<p>然后运行 <code>kubectl apply -f config/crds</code> 创建 CRD。</p>
</blockquote>
<p>然后就可以用 <code>ships.k8s.io/v1beta1</code> 来创建 Kind 为 <code>Sloop</code> 的资源了，比如 </p>
<pre><code class="lang-sh">kubectl apply <span class="hljs-_">-f</span> config/samples/ships_v1beta1_sloop.yaml
</code></pre>
<h4 id="构建镜像并部署控制器">构建镜像并部署控制器</h4>
<pre><code class="lang-sh"><span class="hljs-comment"># 替换 IMG 为你自己的</span>
<span class="hljs-built_in">export</span> IMG=feisky/demo-crd:v1
make docker-build
make docker-push
make deploy
</code></pre>
<blockquote>
<p>kustomize 已经不再支持通配符，因而上述 <code>make deploy</code> 可能会碰到 <code>Load from path ../rbac/*.yaml failed</code> 错误，解决方法是手动修改 <code>config/default/kustomization.yaml</code>:</p>
<p>resources:</p>
<ul>
<li>../rbac/rbac_role.yaml</li>
<li>../rbac/rbac_role_binding.yaml</li>
<li>../manager/manager.yaml</li>
</ul>
<p>然后执行 <code>kustomize build config/default | kubectl apply -f -</code> 部署，默认部署到 <code>demo-system</code> namespace 中。</p>
</blockquote>
<h4 id="文档和测试">文档和测试</h4>
<pre><code class="lang-sh"><span class="hljs-comment"># run unit tests</span>
make <span class="hljs-built_in">test</span>

<span class="hljs-comment"># generate docs</span>
kubebuilder docs
</code></pre>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/extend-api-custom-resource-definitions/#validation" target="_blank">Extend the Kubernetes API with CustomResourceDefinitions</a></li>
<li><a href="https://kubernetes.io/docs/api-reference/v1.8/#customresourcedefinition-v1beta1-apiextensions" target="_blank">CustomResourceDefinition API</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="daemonset" class="level3">DaemonSet</h1>
<p>DaemonSet 保证在每个 Node 上都运行一个容器副本，常用来部署一些集群的日志、监控或者其他系统管理应用。典型的应用包括：</p>
<ul>
<li>日志收集，比如 fluentd，logstash 等</li>
<li>系统监控，比如 Prometheus Node Exporter，collectd，New Relic agent，Ganglia gmond 等</li>
<li>系统程序，比如 kube-proxy, kube-dns, glusterd, ceph 等</li>
</ul>
<h2 id="api-版本对照表">API 版本对照表</h2>
<table>
<thead>
<tr>
<th>Kubernetes 版本</th>
<th>Deployment 版本</th>
</tr>
</thead>
<tbody>
<tr>
<td>v1.5-v1.6</td>
<td>extensions/v1beta1</td>
</tr>
<tr>
<td>v1.7</td>
<td>apps/v1beta1</td>
</tr>
<tr>
<td>v1.8</td>
<td>apps/v1beta2</td>
</tr>
<tr>
<td>v1.9</td>
<td>apps/v1</td>
</tr>
</tbody>
</table>
<p>使用 Fluentd 收集日志的例子：</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> apps/v1
<span class="hljs-attr">kind:</span> DaemonSet
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> fluentd-elasticsearch
<span class="hljs-attr">  namespace:</span> kube-system
<span class="hljs-attr">  labels:</span>
<span class="hljs-attr">    k8s-app:</span> fluentd-logging
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  selector:</span>
<span class="hljs-attr">    matchLabels:</span>
<span class="hljs-attr">      name:</span> fluentd-elasticsearch
<span class="hljs-attr">  template:</span>
<span class="hljs-attr">    metadata:</span>
<span class="hljs-attr">      labels:</span>
<span class="hljs-attr">        name:</span> fluentd-elasticsearch
<span class="hljs-attr">    spec:</span>
<span class="hljs-attr">      tolerations:</span>
<span class="hljs-attr">      - key:</span> node-role.kubernetes.io/master
<span class="hljs-attr">        effect:</span> NoSchedule
<span class="hljs-attr">      containers:</span>
<span class="hljs-attr">      - name:</span> fluentd-elasticsearch
<span class="hljs-attr">        image:</span> gcr.io/google-containers/fluentd-elasticsearch:<span class="hljs-number">1.20</span>
<span class="hljs-attr">        resources:</span>
<span class="hljs-attr">          limits:</span>
<span class="hljs-attr">            memory:</span> <span class="hljs-number">200</span>Mi
<span class="hljs-attr">          requests:</span>
<span class="hljs-attr">            cpu:</span> <span class="hljs-number">100</span>m
<span class="hljs-attr">            memory:</span> <span class="hljs-number">200</span>Mi
<span class="hljs-attr">        volumeMounts:</span>
<span class="hljs-attr">        - name:</span> varlog
<span class="hljs-attr">          mountPath:</span> /var/log
<span class="hljs-attr">        - name:</span> varlibdockercontainers
<span class="hljs-attr">          mountPath:</span> /var/lib/docker/containers
<span class="hljs-attr">          readOnly:</span> <span class="hljs-literal">true</span>
<span class="hljs-attr">      terminationGracePeriodSeconds:</span> <span class="hljs-number">30</span>
<span class="hljs-attr">      volumes:</span>
<span class="hljs-attr">      - name:</span> varlog
<span class="hljs-attr">        hostPath:</span>
<span class="hljs-attr">          path:</span> /var/log
<span class="hljs-attr">      - name:</span> varlibdockercontainers
<span class="hljs-attr">        hostPath:</span>
<span class="hljs-attr">          path:</span> /var/lib/docker/containers
</code></pre>
<h2 id="滚动更新">滚动更新</h2>
<p>v1.6 + 支持 DaemonSet 的滚动更新，可以通过 <code>.spec.updateStrategy.type</code> 设置更新策略。目前支持两种策略</p>
<ul>
<li>OnDelete：默认策略，更新模板后，只有手动删除了旧的 Pod 后才会创建新的 Pod</li>
<li>RollingUpdate：更新 DaemonSet 模版后，自动删除旧的 Pod 并创建新的 Pod</li>
</ul>
<p>在使用 RollingUpdate 策略时，还可以设置</p>
<ul>
<li><code>.spec.updateStrategy.rollingUpdate.maxUnavailable</code>, 默认 1</li>
<li><code>spec.minReadySeconds</code>，默认 0</li>
</ul>
<h3 id="回滚">回滚</h3>
<p>v1.7 + 还支持回滚</p>
<pre><code class="lang-sh"><span class="hljs-comment"># 查询历史版本</span>
$ kubectl rollout <span class="hljs-built_in">history</span> daemonset <daemonset-name>

<span class="hljs-comment"># 查询某个历史版本的详细信息</span>
$ kubectl rollout <span class="hljs-built_in">history</span> daemonset <daemonset-name> --revision=1

<span class="hljs-comment"># 回滚</span>
$ kubectl rollout undo daemonset <daemonset-name> --to-revision=<revision>
<span class="hljs-comment"># 查询回滚状态</span>
$ kubectl rollout status ds/<daemonset-name>
</code></pre>
<h2 id="指定-node-节点">指定 Node 节点</h2>
<p>DaemonSet 会忽略 Node 的 unschedulable 状态，有两种方式来指定 Pod 只运行在指定的 Node 节点上：</p>
<ul>
<li>nodeSelector：只调度到匹配指定 label 的 Node 上</li>
<li>nodeAffinity：功能更丰富的 Node 选择器，比如支持集合操作</li>
<li>podAffinity：调度到满足条件的 Pod 所在的 Node 上</li>
</ul>
<h3 id="nodeselector-示例">nodeSelector 示例</h3>
<p>首先给 Node 打上标签</p>
<pre><code class="lang-sh">kubectl label nodes node-01 disktype=ssd
</code></pre>
<p>然后在 daemonset 中指定 nodeSelector 为 <code>disktype=ssd</code>：</p>
<pre><code class="lang-yaml"><span class="hljs-attr">spec:</span>
<span class="hljs-attr">  nodeSelector:</span>
<span class="hljs-attr">    disktype:</span> ssd
</code></pre>
<h3 id="nodeaffinity-示例">nodeAffinity 示例</h3>
<p>nodeAffinity 目前支持两种：requiredDuringSchedulingIgnoredDuringExecution 和 preferredDuringSchedulingIgnoredDuringExecution，分别代表必须满足条件和优选条件。比如下面的例子代表调度到包含标签 <code>kubernetes.io/e2e-az-name</code> 并且值为 e2e-az1 或 e2e-az2 的 Node 上，并且优选还带有标签 <code>another-node-label-key=another-node-label-value</code> 的 Node。</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> with-node-affinity
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  affinity:</span>
<span class="hljs-attr">    nodeAffinity:</span>
<span class="hljs-attr">      requiredDuringSchedulingIgnoredDuringExecution:</span>
<span class="hljs-attr">        nodeSelectorTerms:</span>
<span class="hljs-attr">        - matchExpressions:</span>
<span class="hljs-attr">          - key:</span> kubernetes.io/e2e-az-name
<span class="hljs-attr">            operator:</span> In
<span class="hljs-attr">            values:</span>
<span class="hljs-bullet">            -</span> e2e-az1
<span class="hljs-bullet">            -</span> e2e-az2
<span class="hljs-attr">      preferredDuringSchedulingIgnoredDuringExecution:</span>
<span class="hljs-attr">      - weight:</span> <span class="hljs-number">1</span>
<span class="hljs-attr">        preference:</span>
<span class="hljs-attr">          matchExpressions:</span>
<span class="hljs-attr">          - key:</span> another-node-label-key
<span class="hljs-attr">            operator:</span> In
<span class="hljs-attr">            values:</span>
<span class="hljs-bullet">            -</span> another-node-label-value
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">  - name:</span> with-node-affinity
<span class="hljs-attr">    image:</span> gcr.io/google_containers/pause:<span class="hljs-number">2.0</span>
</code></pre>
<h3 id="podaffinity-示例">podAffinity 示例</h3>
<p>podAffinity 基于 Pod 的标签来选择 Node，仅调度到满足条件 Pod 所在的 Node 上，支持 podAffinity 和 podAntiAffinity。这个功能比较绕，以下面的例子为例：</p>
<ul>
<li>如果一个 “Node 所在 Zone 中包含至少一个带有 <code>security=S1</code> 标签且运行中的 Pod”，那么可以调度到该 Node</li>
<li>不调度到 “包含至少一个带有 <code>security=S2</code> 标签且运行中 Pod” 的 Node 上</li>
</ul>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> with-pod-affinity
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  affinity:</span>
<span class="hljs-attr">    podAffinity:</span>
<span class="hljs-attr">      requiredDuringSchedulingIgnoredDuringExecution:</span>
<span class="hljs-attr">      - labelSelector:</span>
<span class="hljs-attr">          matchExpressions:</span>
<span class="hljs-attr">          - key:</span> security
<span class="hljs-attr">            operator:</span> In
<span class="hljs-attr">            values:</span>
<span class="hljs-bullet">            -</span> S1
<span class="hljs-attr">        topologyKey:</span> failure-domain.beta.kubernetes.io/zone
<span class="hljs-attr">    podAntiAffinity:</span>
<span class="hljs-attr">      preferredDuringSchedulingIgnoredDuringExecution:</span>
<span class="hljs-attr">      - weight:</span> <span class="hljs-number">100</span>
<span class="hljs-attr">        podAffinityTerm:</span>
<span class="hljs-attr">          labelSelector:</span>
<span class="hljs-attr">            matchExpressions:</span>
<span class="hljs-attr">            - key:</span> security
<span class="hljs-attr">              operator:</span> In
<span class="hljs-attr">              values:</span>
<span class="hljs-bullet">              -</span> S2
<span class="hljs-attr">          topologyKey:</span> kubernetes.io/hostname
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">  - name:</span> with-pod-affinity
<span class="hljs-attr">    image:</span> gcr.io/google_containers/pause:<span class="hljs-number">2.0</span>
</code></pre>
<h2 id="静态-pod">静态 Pod</h2>
<p>除了 DaemonSet，还可以使用静态 Pod 来在每台机器上运行指定的 Pod，这需要 kubelet 在启动的时候指定 manifest 目录：</p>
<pre><code class="lang-sh">kubelet --pod-manifest-path=/etc/kubernetes/manifests
</code></pre>
<p>然后将所需要的 Pod 定义文件放到指定的 manifest 目录中。</p>
<p>注意：静态 Pod 不能通过 API Server 来删除，但可以通过删除 manifest 文件来自动删除对应的 Pod。</p>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="deployment" class="level3">Deployment</h1>
<h2 id="简述">简述</h2>
<p>Deployment 为 Pod 和 ReplicaSet 提供了一个声明式定义 (declarative) 方法，用来替代以前的 ReplicationController 来方便的管理应用。</p>
<h2 id="api-版本对照表">API 版本对照表</h2>
<table>
<thead>
<tr>
<th>Kubernetes 版本</th>
<th>Deployment 版本</th>
</tr>
</thead>
<tbody>
<tr>
<td>v1.5-v1.6</td>
<td>extensions/v1beta1</td>
</tr>
<tr>
<td>v1.7</td>
<td>apps/v1beta1</td>
</tr>
<tr>
<td>v1.8</td>
<td>apps/v1beta2</td>
</tr>
<tr>
<td>v1.9</td>
<td>apps/v1</td>
</tr>
</tbody>
</table>
<p>比如一个简单的 nginx 应用可以定义为</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> apps/v1
<span class="hljs-attr">kind:</span> Deployment
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> nginx-deployment
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  replicas:</span> <span class="hljs-number">3</span>
<span class="hljs-attr">  template:</span>
<span class="hljs-attr">    metadata:</span>
<span class="hljs-attr">      labels:</span>
<span class="hljs-attr">        app:</span> nginx
<span class="hljs-attr">    spec:</span>
<span class="hljs-attr">      containers:</span>
<span class="hljs-attr">      - name:</span> nginx
<span class="hljs-attr">        image:</span> nginx:<span class="hljs-number">1.7</span><span class="hljs-number">.9</span>
<span class="hljs-attr">        ports:</span>
<span class="hljs-attr">        - containerPort:</span> <span class="hljs-number">80</span>
</code></pre>
<p>扩容：</p>
<pre><code>kubectl scale deployment nginx-deployment --replicas 10
</code></pre><p>如果集群支持 horizontal pod autoscaling 的话，还可以为 Deployment 设置自动扩展：</p>
<pre><code>kubectl autoscale deployment nginx-deployment --min=10 --max=15 --cpu-percent=80
</code></pre><p>更新镜像也比较简单:</p>
<pre><code>kubectl set image deployment/nginx-deployment nginx=nginx:1.9.1
</code></pre><p>回滚：</p>
<pre><code>kubectl rollout undo deployment/nginx-deployment
</code></pre><p>Deployment 的 <strong> 典型应用场景 </strong> 包括：</p>
<ul>
<li>定义 Deployment 来创建 Pod 和 ReplicaSet</li>
<li>滚动升级和回滚应用</li>
<li>扩容和缩容</li>
<li>暂停和继续 Deployment</li>
</ul>
<h2 id="deployment-概念解析">Deployment 概念解析</h2>
<h2 id="deployment-是什么？">Deployment 是什么？</h2>
<p>Deployment 为 Pod 和 Replica Set（下一代 Replication Controller）提供声明式更新。</p>
<p>你只需要在 Deployment 中描述你想要的目标状态是什么，Deployment controller 就会帮你将 Pod 和 Replica Set 的实际状态改变到你的目标状态。你可以定义一个全新的 Deployment，也可以创建一个新的替换旧的 Deployment。</p>
<p>一个典型的用例如下：</p>
<ul>
<li>使用 Deployment 来创建 ReplicaSet。ReplicaSet 在后台创建 pod。检查启动状态，看它是成功还是失败。</li>
<li>然后，通过更新 Deployment 的 PodTemplateSpec 字段来声明 Pod 的新状态。这会创建一个新的 ReplicaSet，Deployment 会按照控制的速率将 pod 从旧的 ReplicaSet 移动到新的 ReplicaSet 中。</li>
<li>如果当前状态不稳定，回滚到之前的 Deployment revision。每次回滚都会更新 Deployment 的 revision。</li>
<li>扩容 Deployment 以满足更高的负载。</li>
<li>暂停 Deployment 来应用 PodTemplateSpec 的多个修复，然后恢复上线。</li>
<li>根据 Deployment 的状态判断上线是否 hang 住了。</li>
<li>清除旧的不必要的 ReplicaSet。</li>
</ul>
<h2 id="创建-deployment">创建 Deployment</h2>
<p>下面是一个 Deployment 示例，它创建了一个 Replica Set 来启动 3 个 nginx pod。</p>
<p>下载示例文件并执行命令：</p>
<pre><code class="lang-sh">$ kubectl create <span class="hljs-_">-f</span> docs/user-guide/nginx-deployment.yaml --record
deployment <span class="hljs-string">"nginx-deployment"</span> created
</code></pre>
<p>将 kubectl 的 <code>—record</code> 的 flag 设置为 <code>true</code> 可以在 annotation 中记录当前命令创建或者升级了该资源。这在未来会很有用，例如，查看在每个 Deployment revision 中执行了哪些命令。</p>
<p>然后立即执行 <code>get</code> 将获得如下结果：</p>
<pre><code class="lang-sh">$ kubectl get deployments
NAME               DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   3         0         0            0           1s
</code></pre>
<p>输出结果表明我们希望的 repalica 数是 3（根据 deployment 中的 <code>.spec.replicas</code> 配置）当前 replica 数（ <code>.status.replicas</code>）是 0, 最新的 replica 数（<code>.status.updatedReplicas</code>）是 0，可用的 replica 数（<code>.status.availableReplicas</code>）是 0。</p>
<p>过几秒后再执行 <code>get</code> 命令，将获得如下输出：</p>
<pre><code class="lang-sh">$ kubectl get deployments
NAME               DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   3         3         3            3           18s
</code></pre>
<p>我们可以看到 Deployment 已经创建了 3 个 replica，所有的 replica 都已经是最新的了（包含最新的 pod template），可用的（根据 Deployment 中的 <code>.spec.minReadySeconds</code> 声明，处于已就绪状态的 pod 的最少个数）。执行 <code>kubectl get rs</code> 和 <code>kubectl get pods</code> 会显示 Replica Set（RS）和 Pod 已创建。</p>
<pre><code class="lang-sh">$ kubectl get rs
NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-2035384211   3         3         0       18s
</code></pre>
<p>你可能会注意到 Replica Set 的名字总是 <code><Deployment 的名字>-<pod template 的 hash 值 ></code>。</p>
<pre><code class="lang-sh">$ kubectl get pods --show-labels
NAME                                READY     STATUS    RESTARTS   AGE       LABELS
nginx-deployment-2035384211-7ci7o   1/1       Running   0          18s       app=nginx,pod-template-hash=2035384211
nginx-deployment-2035384211-kzszj   1/1       Running   0          18s       app=nginx,pod-template-hash=2035384211
nginx-deployment-2035384211-qqcnn   1/1       Running   0          18s       app=nginx,pod-template-hash=2035384211
</code></pre>
<p>刚创建的 Replica Set 将保证总是有 3 个 nginx 的 pod 存在。</p>
<p><strong> 注意：</strong>  你必须在 Deployment 中的 selector 指定正确 pod template label（在该示例中是 <code>app = nginx</code>），不要跟其他的 controller 搞混了（包括 Deployment、Replica Set、Replication Controller 等）。<strong>Kubernetes 本身不会阻止你这么做 </strong>，如果你真的这么做了，这些 controller 之间会相互打架，并可能导致不正确的行为。</p>
<h2 id="更新-deployment">更新 Deployment</h2>
<p><strong> 注意：</strong>  Deployment 的 rollout 当且仅当 Deployment 的 pod template（例如 <code>.spec.template</code>）中的 label 更新或者镜像更改时被触发。其他更新，例如扩容 Deployment 不会触发 rollout。</p>
<p>假如我们现在想要让 nginx pod 使用 <code>nginx:1.9.1</code> 的镜像来代替原来的 <code>nginx:1.7.9</code> 的镜像。</p>
<pre><code class="lang-sh">$ kubectl <span class="hljs-built_in">set</span> image deployment/nginx-deployment nginx=nginx:1.9.1
deployment <span class="hljs-string">"nginx-deployment"</span> image updated
</code></pre>
<p>我们可以使用 <code>edit</code> 命令来编辑 Deployment，修改 <code>.spec.template.spec.containers[0].image</code> ，将 <code>nginx:1.7.9</code> 改写成 <code>nginx:1.9.1</code>。</p>
<pre><code class="lang-sh">$ kubectl edit deployment/nginx-deployment
deployment <span class="hljs-string">"nginx-deployment"</span> edited
</code></pre>
<p>查看 rollout 的状态，只要执行：</p>
<pre><code class="lang-sh">$ kubectl rollout status deployment/nginx-deployment
Waiting <span class="hljs-keyword">for</span> rollout to finish: 2 out of 3 new replicas have been updated...
deployment <span class="hljs-string">"nginx-deployment"</span> successfully rolled out
</code></pre>
<p>Rollout 成功后，<code>get</code> Deployment：</p>
<pre><code class="lang-sh">$ kubectl get deployments
NAME               DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   3         3         3            3           36s
</code></pre>
<p>UP-TO-DATE 的 replica 的数目已经达到了配置中要求的数目。</p>
<p>CURRENT 的 replica 数表示 Deployment 管理的 replica 数量，AVAILABLE 的 replica 数是当前可用的 replica 数量。</p>
<p>我们通过执行 <code>kubectl get rs</code> 可以看到 Deployment 更新了 Pod，通过创建一个新的 Replica Set 并扩容了 3 个 replica，同时将原来的 Replica Set 缩容到了 0 个 replica。</p>
<pre><code class="lang-sh">$ kubectl get rs
NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-1564180365   3         3         0       6s
nginx-deployment-2035384211   0         0         0       36s
</code></pre>
<p>执行 <code>get pods</code> 只会看到当前的新的 pod:</p>
<pre><code class="lang-sh">$ kubectl get pods
NAME                                READY     STATUS    RESTARTS   AGE
nginx-deployment-1564180365-khku8   1/1       Running   0          14s
nginx-deployment-1564180365-nacti   1/1       Running   0          14s
nginx-deployment-1564180365-z9gth   1/1       Running   0          14s
</code></pre>
<p>下次更新这些 pod 的时候，只需要更新 Deployment 中的 pod 的 template 即可。</p>
<p>Deployment 可以保证在升级时只有一定数量的 Pod 是 down 的。默认的，它会确保至少有比期望的 Pod 数量少一个的 Pod 是 up 状态（最多一个不可用）。</p>
<p>Deployment 同时也可以确保只创建出超过期望数量的一定数量的 Pod。默认的，它会确保最多比期望的 Pod 数量多一个的 Pod 是 up 的（最多 1 个 surge）。</p>
<p><strong> 在未来的 Kuberentes 版本中，将从 1-1 变成 25%-25%）。</strong></p>
<p>例如，如果你自己看下上面的 Deployment，你会发现，开始创建一个新的 Pod，然后删除一些旧的 Pod 再创建一个新的。当新的 Pod 创建出来之前不会杀掉旧的 Pod。这样能够确保可用的 Pod 数量至少有 2 个，Pod 的总数最多 4 个。</p>
<pre><code class="lang-sh">$ kubectl describe deployments
Name:           nginx-deployment
Namespace:      default
CreationTimestamp:  Tue, 15 Mar 2016 12:01:06 -0700
Labels:         app=nginx
Selector:       app=nginx
Replicas:       3 updated | 3 total | 3 available | 0 unavailable
StrategyType:       RollingUpdate
M<span class="hljs-keyword">in</span>ReadySeconds:    0
RollingUpdateStrategy:  1 max unavailable, 1 max surge
OldReplicaSets:     <none>
NewReplicaSet:      nginx-deployment-1564180365 (3/3 replicas created)
Events:
  FirstSeen LastSeen    Count   From                     SubobjectPath   Type        Reason              Message
  --------- --------    -----   ----                     -------------   --------    ------              -------
  36s       36s         1       {deployment-controller}                 Normal      ScalingReplicaSet   Scaled up replica <span class="hljs-built_in">set</span> nginx-deployment-2035384211 to 3
  23s       23s         1       {deployment-controller}                 Normal      ScalingReplicaSet   Scaled up replica <span class="hljs-built_in">set</span> nginx-deployment-1564180365 to 1
  23s       23s         1       {deployment-controller}                 Normal      ScalingReplicaSet   Scaled down replica <span class="hljs-built_in">set</span> nginx-deployment-2035384211 to 2
  23s       23s         1       {deployment-controller}                 Normal      ScalingReplicaSet   Scaled up replica <span class="hljs-built_in">set</span> nginx-deployment-1564180365 to 2
  21s       21s         1       {deployment-controller}                 Normal      ScalingReplicaSet   Scaled down replica <span class="hljs-built_in">set</span> nginx-deployment-2035384211 to 0
  21s       21s         1       {deployment-controller}                 Normal      ScalingReplicaSet   Scaled up replica <span class="hljs-built_in">set</span> nginx-deployment-1564180365 to 3
</code></pre>
<p>我们可以看到当我们刚开始创建这个 Deployment 的时候，创建了一个 Replica Set（nginx-deployment-2035384211），并直接扩容到了 3 个 replica。</p>
<p>当我们更新这个 Deployment 的时候，它会创建一个新的 Replica Set（nginx-deployment-1564180365），将它扩容到 1 个 replica，然后缩容原先的 Replica Set 到 2 个 replica，此时满足至少 2 个 Pod 是可用状态，同一时刻最多有 4 个 Pod 处于创建的状态。</p>
<p>接着继续使用相同的 rolling update 策略扩容新的 Replica Set 和缩容旧的 Replica Set。最终，将会在新的 Replica Set 中有 3 个可用的 replica，旧的 Replica Set 的 replica 数目变成 0。</p>
<h3 id="rollover（多个-rollout-并行）">Rollover（多个 rollout 并行）</h3>
<p>每当 Deployment controller 观测到有新的 deployment 被创建时，如果没有已存在的 Replica Set 来创建期望个数的 Pod 的话，就会创建出一个新的 Replica Set 来做这件事。已存在的 Replica Set 控制 label 匹配 <code>.spec.selector</code> 但是 template 跟 <code>.spec.template</code> 不匹配的 Pod 缩容。最终，新的 Replica Set 将会扩容出 <code>.spec.replicas</code> 指定数目的 Pod，旧的 Replica Set 会缩容到 0。</p>
<p>如果你更新了一个的已存在并正在进行中的 Deployment，每次更新 Deployment 都会创建一个新的 Replica Set 并扩容它，同时回滚之前扩容的 Replica Set——将它添加到旧的 Replica Set 列表，开始缩容。</p>
<p>例如，假如你创建了一个有 5 个 <code>niginx:1.7.9</code> replica 的 Deployment，但是当还只有 3 个 <code>nginx:1.7.9</code> 的 replica 创建出来的时候你就开始更新含有 5 个 <code>nginx:1.9.1</code> replica 的 Deployment。在这种情况下，Deployment 会立即杀掉已创建的 3 个 <code>nginx:1.7.9</code> 的 Pod，并开始创建 <code>nginx:1.9.1</code> 的 Pod。它不会等到所有的 5 个 <code>nginx:1.7.9</code> 的 Pod 都创建完成后才开始执行滚动更新。</p>
<h2 id="回退-deployment">回退 Deployment</h2>
<p>有时候你可能想回退一个 Deployment，例如，当 Deployment 不稳定时，比如一直 crash looping。</p>
<p>默认情况下，kubernetes 会在系统中保存所有的 Deployment 的 rollout 历史记录，以便你可以随时回退（你可以修改 <code>revision history limit</code> 来更改保存的 revision 数）。</p>
<p><strong> 注意：</strong> 只要 Deployment 的 rollout 被触发就会创建一个 revision。也就是说当且仅当 Deployment 的 Pod template（如 <code>.spec.template</code>）被更改，例如更新 template 中的 label 和容器镜像时，就会创建出一个新的 revision。</p>
<p>其他的更新，比如扩容 Deployment 不会创建 revision——因此我们可以很方便的手动或者自动扩容。这意味着当你回退到历史 revision 时，只有 Deployment 中的 Pod template 部分才会回退。</p>
<p>假设我们在更新 Deployment 的时候犯了一个拼写错误，将镜像的名字写成了 <code>nginx:1.91</code>，而正确的名字应该是 <code>nginx:1.9.1</code>：</p>
<pre><code class="lang-sh">$ kubectl <span class="hljs-built_in">set</span> image deployment/nginx-deployment nginx=nginx:1.91
deployment <span class="hljs-string">"nginx-deployment"</span> image updated
</code></pre>
<p>Rollout 将会卡住。</p>
<pre><code class="lang-sh">$ kubectl rollout status deployments nginx-deployment
Waiting <span class="hljs-keyword">for</span> rollout to finish: 2 out of 3 new replicas have been updated...
</code></pre>
<p>按住 Ctrl-C 停止上面的 rollout 状态监控。</p>
<p>你会看到旧的 replicas（nginx-deployment-1564180365 和 nginx-deployment-2035384211）和新的 replicas （nginx-deployment-3066724191）数目都是 2 个。</p>
<pre><code class="lang-sh">$ kubectl get rs
NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-1564180365   2         2         0       25s
nginx-deployment-2035384211   0         0         0       36s
nginx-deployment-3066724191   2         2         2       6s
</code></pre>
<p>看下创建 Pod，你会看到有两个新的 Replica Set 创建的 Pod 处于 ImagePullBackOff 状态，循环拉取镜像。</p>
<pre><code class="lang-sh">$ kubectl get pods
NAME                                READY     STATUS             RESTARTS   AGE
nginx-deployment-1564180365-70iae   1/1       Running            0          25s
nginx-deployment-1564180365-jbqqo   1/1       Running            0          25s
nginx-deployment-3066724191-08mng   0/1       ImagePullBackOff   0          6s
nginx-deployment-3066724191-eocby   0/1       ImagePullBackOff   0          6s
</code></pre>
<p>注意，Deployment controller 会自动停止坏的 rollout，并停止扩容新的 Replica Set。</p>
<pre><code class="lang-sh">$ kubectl describe deployment
Name:           nginx-deployment
Namespace:      default
CreationTimestamp:  Tue, 15 Mar 2016 14:48:04 -0700
Labels:         app=nginx
Selector:       app=nginx
Replicas:       2 updated | 3 total | 2 available | 2 unavailable
StrategyType:       RollingUpdate
M<span class="hljs-keyword">in</span>ReadySeconds:    0
RollingUpdateStrategy:  1 max unavailable, 1 max surge
OldReplicaSets:     nginx-deployment-1564180365 (2/2 replicas created)
NewReplicaSet:      nginx-deployment-3066724191 (2/2 replicas created)
Events:
  FirstSeen LastSeen    Count   From                    SubobjectPath   Type        Reason              Message
  --------- --------    -----   ----                    -------------   --------    ------              -------
  1m        1m          1       {deployment-controller}                Normal      ScalingReplicaSet   Scaled up replica <span class="hljs-built_in">set</span> nginx-deployment-2035384211 to 3
  22s       22s         1       {deployment-controller}                Normal      ScalingReplicaSet   Scaled up replica <span class="hljs-built_in">set</span> nginx-deployment-1564180365 to 1
  22s       22s         1       {deployment-controller}                Normal      ScalingReplicaSet   Scaled down replica <span class="hljs-built_in">set</span> nginx-deployment-2035384211 to 2
  22s       22s         1       {deployment-controller}                Normal      ScalingReplicaSet   Scaled up replica <span class="hljs-built_in">set</span> nginx-deployment-1564180365 to 2
  21s       21s         1       {deployment-controller}                Normal      ScalingReplicaSet   Scaled down replica <span class="hljs-built_in">set</span> nginx-deployment-2035384211 to 0
  21s       21s         1       {deployment-controller}                Normal      ScalingReplicaSet   Scaled up replica <span class="hljs-built_in">set</span> nginx-deployment-1564180365 to 3
  13s       13s         1       {deployment-controller}                Normal      ScalingReplicaSet   Scaled up replica <span class="hljs-built_in">set</span> nginx-deployment-3066724191 to 1
  13s       13s         1       {deployment-controller}                Normal      ScalingReplicaSet   Scaled down replica <span class="hljs-built_in">set</span> nginx-deployment-1564180365 to 2
  13s       13s         1       {deployment-controller}                Normal      ScalingReplicaSet   Scaled up replica <span class="hljs-built_in">set</span> nginx-deployment-3066724191 to 2
</code></pre>
<p>为了修复这个问题，我们需要回退到稳定的 Deployment revision。</p>
<h3 id="检查-deployment-升级的历史记录">检查 Deployment 升级的历史记录</h3>
<p>首先，检查下 Deployment 的 revision：</p>
<pre><code class="lang-sh">$ kubectl rollout <span class="hljs-built_in">history</span> deployment/nginx-deployment
deployments <span class="hljs-string">"nginx-deployment"</span>:
REVISION    CHANGE-CAUSE
1           kubectl create <span class="hljs-_">-f</span> docs/user-guide/nginx-deployment.yaml --record
2           kubectl <span class="hljs-built_in">set</span> image deployment/nginx-deployment nginx=nginx:1.9.1
3           kubectl <span class="hljs-built_in">set</span> image deployment/nginx-deployment nginx=nginx:1.91
</code></pre>
<p>因为我们创建 Deployment 的时候使用了 <code>—recored</code> 参数可以记录命令，我们可以很方便的查看每次 revison 的变化。</p>
<p>查看单个 revision 的详细信息：</p>
<pre><code class="lang-sh">$ kubectl rollout <span class="hljs-built_in">history</span> deployment/nginx-deployment --revision=2
deployments <span class="hljs-string">"nginx-deployment"</span> revision 2
  Labels:       app=nginx
          pod-template-hash=1159050644
  Annotations:  kubernetes.io/change-cause=kubectl <span class="hljs-built_in">set</span> image deployment/nginx-deployment nginx=nginx:1.9.1
  Containers:
   nginx:
    Image:      nginx:1.9.1
    Port:       80/TCP
     QoS Tier:
        cpu:      BestEffort
        memory:   BestEffort
    Environment Variables:      <none>
  No volumes.
</code></pre>
<h3 id="回退到历史版本">回退到历史版本</h3>
<p>现在，我们可以决定回退当前的 rollout 到之前的版本：</p>
<pre><code class="lang-sh">$ kubectl rollout undo deployment/nginx-deployment
deployment <span class="hljs-string">"nginx-deployment"</span> rolled back
</code></pre>
<p>也可以使用 <code>--to-revision</code> 参数指定某个历史版本：</p>
<pre><code class="lang-sh">$ kubectl rollout undo deployment/nginx-deployment --to-revision=2
deployment <span class="hljs-string">"nginx-deployment"</span> rolled back
</code></pre>
<p>与 rollout 相关的命令详细文档见 <a href="https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#rollout" target="_blank">kubectl rollout</a>。</p>
<p>该 Deployment 现在已经回退到了先前的稳定版本。如你所见，Deployment controller 产生了一个回退到 revison 2 的 <code>DeploymentRollback</code> 的 event。</p>
<pre><code class="lang-sh">$ kubectl get deployment
NAME               DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   3         3         3            3           30m

$ kubectl describe deployment
Name:           nginx-deployment
Namespace:      default
CreationTimestamp:  Tue, 15 Mar 2016 14:48:04 -0700
Labels:         app=nginx
Selector:       app=nginx
Replicas:       3 updated | 3 total | 3 available | 0 unavailable
StrategyType:       RollingUpdate
M<span class="hljs-keyword">in</span>ReadySeconds:    0
RollingUpdateStrategy:  1 max unavailable, 1 max surge
OldReplicaSets:     <none>
NewReplicaSet:      nginx-deployment-1564180365 (3/3 replicas created)
Events:
  FirstSeen LastSeen    Count   From                    SubobjectPath   Type        Reason              Message
  --------- --------    -----   ----                    -------------   --------    ------              -------
  30m       30m         1       {deployment-controller}                Normal      ScalingReplicaSet   Scaled up replica <span class="hljs-built_in">set</span> nginx-deployment-2035384211 to 3
  29m       29m         1       {deployment-controller}                Normal      ScalingReplicaSet   Scaled up replica <span class="hljs-built_in">set</span> nginx-deployment-1564180365 to 1
  29m       29m         1       {deployment-controller}                Normal      ScalingReplicaSet   Scaled down replica <span class="hljs-built_in">set</span> nginx-deployment-2035384211 to 2
  29m       29m         1       {deployment-controller}                Normal      ScalingReplicaSet   Scaled up replica <span class="hljs-built_in">set</span> nginx-deployment-1564180365 to 2
  29m       29m         1       {deployment-controller}                Normal      ScalingReplicaSet   Scaled down replica <span class="hljs-built_in">set</span> nginx-deployment-2035384211 to 0
  29m       29m         1       {deployment-controller}                Normal      ScalingReplicaSet   Scaled up replica <span class="hljs-built_in">set</span> nginx-deployment-3066724191 to 2
  29m       29m         1       {deployment-controller}                Normal      ScalingReplicaSet   Scaled up replica <span class="hljs-built_in">set</span> nginx-deployment-3066724191 to 1
  29m       29m         1       {deployment-controller}                Normal      ScalingReplicaSet   Scaled down replica <span class="hljs-built_in">set</span> nginx-deployment-1564180365 to 2
  2m        2m          1       {deployment-controller}                Normal      ScalingReplicaSet   Scaled down replica <span class="hljs-built_in">set</span> nginx-deployment-3066724191 to 0
  2m        2m          1       {deployment-controller}                Normal      DeploymentRollback  Rolled back deployment <span class="hljs-string">"nginx-deployment"</span> to revision 2
  29m       2m          2       {deployment-controller}                Normal      ScalingReplicaSet   Scaled up replica <span class="hljs-built_in">set</span> nginx-deployment-1564180365 to 3
</code></pre>
<h3 id="清理-policy">清理 Policy</h3>
<p>你可以通过设置 <code>.spec.revisonHistoryLimit</code> 项来指定 deployment 最多保留多少 revison 历史记录。默认的会保留所有的 revision；如果将该项设置为 0，Deployment 就不允许回退了。</p>
<h2 id="deployment-扩容">Deployment 扩容</h2>
<p>你可以使用以下命令扩容 Deployment：</p>
<pre><code class="lang-sh">$ kubectl scale deployment nginx-deployment --replicas 10
deployment <span class="hljs-string">"nginx-deployment"</span> scaled
</code></pre>
<p>假设你的集群中启用了 <a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/" target="_blank">horizontal pod autoscaling (HPA)</a>，你可以给 Deployment 设置一个 autoscaler，基于当前 Pod 的 CPU 利用率选择最少和最多的 Pod 数。</p>
<pre><code class="lang-sh">$ kubectl autoscale deployment nginx-deployment --min=10 --max=15 --cpu-percent=80
deployment <span class="hljs-string">"nginx-deployment"</span> autoscaled
</code></pre>
<h2 id="比例扩容">比例扩容</h2>
<p>RollingUpdate Deployment 支持同时运行一个应用的多个版本。当你或者 autoscaler 扩容一个正在 rollout 中（进行中或者已经暂停）的 RollingUpdate Deployment 的时候，为了降低风险，Deployment controller 将会平衡已存在的 active 的 ReplicaSets（有 Pod 的 ReplicaSets）和新加入的 replicas。这被称为比例扩容。</p>
<p>例如，你正在运行中含有 10 个 replica 的 Deployment。maxSurge=3，maxUnavailable=2。</p>
<pre><code class="lang-sh">$ kubectl get deploy
NAME                 DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment     10        10        10           10          50s
</code></pre>
<p>你更新了一个镜像，而在集群内部无法解析。</p>
<pre><code class="lang-sh">$ kubectl <span class="hljs-built_in">set</span> image deploy/nginx-deployment nginx=nginx:sometag
deployment <span class="hljs-string">"nginx-deployment"</span> image updated
</code></pre>
<p>镜像更新启动了一个包含 ReplicaSet nginx-deployment-1989198191 的新的 rollout，但是它被阻塞了，因为我们上面提到的 maxUnavailable。</p>
<pre><code class="lang-sh">$ kubectl get rs
NAME                          DESIRED   CURRENT   READY     AGE
nginx-deployment-1989198191   5         5         0         9s
nginx-deployment-618515232    8         8         8         1m
</code></pre>
<p>然后发起了一个新的 Deployment 扩容请求。autoscaler 将 Deployment 的 replica 数目增加到了 15 个。Deployment controller 需要判断在哪里增加这 5 个新的 replica。如果我们没有使用比例扩容，所有的 5 个 replica 都会加到一个新的 ReplicaSet 中。如果使用比例扩容，新添加的 replica 将传播到所有的 ReplicaSet 中。大的部分加入 replica 数最多的 ReplicaSet 中，小的部分加入到 replica 数少的 ReplciaSet 中。0 个 replica 的 ReplicaSet 不会被扩容。</p>
<p>在我们上面的例子中，3 个 replica 将添加到旧的 ReplicaSet 中，2 个 replica 将添加到新的 ReplicaSet 中。rollout 进程最终会将所有的 replica 移动到新的 ReplicaSet 中，假设新的 replica 成为健康状态。</p>
<pre><code class="lang-sh">$ kubectl get deploy
NAME                 DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment     15        18        7            8           7m
$ kubectl get rs
NAME                          DESIRED   CURRENT   READY     AGE
nginx-deployment-1989198191   7         7         0         7m
nginx-deployment-618515232    11        11        11        7m
</code></pre>
<h2 id="暂停和恢复-deployment">暂停和恢复 Deployment</h2>
<p>你可以在触发一次或多次更新前暂停一个 Deployment，然后再恢复它。这样你就能多次暂停和恢复 Deployment，在此期间进行一些修复工作，而不会触发不必要的 rollout。</p>
<p>例如使用刚刚创建 Deployment：</p>
<pre><code class="lang-sh">$ kubectl get deploy
NAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx     3         3         3            3           1m
[mkargaki@dhcp129-211 kubernetes]$ kubectl get rs
NAME               DESIRED   CURRENT   READY     AGE
nginx-2142116321   3         3         3         1m
</code></pre>
<p>使用以下命令暂停 Deployment：</p>
<pre><code class="lang-sh">$ kubectl rollout pause deployment/nginx-deployment
deployment <span class="hljs-string">"nginx-deployment"</span> paused
</code></pre>
<p>然后更新 Deplyment 中的镜像：</p>
<pre><code class="lang-sh">$ kubectl <span class="hljs-built_in">set</span> image deploy/nginx nginx=nginx:1.9.1
deployment <span class="hljs-string">"nginx-deployment"</span> image updated
</code></pre>
<p>注意没有启动新的 rollout：</p>
<pre><code class="lang-sh">$ kubectl rollout <span class="hljs-built_in">history</span> deploy/nginx
deployments <span class="hljs-string">"nginx"</span>
REVISION  CHANGE-CAUSE
1   <none>

$ kubectl get rs
NAME               DESIRED   CURRENT   READY     AGE
nginx-2142116321   3         3         3         2m
</code></pre>
<p>你可以进行任意多次更新，例如更新使用的资源：</p>
<pre><code class="lang-sh">$ kubectl <span class="hljs-built_in">set</span> resources deployment nginx -c=nginx --limits=cpu=200m,memory=512Mi
deployment <span class="hljs-string">"nginx"</span> resource requirements updated
</code></pre>
<p>Deployment 暂停前的初始状态将继续它的功能，而不会对 Deployment 的更新产生任何影响，只要 Deployment 是暂停的。</p>
<p>最后，恢复这个 Deployment，观察完成更新的 ReplicaSet 已经创建出来了：</p>
<pre><code class="lang-sh">$ kubectl rollout resume deploy nginx
deployment <span class="hljs-string">"nginx"</span> resumed
$ KUBECTL get rs -w
NAME               DESIRED   CURRENT   READY     AGE
nginx-2142116321   2         2         2         2m
nginx-3926361531   2         2         0         6s
nginx-3926361531   2         2         1         18s
nginx-2142116321   1         2         2         2m
nginx-2142116321   1         2         2         2m
nginx-3926361531   3         2         1         18s
nginx-3926361531   3         2         1         18s
nginx-2142116321   1         1         1         2m
nginx-3926361531   3         3         1         18s
nginx-3926361531   3         3         2         19s
nginx-2142116321   0         1         1         2m
nginx-2142116321   0         1         1         2m
nginx-2142116321   0         0         0         2m
nginx-3926361531   3         3         3         20s
^C
$ KUBECTL get rs
NAME               DESIRED   CURRENT   READY     AGE
nginx-2142116321   0         0         0         2m
nginx-3926361531   3         3         3         28s
</code></pre>
<p><strong> 注意：</strong> 在恢复 Deployment 之前你无法回退一个暂停了的 Deployment。</p>
<h2 id="deployment-状态">Deployment 状态</h2>
<p>Deployment 在生命周期中有多种状态。在创建一个新的 ReplicaSet 的时候它可以是 <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#progressing-deployment" target="_blank">progressing</a> 状态， <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#complete-deployment" target="_blank">complete</a> 状态，或者 <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#failed-deployment" target="_blank">fail to progress</a> 状态。</p>
<h3 id="progressing-deployment">Progressing Deployment</h3>
<p>Kubernetes 将执行过下列任务之一的 Deployment 标记为 <em>progressing</em> 状态：</p>
<ul>
<li>Deployment 正在创建新的 ReplicaSet 过程中。</li>
<li>Deployment 正在扩容一个已有的 ReplicaSet。</li>
<li>Deployment 正在缩容一个已有的 ReplicaSet。</li>
<li>有新的可用的 pod 出现。</li>
</ul>
<p>你可以使用 <code>kubectl rollout status</code> 命令监控 Deployment 的进度。</p>
<h3 id="complete-deployment">Complete Deployment</h3>
<p>Kubernetes 将包括以下特性的 Deployment 标记为 <em>complete</em> 状态：</p>
<ul>
<li>Deployment 最小可用。最小可用意味着 Deployment 的可用 replica 个数等于或者超过 Deployment 策略中的期望个数。</li>
<li>所有与该 Deployment 相关的 replica 都被更新到了你指定版本，也就说更新完成。</li>
<li>该 Deployment 中没有旧的 Pod 存在。</li>
</ul>
<p>你可以用 <code>kubectl rollout status</code> 命令查看 Deployment 是否完成。如果 rollout 成功完成，<code>kubectl rollout status</code> 将返回一个 0 值的 Exit Code。</p>
<pre><code class="lang-sh">$ kubectl rollout status deploy/nginx
Waiting <span class="hljs-keyword">for</span> rollout to finish: 2 of 3 updated replicas are available...
deployment <span class="hljs-string">"nginx"</span> successfully rolled out
$ <span class="hljs-built_in">echo</span> $?
0
</code></pre>
<h3 id="failed-deployment">Failed Deployment</h3>
<p>你的 Deployment 在尝试部署新的 ReplicaSet 的时候可能卡住，永远也不会完成。这可能是因为以下几个因素引起的：</p>
<ul>
<li>无效的引用</li>
<li>不可读的 probe failure</li>
<li>镜像拉取错误</li>
<li>权限不够</li>
<li>范围限制</li>
<li>程序运行时配置错误</li>
</ul>
<p>探测这种情况的一种方式是，在你的 Deployment spec 中指定 <a href="https://github.com/kubernetes/kubernetes.github.io/blob/master/docs/concepts/workloads/controllers/deployment.md#progress-deadline-seconds" target="_blank"><code>spec.progressDeadlineSeconds</code></a>。<code>spec.progressDeadlineSeconds</code> 表示 Deployment controller 等待多少秒才能确定（通过 Deployment status）Deployment 进程是卡住的。</p>
<p>下面的 <code>kubectl</code> 命令设置 <code>progressDeadlineSeconds</code> 使 controller 在 Deployment 在进度卡住 10 分钟后报告：</p>
<pre><code class="lang-sh">$ kubectl patch deployment/nginx-deployment -p <span class="hljs-string">'{"spec":{"progressDeadlineSeconds":600}}'</span>
<span class="hljs-string">"nginx-deployment"</span> patched
</code></pre>
<p>当超过截止时间后，Deployment controller 会在 Deployment 的 <code>status.conditions</code> 中增加一条 DeploymentCondition，它包括如下属性：</p>
<ul>
<li>Type=Progressing</li>
<li>Status=False</li>
<li>Reason=ProgressDeadlineExceeded</li>
</ul>
<p>浏览 <a href="https://github.com/kubernetes/community/blob/master/contributors/devel/api-conventions.md#typical-status-properties" target="_blank">Kubernetes API conventions</a> 查看关于 status conditions 的更多信息。</p>
<p><strong>注意:</strong> kubernetes 除了报告 <code>Reason=ProgressDeadlineExceeded</code> 状态信息外不会对卡住的 Deployment 做任何操作。更高层次的协调器可以利用它并采取相应行动，例如，回滚 Deployment 到之前的版本。</p>
<p><strong> 注意：</strong> 如果你暂停了一个 Deployment，在暂停的这段时间内 kubernetnes 不会检查你指定的 deadline。你可以在 Deployment 的 rollout 途中安全的暂停它，然后再恢复它，这不会触发超过 deadline 的状态。</p>
<p>你可能在使用 Deployment 的时候遇到一些短暂的错误，这些可能是由于你设置了太短的 timeout，也有可能是因为各种其他错误导致的短暂错误。例如，假设你使用了无效的引用。当你 Describe Deployment 的时候可能会注意到如下信息：</p>
<pre><code class="lang-sh">$ kubectl describe deployment nginx-deployment
<...>
Conditions:
  Type            Status  Reason
  ----            ------  ------
  Available       True    MinimumReplicasAvailable
  Progressing     True    ReplicaSetUpdated
  ReplicaFailure  True    FailedCreate
<...>
</code></pre>
<p>执行 <code>kubectl get deployment nginx-deployment -o yaml</code>，Deployement 的状态可能看起来像这个样子：</p>
<pre><code class="lang-yaml"><span class="hljs-attr">status:</span>
<span class="hljs-attr">  availableReplicas:</span> <span class="hljs-number">2</span>
<span class="hljs-attr">  conditions:</span>
<span class="hljs-attr">  - lastTransitionTime:</span> <span class="hljs-number">2016</span><span class="hljs-bullet">-10</span><span class="hljs-bullet">-04</span>T12:<span class="hljs-number">25</span>:<span class="hljs-number">39</span>Z
<span class="hljs-attr">    lastUpdateTime:</span> <span class="hljs-number">2016</span><span class="hljs-bullet">-10</span><span class="hljs-bullet">-04</span>T12:<span class="hljs-number">25</span>:<span class="hljs-number">39</span>Z
<span class="hljs-attr">    message:</span> Replica set <span class="hljs-string">"nginx-deployment-4262182780"</span> is progressing.
<span class="hljs-attr">    reason:</span> ReplicaSetUpdated
<span class="hljs-attr">    status:</span> <span class="hljs-string">"True"</span>
<span class="hljs-attr">    type:</span> Progressing
<span class="hljs-attr">  - lastTransitionTime:</span> <span class="hljs-number">2016</span><span class="hljs-bullet">-10</span><span class="hljs-bullet">-04</span>T12:<span class="hljs-number">25</span>:<span class="hljs-number">42</span>Z
<span class="hljs-attr">    lastUpdateTime:</span> <span class="hljs-number">2016</span><span class="hljs-bullet">-10</span><span class="hljs-bullet">-04</span>T12:<span class="hljs-number">25</span>:<span class="hljs-number">42</span>Z
<span class="hljs-attr">    message:</span> Deployment has minimum availability.
<span class="hljs-attr">    reason:</span> MinimumReplicasAvailable
<span class="hljs-attr">    status:</span> <span class="hljs-string">"True"</span>
<span class="hljs-attr">    type:</span> Available
<span class="hljs-attr">  - lastTransitionTime:</span> <span class="hljs-number">2016</span><span class="hljs-bullet">-10</span><span class="hljs-bullet">-04</span>T12:<span class="hljs-number">25</span>:<span class="hljs-number">39</span>Z
<span class="hljs-attr">    lastUpdateTime:</span> <span class="hljs-number">2016</span><span class="hljs-bullet">-10</span><span class="hljs-bullet">-04</span>T12:<span class="hljs-number">25</span>:<span class="hljs-number">39</span>Z
<span class="hljs-attr">    message:</span> <span class="hljs-string">'Error creating: pods"nginx-deployment-4262182780-" is forbidden: exceeded quota:
      object-counts, requested: pods=1, used: pods=3, limited: pods=2'</span>
<span class="hljs-attr">    reason:</span> FailedCreate
<span class="hljs-attr">    status:</span> <span class="hljs-string">"True"</span>
<span class="hljs-attr">    type:</span> ReplicaFailure
<span class="hljs-attr">  observedGeneration:</span> <span class="hljs-number">3</span>
<span class="hljs-attr">  replicas:</span> <span class="hljs-number">2</span>
<span class="hljs-attr">  unavailableReplicas:</span> <span class="hljs-number">2</span>
</code></pre>
<p>最终，一旦超过 Deployment 进程的 deadline，kuberentes 会更新状态和导致 Progressing 状态的原因：</p>
<pre><code class="lang-sh">Conditions:
  Type            Status  Reason
  ----            ------  ------
  Available       True    MinimumReplicasAvailable
  Progressing     False   ProgressDeadlineExceeded
  ReplicaFailure  True    FailedCreate
</code></pre>
<p>你可以通过缩容 Deployment 的方式解决配额不足的问题，或者增加你的 namespace 的配额。如果你满足了配额条件后，Deployment controller 就会完成你的 Deployment rollout，你将看到 Deployment 的状态更新为成功状态（<code>Status=True</code> 并且 <code>Reason=NewReplicaSetAvailable</code>）。</p>
<pre><code class="lang-sh">Conditions:
  Type          Status  Reason
  ----          ------  ------
  Available     True    MinimumReplicasAvailable
  Progressing   True    NewReplicaSetAvailable
</code></pre>
<p><code>Type=Available</code>、 <code>Status=True</code> 意味着你的 Deployment 有最小可用性。 最小可用性是在 Deployment 策略中指定的参数。
<code>Type=Progressing</code> 、 <code>Status=True</code> 意味着你的 Deployment 或者在部署过程中，或者已经成功部署，达到了期望的最少的可用 replica 数量（查看特定状态的 Reason——在我们的例子中 <code>Reason=NewReplicaSetAvailable</code> 意味着 Deployment 已经完成）。</p>
<p>你可以使用 <code>kubectl rollout status</code> 命令查看 Deployment 进程是否失败。当 Deployment 过程超过了 deadline，<code>kubectl rollout status</code> 将返回非 0 的 exit code。</p>
<pre><code class="lang-sh">$ kubectl rollout status deploy/nginx
Waiting <span class="hljs-keyword">for</span> rollout to finish: 2 out of 3 new replicas have been updated...
error: deployment <span class="hljs-string">"nginx"</span> exceeded its progress deadline
$ <span class="hljs-built_in">echo</span> $?
1
</code></pre>
<h3 id="操作失败的-deployment">操作失败的 Deployment</h3>
<p>所有对完成的 Deployment 的操作都适用于失败的 Deployment。你可以对它扩／缩容，回退到历史版本，你甚至可以多次暂停它来应用 Deployment pod template。</p>
<h2 id="清理-policy">清理 Policy</h2>
<p>你可以设置 Deployment 中的 <code>.spec.revisionHistoryLimit</code> 项来指定保留多少旧的 ReplicaSet。 余下的将在后台被当作垃圾收集。默认的，所有的 revision 历史都会被保留。在未来的版本中，将会更改为 2。</p>
<p><strong>注意：</strong> 将该值设置为 0，将导致该 Deployment 的所有历史记录都被清除，也就无法回退了。</p>
<h2 id="用例">用例</h2>
<h3 id="canary-deployment">Canary Deployment</h3>
<p>如果你想要使用 Deployment 对部分用户或服务器发布 release，你可以创建多个 Deployment，每个对一个 release，参照 <a href="https://github.com/kubernetes/kubernetes.github.io/blob/master/docs/concepts/cluster-administration/manage-deployment.md#canary-deployments" target="_blank">managing resources</a> 中对 canary 模式的描述。</p>
<h2 id="编写-deployment-spec">编写 Deployment Spec</h2>
<p>在所有的 Kubernetes 配置中，Deployment 也需要 <code>apiVersion</code>，<code>kind</code> 和 <code>metadata</code> 这些配置项。配置文件的通用使用说明查看 <a href="https://kubernetes.io/docs/tasks/run-application/run-stateless-application-deployment/" target="_blank">部署应用</a>，配置容器，和<a href="https://github.com/kubernetes/kubernetes.github.io/blob/master/docs/tutorials/object-management-kubectl/object-management.md" target="_blank">使用 kubeclt 管理资源</a> 文档。</p>
<p>Deployment 也需要 <a href="https://github.com/kubernetes/community/blob/master/contributors/devel/api-conventions.md#spec-and-status" target="_blank"><code>.spec</code> section</a>.</p>
<h3 id="pod-template">Pod Template</h3>
<p> <code>.spec.template</code> 是 <code>.spec</code> 中唯一要求的字段。</p>
<p><code>.spec.template</code> 是 <a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/#pod-template" target="_blank">pod template</a>. 它跟 <a href="https://github.com/kubernetes/kubernetes.github.io/blob/master/docs/user-guide/pods" target="_blank">Pod</a> 有一模一样的 schema，除了它是嵌套的并且不需要 <code>apiVersion</code> 和 <code>kind</code> 字段。</p>
<p>另外为了划分 Pod 的范围，Deployment 中的 pod template 必须指定适当的 label（不要跟其他 controller 重复了，参考 <a href="https://github.com/kubernetes/kubernetes.github.io/blob/master/docs/concepts/workloads/controllers/deployment.md#selector" target="_blank">selector</a>）和适当的重启策略。</p>
<p><a href="https://github.com/kubernetes/kubernetes.github.io/blob/master/docs/concepts/workloads/pods/pod-lifecycle.md" target="_blank"><code>.spec.template.spec.restartPolicy</code></a> 可以设置为 <code>Always</code> , 如果不指定的话这就是默认配置。</p>
<h3 id="replicas">Replicas</h3>
<p><code>.spec.replicas</code> 是可以选字段，指定期望的 pod 数量，默认是 1。</p>
<h3 id="selector">Selector</h3>
<p><code>.spec.selector</code> 是可选字段，用来指定 <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/" target="_blank">label selector</a> ，圈定 Deployment 管理的 pod 范围。</p>
<p>如果被指定， <code>.spec.selector</code> 必须匹配 <code>.spec.template.metadata.labels</code>，否则它将被 API 拒绝。如果 <code>.spec.selector</code> 没有被指定， <code>.spec.selector.matchLabels</code> 默认是 <code>.spec.template.metadata.labels</code>。</p>
<p>在 Pod 的 template 跟 <code>.spec.template</code> 不同或者数量超过了 <code>.spec.replicas</code> 规定的数量的情况下，Deployment 会杀掉 label 跟 selector 不同的 Pod。</p>
<p><strong> 注意：</strong> 你不应该再创建其他 label 跟这个 selector 匹配的 pod，或者通过其他 Deployment，或者通过其他 Controller，例如 ReplicaSet 和 ReplicationController。否则该 Deployment 会被把它们当成都是自己创建的。Kubernetes 不会阻止你这么做。</p>
<p>如果你有多个 controller 使用了重复的 selector，controller 们就会互相打架并导致不正确的行为。</p>
<h3 id="策略">策略</h3>
<p><code>.spec.strategy</code> 指定新的 Pod 替换旧的 Pod 的策略。 <code>.spec.strategy.type</code> 可以是 "Recreate" 或者是 "RollingUpdate"。"RollingUpdate" 是默认值。</p>
<h4 id="recreate-deployment">Recreate Deployment</h4>
<p><code>.spec.strategy.type==Recreate</code> 时，在创建出新的 Pod 之前会先杀掉所有已存在的 Pod。</p>
<h4 id="rolling-update-deployment">Rolling Update Deployment</h4>
<p><code>.spec.strategy.type==RollingUpdate</code> 时，Deployment 使用 <a href="https://kubernetes.io/docs/tasks/run-application/rolling-update-replication-controller/" target="_blank">rolling update</a> 的方式更新 Pod 。你可以指定 <code>maxUnavailable</code> 和 <code>maxSurge</code> 来控制 rolling update 进程。</p>
<h5 id="max-unavailable">Max Unavailable</h5>
<p><code>.spec.strategy.rollingUpdate.maxUnavailable</code> 是可选配置项，用来指定在升级过程中不可用 Pod 的最大数量。该值可以是一个绝对值（例如 5），也可以是期望 Pod 数量的百分比（例如 10%）。通过计算百分比的绝对值向下取整。如果 <code>.spec.strategy.rollingUpdate.maxSurge</code> 为 0 时，这个值不可以为 0。默认值是 1。</p>
<p>例如，该值设置成 30%，启动 rolling update 后旧的 ReplicatSet 将会立即缩容到期望的 Pod 数量的 70%。新的 Pod ready 后，随着新的 ReplicaSet 的扩容，旧的 ReplicaSet 会进一步缩容，确保在升级的所有时刻可以用的 Pod 数量至少是期望 Pod 数量的 70%。</p>
<h5 id="max-surge">Max Surge</h5>
<p><code>.spec.strategy.rollingUpdate.maxSurge</code> 是可选配置项，用来指定可以超过期望的 Pod 数量的最大个数。该值可以是一个绝对值（例如 5）或者是期望的 Pod 数量的百分比（例如 10%）。当 <code>MaxUnavailable</code> 为 0 时该值不可以为 0。通过百分比计算的绝对值向上取整。默认值是 1。</p>
<p>例如，该值设置成 30%，启动 rolling update 后新的 ReplicatSet 将会立即扩容，新老 Pod 的总数不能超过期望的 Pod 数量的 130%。旧的 Pod 被杀掉后，新的 ReplicaSet 将继续扩容，旧的 ReplicaSet 会进一步缩容，确保在升级的所有时刻所有的 Pod 数量和不会超过期望 Pod 数量的 130%。</p>
<h3 id="progress-deadline-seconds">Progress Deadline Seconds</h3>
<p><code>.spec.progressDeadlineSeconds</code> 是可选配置项，用来指定在系统报告 Deployment 的 <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#failed-deployment" target="_blank">failed progressing</a> ——表现为 resource 的状态中 <code>type=Progressing</code>、<code>Status=False</code>、 <code>Reason=ProgressDeadlineExceeded</code> 前可以等待的 Deployment 进行的秒数。Deployment controller 会继续重试该 Deployment。未来，在实现了自动回滚后， deployment controller 在观察到这种状态时就会自动回滚。</p>
<p>如果设置该参数，该值必须大于 <code>.spec.minReadySeconds</code>。</p>
<h3 id="min-ready-seconds">Min Ready Seconds</h3>
<p><code>.spec.minReadySeconds</code> 是一个可选配置项，用来指定没有任何容器 crash 的 Pod 并被认为是可用状态的最小秒数。默认是 0（Pod 在 ready 后就会被认为是可用状态）。进一步了解什么时候 Pod 会被认为是 ready 状态，参阅 <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes" target="_blank">Container Probes</a>。</p>
<h3 id="rollback-to">Rollback To</h3>
<p><code>.spec.rollbackTo</code> 是一个可以选配置项，用来配置 Deployment 回退的配置。设置该参数将触发回退操作，每次回退完成后，该值就会被清除。</p>
<h4 id="revision">Revision</h4>
<p><code>.spec.rollbackTo.revision</code> 是一个可选配置项，用来指定回退到的 revision。默认是 0，意味着回退到上一个 revision。</p>
<h3 id="revision-history-limit">Revision History Limit</h3>
<p>Deployment revision history 存储在它控制的 ReplicaSets 中。</p>
<p><code>.spec.revisionHistoryLimit</code> 是一个可选配置项，用来指定可以保留的旧的 ReplicaSet 数量。该理想值取决于新 Deployment 的频率和稳定性。如果该值没有设置的话，默认所有旧的 Replicaset 或会被保留，将资源存储在 etcd 中，使用 <code>kubectl get rs</code> 查看输出。每个 Deployment 的该配置都保存在 ReplicaSet 中，然而，一旦你删除的旧的 RepelicaSet，你的 Deployment 就无法再回退到那个 revison 了。</p>
<p>如果你将该值设置为 0，所有具有 0 个 replica 的 ReplicaSet 都会被删除。在这种情况下，新的 Deployment rollout 无法撤销，因为 revision history 都被清理掉了。</p>
<h3 id="paused">Paused</h3>
<p><code>.spec.paused</code> 是可选配置项，boolean 值。用来指定暂停和恢复 Deployment。Paused 和非 paused 的 Deployment 之间的唯一区别就是，所有对 paused deployment 中的 PodTemplateSpec 的修改都不会触发新的 rollout。Deployment 被创建之后默认是非 paused。</p>
<h2 id="alternative-to-deployments">Alternative to Deployments</h2>
<h3 id="kubectl-rolling-update">kubectl rolling update</h3>
<p><a href="https://kubernetes.io/docs/tasks/run-application/rolling-update-replication-controller/" target="_blank">Kubectl rolling update</a> 虽然使用类似的方式更新 Pod 和 ReplicationController。但是我们推荐使用 Deployment，因为它是声明式的，客户端侧，具有附加特性，例如即使滚动升级结束后也可以回滚到任何历史版本。</p>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="ingress" class="level3">Ingress</h1>
<p>在本篇文章中你将会看到一些在其他地方被交叉使用的术语，为了防止产生歧义，我们首先来澄清下。</p>
<ul>
<li>节点：Kubernetes 集群中的服务器；</li>
<li>集群：Kubernetes 管理的一组服务器集合；</li>
<li>边界路由器：为局域网和 Internet 路由数据包的路由器，执行防火墙保护局域网络；</li>
<li>集群网络：遵循 Kubernetes<a href="https://kubernetes.io/docs/admin/networking/" target="_blank">网络模型</a> 实现集群内的通信的具体实现，比如 <a href="https://github.com/coreos/flannel#flannel" target="_blank">flannel</a> 和 <a href="https://github.com/openvswitch/ovn-kubernetes" target="_blank">OVS</a>。</li>
<li>服务：Kubernetes 的服务 (Service) 是使用标签选择器标识的一组 pod <a href="https://kubernetes.io/docs/user-guide/services/" target="_blank">Service</a>。 除非另有说明，否则服务的虚拟 IP 仅可在集群内部访问。</li>
</ul>
<h2 id="什么是-ingress？">什么是 Ingress？</h2>
<p>通常情况下，service 和 pod 的 IP 仅可在集群内部访问。集群外部的请求需要通过负载均衡转发到 service 在 Node 上暴露的 NodePort 上，然后再由 kube-proxy 通过边缘路由器 (edge router) 将其转发给相关的 Pod 或者丢弃。如下图所示</p>
<pre><code>   internet
        |
  ------------
  [Services]
</code></pre><p>而 Ingress 就是为进入集群的请求提供路由规则的集合，如下图所示</p>
<pre><code>    internet
        |
   [Ingress]
   --|-----|--
   [Services]
</code></pre><p>Ingress 可以给 service 提供集群外部访问的 URL、负载均衡、SSL 终止、HTTP 路由等。为了配置这些 Ingress 规则，集群管理员需要部署一个 <a href="../plugins/ingress.html">Ingress controller</a>，它监听 Ingress 和 service 的变化，并根据规则配置负载均衡并提供访问入口。</p>
<h2 id="ingress-格式">Ingress 格式</h2>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> extensions/v1beta1
<span class="hljs-attr">kind:</span> Ingress
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> test-ingress
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  rules:</span>
<span class="hljs-attr">  - http:</span>
<span class="hljs-attr">      paths:</span>
<span class="hljs-attr">      - path:</span> /testpath
<span class="hljs-attr">        backend:</span>
<span class="hljs-attr">          serviceName:</span> test
<span class="hljs-attr">          servicePort:</span> <span class="hljs-number">80</span>
</code></pre>
<p>每个 Ingress 都需要配置 <code>rules</code>，目前 Kubernetes 仅支持 http 规则。上面的示例表示请求 <code>/testpath</code> 时转发到服务 <code>test</code> 的 80 端口。</p>
<h2 id="api-版本对照表">API 版本对照表</h2>
<table>
<thead>
<tr>
<th>Kubernetes 版本</th>
<th>Extension 版本</th>
</tr>
</thead>
<tbody>
<tr>
<td>v1.5-v1.9</td>
<td>extensions/v1beta1</td>
</tr>
</tbody>
</table>
<h2 id="ingress-类型">Ingress 类型</h2>
<p>根据 Ingress Spec 配置的不同，Ingress 可以分为以下几种类型：</p>
<h3 id="单服务-ingress">单服务 Ingress</h3>
<p>单服务 Ingress 即该 Ingress 仅指定一个没有任何规则的后端服务。</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> extensions/v1beta1
<span class="hljs-attr">kind:</span> Ingress
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> test-ingress
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  backend:</span>
<span class="hljs-attr">    serviceName:</span> testsvc
<span class="hljs-attr">    servicePort:</span> <span class="hljs-number">80</span>
</code></pre>
<blockquote>
<p>注：单个服务还可以通过设置 <code>Service.Type=NodePort</code> 或者 <code>Service.Type=LoadBalancer</code> 来对外暴露。</p>
</blockquote>
<h3 id="多服务的-ingress">多服务的 Ingress</h3>
<p>路由到多服务的 Ingress 即根据请求路径的不同转发到不同的后端服务上，比如</p>
<pre><code>foo.bar.com -> 178.91.123.132 -> / foo    s1:80
                                 / bar    s2:80
</code></pre><p>可以通过下面的 Ingress 来定义：</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> extensions/v1beta1
<span class="hljs-attr">kind:</span> Ingress
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> test
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  rules:</span>
<span class="hljs-attr">  - host:</span> foo.bar.com
<span class="hljs-attr">    http:</span>
<span class="hljs-attr">      paths:</span>
<span class="hljs-attr">      - path:</span> /foo
<span class="hljs-attr">        backend:</span>
<span class="hljs-attr">          serviceName:</span> s1
<span class="hljs-attr">          servicePort:</span> <span class="hljs-number">80</span>
<span class="hljs-attr">      - path:</span> /bar
<span class="hljs-attr">        backend:</span>
<span class="hljs-attr">          serviceName:</span> s2
<span class="hljs-attr">          servicePort:</span> <span class="hljs-number">80</span>
</code></pre>
<p>使用 <code>kubectl create -f</code> 创建完 ingress 后：</p>
<pre><code class="lang-bash">$ kubectl get ing
NAME      RULE          BACKEND   ADDRESS
<span class="hljs-built_in">test</span>      -
          foo.bar.com
          /foo          s1:80
          /bar          s2:80
</code></pre>
<h3 id="虚拟主机-ingress">虚拟主机 Ingress</h3>
<p>虚拟主机 Ingress 即根据名字的不同转发到不同的后端服务上，而他们共用同一个的 IP 地址，如下所示</p>
<pre><code>foo.bar.com --|                 |-> foo.bar.com s1:80
              | 178.91.123.132  |
bar.foo.com --|                 |-> bar.foo.com s2:80
</code></pre><p>下面是一个基于 <a href="https://tools.ietf.org/html/rfc7230#section-5.4" target="_blank">Host header</a> 路由请求的 Ingress：</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> extensions/v1beta1
<span class="hljs-attr">kind:</span> Ingress
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> test
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  rules:</span>
<span class="hljs-attr">  - host:</span> foo.bar.com
<span class="hljs-attr">    http:</span>
<span class="hljs-attr">      paths:</span>
<span class="hljs-attr">      - backend:</span>
<span class="hljs-attr">          serviceName:</span> s1
<span class="hljs-attr">          servicePort:</span> <span class="hljs-number">80</span>
<span class="hljs-attr">  - host:</span> bar.foo.com
<span class="hljs-attr">    http:</span>
<span class="hljs-attr">      paths:</span>
<span class="hljs-attr">      - backend:</span>
<span class="hljs-attr">          serviceName:</span> s2
<span class="hljs-attr">          servicePort:</span> <span class="hljs-number">80</span>
</code></pre>
<blockquote>
<p>注：没有定义规则的后端服务称为默认后端服务，可以用来方便的处理 404 页面。</p>
</blockquote>
<h3 id="tls-ingress">TLS Ingress</h3>
<p>TLS Ingress 通过 Secret 获取 TLS 私钥和证书 (名为 <code>tls.crt</code> 和 <code>tls.key</code>)，来执行 TLS 终止。如果 Ingress 中的 TLS 配置部分指定了不同的主机，则它们将根据通过 SNI TLS 扩展指定的主机名（假如 Ingress controller 支持 SNI）在多个相同端口上进行复用。</p>
<p>定义一个包含 <code>tls.crt</code> 和 <code>tls.key</code> 的 secret：</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">data:</span>
  tls.crt: base64 encoded cert
  tls.key: base64 encoded key
<span class="hljs-attr">kind:</span> Secret
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> testsecret
<span class="hljs-attr">  namespace:</span> default
<span class="hljs-attr">type:</span> Opaque
</code></pre>
<p>Ingress 中引用 secret：</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> extensions/v1beta1
<span class="hljs-attr">kind:</span> Ingress
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> <span class="hljs-literal">no</span>-rules-map
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  tls:</span>
<span class="hljs-attr">    - secretName:</span> testsecret
<span class="hljs-attr">  backend:</span>
<span class="hljs-attr">    serviceName:</span> s1
<span class="hljs-attr">    servicePort:</span> <span class="hljs-number">80</span>
</code></pre>
<p>注意，不同 Ingress controller 支持的 TLS 功能不尽相同。 请参阅有关 <a href="https://github.com/kubernetes/ingress/blob/master/controllers/nginx/README.md#https" target="_blank">nginx</a>，<a href="https://github.com/kubernetes/ingress/blob/master/controllers/gce/README.md#tls" target="_blank">GCE</a> 或任何其他 Ingress controller 的文档，以了解 TLS 的支持情况。</p>
<h2 id="更新-ingress">更新 Ingress</h2>
<p>可以通过 <code>kubectl edit ing name</code> 的方法来更新 ingress：</p>
<pre><code class="lang-Bash">$ kubectl get ing
NAME      RULE          BACKEND   ADDRESS
<span class="hljs-built_in">test</span>      -                       178.91.123.132
          foo.bar.com
          /foo          s1:80
$ kubectl edit ing <span class="hljs-built_in">test</span>
</code></pre>
<p>这会弹出一个包含已有 IngressSpec yaml 文件的编辑器，修改并保存就会将其更新到 kubernetes API server，进而触发 Ingress Controller 重新配置负载均衡：</p>
<pre><code class="lang-yaml"><span class="hljs-attr">spec:</span>
<span class="hljs-attr">  rules:</span>
<span class="hljs-attr">  - host:</span> foo.bar.com
<span class="hljs-attr">    http:</span>
<span class="hljs-attr">      paths:</span>
<span class="hljs-attr">      - backend:</span>
<span class="hljs-attr">          serviceName:</span> s1
<span class="hljs-attr">          servicePort:</span> <span class="hljs-number">80</span>
<span class="hljs-attr">        path:</span> /foo
<span class="hljs-attr">  - host:</span> bar.baz.com
<span class="hljs-attr">    http:</span>
<span class="hljs-attr">      paths:</span>
<span class="hljs-attr">      - backend:</span>
<span class="hljs-attr">          serviceName:</span> s2
<span class="hljs-attr">          servicePort:</span> <span class="hljs-number">80</span>
<span class="hljs-attr">        path:</span> /foo
..
</code></pre>
<p>更新后：</p>
<pre><code class="lang-bash">$ kubectl get ing
NAME      RULE          BACKEND   ADDRESS
<span class="hljs-built_in">test</span>      -                       178.91.123.132
          foo.bar.com
          /foo          s1:80
          bar.baz.com
          /foo          s2:80
</code></pre>
<p>当然，也可以通过 <code>kubectl replace -f new-ingress.yaml</code> 命令来更新，其中 new-ingress.yaml 是修改过的 Ingress yaml。</p>
<h2 id="ingress-controller">Ingress Controller</h2>
<p>Ingress 正常工作需要集群中运行 Ingress Controller。Ingress Controller 与其他作为 kube-controller-manager 中的在集群创建时自动启动的 controller 成员不同，需要用户选择最适合自己集群的 Ingress Controller，或者自己实现一个。</p>
<p>Ingress Controller 以 Kubernetes Pod 的方式部署，以 daemon 方式运行，保持 watch Apiserver 的 /ingress 接口以更新 Ingress 资源，以满足 Ingress 的请求。比如可以使用 <a href="https://github.com/kubernetes/ingress-nginx" target="_blank">Nginx Ingress Controller</a>：</p>
<pre><code class="lang-sh">helm install stable/nginx-ingress --name nginx-ingress --set rbac.create=<span class="hljs-literal">true</span>
</code></pre>
<p>其他 Ingress Controller 还有：</p>
<ul>
<li><a href="../practice/service-discovery-lb/service-discovery-and-load-balancing.html">traefik ingress</a> 提供了一个 Traefik Ingress Controller 的实践案例</li>
<li><a href="https://github.com/kubernetes/ingress-nginx" target="_blank">kubernetes/ingress-nginx</a> 提供了一个详细的 Nginx Ingress Controller 示例</li>
<li><a href="https://github.com/kubernetes/ingress-gce" target="_blank">kubernetes/ingress-gce</a> 提供了一个用于 GCE 的 Ingress Controller 示例</li>
</ul>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://kubernetes.io/docs/concepts/services-networking/ingress/" target="_blank">Kubernetes Ingress Resource</a></li>
<li><a href="https://github.com/kubernetes/ingress/tree/master" target="_blank">Kubernetes Ingress Controller</a></li>
<li><a href="http://dockone.io/article/957" target="_blank">使用 NGINX Plus 负载均衡 Kubernetes 服务</a></li>
<li><a href="http://www.cnblogs.com/276815076/p/6407101.html" target="_blank">使用 NGINX 和 NGINX Plus 的 Ingress Controller 进行 Kubernetes 的负载均衡</a></li>
<li><a href="https://blog.osones.com/en/kubernetes-ingress-controller-with-traefik-and-lets-encrypt.html" target="_blank">Kubernetes : Ingress Controller with Træfɪk and Let's Encrypt</a></li>
<li><a href="https://blog.osones.com/en/kubernetes-traefik-and-lets-encrypt-at-scale.html" target="_blank">Kubernetes : Træfɪk and Let's Encrypt at scale</a></li>
<li><a href="https://docs.traefik.io/user-guide/kubernetes/" target="_blank">Kubernetes Ingress Controller-Træfɪk</a></li>
<li><a href="http://blog.kubernetes.io/2016/03/Kubernetes-1.2-and-simplifying-advanced-networking-with-Ingress.html" target="_blank">Kubernetes 1.2 and simplifying advanced networking with Ingress</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="job" class="level3">Job</h1>
<p>Job 负责批量处理短暂的一次性任务 (short lived one-off tasks)，即仅执行一次的任务，它保证批处理任务的一个或多个 Pod 成功结束。</p>
<h2 id="api-版本对照表">API 版本对照表</h2>
<table>
<thead>
<tr>
<th>Kubernetes 版本</th>
<th>Batch API 版本</th>
<th>默认开启</th>
</tr>
</thead>
<tbody>
<tr>
<td>v1.5+</td>
<td>batch/v1</td>
<td>是</td>
</tr>
</tbody>
</table>
<h2 id="job-类型">Job 类型</h2>
<p>Kubernetes 支持以下几种 Job：</p>
<ul>
<li>非并行 Job：通常创建一个 Pod 直至其成功结束</li>
<li>固定结束次数的 Job：设置 <code>.spec.completions</code>，创建多个 Pod，直到 <code>.spec.completions</code> 个 Pod 成功结束</li>
<li>带有工作队列的并行 Job：设置 <code>.spec.Parallelism</code> 但不设置 <code>.spec.completions</code>，当所有 Pod 结束并且至少一个成功时，Job 就认为是成功</li>
</ul>
<p>根据 <code>.spec.completions</code> 和 <code>.spec.Parallelism</code> 的设置，可以将 Job 划分为以下几种 pattern：</p>
<table>
<thead>
<tr>
<th>Job 类型</th>
<th>使用示例</th>
<th>行为</th>
<th>completions</th>
<th>Parallelism</th>
</tr>
</thead>
<tbody>
<tr>
<td>一次性 Job</td>
<td>数据库迁移</td>
<td>创建一个 Pod 直至其成功结束</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>固定结束次数的 Job</td>
<td>处理工作队列的 Pod</td>
<td>依次创建一个 Pod 运行直至 completions 个成功结束</td>
<td>2+</td>
<td>1</td>
</tr>
<tr>
<td>固定结束次数的并行 Job</td>
<td>多个 Pod 同时处理工作队列</td>
<td>依次创建多个 Pod 运行直至 completions 个成功结束</td>
<td>2+</td>
<td>2+</td>
</tr>
<tr>
<td>并行 Job</td>
<td>多个 Pod 同时处理工作队列</td>
<td>创建一个或多个 Pod 直至有一个成功结束</td>
<td>1</td>
<td>2+</td>
</tr>
</tbody>
</table>
<h2 id="job-controller">Job Controller</h2>
<p>Job Controller 负责根据 Job Spec 创建 Pod，并持续监控 Pod 的状态，直至其成功结束。如果失败，则根据 restartPolicy（只支持 OnFailure 和 Never，不支持 Always）决定是否创建新的 Pod 再次重试任务。</p>
<p><img src="images/job.png" alt=""/></p>
<h2 id="job-spec-格式">Job Spec 格式</h2>
<ul>
<li>spec.template 格式同 Pod</li>
<li>RestartPolicy 仅支持 Never 或 OnFailure</li>
<li>单个 Pod 时，默认 Pod 成功运行后 Job 即结束</li>
<li><code>.spec.completions</code> 标志 Job 结束需要成功运行的 Pod 个数，默认为 1</li>
<li><code>.spec.parallelism</code> 标志并行运行的 Pod 的个数，默认为 1</li>
<li><code>spec.activeDeadlineSeconds</code> 标志失败 Pod 的重试最大时间，超过这个时间不会继续重试</li>
</ul>
<p>一个简单的例子：</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> batch/v1
<span class="hljs-attr">kind:</span> Job
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> pi
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  template:</span>
<span class="hljs-attr">    metadata:</span>
<span class="hljs-attr">      name:</span> pi
<span class="hljs-attr">    spec:</span>
<span class="hljs-attr">      containers:</span>
<span class="hljs-attr">      - name:</span> pi
<span class="hljs-attr">        image:</span> perl
<span class="hljs-attr">        command:</span> [<span class="hljs-string">"perl"</span>,  <span class="hljs-string">"-Mbignum=bpi"</span>, <span class="hljs-string">"-wle"</span>, <span class="hljs-string">"print bpi(2000)"</span>]
<span class="hljs-attr">      restartPolicy:</span> Never
</code></pre>
<pre><code class="lang-sh"><span class="hljs-comment"># 创建 Job</span>
$ kubectl create <span class="hljs-_">-f</span> ./job.yaml
job <span class="hljs-string">"pi"</span> created
<span class="hljs-comment"># 查看 Job 的状态</span>
$ kubectl describe job pi
Name:        pi
Namespace:    default
Selector:    controller-uid=<span class="hljs-built_in">cd</span>37a621-5b02-11e7-b56e-76933ddd7f55
Labels:        controller-uid=<span class="hljs-built_in">cd</span>37a621-5b02-11e7-b56e-76933ddd7f55
        job-name=pi
Annotations:    <none>
Parallelism:    1
Completions:    1
Start Time:    Tue, 27 Jun 2017 14:35:24 +0800
Pods Statuses:    0 Running / 1 Succeeded / 0 Failed
Pod Template:
  Labels:    controller-uid=<span class="hljs-built_in">cd</span>37a621-5b02-11e7-b56e-76933ddd7f55
        job-name=pi
  Containers:
   pi:
    Image:    perl
    Port:
    Command:
      perl
      -Mbignum=bpi
      -wle
      <span class="hljs-built_in">print</span> bpi(2000)
    Environment:    <none>
    Mounts:        <none>
  Volumes:        <none>
Events:
  FirstSeen    LastSeen    Count    From        SubObjectPath    Type        Reason            Message
  ---------    --------    -----    ----        -------------    --------    ------            -------
  2m        2m        1    job-controller            Normal        SuccessfulCreate    Created pod: pi-nltxv

<span class="hljs-comment"># 使用'job-name=pi'标签查询属于该 Job 的 Pod</span>
<span class="hljs-comment"># 注意不要忘记'--show-all'选项显示已经成功（或失败）的 Pod</span>
$ kubectl get pod --show-all <span class="hljs-_">-l</span> job-name=pi
NAME       READY     STATUS      RESTARTS   AGE
pi-nltxv   0/1       Completed   0          3m

<span class="hljs-comment"># 使用 jsonpath 获取 pod ID 并查看 Pod 的日志</span>
$ pods=$(kubectl get pods --selector=job-name=pi --output=jsonpath={.items..metadata.name})
$ kubectl logs <span class="hljs-variable">$pods</span>
3.141592653589793238462643383279502...
</code></pre>
<p>固定结束次数的 Job 示例</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> batch/v1
<span class="hljs-attr">kind:</span> Job
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> busybox
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  completions:</span> <span class="hljs-number">3</span>
<span class="hljs-attr">  template:</span>
<span class="hljs-attr">    metadata:</span>
<span class="hljs-attr">      name:</span> busybox
<span class="hljs-attr">    spec:</span>
<span class="hljs-attr">      containers:</span>
<span class="hljs-attr">      - name:</span> busybox
<span class="hljs-attr">        image:</span> busybox
<span class="hljs-attr">        command:</span> [<span class="hljs-string">"echo"</span>, <span class="hljs-string">"hello"</span>]
<span class="hljs-attr">      restartPolicy:</span> Never
</code></pre>
<h2 id="bare-pods">Bare Pods</h2>
<p>所谓 Bare Pods 是指直接用 PodSpec 来创建的 Pod（即不在 ReplicaSets 或者 ReplicationCtroller 的管理之下的 Pods）。这些 Pod 在 Node 重启后不会自动重启，但 Job 则会创建新的 Pod 继续任务。所以，推荐使用 Job 来替代 Bare Pods，即便是应用只需要一个 Pod。</p>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/" target="_blank">Jobs - Run to Completion</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="本地数据卷" class="level3">LocalVolume</h1>
<blockquote>
<p>注意：仅在 v1.7 + 中支持，并从 v1.10 开始升级为 beta 版本。</p>
</blockquote>
<p>本地数据卷（Local Volume）代表一个本地存储设备，比如磁盘、分区或者目录等。主要的应用场景包括分布式存储和数据库等需要高性能和高可靠性的环境里。本地数据卷同时支持块设备和文件系统，通过 <code>spec.local.path</code> 指定；但对于文件系统来说，kubernetes 并不会限制该目录可以使用的存储空间大小。</p>
<p>本地数据卷只能以静态创建的 PV 使用。相对于 <a href="volume.html#hostPath">HostPath</a>，本地数据卷可以直接以持久化的方式使用（它总是通过 NodeAffinity 调度在某个指定的节点上）。</p>
<p>另外，社区还提供了一个 <a href="https://github.com/kubernetes-incubator/external-storage/tree/master/local-volume/provisioner" target="_blank">local-volume-provisioner</a>，用于自动创建和清理本地数据卷。</p>
<h2 id="示例">示例</h2>
<p>StorageClass</p>
<pre><code class="lang-yaml"><span class="hljs-attr">kind:</span> StorageClass
<span class="hljs-attr">apiVersion:</span> storage.k8s.io/v1
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> local-storage
<span class="hljs-attr">provisioner:</span> kubernetes.io/<span class="hljs-literal">no</span>-provisioner
<span class="hljs-attr">volumeBindingMode:</span> WaitForFirstConsumer
</code></pre>
<p>创建一个调度到 hostname 为 <code>example-node</code> 的本地数据卷：</p>
<pre><code class="lang-yaml"><span class="hljs-comment"># For kubernetes v1.10</span>
<span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> PersistentVolume
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> example-local-pv
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  capacity:</span>
<span class="hljs-attr">    storage:</span> <span class="hljs-number">100</span>Gi
<span class="hljs-attr">  accessModes:</span>
<span class="hljs-bullet">  -</span> ReadWriteOnce
<span class="hljs-attr">  persistentVolumeReclaimPolicy:</span> Delete
<span class="hljs-attr">  storageClassName:</span> local-storage
<span class="hljs-attr">  local:</span>
<span class="hljs-attr">    path:</span> /mnt/disks/ssd1
<span class="hljs-attr">  nodeAffinity:</span>
<span class="hljs-attr">    required:</span>
<span class="hljs-attr">      nodeSelectorTerms:</span>
<span class="hljs-attr">      - matchExpressions:</span>
<span class="hljs-attr">        - key:</span> kubernetes.io/hostname
<span class="hljs-attr">          operator:</span> In
<span class="hljs-attr">          values:</span>
<span class="hljs-bullet">          -</span> example-node
</code></pre>
<pre><code class="lang-yaml"><span class="hljs-comment"># For kubernetes v1.7-1.9</span>
<span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> PersistentVolume
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> example-local-pv
<span class="hljs-attr">  annotations:</span>
    <span class="hljs-string">"volume.alpha.kubernetes.io/node-affinity"</span>: <span class="hljs-string">'{
      "requiredDuringSchedulingIgnoredDuringExecution": {
        "nodeSelectorTerms": [
          { "matchExpressions": [
            { "key": "kubernetes.io/hostname",
              "operator": "In",
              "values": ["example-node"]
            }
          ]}
         ]}
        }'</span>,
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  capacity:</span>
<span class="hljs-attr">    storage:</span> <span class="hljs-number">5</span>Gi
<span class="hljs-attr">  accessModes:</span>
<span class="hljs-bullet">  -</span> ReadWriteOnce
<span class="hljs-attr">  persistentVolumeReclaimPolicy:</span> Delete
<span class="hljs-attr">  storageClassName:</span> local-storage
<span class="hljs-attr">  local:</span>
<span class="hljs-attr">    path:</span> /mnt/disks/ssd1
</code></pre>
<p>创建 PVC：</p>
<pre><code class="lang-yaml"><span class="hljs-attr">kind:</span> PersistentVolumeClaim
<span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> example-local-claim
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  accessModes:</span>
<span class="hljs-bullet">  -</span> ReadWriteOnce
<span class="hljs-attr">  resources:</span>
<span class="hljs-attr">    requests:</span>
<span class="hljs-attr">      storage:</span> <span class="hljs-number">5</span>Gi
<span class="hljs-attr">  storageClassName:</span> local-storage
</code></pre>
<p>创建 Pod，引用 PVC：</p>
<pre><code class="lang-yaml"><span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> mypod
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">    - name:</span> myfrontend
<span class="hljs-attr">      image:</span> nginx
<span class="hljs-attr">      volumeMounts:</span>
<span class="hljs-attr">      - mountPath:</span> <span class="hljs-string">"/var/www/html"</span>
<span class="hljs-attr">        name:</span> mypd
<span class="hljs-attr">  volumes:</span>
<span class="hljs-attr">    - name:</span> mypd
<span class="hljs-attr">      persistentVolumeClaim:</span>
<span class="hljs-attr">        claimName:</span> example-local-claim
</code></pre>
<h2 id="限制">限制</h2>
<ul>
<li>暂不支持一个 Pod 绑定多个本地数据卷的 PVC（计划 v1.9 支持）</li>
<li>有可能导致调度冲突，比如 CPU 或者内存资源不足（计划 v1.9 增强）</li>
<li>外部 Provisoner 在启动后无法正确检测挂载点的空间大小（需要 Mount Propagation，计划 v1.9 支持）</li>
</ul>
<h2 id="最佳实践">最佳实践</h2>
<ul>
<li>推荐为每个存储卷分配独立的磁盘，以便隔离 IO 请求</li>
<li>推荐为每个存储卷分配独立的分区，以便隔离存储空间</li>
<li>避免重新创建同名的 Node，否则会导致新 Node 无法识别已绑定旧 Node 的 PV</li>
<li>推荐使用 UUID 而不是文件路径，以避免文件路径误配的问题</li>
<li>对于不带文件系统的块存储，推荐使用唯一 ID（如 <code>/dev/disk/by-id/</code>），以避免块设备路径误配的问题</li>
</ul>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://github.com/kubernetes-incubator/external-storage/tree/master/local-volume" target="_blank">Local Persistent Storage User Guide</a> </li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="namespace" class="level3">Namespace</h1>
<p>Namespace 是对一组资源和对象的抽象集合，比如可以用来将系统内部的对象划分为不同的项目组或用户组。常见的 pod, service, replication controller 和 deployment 等都是属于某一个 namespace 的（默认是 default），而 node, persistent volume，namespace 等资源则不属于任何 namespace。</p>
<p>Namespace 常用来隔离不同的用户，比如 Kubernetes 自带的服务一般运行在 <code>kube-system</code> namespace 中。</p>
<h2 id="namespace-操作">Namespace 操作</h2>
<blockquote>
<p><code>kubectl</code> 可以通过 <code>--namespace</code> 或者 <code>-n</code> 选项指定 namespace。如果不指定，默认为 default。查看操作下, 也可以通过设置 --all-namespace=true 来查看所有 namespace 下的资源。</p>
</blockquote>
<h3 id="查询">查询</h3>
<pre><code class="lang-sh">$ kubectl get namespaces
NAME          STATUS    AGE
default       Active    11d
kube-system   Active    11d
</code></pre>
<p>注意：namespace 包含两种状态 "Active" 和 "Terminating"。在 namespace 删除过程中，namespace 状态被设置成 "Terminating"。</p>
<h3 id="创建">创建</h3>
<pre><code class="lang-sh">(1) 命令行直接创建
$ kubectl create namespace new-namespace

(2) 通过文件创建
$ cat my-namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: new-namespace

$ kubectl create <span class="hljs-_">-f</span> ./my-namespace.yaml
</code></pre>
<p>注意：命名空间名称满足正则表达式 <code>[a-z0-9]([-a-z0-9]*[a-z0-9])?</code>, 最大长度为 63 位</p>
<h3 id="删除">删除</h3>
<pre><code class="lang-sh">$ kubectl delete namespaces new-namespace
</code></pre>
<p>注意：</p>
<ol>
<li>删除一个 namespace 会自动删除所有属于该 namespace 的资源。</li>
<li><code>default</code> 和 <code>kube-system</code> 命名空间不可删除。</li>
<li>PersistentVolume 是不属于任何 namespace 的，但 PersistentVolumeClaim 是属于某个特定 namespace 的。</li>
<li>Event 是否属于 namespace 取决于产生 event 的对象。</li>
<li>v1.7 版本增加了 <code>kube-public</code> 命名空间，该命名空间用来存放公共的信息，一般以 ConfigMap 的形式存放。</li>
</ol>
<pre><code class="lang-sh">$ kubectl get configmap  -n=kube-public
NAME           DATA      AGE
cluster-info   2         29d
</code></pre>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/" target="_blank">Kubernetes Namespace</a></li>
<li><a href="https://kubernetes.io/docs/tasks/administer-cluster/namespaces/" target="_blank">Share a Cluster with Namespaces</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="network-policy" class="level3">NetworkPolicy</h1>
<p>随着微服务的流行，越来越多的云服务平台需要大量模块之间的网络调用。Kubernetes 在 1.3 引入了 Network Policy，Network Policy 提供了基于策略的网络控制，用于隔离应用并减少攻击面。它使用标签选择器模拟传统的分段网络，并通过策略控制它们之间的流量以及来自外部的流量。</p>
<p>在使用 Network Policy 时，需要注意</p>
<ul>
<li>v1.6 以及以前的版本需要在 kube-apiserver 中开启 <code>extensions/v1beta1/networkpolicies</code></li>
<li>v1.7 版本 Network Policy 已经 GA，API 版本为 <code>networking.k8s.io/v1</code></li>
<li>v1.8 版本新增 <strong>Egress</strong> 和 <strong>IPBlock</strong> 的支持</li>
<li>网络插件要支持 Network Policy，如 Calico、Romana、Weave Net 和 trireme 等，参考 <a href="../plugins/network-policy.html">这里</a></li>
</ul>
<h2 id="api-版本对照表">API 版本对照表</h2>
<table>
<thead>
<tr>
<th>Kubernetes 版本</th>
<th>Networking API 版本</th>
</tr>
</thead>
<tbody>
<tr>
<td>v1.5-v1.6</td>
<td>extensions/v1beta1</td>
</tr>
<tr>
<td>v1.7+</td>
<td>networking.k8s.io/v1</td>
</tr>
</tbody>
</table>
<h2 id="网络策略">网络策略</h2>
<h3 id="namespace-隔离">Namespace 隔离</h3>
<p>默认情况下，所有 Pod 之间是全通的。每个 Namespace 可以配置独立的网络策略，来隔离 Pod 之间的流量。</p>
<p>v1.7 + 版本通过创建匹配所有 Pod 的 Network Policy 来作为默认的网络策略，比如默认拒绝所有 Pod 之间 Ingress 通信</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> networking.k8s.io/v1
<span class="hljs-attr">kind:</span> NetworkPolicy
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> default-deny
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  podSelector:</span> {}
<span class="hljs-attr">  policyTypes:</span>
<span class="hljs-bullet">  -</span> Ingress
</code></pre>
<p>默认拒绝所有 Pod 之间 Egress 通信的策略为</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> networking.k8s.io/v1
<span class="hljs-attr">kind:</span> NetworkPolicy
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> default-deny
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  podSelector:</span> {}
<span class="hljs-attr">  policyTypes:</span>
<span class="hljs-bullet">  -</span> Egress
</code></pre>
<p>甚至是默认拒绝所有 Pod 之间 Ingress 和 Egress 通信的策略为</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> networking.k8s.io/v1
<span class="hljs-attr">kind:</span> NetworkPolicy
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> default-deny
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  podSelector:</span> {}
<span class="hljs-attr">  policyTypes:</span>
<span class="hljs-bullet">  -</span> Ingress
<span class="hljs-bullet">  -</span> Egress
</code></pre>
<p>而默认允许所有 Pod 之间 Ingress 通信的策略为</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> networking.k8s.io/v1
<span class="hljs-attr">kind:</span> NetworkPolicy
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> allow-all
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  podSelector:</span> {}
<span class="hljs-attr">  ingress:</span>
<span class="hljs-bullet">  -</span> {}
</code></pre>
<p>默认允许所有 Pod 之间 Egress 通信的策略为</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> networking.k8s.io/v1
<span class="hljs-attr">kind:</span> NetworkPolicy
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> allow-all
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  podSelector:</span> {}
<span class="hljs-attr">  egress:</span>
<span class="hljs-bullet">  -</span> {}
</code></pre>
<p>而 v1.6 版本则通过 Annotation 来隔离 namespace 的所有 Pod 之间的流量，包括从外部到该 namespace 中所有 Pod 的流量以及 namespace 内部 Pod 相互之间的流量：</p>
<pre><code class="lang-sh">kubectl annotate ns <namespace> <span class="hljs-string">"net.beta.kubernetes.io/network-policy={\"ingress\": {\"isolation\": \"DefaultDeny\"}}"</span>
</code></pre>
<h3 id="pod-隔离">Pod 隔离</h3>
<p>通过使用标签选择器（包括 namespaceSelector 和 podSelector）来控制 Pod 之间的流量。比如下面的 Network Policy</p>
<ul>
<li>允许 default namespace 中带有 <code>role=frontend</code> 标签的 Pod 访问 default namespace 中带有 <code>role=db</code> 标签 Pod 的 6379 端口</li>
<li>允许带有 <code>project=myprojects</code> 标签的 namespace 中所有 Pod 访问 default namespace 中带有 <code>role=db</code> 标签 Pod 的 6379 端口</li>
</ul>
<pre><code class="lang-yaml"><span class="hljs-comment"># v1.6 以及更老的版本应该使用 extensions/v1beta1</span>
<span class="hljs-comment"># apiVersion: extensions/v1beta1</span>
<span class="hljs-attr">apiVersion:</span> networking.k8s.io/v1
<span class="hljs-attr">kind:</span> NetworkPolicy
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> test-network-policy
<span class="hljs-attr">  namespace:</span> default
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  podSelector:</span>
<span class="hljs-attr">    matchLabels:</span>
<span class="hljs-attr">      role:</span> db
<span class="hljs-attr">  ingress:</span>
<span class="hljs-attr">  - from:</span>
<span class="hljs-attr">    - namespaceSelector:</span>
<span class="hljs-attr">        matchLabels:</span>
<span class="hljs-attr">          project:</span> myproject
<span class="hljs-attr">    - podSelector:</span>
<span class="hljs-attr">        matchLabels:</span>
<span class="hljs-attr">          role:</span> frontend
<span class="hljs-attr">    ports:</span>
<span class="hljs-attr">    - protocol:</span> tcp
<span class="hljs-attr">      port:</span> <span class="hljs-number">6379</span>
</code></pre>
<p>另外一个同时开启 Ingress 和 Egress 通信的策略为</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> networking.k8s.io/v1
<span class="hljs-attr">kind:</span> NetworkPolicy
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> test-network-policy
<span class="hljs-attr">  namespace:</span> default
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  podSelector:</span>
<span class="hljs-attr">    matchLabels:</span>
<span class="hljs-attr">      role:</span> db
<span class="hljs-attr">  policyTypes:</span>
<span class="hljs-bullet">  -</span> Ingress
<span class="hljs-bullet">  -</span> Egress
<span class="hljs-attr">  ingress:</span>
<span class="hljs-attr">  - from:</span>
<span class="hljs-attr">    - ipBlock:</span>
<span class="hljs-attr">        cidr:</span> <span class="hljs-number">172.17</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span>/<span class="hljs-number">16</span>
<span class="hljs-attr">        except:</span>
<span class="hljs-bullet">        -</span> <span class="hljs-number">172.17</span><span class="hljs-number">.1</span><span class="hljs-number">.0</span>/<span class="hljs-number">24</span>
<span class="hljs-attr">    - namespaceSelector:</span>
<span class="hljs-attr">        matchLabels:</span>
<span class="hljs-attr">          project:</span> myproject
<span class="hljs-attr">    - podSelector:</span>
<span class="hljs-attr">        matchLabels:</span>
<span class="hljs-attr">          role:</span> frontend
<span class="hljs-attr">    ports:</span>
<span class="hljs-attr">    - protocol:</span> TCP
<span class="hljs-attr">      port:</span> <span class="hljs-number">6379</span>
<span class="hljs-attr">  egress:</span>
<span class="hljs-attr">  - to:</span>
<span class="hljs-attr">    - ipBlock:</span>
<span class="hljs-attr">        cidr:</span> <span class="hljs-number">10.0</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span>/<span class="hljs-number">24</span>
<span class="hljs-attr">    ports:</span>
<span class="hljs-attr">    - protocol:</span> TCP
<span class="hljs-attr">      port:</span> <span class="hljs-number">5978</span>
</code></pre>
<p>它用来隔离 default namespace 中带有 <code>role=db</code> 标签的 Pod：</p>
<ul>
<li>允许 default namespace 中带有 <code>role=frontend</code> 标签的 Pod 访问 default namespace 中带有 <code>role=db</code> 标签 Pod 的 6379 端口</li>
<li>允许带有 <code>project=myprojects</code> 标签的 namespace 中所有 Pod 访问 default namespace 中带有 <code>role=db</code> 标签 Pod 的 6379 端口</li>
<li>允许 default namespace 中带有 <code>role=db</code> 标签的 Pod 访问 <code>10.0.0.0/24</code> 网段的 TCP 5987 端口</li>
</ul>
<h2 id="简单示例">简单示例</h2>
<p>以 calico 为例看一下 Network Policy 的具体用法。</p>
<p>首先配置 kubelet 使用 CNI 网络插件</p>
<pre><code class="lang-sh">kubelet --network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin ...
</code></pre>
<p>安装 calio 网络插件</p>
<pre><code class="lang-sh"><span class="hljs-comment"># 注意修改 CIDR，需要跟 k8s pod-network-cidr 一致，默认为 192.168.0.0/16</span>
kubectl apply <span class="hljs-_">-f</span> https://docs.projectcalico.org/v3.0/getting-started/kubernetes/installation/hosted/kubeadm/1.7/calico.yaml
</code></pre>
<p>首先部署一个 nginx 服务</p>
<pre><code class="lang-sh">$ kubectl run nginx --image=nginx --replicas=2
deployment <span class="hljs-string">"nginx"</span> created
$ kubectl expose deployment nginx --port=80
service <span class="hljs-string">"nginx"</span> exposed
</code></pre>
<p>此时，通过其他 Pod 是可以访问 nginx 服务的</p>
<pre><code class="lang-sh">$ kubectl get svc,pod
NAME                        CLUSTER-IP    EXTERNAL-IP   PORT(S)    AGE
svc/kubernetes              10.100.0.1    <none>        443/TCP    46m
svc/nginx                   10.100.0.16   <none>        80/TCP     33s

NAME                        READY         STATUS        RESTARTS   AGE
po/nginx-701339712<span class="hljs-_">-e</span>0qfq    1/1           Running       0          35s
po/nginx-701339712-o00ef    1/1           Running       0

$ kubectl run busybox --rm -ti --image=busybox /bin/sh
Waiting <span class="hljs-keyword">for</span> pod default/busybox-472357175-y0m47 to be running, status is Pending, pod ready: <span class="hljs-literal">false</span>

Hit enter <span class="hljs-keyword">for</span> <span class="hljs-built_in">command</span> prompt

/ <span class="hljs-comment"># wget --spider --timeout=1 nginx</span>
Connecting to nginx (10.100.0.16:80)
/ <span class="hljs-comment">#</span>
</code></pre>
<p>开启 default namespace 的 DefaultDeny Network Policy 后，其他 Pod（包括 namespace 外部）不能访问 nginx 了：</p>
<pre><code class="lang-sh">$ cat default-deny.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny
spec:
  podSelector: {}
  policyTypes:
  - Ingress

$ kubectl create <span class="hljs-_">-f</span> default-deny.yaml

$ kubectl run busybox --rm -ti --image=busybox /bin/sh
Waiting <span class="hljs-keyword">for</span> pod default/busybox-472357175-y0m47 to be running, status is Pending, pod ready: <span class="hljs-literal">false</span>

Hit enter <span class="hljs-keyword">for</span> <span class="hljs-built_in">command</span> prompt

/ <span class="hljs-comment"># wget --spider --timeout=1 nginx</span>
Connecting to nginx (10.100.0.16:80)
wget: download timed out
/ <span class="hljs-comment">#</span>
</code></pre>
<p>最后再创建一个运行带有 <code>access=true</code> 的 Pod 访问的网络策略</p>
<pre><code class="lang-sh">$ cat nginx-policy.yaml
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: access-nginx
spec:
  podSelector:
    matchLabels:
      run: nginx
  ingress:
  - from:
    - podSelector:
        matchLabels:
          access: <span class="hljs-string">"true"</span>

$ kubectl create <span class="hljs-_">-f</span> nginx-policy.yaml
networkpolicy <span class="hljs-string">"access-nginx"</span> created

<span class="hljs-comment"># 不带 access=true 标签的 Pod 还是无法访问 nginx 服务</span>
$ kubectl run busybox --rm -ti --image=busybox /bin/sh
Waiting <span class="hljs-keyword">for</span> pod default/busybox-472357175-y0m47 to be running, status is Pending, pod ready: <span class="hljs-literal">false</span>

Hit enter <span class="hljs-keyword">for</span> <span class="hljs-built_in">command</span> prompt

/ <span class="hljs-comment"># wget --spider --timeout=1 nginx</span>
Connecting to nginx (10.100.0.16:80)
wget: download timed out
/ <span class="hljs-comment">#</span>


<span class="hljs-comment"># 而带有 access=true 标签的 Pod 可以访问 nginx 服务</span>
$ kubectl run busybox --rm -ti --labels=<span class="hljs-string">"access=true"</span> --image=busybox /bin/sh
Waiting <span class="hljs-keyword">for</span> pod default/busybox-472357175-y0m47 to be running, status is Pending, pod ready: <span class="hljs-literal">false</span>

Hit enter <span class="hljs-keyword">for</span> <span class="hljs-built_in">command</span> prompt

/ <span class="hljs-comment"># wget --spider --timeout=1 nginx</span>
Connecting to nginx (10.100.0.16:80)
/ <span class="hljs-comment">#</span>
</code></pre>
<p>最后开启 nginx 服务的外部访问：</p>
<pre><code class="lang-sh">$ cat nginx-external-policy.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: front-end-access
  namespace: sock-shop
spec:
  podSelector:
    matchLabels:
      run: nginx
  ingress:
    - ports:
        - protocol: TCP
          port: 80

$ kubectl create <span class="hljs-_">-f</span> nginx-external-policy.yaml
</code></pre>
<h2 id="使用场景">使用场景</h2>
<h3 id="禁止访问指定服务">禁止访问指定服务</h3>
<pre><code class="lang-sh">kubectl run web --image=nginx --labels app=web,env=prod --expose --port 80
</code></pre>
<p><img src="images/15022447799137.jpg" alt=""/></p>
<p>网络策略</p>
<pre><code class="lang-yaml"><span class="hljs-attr">kind:</span> NetworkPolicy
<span class="hljs-attr">apiVersion:</span> networking.k8s.io/v1
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> web-deny-all
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  podSelector:</span>
<span class="hljs-attr">    matchLabels:</span>
<span class="hljs-attr">      app:</span> web
<span class="hljs-attr">      env:</span> prod
</code></pre>
<h3 id="只允许指定-pod-访问服务">只允许指定 Pod 访问服务</h3>
<pre><code class="lang-sh">kubectl run apiserver --image=nginx --labels app=bookstore,role=api --expose --port 80
</code></pre>
<p><img src="images/15022448622429.jpg" alt=""/></p>
<p>网络策略</p>
<pre><code class="lang-yaml"><span class="hljs-attr">kind:</span> NetworkPolicy
<span class="hljs-attr">apiVersion:</span> networking.k8s.io/v1
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> api-allow
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  podSelector:</span>
<span class="hljs-attr">    matchLabels:</span>
<span class="hljs-attr">      app:</span> bookstore
<span class="hljs-attr">      role:</span> api
<span class="hljs-attr">  ingress:</span>
<span class="hljs-attr">  - from:</span>
<span class="hljs-attr">      - podSelector:</span>
<span class="hljs-attr">          matchLabels:</span>
<span class="hljs-attr">            app:</span> bookstore
</code></pre>
<h3 id="禁止-namespace-中所有-pod-之间的相互访问">禁止 namespace 中所有 Pod 之间的相互访问</h3>
<p><img src="images/15022451724392.gif" alt=""/></p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> networking.k8s.io/v1
<span class="hljs-attr">kind:</span> NetworkPolicy
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> default-deny
<span class="hljs-attr">  namespace:</span> default
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  podSelector:</span> {}
</code></pre>
<h3 id="禁止其他-namespace-访问服务">禁止其他 namespace 访问服务</h3>
<pre><code class="lang-sh">kubectl create namespace secondary
kubectl run web --namespace secondary --image=nginx \
    --labels=app=web --expose --port 80
</code></pre>
<p><img src="images/15022452203435.gif" alt=""/></p>
<p>网络策略</p>
<pre><code class="lang-yaml"><span class="hljs-attr">kind:</span> NetworkPolicy
<span class="hljs-attr">apiVersion:</span> networking.k8s.io/v1
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  namespace:</span> secondary
<span class="hljs-attr">  name:</span> web-deny-other-namespaces
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  podSelector:</span>
<span class="hljs-attr">    matchLabels:</span>
<span class="hljs-attr">  ingress:</span>
<span class="hljs-attr">  - from:</span>
<span class="hljs-attr">    - podSelector:</span> {}
</code></pre>
<h3 id="只允许指定-namespace-访问服务">只允许指定 namespace 访问服务</h3>
<pre><code class="lang-sh">kubectl run web --image=nginx \
    --labels=app=web --expose --port 80
</code></pre>
<p><img src="images/15022453441751.gif" alt=""/></p>
<p>网络策略</p>
<pre><code class="lang-yaml"><span class="hljs-attr">kind:</span> NetworkPolicy
<span class="hljs-attr">apiVersion:</span> networking.k8s.io/v1
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> web-allow-prod
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  podSelector:</span>
<span class="hljs-attr">    matchLabels:</span>
<span class="hljs-attr">      app:</span> web
<span class="hljs-attr">  ingress:</span>
<span class="hljs-attr">  - from:</span>
<span class="hljs-attr">    - namespaceSelector:</span>
<span class="hljs-attr">        matchLabels:</span>
<span class="hljs-attr">          purpose:</span> production
</code></pre>
<h3 id="允许外网访问服务">允许外网访问服务</h3>
<pre><code class="lang-sh">kubectl run web --image=nginx --labels=app=web --port 80
kubectl expose deployment/web --type=LoadBalancer
</code></pre>
<p><img src="images/15022454444461.gif" alt=""/></p>
<p>网络策略</p>
<pre><code class="lang-yaml"><span class="hljs-attr">kind:</span> NetworkPolicy
<span class="hljs-attr">apiVersion:</span> networking.k8s.io/v1
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> web-allow-external
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  podSelector:</span>
<span class="hljs-attr">    matchLabels:</span>
<span class="hljs-attr">      app:</span> web
<span class="hljs-attr">  ingress:</span>
<span class="hljs-attr">  - ports:</span>
<span class="hljs-attr">    - port:</span> <span class="hljs-number">80</span>
<span class="hljs-attr">    from:</span> []
</code></pre>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/" target="_blank">Kubernetes network policies</a></li>
<li><a href="https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/" target="_blank">Declare Network Policy</a></li>
<li><a href="https://ahmet.im/blog/kubernetes-network-policy/" target="_blank">Securing Kubernetes Cluster Networking</a></li>
<li><a href="https://github.com/ahmetb/kubernetes-networkpolicy-tutorial" target="_blank">Kubernetes Network Policy Recipes</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="node" class="level3">Node</h1>
<p>Node 是 Pod 真正运行的主机，可以是物理机，也可以是虚拟机。为了管理 Pod，每个 Node 节点上至少要运行 container runtime（比如 <code>docker</code> 或者 <code>rkt</code>）、<code>kubelet</code> 和 <code>kube-proxy</code> 服务。</p>
<p><img src="images/node.png" alt="node"/></p>
<h2 id="node-管理">Node 管理</h2>
<p>不像其他的资源（如 Pod 和 Namespace），Node 本质上不是 Kubernetes 来创建的，Kubernetes 只是管理 Node 上的资源。虽然可以通过 Manifest 创建一个 Node 对象（如下 yaml 所示），但 Kubernetes 也只是去检查是否真的是有这么一个 Node，如果检查失败，也不会往上调度 Pod。</p>
<pre><code class="lang-yaml"><span class="hljs-attr">kind:</span> Node
<span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> <span class="hljs-number">10</span><span class="hljs-bullet">-240</span><span class="hljs-bullet">-79</span><span class="hljs-bullet">-157</span>
<span class="hljs-attr">  labels:</span>
<span class="hljs-attr">    name:</span> my-first-k8s-node
</code></pre>
<p>这个检查是由 Node Controller 来完成的。Node Controller 负责</p>
<ul>
<li>维护 Node 状态</li>
<li>与 Cloud Provider 同步 Node</li>
<li>给 Node 分配容器 CIDR</li>
<li>删除带有 <code>NoExecute</code> taint 的 Node 上的 Pods</li>
</ul>
<p>默认情况下，kubelet 在启动时会向 master 注册自己，并创建 Node 资源。</p>
<h2 id="node-的状态">Node 的状态</h2>
<p>每个 Node 都包括以下状态信息：</p>
<ul>
<li>地址：包括 hostname、外网 IP 和内网 IP</li>
<li>条件（Condition）：包括 OutOfDisk、Ready、MemoryPressure 和 DiskPressure</li>
<li>容量（Capacity）：Node 上的可用资源，包括 CPU、内存和 Pod 总数</li>
<li>基本信息（Info）：包括内核版本、容器引擎版本、OS 类型等</li>
</ul>
<h2 id="taints-和-tolerations">Taints 和 tolerations</h2>
<p>Taints 和 tolerations 用于保证 Pod 不被调度到不合适的 Node 上，Taint 应用于 Node 上，而 toleration 则应用于 Pod 上（Toleration 是可选的）。</p>
<p>比如，可以使用 taint 命令给 node1 添加 taints：</p>
<pre><code class="lang-sh">kubectl taint nodes node1 key1=value1:NoSchedule
kubectl taint nodes node1 key1=value2:NoExecute
</code></pre>
<p>Taints 和 tolerations 的具体使用方法请参考 <a href="../components/scheduler.html#Taints%20和%20tolerations">调度器章节</a>。</p>
<h2 id="node-维护模式">Node 维护模式</h2>
<p>标志 Node 不可调度但不影响其上正在运行的 Pod，这种维护 Node 时是非常有用的</p>
<pre><code class="lang-sh">kubectl cordon <span class="hljs-variable">$NODENAME</span>
</code></pre>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://kubernetes.io/docs/concepts/architecture/nodes/" target="_blank">Kubernetes Node</a></li>
<li><a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#taints-and-tolerations-beta-feature" target="_blank">Taints 和 tolerations</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="persistent-volume" class="level3">PersistentVolume</h1>
<p>PersistentVolume (PV) 和 PersistentVolumeClaim (PVC) 提供了方便的持久化卷：PV 提供网络存储资源，而 PVC 请求存储资源。这样，设置持久化的工作流包括配置底层文件系统或者云数据卷、创建持久性数据卷、最后创建 PVC 来将 Pod 跟数据卷关联起来。PV 和 PVC 可以将 pod 和数据卷解耦，pod 不需要知道确切的文件系统或者支持它的持久化引擎。</p>
<h2 id="volume-生命周期">Volume 生命周期</h2>
<p>Volume 的生命周期包括 5 个阶段</p>
<ol>
<li>Provisioning，即 PV 的创建，可以直接创建 PV（静态方式），也可以使用 StorageClass 动态创建</li>
<li>Binding，将 PV 分配给 PVC</li>
<li>Using，Pod 通过 PVC 使用该 Volume，并可以通过准入控制 StorageObjectInUseProtection（1.9 及以前版本为 PVCProtection）阻止删除正在使用的 PVC</li>
<li>Releasing，Pod 释放 Volume 并删除 PVC</li>
<li>Reclaiming，回收 PV，可以保留 PV 以便下次使用，也可以直接从云存储中删除</li>
<li>Deleting，删除 PV 并从云存储中删除后段存储</li>
</ol>
<p>根据这 5 个阶段，Volume 的状态有以下 4 种</p>
<ul>
<li>Available：可用</li>
<li>Bound：已经分配给 PVC</li>
<li>Released：PVC 解绑但还未执行回收策略</li>
<li>Failed：发生错误</li>
</ul>
<h2 id="api-版本对照表">API 版本对照表</h2>
<table>
<thead>
<tr>
<th>Kubernetes 版本</th>
<th>PV/PVC 版本</th>
<th>StorageClass 版本</th>
</tr>
</thead>
<tbody>
<tr>
<td>v1.5-v1.6</td>
<td>core/v1</td>
<td>storage.k8s.io/v1beta1</td>
</tr>
<tr>
<td>v1.7+</td>
<td>core/v1</td>
<td>storage.k8s.io/v1</td>
</tr>
</tbody>
</table>
<h2 id="pv">PV</h2>
<p>PersistentVolume（PV）是集群之中的一块网络存储。跟 Node 一样，也是集群的资源。PV 跟 Volume (卷) 类似，不过会有独立于 Pod 的生命周期。比如一个 NFS 的 PV 可以定义为</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> PersistentVolume
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> pv0003
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  capacity:</span>
<span class="hljs-attr">    storage:</span> <span class="hljs-number">5</span>Gi
<span class="hljs-attr">  accessModes:</span>
<span class="hljs-bullet">    -</span> ReadWriteOnce
<span class="hljs-attr">  persistentVolumeReclaimPolicy:</span> Recycle
<span class="hljs-attr">  nfs:</span>
<span class="hljs-attr">    path:</span> /tmp
<span class="hljs-attr">    server:</span> <span class="hljs-number">172.17</span><span class="hljs-number">.0</span><span class="hljs-number">.2</span>
</code></pre>
<p>PV 的访问模式（accessModes）有三种：</p>
<ul>
<li>ReadWriteOnce（RWO）：是最基本的方式，可读可写，但只支持被单个节点挂载。</li>
<li>ReadOnlyMany（ROX）：可以以只读的方式被多个节点挂载。</li>
<li>ReadWriteMany（RWX）：这种存储可以以读写的方式被多个节点共享。不是每一种存储都支持这三种方式，像共享方式，目前支持的还比较少，比较常用的是 NFS。在 PVC 绑定 PV 时通常根据两个条件来绑定，一个是存储的大小，另一个就是访问模式。</li>
</ul>
<p>PV 的回收策略（persistentVolumeReclaimPolicy，即 PVC 释放卷的时候 PV 该如何操作）也有三种</p>
<ul>
<li>Retain，不清理, 保留 Volume（需要手动清理）</li>
<li>Recycle，删除数据，即 <code>rm -rf /thevolume/*</code>（只有 NFS 和 HostPath 支持）</li>
<li>Delete，删除存储资源，比如删除 AWS EBS 卷（只有 AWS EBS, GCE PD, Azure Disk 和 Cinder 支持）</li>
</ul>
<h2 id="storageclass">StorageClass</h2>
<p>上面通过手动的方式创建了一个 NFS Volume，这在管理很多 Volume 的时候不太方便。Kubernetes 还提供了 <a href="https://kubernetes.io/docs/user-guide/persistent-volumes/#storageclasses" target="_blank">StorageClass</a> 来动态创建 PV，不仅节省了管理员的时间，还可以封装不同类型的存储供 PVC 选用。</p>
<p>StorageClass 包括四个部分</p>
<ul>
<li>provisioner：指定 Volume 插件的类型，包括内置插件（如 <code>kubernetes.io/glusterfs</code>）和外部插件（如 <a href="https://github.com/kubernetes-incubator/external-storage/tree/master/ceph/cephfs" target="_blank">external-storage</a> 提供的 <code>ceph.com/cephfs</code>）。</li>
<li>mountOptions：指定挂载选项，当 PV 不支持指定的选项时会直接失败。比如 NFS 支持 <code>hard</code> 和 <code>nfsvers=4.1</code> 等选项。</li>
<li>parameters：指定 provisioner 的选项，比如 <code>kubernetes.io/aws-ebs</code> 支持 <code>type</code>、<code>zone</code>、<code>iopsPerGB</code> 等参数。</li>
<li>reclaimPolicy：指定回收策略，同 PV 的回收策略。</li>
</ul>
<p>在使用 PVC 时，可以通过 <code>DefaultStorageClass</code> 准入控制设置默认 StorageClass, 即给未设置 storageClassName 的 PVC 自动添加默认的 StorageClass。而默认的 StorageClass 带有 annotation <code>storageclass.kubernetes.io/is-default-class=true</code>。</p>
<table>
<thead>
<tr>
<th>Volume Plugin</th>
<th>Internal Provisioner</th>
<th>Config Example</th>
</tr>
</thead>
<tbody>
<tr>
<td>AWSElasticBlockStore</td>
<td>✓</td>
<td><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#aws" target="_blank">AWS</a></td>
</tr>
<tr>
<td>AzureFile</td>
<td>✓</td>
<td><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#azure-file" target="_blank">Azure File</a></td>
</tr>
<tr>
<td>AzureDisk</td>
<td>✓</td>
<td><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#azure-disk" target="_blank">Azure Disk</a></td>
</tr>
<tr>
<td>CephFS</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>Cinder</td>
<td>✓</td>
<td><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#openstack-cinder" target="_blank">OpenStack Cinder</a></td>
</tr>
<tr>
<td>FC</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>FlexVolume</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>Flocker</td>
<td>✓</td>
<td>-</td>
</tr>
<tr>
<td>GCEPersistentDisk</td>
<td>✓</td>
<td><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#gce" target="_blank">GCE</a></td>
</tr>
<tr>
<td>Glusterfs</td>
<td>✓</td>
<td><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#glusterfs" target="_blank">Glusterfs</a></td>
</tr>
<tr>
<td>iSCSI</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>PhotonPersistentDisk</td>
<td>✓</td>
<td>-</td>
</tr>
<tr>
<td>Quobyte</td>
<td>✓</td>
<td><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#quobyte" target="_blank">Quobyte</a></td>
</tr>
<tr>
<td>NFS</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>RBD</td>
<td>✓</td>
<td><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#ceph-rbd" target="_blank">Ceph RBD</a></td>
</tr>
<tr>
<td>VsphereVolume</td>
<td>✓</td>
<td><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#vsphere" target="_blank">vSphere</a></td>
</tr>
<tr>
<td>PortworxVolume</td>
<td>✓</td>
<td><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#portworx-volume" target="_blank">Portworx Volume</a></td>
</tr>
<tr>
<td>ScaleIO</td>
<td>✓</td>
<td><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#scaleio" target="_blank">ScaleIO</a></td>
</tr>
<tr>
<td>StorageOS</td>
<td>✓</td>
<td><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#storageos" target="_blank">StorageOS</a></td>
</tr>
<tr>
<td>Local</td>
<td>-</td>
<td><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#local" target="_blank">Local</a></td>
</tr>
</tbody>
</table>
<h4 id="修改默认-storageclass">修改默认 StorageClass</h4>
<p>取消原来的默认 StorageClass</p>
<pre><code class="lang-sh">kubectl patch storageclass <default-class-name> -p <span class="hljs-string">'{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"false"}}}'</span>
</code></pre>
<p>标记新的默认 StorageClass</p>
<pre><code class="lang-sh">kubectl patch storageclass <your-class-name> -p <span class="hljs-string">'{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'</span>
</code></pre>
<h4 id="gce-示例">GCE 示例</h4>
<blockquote>
<p>单个 GCE 节点最大支持挂载 16 个 Google Persistent Disk。开启 <code>AttachVolumeLimit</code> 特性后，根据节点的类型最大可以挂载 128 个。</p>
</blockquote>
<pre><code class="lang-yaml"><span class="hljs-attr">kind:</span> StorageClass
<span class="hljs-attr">apiVersion:</span> storage.k8s.io/v1
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> slow
<span class="hljs-attr">provisioner:</span> kubernetes.io/gce-pd
<span class="hljs-attr">parameters:</span>
<span class="hljs-attr">  type:</span> pd-standard
<span class="hljs-attr">  zone:</span> us-central1-a
</code></pre>
<h4 id="glusterfs-示例">Glusterfs 示例</h4>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> storage.k8s.io/v1
<span class="hljs-attr">kind:</span> StorageClass
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> slow
<span class="hljs-attr">provisioner:</span> kubernetes.io/glusterfs
<span class="hljs-attr">parameters:</span>
<span class="hljs-attr">  resturl:</span> <span class="hljs-string">"http://127.0.0.1:8081"</span>
<span class="hljs-attr">  clusterid:</span> <span class="hljs-string">"630372ccdc720a92c681fb928f27b53f"</span>
<span class="hljs-attr">  restauthenabled:</span> <span class="hljs-string">"true"</span>
<span class="hljs-attr">  restuser:</span> <span class="hljs-string">"admin"</span>
<span class="hljs-attr">  secretNamespace:</span> <span class="hljs-string">"default"</span>
<span class="hljs-attr">  secretName:</span> <span class="hljs-string">"heketi-secret"</span>
<span class="hljs-attr">  gidMin:</span> <span class="hljs-string">"40000"</span>
<span class="hljs-attr">  gidMax:</span> <span class="hljs-string">"50000"</span>
<span class="hljs-attr">  volumetype:</span> <span class="hljs-string">"replicate:3"</span>
</code></pre>
<h4 id="openstack-cinder-示例">OpenStack Cinder 示例</h4>
<pre><code class="lang-yaml"><span class="hljs-attr">kind:</span> StorageClass
<span class="hljs-attr">apiVersion:</span> storage.k8s.io/v1
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> gold
<span class="hljs-attr">provisioner:</span> kubernetes.io/cinder
<span class="hljs-attr">parameters:</span>
<span class="hljs-attr">  type:</span> fast
<span class="hljs-attr">  availability:</span> nova
</code></pre>
<h4 id="ceph-rbd-示例">Ceph RBD 示例</h4>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> storage.k8s.io/v1
<span class="hljs-attr">  kind:</span> StorageClass
<span class="hljs-attr">  metadata:</span>
<span class="hljs-attr">    name:</span> fast
<span class="hljs-attr">  provisioner:</span> kubernetes.io/rbd
<span class="hljs-attr">  parameters:</span>
<span class="hljs-attr">    monitors:</span> <span class="hljs-number">10.16</span><span class="hljs-number">.153</span><span class="hljs-number">.105</span>:<span class="hljs-number">6789</span>
<span class="hljs-attr">    adminId:</span> kube
<span class="hljs-attr">    adminSecretName:</span> ceph-secret
<span class="hljs-attr">    adminSecretNamespace:</span> kube-system
<span class="hljs-attr">    pool:</span> kube
<span class="hljs-attr">    userId:</span> kube
<span class="hljs-attr">    userSecretName:</span> ceph-secret-user
</code></pre>
<h3 id="local-volume">Local Volume</h3>
<p>Local Volume 允许将 Node 本地的磁盘、分区或者目录作为持久化存储使用。注意，Local Volume 不支持动态创建，使用前需要预先创建好 PV。</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> PersistentVolume
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> example-pv
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  capacity:</span>
<span class="hljs-attr">    storage:</span> <span class="hljs-number">100</span>Gi
  <span class="hljs-comment"># volumeMode field requires BlockVolume Alpha feature gate to be enabled.</span>
<span class="hljs-attr">  volumeMode:</span> Filesystem
<span class="hljs-attr">  accessModes:</span>
<span class="hljs-bullet">  -</span> ReadWriteOnce
<span class="hljs-attr">  persistentVolumeReclaimPolicy:</span> Delete
<span class="hljs-attr">  storageClassName:</span> local-storage
<span class="hljs-attr">  local:</span>
<span class="hljs-attr">    path:</span> /mnt/disks/ssd1
<span class="hljs-attr">  nodeAffinity:</span>
<span class="hljs-attr">    required:</span>
<span class="hljs-attr">      nodeSelectorTerms:</span>
<span class="hljs-attr">      - matchExpressions:</span>
<span class="hljs-attr">        - key:</span> kubernetes.io/hostname
<span class="hljs-attr">          operator:</span> In
<span class="hljs-attr">          values:</span>
<span class="hljs-bullet">          -</span> example-node
<span class="hljs-meta">---</span>
<span class="hljs-attr">kind:</span> StorageClass
<span class="hljs-attr">apiVersion:</span> storage.k8s.io/v1
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> local-storage
<span class="hljs-attr">provisioner:</span> kubernetes.io/<span class="hljs-literal">no</span>-provisioner
<span class="hljs-attr">volumeBindingMode:</span> WaitForFirstConsumer
</code></pre>
<p>推荐配置</p>
<ul>
<li>对于需要强 IO 隔离的场景，推荐使用整块磁盘作为 Volume</li>
<li>对于需要容量隔离的场景，推荐使用分区作为 Volume</li>
<li>避免在集群中重新创建同名的 Node（无法避免时需要先删除通过 Affinity 引用该 Node 的 PV）</li>
<li>对于文件系统类型的本地存储，推荐使用 UUID （如 <code>ls -l /dev/disk/by-uuid</code>）作为系统挂载点</li>
<li>对于无文件系统的块存储，推荐生成一个唯一 ID 作软链接（如 <code>/dev/dis/by-id</code>）。这可以保证 Volume 名字唯一，并不会与其他 Node 上面的同名 Volume 混淆</li>
</ul>
<h2 id="pvc">PVC</h2>
<p>PV 是存储资源，而 PersistentVolumeClaim (PVC) 是对 PV 的请求。PVC 跟 Pod 类似：Pod 消费 Node 资源，而 PVC 消费 PV 资源；Pod 能够请求 CPU 和内存资源，而 PVC 请求特定大小和访问模式的数据卷。</p>
<pre><code class="lang-yaml"><span class="hljs-attr">kind:</span> PersistentVolumeClaim
<span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> myclaim
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  accessModes:</span>
<span class="hljs-bullet">    -</span> ReadWriteOnce
<span class="hljs-attr">  resources:</span>
<span class="hljs-attr">    requests:</span>
<span class="hljs-attr">      storage:</span> <span class="hljs-number">8</span>Gi
<span class="hljs-attr">  storageClassName:</span> slow
<span class="hljs-attr">  selector:</span>
<span class="hljs-attr">    matchLabels:</span>
<span class="hljs-attr">      release:</span> <span class="hljs-string">"stable"</span>
<span class="hljs-attr">    matchExpressions:</span>
<span class="hljs-bullet">      -</span> {key: environment, operator: In, values: [dev]}
</code></pre>
<p>PVC 可以直接挂载到 Pod 中：</p>
<pre><code class="lang-yaml"><span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> mypod
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">    - name:</span> myfrontend
<span class="hljs-attr">      image:</span> dockerfile/nginx
<span class="hljs-attr">      volumeMounts:</span>
<span class="hljs-attr">      - mountPath:</span> <span class="hljs-string">"/var/www/html"</span>
<span class="hljs-attr">        name:</span> mypd
<span class="hljs-attr">  volumes:</span>
<span class="hljs-attr">    - name:</span> mypd
<span class="hljs-attr">      persistentVolumeClaim:</span>
<span class="hljs-attr">        claimName:</span> myclaim
</code></pre>
<h2 id="扩展-pv-空间">扩展 PV 空间</h2>
<blockquote>
<p>ExpandPersistentVolumes 在 v1.8 开始 Alpha，v1.11 升级为 Beta 版。</p>
</blockquote>
<p>v1.8 开始支持扩展 PV 空间，支持在不丢失数据和重启容器的情况下扩展 PV 的大小。注意，<strong> 当前的实现仅支持不需要调整文件系统大小（XFS、Ext3、Ext4）的 PV，并且只支持以下几种存储插件 </strong>：</p>
<ul>
<li>AzureDisk</li>
<li>AzureFile</li>
<li>gcePersistentDisk</li>
<li>awsElasticBlockStore</li>
<li>Cinder</li>
<li>glusterfs</li>
<li>rbd</li>
<li>Portworx</li>
</ul>
<p>开启扩展 PV 空间的功能需要配置</p>
<ul>
<li>开启 <code>ExpandPersistentVolumes</code> 功能，即配置 <code>--feature-gates=ExpandPersistentVolumes=true</code></li>
<li>开启准入控制插件 <code>PersistentVolumeClaimResize</code>，它只允许扩展明确配置 <code>allowVolumeExpansion=true</code> 的 StorageClass，比如</li>
</ul>
<pre><code class="lang-yaml"><span class="hljs-attr">kind:</span> StorageClass
<span class="hljs-attr">apiVersion:</span> storage.k8s.io/v1
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> gluster-vol-default
<span class="hljs-attr">provisioner:</span> kubernetes.io/glusterfs
<span class="hljs-attr">parameters:</span>
<span class="hljs-attr">  resturl:</span> <span class="hljs-string">"http://192.168.10.100:8080"</span>
<span class="hljs-attr">  restuser:</span> <span class="hljs-string">""</span>
<span class="hljs-attr">  secretNamespace:</span> <span class="hljs-string">""</span>
<span class="hljs-attr">  secretName:</span> <span class="hljs-string">""</span>
<span class="hljs-attr">allowVolumeExpansion:</span> <span class="hljs-literal">true</span>
</code></pre>
<p>这样，用户就可以修改 PVC 中请求存储的大小（如通过 <code>kubectl edit</code> 命令）请求更大的存储空间。</p>
<h2 id="块存储（raw-block-volume）">块存储（Raw Block Volume）</h2>
<p>Kubernetes v1.9 新增了 Alpha 版的 Raw Block Volume，可通过设置 <code>volumeMode: Block</code>（可选项为 <code>Filesystem</code> 和 <code>Block</code>）来使用块存储。</p>
<blockquote>
<p>注意：使用前需要为 kube-apiserver、kube-controller-manager 和 kubelet 开启 <code>BlockVolume</code> 特性，即添加命令行选项 <code>--feature-gates=BlockVolume=true,...</code>。</p>
</blockquote>
<p>支持块存储的 PV 插件包括</p>
<ul>
<li>Local Volume</li>
<li>fc</li>
<li>iSCSI</li>
<li>Ceph RBD</li>
<li>AWS EBS</li>
<li>GCE PD</li>
<li>AzureDisk</li>
<li>Cinder</li>
</ul>
<p>使用示例</p>
<pre><code class="lang-yaml"><span class="hljs-comment"># Persistent Volumes using a Raw Block Volume</span>
<span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> PersistentVolume
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> block-pv
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  capacity:</span>
<span class="hljs-attr">    storage:</span> <span class="hljs-number">10</span>Gi
<span class="hljs-attr">  accessModes:</span>
<span class="hljs-bullet">    -</span> ReadWriteOnce
<span class="hljs-attr">  volumeMode:</span> Block
<span class="hljs-attr">  persistentVolumeReclaimPolicy:</span> Retain
<span class="hljs-attr">  fc:</span>
<span class="hljs-attr">    targetWWNs:</span> [<span class="hljs-string">"50060e801049cfd1"</span>]
<span class="hljs-attr">    lun:</span> <span class="hljs-number">0</span>
<span class="hljs-attr">    readOnly:</span> <span class="hljs-literal">false</span>
<span class="hljs-meta">---</span>
<span class="hljs-comment"># Persistent Volume Claim requesting a Raw Block Volume</span>
<span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> PersistentVolumeClaim
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> block-pvc
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  accessModes:</span>
<span class="hljs-bullet">    -</span> ReadWriteOnce
<span class="hljs-attr">  volumeMode:</span> Block
<span class="hljs-attr">  resources:</span>
<span class="hljs-attr">    requests:</span>
<span class="hljs-attr">      storage:</span> <span class="hljs-number">10</span>Gi
<span class="hljs-meta">---</span>
<span class="hljs-comment"># Pod specification adding Raw Block Device path in container</span>
<span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> pod-with-block-volume
<span class="hljs-attr">  annotations:</span>
    <span class="hljs-comment"># apparmor should be unconfied for mounting the device inside container.</span>
    container.apparmor.security.beta.kubernetes.io/fc-container: unconfined
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">    - name:</span> fc-container
<span class="hljs-attr">      image:</span> fedora:<span class="hljs-number">26</span>
<span class="hljs-attr">      command:</span> [<span class="hljs-string">"/bin/sh"</span>, <span class="hljs-string">"-c"</span>]
<span class="hljs-attr">      args:</span> [<span class="hljs-string">"tail -f /dev/null"</span>]
<span class="hljs-attr">      securityContext:</span>
<span class="hljs-attr">        capabilities:</span>
          <span class="hljs-comment"># CAP_SYS_ADMIN is required for mount() syscall.</span>
<span class="hljs-attr">          add:</span> [<span class="hljs-string">"SYS_ADMIN"</span>]
<span class="hljs-attr">      volumeDevices:</span>
<span class="hljs-attr">        - name:</span> data
<span class="hljs-attr">          devicePath:</span> /dev/xvda
<span class="hljs-attr">  volumes:</span>
<span class="hljs-attr">    - name:</span> data
<span class="hljs-attr">      persistentVolumeClaim:</span>
<span class="hljs-attr">        claimName:</span> block-pvc
</code></pre>
<h2 id="storageobjectinuseprotection">StorageObjectInUseProtection</h2>
<blockquote>
<p>准入控制 StorageObjectInUseProtection 在 v1.11 版本 GA。</p>
</blockquote>
<p>当开启准入控制 StorageObjectInUseProtection（<code>--admission-control=StorageObjectInUseProtection</code>）时，删除使用中的 PV 和 PVC 后，它们会等待使用者删除后才删除（而不是之前的立即删除）。而在使用者删除之前，它们会一直处于 Terminating 状态。</p>
<h2 id="拓扑感知动态调度">拓扑感知动态调度</h2>
<p>拓扑感知动态存储卷调度（topology-aware dynamic provisioning）是 v1.12 版本的一个 Beta 特性，用来支持在多可用区集群中动态创建和调度持久化存储卷。目前的实现支持以下几种存储：</p>
<ul>
<li>AWS EBS</li>
<li>Azure Disk</li>
<li>GCE PD (including Regional PD)</li>
<li>CSI (alpha) - currently only the GCE PD CSI driver has implemented topology support</li>
</ul>
<p>使用示例</p>
<pre><code class="lang-yaml"><span class="hljs-comment"># set WaitForFirstConsumer in storage class</span>
<span class="hljs-attr">kind:</span> StorageClass
<span class="hljs-attr">apiVersion:</span> storage.k8s.io/v1
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> topology-aware-standard
<span class="hljs-attr">provisioner:</span> kubernetes.io/gce-pd
<span class="hljs-attr">volumeBindingMode:</span> WaitForFirstConsumer
<span class="hljs-attr">parameters:</span>
<span class="hljs-attr">  type:</span> pd-standard

<span class="hljs-comment"># Refer storage class</span>
<span class="hljs-attr">apiVersion:</span> apps/v1
<span class="hljs-attr">kind:</span> StatefulSet
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> web
<span class="hljs-attr">spec:</span>   
<span class="hljs-attr">  serviceName:</span> <span class="hljs-string">"nginx"</span>
<span class="hljs-attr">  replicas:</span> <span class="hljs-number">2</span>
<span class="hljs-attr">  selector:</span>
<span class="hljs-attr">    matchLabels:</span>
<span class="hljs-attr">      app:</span> nginx
<span class="hljs-attr">  template:</span>
<span class="hljs-attr">    metadata:</span>
<span class="hljs-attr">      labels:</span>
<span class="hljs-attr">        app:</span> nginx
<span class="hljs-attr">    spec:</span>
<span class="hljs-attr">      affinity:</span>
<span class="hljs-attr">        nodeAffinity:</span>
<span class="hljs-attr">          requiredDuringSchedulingIgnoredDuringExecution:</span>
<span class="hljs-attr">            nodeSelectorTerms:</span>
<span class="hljs-attr">            - matchExpressions:</span>
<span class="hljs-attr">              - key:</span> failure-domain.beta.kubernetes.io/zone
<span class="hljs-attr">                operator:</span> In
<span class="hljs-attr">                values:</span>
<span class="hljs-bullet">                -</span> us-central1-a
<span class="hljs-bullet">                -</span> us-central1-f
<span class="hljs-attr">        podAntiAffinity:</span>
<span class="hljs-attr">          requiredDuringSchedulingIgnoredDuringExecution:</span>
<span class="hljs-attr">          - labelSelector:</span>
<span class="hljs-attr">              matchExpressions:</span>
<span class="hljs-attr">              - key:</span> app
<span class="hljs-attr">                operator:</span> In
<span class="hljs-attr">                values:</span>
<span class="hljs-bullet">                -</span> nginx
<span class="hljs-attr">            topologyKey:</span> failure-domain.beta.kubernetes.io/zone
<span class="hljs-attr">      containers:</span>
<span class="hljs-attr">      - name:</span> nginx
<span class="hljs-attr">        image:</span> gcr.io/google_containers/nginx-slim:<span class="hljs-number">0.8</span>
<span class="hljs-attr">        ports:</span>
<span class="hljs-attr">        - containerPort:</span> <span class="hljs-number">80</span>
<span class="hljs-attr">          name:</span> web
<span class="hljs-attr">        volumeMounts:</span>
<span class="hljs-attr">        - name:</span> www
<span class="hljs-attr">          mountPath:</span> /usr/share/nginx/html
<span class="hljs-attr">        - name:</span> logs
<span class="hljs-attr">          mountPath:</span> /logs
<span class="hljs-attr"> volumeClaimTemplates:</span>
<span class="hljs-attr">  - metadata:</span>
<span class="hljs-attr">      name:</span> www
<span class="hljs-attr">    spec:</span>
<span class="hljs-attr">      accessModes:</span> [ <span class="hljs-string">"ReadWriteOnce"</span> ]
<span class="hljs-attr">      storageClassName:</span> topology-aware-standard
<span class="hljs-attr">      resources:</span>
<span class="hljs-attr">        requests:</span>
<span class="hljs-attr">          storage:</span> <span class="hljs-number">10</span>Gi
<span class="hljs-attr">  - metadata:</span>
<span class="hljs-attr">      name:</span> logs
<span class="hljs-attr">    spec:</span>
<span class="hljs-attr">      accessModes:</span> [ <span class="hljs-string">"ReadWriteOnce"</span> ]
<span class="hljs-attr">      storageClassName:</span> topology-aware-standard
<span class="hljs-attr">      resources:</span>
<span class="hljs-attr">        requests:</span>
<span class="hljs-attr">          storage:</span> <span class="hljs-number">1</span>Gi
</code></pre>
<p>然后查看 PV，可以发现它们创建在不同的可用区内</p>
<pre><code class="lang-sh">$ kubectl get pv -o=jsonpath=<span class="hljs-string">'{range .items[*]}{.spec.claimRef.name}{"\t"}{.metadata.labels.failure\-domain\.beta\.kubernetes\.io/zone}{"\n"}{end}'</span>
www-web-0       us-central1<span class="hljs-_">-f</span>
logs-web-0      us-central1<span class="hljs-_">-f</span>
www-web-1       us-central1<span class="hljs-_">-a</span>
logs-web-1      us-central1<span class="hljs-_">-a</span>
</code></pre>
<h2 id="存储快照">存储快照</h2>
<p>存储快照是 v1.12 新增的 Alpha 特性，用来支持给存储卷创建快照。支持的插件包括</p>
<ul>
<li><a href="https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver" target="_blank">GCE Persistent Disk CSI Driver</a></li>
<li><a href="https://github.com/opensds/nbp/tree/master/csi/server" target="_blank">OpenSDS CSI Driver</a></li>
<li><a href="https://github.com/ceph/ceph-csi/tree/master/pkg/rbd" target="_blank">Ceph RBD CSI Driver</a></li>
<li><a href="https://github.com/libopenstorage/openstorage/tree/master/csi" target="_blank">Portworx CSI Driver</a></li>
</ul>
<p><img src="assets/image-20181014215558480.png" alt="image-20181014215558480"/></p>
<p>在使用前需要开启特性开关 VolumeSnapshotDataSource。</p>
<p>使用示例：</p>
<pre><code class="lang-yaml"><span class="hljs-comment"># create snapshot</span>
<span class="hljs-attr">apiVersion:</span> snapshot.storage.k8s.io/v1alpha1
<span class="hljs-attr">kind:</span> VolumeSnapshot
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> new-snapshot-demo
<span class="hljs-attr">  namespace:</span> demo-namespace
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  snapshotClassName:</span> csi-snapclass
<span class="hljs-attr">  source:</span>
<span class="hljs-attr">    name:</span> mypvc
<span class="hljs-attr">    kind:</span> PersistentVolumeClaim

<span class="hljs-comment"># import from snapshot</span>
<span class="hljs-attr">apiVersion:</span> snapshot.storage.k8s.io/v1alpha1
<span class="hljs-attr">kind:</span> VolumeSnapshotContent
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> static-snapshot-content
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  csiVolumeSnapshotSource:</span>
<span class="hljs-attr">    driver:</span> com.example.csi-driver
<span class="hljs-attr">    snapshotHandle:</span> snapshotcontent-example-id
<span class="hljs-attr">  volumeSnapshotRef:</span>
<span class="hljs-attr">    kind:</span> VolumeSnapshot
<span class="hljs-attr">    name:</span> static-snapshot-demo
<span class="hljs-attr">    namespace:</span> demo-namespace

<span class="hljs-comment"># provision volume from snapshot</span>
<span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> PersistentVolumeClaim
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> pvc-restore
<span class="hljs-attr">  Namespace:</span> demo-namespace
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  storageClassName:</span> csi-storageclass
<span class="hljs-attr">  dataSource:</span>
<span class="hljs-attr">    name:</span> new-snapshot-demo
<span class="hljs-attr">    kind:</span> VolumeSnapshot
<span class="hljs-attr">    apiGroup:</span> snapshot.storage.k8s.io
<span class="hljs-attr">  accessModes:</span>
<span class="hljs-bullet">    -</span> ReadWriteOnce
<span class="hljs-attr">  resources:</span>
<span class="hljs-attr">    requests:</span>
<span class="hljs-attr">      storage:</span> <span class="hljs-number">1</span>Gi
</code></pre>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/" target="_blank">Kubernetes Persistent Volumes</a></li>
<li><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/" target="_blank">Kubernetes Storage Classes</a></li>
<li><a href="https://kubernetes.io/docs/concepts/storage/dynamic-provisioning/" target="_blank">Dynamic Volume Provisioning</a></li>
<li><a href="https://kubernetes-csi.github.io/docs/" target="_blank">Kubernetes CSI Documentation</a></li>
<li><a href="https://kubernetes.io/docs/concepts/storage/volume-snapshots/" target="_blank">Volume Snapshots Documentation</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="pod" class="level3">Pod</h1>
<p>Pod 是一组紧密关联的容器集合，它们共享 IPC、Network 和 UTS namespace，是 Kubernetes 调度的基本单位。Pod 的设计理念是支持多个容器在一个 Pod 中共享网络和文件系统，可以通过进程间通信和文件共享这种简单高效的方式组合完成服务。</p>
<p><img src="images/pod.png" alt="pod"/></p>
<p>Pod 的特征</p>
<ul>
<li>包含多个共享 IPC、Network 和 UTC namespace 的容器，可直接通过 localhost 通信</li>
<li>所有 Pod 内容器都可以访问共享的 Volume，可以访问共享数据</li>
<li>无容错性：直接创建的 Pod 一旦被调度后就跟 Node 绑定，即使 Node 挂掉也不会被重新调度（而是被自动删除），因此推荐使用 Deployment、Daemonset 等控制器来容错</li>
<li>优雅终止：Pod 删除的时候先给其内的进程发送 SIGTERM，等待一段时间（grace period）后才强制停止依然还在运行的进程</li>
<li>特权容器（通过 SecurityContext 配置）具有改变系统配置的权限（在网络插件中大量应用）</li>
</ul>
<blockquote>
<p>Kubernetes v1.8+ 还支持容器间共享 PID namespace，需要 docker >= 1.13.1，并配置 kubelet <code>--docker-disable-shared-pid=false</code>。</p>
</blockquote>
<h2 id="api-版本对照表">API 版本对照表</h2>
<table>
<thead>
<tr>
<th>Kubernetes 版本</th>
<th>Core API 版本</th>
<th>默认开启</th>
</tr>
</thead>
<tbody>
<tr>
<td>v1.5+</td>
<td>core/v1</td>
<td>是</td>
</tr>
</tbody>
</table>
<h2 id="pod-定义">Pod 定义</h2>
<p>通过 <a href="https://v1-9.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#pod-v1-core" target="_blank">yaml 或 json 描述 Pod</a> 和其内容器的运行环境以及期望状态，比如一个最简单的 nginx pod 可以定义为</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> nginx
<span class="hljs-attr">  labels:</span>
<span class="hljs-attr">    app:</span> nginx
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">  - name:</span> nginx
<span class="hljs-attr">    image:</span> nginx
<span class="hljs-attr">    ports:</span>
<span class="hljs-attr">    - containerPort:</span> <span class="hljs-number">80</span>
</code></pre>
<blockquote>
<p>在生产环境中，推荐使用 Deployment、StatefulSet、Job 或者 CronJob 等控制器来创建 Pod，而不推荐直接创建 Pod。</p>
</blockquote>
<h3 id="docker-镜像支持">Docker 镜像支持</h3>
<p>目前，Kubernetes 仅支持使用 Docker 镜像来创建容器，但并非支持 <a href="https://docs.docker.com/engine/reference/builder/" target="_blank">Dockerfile</a> 定义的所有行为。如下表所示</p>
<table>
<thead>
<tr>
<th>Dockerfile 指令</th>
<th>描述</th>
<th>支持</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>ENTRYPOINT</td>
<td>启动命令</td>
<td>是</td>
<td>containerSpec.command</td>
</tr>
<tr>
<td>CMD</td>
<td>命令的参数列表</td>
<td>是</td>
<td>containerSpec.args</td>
</tr>
<tr>
<td>ENV</td>
<td>环境变量</td>
<td>是</td>
<td>containerSpec.env</td>
</tr>
<tr>
<td>EXPOSE</td>
<td>对外开放的端口</td>
<td>否</td>
<td>使用 containerSpec.ports.containerPort 替代</td>
</tr>
<tr>
<td>VOLUME</td>
<td>数据卷</td>
<td>是</td>
<td>使用 volumes 和 volumeMounts</td>
</tr>
<tr>
<td>USER</td>
<td>进程运行用户以及用户组</td>
<td>是</td>
<td>securityContext.runAsUser/supplementalGroups</td>
</tr>
<tr>
<td>WORKDIR</td>
<td>工作目录</td>
<td>是</td>
<td>containerSpec.workingDir</td>
</tr>
<tr>
<td>STOPSIGNAL</td>
<td>停止容器时给进程发送的信号</td>
<td>是</td>
<td>SIGKILL</td>
</tr>
<tr>
<td>HEALTHCHECK</td>
<td>健康检查</td>
<td>否</td>
<td>使用 livenessProbe 和 readinessProbe 替代</td>
</tr>
<tr>
<td>SHELL</td>
<td>运行启动命令的 SHELL</td>
<td>否</td>
<td>使用镜像默认 SHELL 启动命令</td>
</tr>
</tbody>
</table>
<h2 id="pod-生命周期">Pod 生命周期</h2>
<p>Kubernetes 以 <code>PodStatus.Phase</code> 抽象 Pod 的状态（但并不直接反映所有容器的状态）。可能的 Phase 包括</p>
<ul>
<li>Pending: Pod 已经在 apiserver 中创建，但还没有调度到 Node 上面</li>
<li>Running: Pod 已经调度到 Node 上面，所有容器都已经创建，并且至少有一个容器还在运行或者正在启动</li>
<li>Succeeded: Pod 调度到 Node 上面后成功运行结束，并且不会重启</li>
<li>Failed: Pod 调度到 Node 上面后至少有一个容器运行失败（即退出码不为 0 或者被系统终止）</li>
<li>Unknonwn: 状态未知，通常是由于 apiserver 无法与 kubelet 通信导致</li>
</ul>
<p>可以用 kubectl 命令查询 Pod Phase：</p>
<pre><code class="lang-sh">$ kubectl get pod reviews-v1-5bdc544bbd-5qgxj -o jsonpath=<span class="hljs-string">"{.status.phase}"</span>
Running
</code></pre>
<p>PodSpec 中的 <code>restartPolicy</code> 可以用来设置是否对退出的 Pod 重启，可选项包括 <code>Always</code>、<code>OnFailure</code>、以及 <code>Never</code>。比如</p>
<ul>
<li>单容器的 Pod，容器成功退出时，不同 <code>restartPolicy</code> 时的动作为<ul>
<li>Always: 重启 Container; Pod <code>phase</code> 保持 Running.</li>
<li>OnFailure: Pod <code>phase</code> 变成 Succeeded.</li>
<li>Never: Pod <code>phase</code> 变成 Succeeded.</li>
</ul>
</li>
<li>单容器的 Pod，容器失败退出时，不同 <code>restartPolicy</code> 时的动作为<ul>
<li>Always: 重启 Container; Pod <code>phase</code> 保持 Running.</li>
<li>OnFailure: 重启 Container; Pod <code>phase</code> 保持 Running.</li>
<li>Never: Pod <code>phase</code> 变成 Failed.</li>
</ul>
</li>
<li>2个容器的 Pod，其中一个容器在运行而另一个失败退出时，不同 <code>restartPolicy</code> 时的动作为<ul>
<li>Always: 重启 Container; Pod <code>phase</code> 保持 Running.</li>
<li>OnFailure: 重启 Container; Pod <code>phase</code> 保持 Running.</li>
<li>Never: 不重启 Container; Pod <code>phase</code> 保持 Running.</li>
</ul>
</li>
<li>2个容器的 Pod，其中一个容器停止而另一个失败退出时，不同 <code>restartPolicy</code> 时的动作为<ul>
<li>Always: 重启 Container; Pod <code>phase</code> 保持 Running.</li>
<li>OnFailure: 重启 Container; Pod <code>phase</code> 保持 Running.</li>
<li>Never: Pod <code>phase</code> 变成 Failed.</li>
</ul>
</li>
<li>单容器的 Pod，容器内存不足（OOM），不同 <code>restartPolicy</code> 时的动作为<ul>
<li>Always: 重启 Container; Pod <code>phase</code> 保持 Running.</li>
<li>OnFailure: 重启 Container; Pod <code>phase</code> 保持 Running.</li>
<li>Never: 记录失败事件; Pod <code>phase</code> 变成 Failed.</li>
</ul>
</li>
<li>Pod 还在运行，但磁盘不可访问时<ul>
<li>终止所有容器</li>
<li>Pod <code>phase</code> 变成 Failed</li>
<li>如果 Pod 是由某个控制器管理的，则重新创建一个 Pod 并调度到其他 Node 运行</li>
</ul>
</li>
<li>Pod 还在运行，但由于网络分区故障导致 Node 无法访问<ul>
<li>Node controller等待 Node 事件超时</li>
<li>Node controller 将 Pod <code>phase</code> 设置为 Failed.</li>
<li>如果 Pod 是由某个控制器管理的，则重新创建一个 Pod 并调度到其他 Node 运行</li>
</ul>
</li>
</ul>
<h2 id="使用-volume">使用 Volume</h2>
<p>Volume 可以为容器提供持久化存储，比如</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> redis
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">  - name:</span> redis
<span class="hljs-attr">    image:</span> redis
<span class="hljs-attr">    volumeMounts:</span>
<span class="hljs-attr">    - name:</span> redis-storage
<span class="hljs-attr">      mountPath:</span> /data/redis
<span class="hljs-attr">  volumes:</span>
<span class="hljs-attr">  - name:</span> redis-storage
<span class="hljs-attr">    emptyDir:</span> {}
</code></pre>
<p>更多挂载存储卷的方法参考 <a href="volume.html">Volume</a>。</p>
<h2 id="私有镜像">私有镜像</h2>
<p>在使用私有镜像时，需要创建一个 docker registry secret，并在容器中引用。</p>
<p>创建 docker registry secret：</p>
<pre><code class="lang-sh">kubectl create secret docker-registry regsecret --docker-server=<your-registry-server> --docker-username=<your-name> --docker-password=<your-pword> --docker-email=<your-email>
</code></pre>
<p>比如使用 Azure Container Registry（ACR）：</p>
<pre><code class="lang-sh">ACR_NAME=dregistry
SERVICE_PRINCIPAL_NAME=acr-service-principal

<span class="hljs-comment"># Populate the ACR login server and resource id.</span>
ACR_LOGIN_SERVER=$(az acr show --name <span class="hljs-variable">$ACR_NAME</span> --query loginServer --output tsv)
ACR_REGISTRY_ID=$(az acr show --name <span class="hljs-variable">$ACR_NAME</span> --query id --output tsv)

<span class="hljs-comment"># Create a contributor role assignment with a scope of the ACR resource.</span>
SP_PASSWD=$(az ad sp create-for-rbac --name <span class="hljs-variable">$SERVICE_PRINCIPAL_NAME</span> --role Reader --scopes <span class="hljs-variable">$ACR_REGISTRY_ID</span> --query password --output tsv)

<span class="hljs-comment"># Get the service principle client id.</span>
CLIENT_ID=$(az ad sp show --id http://<span class="hljs-variable">$SERVICE_PRINCIPAL_NAME</span> --query appId --output tsv)

<span class="hljs-comment"># Create secret</span>
kubectl create secret docker-registry acr-auth --docker-server <span class="hljs-variable">$ACR_LOGIN_SERVER</span> --docker-username <span class="hljs-variable">$CLIENT_ID</span> --docker-password <span class="hljs-variable">$SP_PASSWD</span> --docker-email <span class="hljs-built_in">local</span>@local.domain
</code></pre>
<p>容器中引用该 secret：</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> private-reg
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">    - name:</span> private-reg-container
<span class="hljs-attr">      image:</span> dregistry.azurecr.io/acr-auth-example
<span class="hljs-attr">  imagePullSecrets:</span>
<span class="hljs-attr">    - name:</span> acr-auth
</code></pre>
<h2 id="restartpolicy">RestartPolicy</h2>
<p>支持三种 RestartPolicy</p>
<ul>
<li>Always：只要退出就重启</li>
<li>OnFailure：失败退出（exit code 不等于 0）时重启</li>
<li>Never：只要退出就不再重启</li>
</ul>
<p>注意，这里的重启是指在 Pod 所在 Node 上面本地重启，并不会调度到其他 Node 上去。</p>
<h2 id="环境变量">环境变量</h2>
<p>环境变量为容器提供了一些重要的资源，包括容器和 Pod 的基本信息以及集群中服务的信息等：</p>
<p>(1) hostname</p>
<p><code>HOSTNAME</code> 环境变量保存了该 Pod 的 hostname。</p>
<p>（2）容器和 Pod 的基本信息</p>
<p>Pod 的名字、命名空间、IP 以及容器的计算资源限制等可以以 <a href="https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/" target="_blank">Downward API</a> 的方式获取并存储到环境变量中。</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> test
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">    - name:</span> test-container
<span class="hljs-attr">      image:</span> gcr.io/google_containers/busybox
<span class="hljs-attr">      command:</span> [<span class="hljs-string">"sh"</span>, <span class="hljs-string">"-c"</span>]
<span class="hljs-attr">      args:</span>
<span class="hljs-bullet">      -</span> env
<span class="hljs-attr">      resources:</span>
<span class="hljs-attr">        requests:</span>
<span class="hljs-attr">          memory:</span> <span class="hljs-string">"32Mi"</span>
<span class="hljs-attr">          cpu:</span> <span class="hljs-string">"125m"</span>
<span class="hljs-attr">        limits:</span>
<span class="hljs-attr">          memory:</span> <span class="hljs-string">"64Mi"</span>
<span class="hljs-attr">          cpu:</span> <span class="hljs-string">"250m"</span>
<span class="hljs-attr">      env:</span>
<span class="hljs-attr">        - name:</span> MY_NODE_NAME
<span class="hljs-attr">          valueFrom:</span>
<span class="hljs-attr">            fieldRef:</span>
<span class="hljs-attr">              fieldPath:</span> spec.nodeName
<span class="hljs-attr">        - name:</span> MY_POD_NAME
<span class="hljs-attr">          valueFrom:</span>
<span class="hljs-attr">            fieldRef:</span>
<span class="hljs-attr">              fieldPath:</span> metadata.name
<span class="hljs-attr">        - name:</span> MY_POD_NAMESPACE
<span class="hljs-attr">          valueFrom:</span>
<span class="hljs-attr">            fieldRef:</span>
<span class="hljs-attr">              fieldPath:</span> metadata.namespace
<span class="hljs-attr">        - name:</span> MY_POD_IP
<span class="hljs-attr">          valueFrom:</span>
<span class="hljs-attr">            fieldRef:</span>
<span class="hljs-attr">              fieldPath:</span> status.podIP
<span class="hljs-attr">        - name:</span> MY_POD_SERVICE_ACCOUNT
<span class="hljs-attr">          valueFrom:</span>
<span class="hljs-attr">            fieldRef:</span>
<span class="hljs-attr">              fieldPath:</span> spec.serviceAccountName
<span class="hljs-attr">        - name:</span> MY_CPU_REQUEST
<span class="hljs-attr">          valueFrom:</span>
<span class="hljs-attr">            resourceFieldRef:</span>
<span class="hljs-attr">              containerName:</span> test-container
<span class="hljs-attr">              resource:</span> requests.cpu
<span class="hljs-attr">        - name:</span> MY_CPU_LIMIT
<span class="hljs-attr">          valueFrom:</span>
<span class="hljs-attr">            resourceFieldRef:</span>
<span class="hljs-attr">              containerName:</span> test-container
<span class="hljs-attr">              resource:</span> limits.cpu
<span class="hljs-attr">        - name:</span> MY_MEM_REQUEST
<span class="hljs-attr">          valueFrom:</span>
<span class="hljs-attr">            resourceFieldRef:</span>
<span class="hljs-attr">              containerName:</span> test-container
<span class="hljs-attr">              resource:</span> requests.memory
<span class="hljs-attr">        - name:</span> MY_MEM_LIMIT
<span class="hljs-attr">          valueFrom:</span>
<span class="hljs-attr">            resourceFieldRef:</span>
<span class="hljs-attr">              containerName:</span> test-container
<span class="hljs-attr">              resource:</span> limits.memory
<span class="hljs-attr">  restartPolicy:</span> Never
</code></pre>
<p>(3) 集群中服务的信息</p>
<p>容器的环境变量中还可以引用容器运行前创建的所有服务的信息，比如默认的 kubernetes 服务对应以下环境变量：</p>
<pre><code class="lang-sh">KUBERNETES_PORT_443_TCP_ADDR=10.0.0.1
KUBERNETES_SERVICE_HOST=10.0.0.1
KUBERNETES_SERVICE_PORT=443
KUBERNETES_SERVICE_PORT_HTTPS=443
KUBERNETES_PORT=tcp://10.0.0.1:443
KUBERNETES_PORT_443_TCP=tcp://10.0.0.1:443
KUBERNETES_PORT_443_TCP_PROTO=tcp
KUBERNETES_PORT_443_TCP_PORT=443
</code></pre>
<p>由于环境变量存在创建顺序的局限性（环境变量中不包含后来创建的服务），推荐使用 <a href="../components/kube-dns.html">DNS</a> 来解析服务。</p>
<h2 id="镜像拉取策略">镜像拉取策略</h2>
<p>支持三种 ImagePullPolicy</p>
<ul>
<li>Always：不管镜像是否存在都会进行一次拉取</li>
<li>Never：不管镜像是否存在都不会进行拉取</li>
<li>IfNotPresent：只有镜像不存在时，才会进行镜像拉取</li>
</ul>
<p>注意：</p>
<ul>
<li>默认为 <code>IfNotPresent</code>，但 <code>:latest</code> 标签的镜像默认为 <code>Always</code>。</li>
<li>拉取镜像时 docker 会进行校验，如果镜像中的 MD5 码没有变，则不会拉取镜像数据。</li>
<li>生产环境中应该尽量避免使用 <code>:latest</code> 标签，而开发环境中可以借助 <code>:latest</code> 标签自动拉取最新的镜像。</li>
</ul>
<h2 id="访问-dns-的策略">访问 DNS 的策略</h2>
<p>通过设置 dnsPolicy 参数，设置 Pod 中容器访问 DNS 的策略</p>
<ul>
<li>ClusterFirst：优先基于 cluster domain （如 <code>default.svc.cluster.local</code>） 后缀，通过 kube-dns 查询 (默认策略)</li>
<li>Default：优先从 Node 中配置的 DNS 查询</li>
</ul>
<h2 id="使用主机的-ipc-命名空间">使用主机的 IPC 命名空间</h2>
<p>通过设置 <code>spec.hostIPC</code> 参数为 true，使用主机的 IPC 命名空间，默认为 false。</p>
<h2 id="使用主机的网络命名空间">使用主机的网络命名空间</h2>
<p>通过设置 <code>spec.hostNetwork</code> 参数为 true，使用主机的网络命名空间，默认为 false。</p>
<h2 id="使用主机的-pid-空间">使用主机的 PID 空间</h2>
<p>通过设置 <code>spec.hostPID</code> 参数为 true，使用主机的 PID 命名空间，默认为 false。</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> busybox1
<span class="hljs-attr">  labels:</span>
<span class="hljs-attr">    name:</span> busybox
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  hostIPC:</span> <span class="hljs-literal">true</span>
<span class="hljs-attr">  hostPID:</span> <span class="hljs-literal">true</span>
<span class="hljs-attr">  hostNetwork:</span> <span class="hljs-literal">true</span>
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">  - image:</span> busybox
<span class="hljs-attr">    command:</span>
<span class="hljs-bullet">      -</span> sleep
<span class="hljs-bullet">      -</span> <span class="hljs-string">"3600"</span>
<span class="hljs-attr">    name:</span> busybox
</code></pre>
<h2 id="设置-pod-的-hostname">设置 Pod 的 hostname</h2>
<p>通过 <code>spec.hostname</code> 参数实现，如果未设置默认使用 <code>metadata.name</code> 参数的值作为 Pod 的 hostname。</p>
<h2 id="设置-pod-的子域名">设置 Pod 的子域名</h2>
<p>通过 <code>spec.subdomain</code> 参数设置 Pod 的子域名，默认为空。</p>
<p>比如，指定 hostname 为 busybox-2 和 subdomain 为 default-subdomain，完整域名为 <code>busybox-2.default-subdomain.default.svc.cluster.local</code>，也可以简写为 <code>busybox-2.default-subdomain.default</code>：</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> busybox2
<span class="hljs-attr">  labels:</span>
<span class="hljs-attr">    name:</span> busybox
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  hostname:</span> busybox<span class="hljs-bullet">-2</span>
<span class="hljs-attr">  subdomain:</span> default-subdomain
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">  - image:</span> busybox
<span class="hljs-attr">    command:</span>
<span class="hljs-bullet">      -</span> sleep
<span class="hljs-bullet">      -</span> <span class="hljs-string">"3600"</span>
<span class="hljs-attr">    name:</span> busybox
</code></pre>
<p>注意：</p>
<ul>
<li>默认情况下，DNS 为 Pod 生成的 A 记录格式为 <code>pod-ip-address.my-namespace.pod.cluster.local</code>，如 <code>1-2-3-4.default.pod.cluster.local</code></li>
<li>上面的示例还需要在 default namespace 中创建一个名为 <code>default-subdomain</code>（即 subdomain）的 headless service，否则其他 Pod 无法通过完整域名访问到该 Pod（只能自己访问到自己）</li>
</ul>
<pre><code class="lang-yaml"><span class="hljs-attr">kind:</span> Service
<span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> default-subdomain
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  clusterIP:</span> None
<span class="hljs-attr">  selector:</span>
<span class="hljs-attr">    name:</span> busybox
<span class="hljs-attr">  ports:</span>
<span class="hljs-attr">  - name:</span> foo <span class="hljs-comment"># Actually, no port is needed.</span>
<span class="hljs-attr">    port:</span> <span class="hljs-number">1234</span>
<span class="hljs-attr">    targetPort:</span> <span class="hljs-number">1234</span>
</code></pre>
<p>注意，必须为 headless service 设置至少一个服务端口（<code>spec.ports</code>，即便它看起来并不需要），否则 Pod 与 Pod 之间依然无法通过完整域名来访问。</p>
<h2 id="设置-pod-的-dns-选项">设置 Pod 的 DNS 选项</h2>
<p>从 v1.9 开始，可以在 kubelet 和 kube-apiserver 中设置 <code>--feature-gates=CustomPodDNS=true</code> 开启设置每个 Pod DNS 地址的功能。</p>
<blockquote>
<p>注意该功能在 v1.10 中为 Beta 版，v1.9 中为 Alpha 版。</p>
</blockquote>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  namespace:</span> default
<span class="hljs-attr">  name:</span> dns-example
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">    - name:</span> test
<span class="hljs-attr">      image:</span> nginx
<span class="hljs-attr">  dnsPolicy:</span> <span class="hljs-string">"None"</span>
<span class="hljs-attr">  dnsConfig:</span>
<span class="hljs-attr">    nameservers:</span>
<span class="hljs-bullet">      -</span> <span class="hljs-number">1.2</span><span class="hljs-number">.3</span><span class="hljs-number">.4</span>
<span class="hljs-attr">    searches:</span>
<span class="hljs-bullet">      -</span> ns1.svc.cluster.local
<span class="hljs-bullet">      -</span> my.dns.search.suffix
<span class="hljs-attr">    options:</span>
<span class="hljs-attr">      - name:</span> ndots
<span class="hljs-attr">        value:</span> <span class="hljs-string">"2"</span>
<span class="hljs-attr">      - name:</span> edns0
</code></pre>
<p>对于旧版本的集群，可以使用 ConfigMap 来自定义 Pod 的 <code>/etc/resolv.conf</code>，如</p>
<pre><code class="lang-yaml"><span class="hljs-attr">kind:</span> ConfigMap
<span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> resolvconf
<span class="hljs-attr">  namespace:</span> default
<span class="hljs-attr">data:</span>
  resolv.conf: <span class="hljs-string">|
    search default.svc.cluster.local svc.cluster.local cluster.local
    nameserver 10.0.0.10

---
</span><span class="hljs-attr">kind:</span> Deployment
<span class="hljs-attr">apiVersion:</span> extensions/v1beta1
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> dns-test
<span class="hljs-attr">  namespace:</span> default
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  replicas:</span> <span class="hljs-number">1</span>
<span class="hljs-attr">  template:</span>
<span class="hljs-attr">    metadata:</span>
<span class="hljs-attr">      labels:</span>
<span class="hljs-attr">        name:</span> dns-test
<span class="hljs-attr">    spec:</span>
<span class="hljs-attr">      containers:</span>
<span class="hljs-attr">        - name:</span> dns-test
<span class="hljs-attr">          image:</span> alpine
<span class="hljs-attr">          stdin:</span> <span class="hljs-literal">true</span>
<span class="hljs-attr">          tty:</span> <span class="hljs-literal">true</span>
<span class="hljs-attr">          command:</span> [<span class="hljs-string">"sh"</span>]
<span class="hljs-attr">          volumeMounts:</span>
<span class="hljs-attr">            - name:</span> resolv-conf
<span class="hljs-attr">              mountPath:</span> /etc/resolv.conf
<span class="hljs-attr">              subPath:</span> resolv.conf
<span class="hljs-attr">      volumes:</span>
<span class="hljs-attr">        - name:</span> resolv-conf
<span class="hljs-attr">          configMap:</span>
<span class="hljs-attr">            name:</span> resolvconf
<span class="hljs-attr">            items:</span>
<span class="hljs-attr">            - key:</span> resolv.conf
<span class="hljs-attr">              path:</span> resolv.conf
</code></pre>
<h2 id="资源限制">资源限制</h2>
<p>Kubernetes 通过 cgroups 限制容器的 CPU 和内存等计算资源，包括 requests（请求，<strong>调度器保证调度到资源充足的 Node 上，如果无法满足会调度失败</strong>）和 limits（上限）等：</p>
<ul>
<li><code>spec.containers[].resources.limits.cpu</code>：CPU 上限，可以短暂超过，容器也不会被停止</li>
<li><code>spec.containers[].resources.limits.memory</code>：内存上限，不可以超过；如果超过，容器可能会被终止或调度到其他资源充足的机器上</li>
<li><code>spec.containers[].resources.limits.ephemeral-storage</code>：临时存储（容器可写层、日志以及 EmptyDir 等）的上限，超过后 Pod 会被驱逐</li>
<li><code>spec.containers[].resources.requests.cpu</code>：CPU 请求，也是调度 CPU 资源的依据，可以超过</li>
<li><code>spec.containers[].resources.requests.memory</code>：内存请求，也是调度内存资源的依据，可以超过；但如果超过，容器可能会在 Node 内存不足时清理</li>
<li><code>spec.containers[].resources.requests.ephemeral-storage</code>：临时存储（容器可写层、日志以及 EmptyDir 等）的请求，调度容器存储的依据</li>
</ul>
<p>比如 nginx 容器请求 30% 的 CPU 和 56MB 的内存，但限制最多只用 50% 的 CPU 和 128MB 的内存：</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  labels:</span>
<span class="hljs-attr">    app:</span> nginx
<span class="hljs-attr">  name:</span> nginx
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">    - image:</span> nginx
<span class="hljs-attr">      name:</span> nginx
<span class="hljs-attr">      resources:</span>
<span class="hljs-attr">        requests:</span>
<span class="hljs-attr">          cpu:</span> <span class="hljs-string">"300m"</span>
<span class="hljs-attr">          memory:</span> <span class="hljs-string">"56Mi"</span>
<span class="hljs-attr">        limits:</span>
<span class="hljs-attr">          cpu:</span> <span class="hljs-string">"1"</span>
<span class="hljs-attr">          memory:</span> <span class="hljs-string">"128Mi"</span>
</code></pre>
<p>注意</p>
<ul>
<li>CPU 的单位是 CPU 个数，可以用 <code>millicpu (m)</code> 表示少于 1 个 CPU 的情况，如 <code>500m = 500millicpu = 0.5cpu</code>，而一个 CPU 相当于<ul>
<li>AWS 上的一个 vCPU</li>
<li>GCP 上的一个 Core</li>
<li>Azure 上的一个 vCore</li>
<li>物理机上开启超线程时的一个超线程</li>
</ul>
</li>
<li>内存的单位则包括 <code>E, P, T, G, M, K, Ei, Pi, Ti, Gi, Mi, Ki</code> 等。</li>
<li>从 v1.10 开始，可以设置 <code>kubelet ----cpu-manager-policy=static</code> 为 Guaranteed（即 requests.cpu 与 limits.cpu 相等）Pod 绑定 CPU（通过 cpuset cgroups）。</li>
</ul>
<h2 id="健康检查">健康检查</h2>
<p>为了确保容器在部署后确实处在正常运行状态，Kubernetes 提供了两种探针（Probe）来探测容器的状态：</p>
<ul>
<li>LivenessProbe：探测应用是否处于健康状态，如果不健康则删除并重新创建容器</li>
<li>ReadinessProbe：探测应用是否启动完成并且处于正常服务状态，如果不正常则不会接收来自 Kubernetes Service 的流量</li>
</ul>
<p>Kubernetes 支持三种方式来执行探针：</p>
<ul>
<li>exec：在容器中执行一个命令，如果 <a href="http://www.tldp.org/LDP/abs/html/exitcodes.html" target="_blank">命令退出码</a> 返回 <code>0</code> 则表示探测成功，否则表示失败</li>
<li>tcpSocket：对指定的容器 IP 及端口执行一个 TCP 检查，如果端口是开放的则表示探测成功，否则表示失败</li>
<li>httpGet：对指定的容器 IP、端口及路径执行一个 HTTP Get 请求，如果返回的 <a href="https://en.wikipedia.org/wiki/List_of_HTTP_status_codes" target="_blank">状态码</a> 在 <code>[200,400)</code> 之间则表示探测成功，否则表示失败</li>
</ul>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  labels:</span>
<span class="hljs-attr">    app:</span> nginx
<span class="hljs-attr">  name:</span> nginx
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">    containers:</span>
<span class="hljs-attr">    - image:</span> nginx
<span class="hljs-attr">      imagePullPolicy:</span> Always
<span class="hljs-attr">      name:</span> http
<span class="hljs-attr">      livenessProbe:</span>
<span class="hljs-attr">        httpGet:</span>
<span class="hljs-attr">          path:</span> /
<span class="hljs-attr">          port:</span> <span class="hljs-number">80</span>
<span class="hljs-attr">          httpHeaders:</span>
<span class="hljs-attr">          - name:</span> X-Custom-Header
<span class="hljs-attr">            value:</span> Awesome
<span class="hljs-attr">        initialDelaySeconds:</span> <span class="hljs-number">15</span>
<span class="hljs-attr">        timeoutSeconds:</span> <span class="hljs-number">1</span>
<span class="hljs-attr">      readinessProbe:</span>
<span class="hljs-attr">        exec:</span>
<span class="hljs-attr">          command:</span>
<span class="hljs-bullet">          -</span> cat
<span class="hljs-bullet">          -</span> /usr/share/nginx/html/index.html
<span class="hljs-attr">        initialDelaySeconds:</span> <span class="hljs-number">5</span>
<span class="hljs-attr">        timeoutSeconds:</span> <span class="hljs-number">1</span>
<span class="hljs-attr">    - name:</span> goproxy
<span class="hljs-attr">      image:</span> gcr.io/google_containers/goproxy:<span class="hljs-number">0.1</span>
<span class="hljs-attr">      ports:</span>
<span class="hljs-attr">      - containerPort:</span> <span class="hljs-number">8080</span>
<span class="hljs-attr">      readinessProbe:</span>
<span class="hljs-attr">        tcpSocket:</span>
<span class="hljs-attr">          port:</span> <span class="hljs-number">8080</span>
<span class="hljs-attr">        initialDelaySeconds:</span> <span class="hljs-number">5</span>
<span class="hljs-attr">        periodSeconds:</span> <span class="hljs-number">10</span>
<span class="hljs-attr">      livenessProbe:</span>
<span class="hljs-attr">        tcpSocket:</span>
<span class="hljs-attr">          port:</span> <span class="hljs-number">8080</span>
<span class="hljs-attr">        initialDelaySeconds:</span> <span class="hljs-number">15</span>
<span class="hljs-attr">        periodSeconds:</span> <span class="hljs-number">20</span>
</code></pre>
<h2 id="init-container">Init Container</h2>
<p>Pod 能够具有多个容器，应用运行在容器里面，但是它也可能有一个或多个先于应用容器启动的 Init 容器。Init 容器在所有容器运行之前执行（run-to-completion），常用来初始化配置。</p>
<p>如果为一个 Pod 指定了多个 Init 容器，那些容器会按顺序一次运行一个。 每个 Init 容器必须运行成功，下一个才能够运行。 当所有的 Init 容器运行完成时，Kubernetes 初始化 Pod 并像平常一样运行应用容器。</p>
<p>下面是一个 Init 容器的示例：</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> init-demo
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">  - name:</span> nginx
<span class="hljs-attr">    image:</span> nginx
<span class="hljs-attr">    ports:</span>
<span class="hljs-attr">    - containerPort:</span> <span class="hljs-number">80</span>
<span class="hljs-attr">    volumeMounts:</span>
<span class="hljs-attr">    - name:</span> workdir
<span class="hljs-attr">      mountPath:</span> /usr/share/nginx/html
  <span class="hljs-comment"># These containers are run during pod initialization</span>
<span class="hljs-attr">  initContainers:</span>
<span class="hljs-attr">  - name:</span> install
<span class="hljs-attr">    image:</span> busybox
<span class="hljs-attr">    command:</span>
<span class="hljs-bullet">    -</span> wget
<span class="hljs-bullet">    -</span> <span class="hljs-string">"-O"</span>
<span class="hljs-bullet">    -</span> <span class="hljs-string">"/work-dir/index.html"</span>
<span class="hljs-attr">    - http:</span>//kubernetes.io
<span class="hljs-attr">    volumeMounts:</span>
<span class="hljs-attr">    - name:</span> workdir
<span class="hljs-attr">      mountPath:</span> <span class="hljs-string">"/work-dir"</span>
<span class="hljs-attr">  dnsPolicy:</span> Default
<span class="hljs-attr">  volumes:</span>
<span class="hljs-attr">  - name:</span> workdir
<span class="hljs-attr">    emptyDir:</span> {}
</code></pre>
<p>因为 Init 容器具有与应用容器分离的单独镜像，使用 init 容器启动相关代码具有如下优势：</p>
<ul>
<li>它们可以包含并运行实用工具，出于安全考虑，是不建议在应用容器镜像中包含这些实用工具的。</li>
<li>它们可以包含使用工具和定制化代码来安装，但是不能出现在应用镜像中。例如，创建镜像没必要 FROM 另一个镜像，只需要在安装过程中使用类似 sed、 awk、 python 或 dig 这样的工具。</li>
<li>应用镜像可以分离出创建和部署的角色，而没有必要联合它们构建一个单独的镜像。</li>
<li>它们使用 Linux Namespace，所以对应用容器具有不同的文件系统视图。因此，它们能够具有访问 Secret 的权限，而应用容器不能够访问。</li>
<li>它们在应用容器启动之前运行完成，然而应用容器并行运行，所以 Init 容器提供了一种简单的方式来阻塞或延迟应用容器的启动，直到满足了一组先决条件。</li>
</ul>
<p>Init 容器的资源计算，选择一下两者的较大值：</p>
<ul>
<li>所有 Init 容器中的资源使用的最大值</li>
<li>Pod 中所有容器资源使用的总和</li>
</ul>
<p>Init 容器的重启策略：</p>
<ul>
<li>如果 Init 容器执行失败，Pod 设置的 restartPolicy 为 Never，则 pod 将处于 fail 状态。否则 Pod 将一直重新执行每一个 Init 容器直到所有的 Init 容器都成功。</li>
<li>如果 Pod 异常退出，重新拉取 Pod 后，Init 容器也会被重新执行。所以在 Init 容器中执行的任务，需要保证是幂等的。</li>
</ul>
<h2 id="容器生命周期钩子">容器生命周期钩子</h2>
<p>容器生命周期钩子（Container Lifecycle Hooks）监听容器生命周期的特定事件，并在事件发生时执行已注册的回调函数。支持两种钩子：</p>
<ul>
<li>postStart： 容器创建后立即执行，注意由于是异步执行，它无法保证一定在 ENTRYPOINT 之前运行。如果失败，容器会被杀死，并根据 RestartPolicy 决定是否重启</li>
<li>preStop：容器终止前执行，常用于资源清理。如果失败，容器同样也会被杀死</li>
</ul>
<p>而钩子的回调函数支持两种方式：</p>
<ul>
<li>exec：在容器内执行命令，如果命令的退出状态码是 <code>0</code> 表示执行成功，否则表示失败</li>
<li>httpGet：向指定 URL 发起 GET 请求，如果返回的 HTTP 状态码在 <code>[200, 400)</code> 之间表示请求成功，否则表示失败</li>
</ul>
<p>postStart 和 preStop 钩子示例：</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> lifecycle-demo
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">  - name:</span> lifecycle-demo-container
<span class="hljs-attr">    image:</span> nginx
<span class="hljs-attr">    lifecycle:</span>
<span class="hljs-attr">      postStart:</span>
<span class="hljs-attr">        httpGet:</span>
<span class="hljs-attr">          path:</span> /
<span class="hljs-attr">          port:</span> <span class="hljs-number">80</span>
<span class="hljs-attr">      preStop:</span>
<span class="hljs-attr">        exec:</span>
<span class="hljs-attr">          command:</span> [<span class="hljs-string">"/usr/sbin/nginx"</span>,<span class="hljs-string">"-s"</span>,<span class="hljs-string">"quit"</span>]
</code></pre>
<h2 id="使用-capabilities">使用 Capabilities</h2>
<p>默认情况下，容器都是以非特权容器的方式运行。比如，不能在容器中创建虚拟网卡、配置虚拟网络。</p>
<p>Kubernetes 提供了修改 <a href="http://man7.org/linux/man-pages/man7/capabilities.7.html" target="_blank">Capabilities</a> 的机制，可以按需要给容器增加或删除。比如下面的配置给容器增加了 <code>CAP_NET_ADMIN</code> 并删除了 <code>CAP_KILL</code>。</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> cap-pod
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">  - name:</span> friendly-container
<span class="hljs-attr">    image:</span> <span class="hljs-string">"alpine:3.4"</span>
<span class="hljs-attr">    command:</span> [<span class="hljs-string">"/bin/sleep"</span>, <span class="hljs-string">"3600"</span>]
<span class="hljs-attr">    securityContext:</span>
<span class="hljs-attr">      capabilities:</span>
<span class="hljs-attr">        add:</span>
<span class="hljs-bullet">        -</span> NET_ADMIN
<span class="hljs-attr">        drop:</span>
<span class="hljs-bullet">        -</span> KILL
</code></pre>
<h2 id="限制网络带宽">限制网络带宽</h2>
<p>可以通过给 Pod 增加 <code>kubernetes.io/ingress-bandwidth</code> 和 <code>kubernetes.io/egress-bandwidth</code> 这两个 annotation 来限制 Pod 的网络带宽</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> qos
<span class="hljs-attr">  annotations:</span>
    kubernetes.io/ingress-bandwidth: <span class="hljs-number">3</span>M
    kubernetes.io/egress-bandwidth: <span class="hljs-number">4</span>M
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">  - name:</span> iperf3
<span class="hljs-attr">    image:</span> networkstatic/iperf3
<span class="hljs-attr">    command:</span>
<span class="hljs-bullet">    -</span> iperf3
<span class="hljs-bullet">    -</span> -s
</code></pre>
<blockquote>
<p><strong>仅 kubenet 支持限制带宽</strong></p>
<p>目前只有 kubenet 网络插件支持限制网络带宽，其他 CNI 网络插件暂不支持这个功能。</p>
</blockquote>
<p>kubenet 的网络带宽限制其实是通过 tc 来实现的</p>
<pre><code class="lang-sh"><span class="hljs-comment"># setup qdisc (only once)</span>
tc qdisc add dev cbr0 root handle 1: htb default 30
<span class="hljs-comment"># download rate</span>
tc class add dev cbr0 parent 1: classid 1:2 htb rate 3Mbit
tc filter add dev cbr0 protocol ip parent 1:0 prio 1 u32 match ip dst 10.1.0.3/32 flowid 1:2
<span class="hljs-comment"># upload rate</span>
tc class add dev cbr0 parent 1: classid 1:3 htb rate 4Mbit
tc filter add dev cbr0 protocol ip parent 1:0 prio 1 u32 match ip src 10.1.0.3/32 flowid 1:3
</code></pre>
<h2 id="调度到指定的-node-上">调度到指定的 Node 上</h2>
<p>可以通过 nodeSelector、nodeAffinity、podAffinity 以及 Taints 和 tolerations 等来将 Pod 调度到需要的 Node 上。</p>
<p>也可以通过设置 nodeName 参数，将 Pod 调度到指定 node 节点上。</p>
<p>比如，使用 nodeSelector，首先给 Node 加上标签：</p>
<pre><code class="lang-sh">kubectl label nodes <your-node-name> disktype=ssd
</code></pre>
<p>接着，指定该 Pod 只想运行在带有 <code>disktype=ssd</code> 标签的 Node 上：</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> nginx
<span class="hljs-attr">  labels:</span>
<span class="hljs-attr">    env:</span> test
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">  - name:</span> nginx
<span class="hljs-attr">    image:</span> nginx
<span class="hljs-attr">    imagePullPolicy:</span> IfNotPresent
<span class="hljs-attr">  nodeSelector:</span>
<span class="hljs-attr">    disktype:</span> ssd
</code></pre>
<p>nodeAffinity、podAffinity 以及 Taints 和 tolerations 等的使用方法请参考 <a href="../components/scheduler.html">调度器章节</a>。</p>
<h2 id="自定义-hosts">自定义 hosts</h2>
<p>默认情况下，容器的 <code>/etc/hosts</code> 是 kubelet 自动生成的，并且仅包含 localhost 和 podName 等。不建议在容器内直接修改 <code>/etc/hosts</code> 文件，因为在 Pod 启动或重启时会被覆盖。</p>
<p>默认的 <code>/etc/hosts</code> 文件格式如下，其中 <code>nginx-4217019353-fb2c5</code> 是 podName：</p>
<pre><code class="lang-sh">$ kubectl <span class="hljs-built_in">exec</span> nginx-4217019353-fb2c5 -- cat /etc/hosts
<span class="hljs-comment"># Kubernetes-managed hosts file.</span>
127.0.0.1    localhost
::1    localhost ip6-localhost ip6-loopback
fe00::0    ip6-localnet
fe00::0    ip6-mcastprefix
fe00::1    ip6-allnodes
fe00::2    ip6-allrouters
10.244.1.4    nginx-4217019353-fb2c5
</code></pre>
<p>从 v1.7 开始，可以通过 <code>pod.Spec.HostAliases</code> 来增加 hosts 内容，如</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> hostaliases-pod
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  hostAliases:</span>
<span class="hljs-attr">  - ip:</span> <span class="hljs-string">"127.0.0.1"</span>
<span class="hljs-attr">    hostnames:</span>
<span class="hljs-bullet">    -</span> <span class="hljs-string">"foo.local"</span>
<span class="hljs-bullet">    -</span> <span class="hljs-string">"bar.local"</span>
<span class="hljs-attr">  - ip:</span> <span class="hljs-string">"10.1.2.3"</span>
<span class="hljs-attr">    hostnames:</span>
<span class="hljs-bullet">    -</span> <span class="hljs-string">"foo.remote"</span>
<span class="hljs-bullet">    -</span> <span class="hljs-string">"bar.remote"</span>
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">  - name:</span> cat-hosts
<span class="hljs-attr">    image:</span> busybox
<span class="hljs-attr">    command:</span>
<span class="hljs-bullet">    -</span> cat
<span class="hljs-attr">    args:</span>
<span class="hljs-bullet">    -</span> <span class="hljs-string">"/etc/hosts"</span>
</code></pre>
<pre><code class="lang-sh">$ kubectl logs hostaliases-pod
<span class="hljs-comment"># Kubernetes-managed hosts file.</span>
127.0.0.1    localhost
::1    localhost ip6-localhost ip6-loopback
fe00::0    ip6-localnet
fe00::0    ip6-mcastprefix
fe00::1    ip6-allnodes
fe00::2    ip6-allrouters
10.244.1.5    hostaliases-pod
127.0.0.1    foo.local
127.0.0.1    bar.local
10.1.2.3    foo.remote
10.1.2.3    bar.remote
</code></pre>
<h2 id="hugepages">HugePages</h2>
<p>v1.8 + 支持给容器分配 HugePages，资源格式为 <code>hugepages-<size></code>（如 <code>hugepages-2Mi</code>）。使用前要配置</p>
<ul>
<li>开启 <code>--feature-gates="HugePages=true"</code></li>
<li>在所有 Node 上面预分配好 HugePage ，以便 Kubelet 统计所在 Node 的 HugePage 容量</li>
</ul>
<p>使用示例</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  generateName:</span> hugepages-volume-
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">  - image:</span> fedora:latest
<span class="hljs-attr">    command:</span>
<span class="hljs-bullet">    -</span> sleep
<span class="hljs-bullet">    -</span> inf
<span class="hljs-attr">    name:</span> example
<span class="hljs-attr">    volumeMounts:</span>
<span class="hljs-attr">    - mountPath:</span> /hugepages
<span class="hljs-attr">      name:</span> hugepage
<span class="hljs-attr">    resources:</span>
<span class="hljs-attr">      limits:</span>
<span class="hljs-attr">        hugepages-2Mi:</span> <span class="hljs-number">100</span>Mi
<span class="hljs-attr">  volumes:</span>
<span class="hljs-attr">  - name:</span> hugepage
<span class="hljs-attr">    emptyDir:</span>
<span class="hljs-attr">      medium:</span> HugePages
</code></pre>
<p>注意事项</p>
<ul>
<li>HugePage 资源的请求和限制必须相同</li>
<li>HugePage 以 Pod 级别隔离，未来可能会支持容器级的隔离</li>
<li>基于 HugePage 的 EmptyDir 存储卷最多只能使用请求的 HugePage 内存</li>
<li>使用 <code>shmget()</code> 的 <code>SHM_HUGETLB</code> 选项时，应用必须运行在匹配 <code>proc/sys/vm/hugetlb_shm_group</code> 的用户组（supplemental group）中</li>
</ul>
<h2 id="优先级">优先级</h2>
<p>从 v1.8 开始，可以为 Pod 设置一个优先级，保证高优先级的 Pod 优先调度。</p>
<p>优先级调度功能目前为 Beta 版，在 v1.11 版本中默认开启。对 v1.8-1.10 版本中使用前需要开启：</p>
<ul>
<li><code>--feature-gates=PodPriority=true</code></li>
<li><code>--runtime-config=scheduling.k8s.io/v1alpha1=true --admission-control=Controller-Foo,Controller-Bar,...,Priority</code></li>
</ul>
<p>为 Pod 设置优先级前，先创建一个 PriorityClass，并设置优先级（数值越大优先级越高）：</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> scheduling.k8s.io/v1alpha1
<span class="hljs-attr">kind:</span> PriorityClass
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> high-priority
<span class="hljs-attr">value:</span> <span class="hljs-number">1000000</span>
<span class="hljs-attr">globalDefault:</span> <span class="hljs-literal">false</span>
<span class="hljs-attr">description:</span> <span class="hljs-string">"This priority class should be used for XYZ service pods only."</span>
</code></pre>
<blockquote>
<p>Kubernetes 自动创建了 <code>system-cluster-critical</code> 和 <code>system-node-critical</code> 等两个 PriorityClass，用于 Kubernetes 核心组件。</p>
</blockquote>
<p>为 Pod 指定优先级</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> nginx
<span class="hljs-attr">  labels:</span>
<span class="hljs-attr">    env:</span> test
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">  - name:</span> nginx
<span class="hljs-attr">    image:</span> nginx
<span class="hljs-attr">    imagePullPolicy:</span> IfNotPresent
<span class="hljs-attr">  priorityClassName:</span> high-priority
</code></pre>
<p>当调度队列有多个 Pod 需要调度时，优先调度高优先级的 Pod。而当高优先级的 Pod 无法调度时，Kubernetes 会尝试先删除低优先级的 Pod 再将其调度到对应 Node 上（Preemption）。</p>
<p>注意：<strong>受限于 Kubernetes 的调度策略，抢占并不总是成功</strong>。</p>
<h2 id="poddisruptionbudget">PodDisruptionBudget</h2>
<p><a href="https://kubernetes.io/docs/concepts/workloads/pods/disruptions/" target="_blank">PodDisruptionBudget (PDB)</a> 用来保证一组 Pod 同时运行的数量，这些 Pod 需要使用 Deployment、ReplicationController、ReplicaSet 或者 StatefulSet 管理。</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> policy/v1beta1
<span class="hljs-attr">kind:</span> PodDisruptionBudget
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> zk-pdb
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  maxUnavailable:</span> <span class="hljs-number">1</span>
<span class="hljs-attr">  selector:</span>
<span class="hljs-attr">    matchLabels:</span>
<span class="hljs-attr">      app:</span> zookeeper
</code></pre>
<h2 id="sysctls">Sysctls</h2>
<p>Sysctls 允许容器设置内核参数，分为安全 Sysctls 和非安全 Sysctls：</p>
<ul>
<li>安全 Sysctls：即设置后不影响其他 Pod 的内核选项，只作用在容器 namespace 中，默认开启。包括以下几种<ul>
<li><code>kernel.shm_rmid_forced</code></li>
<li><code>net.ipv4.ip_local_port_range</code></li>
<li><code>net.ipv4.tcp_syncookies</code></li>
</ul>
</li>
<li>非安全 Sysctls：即设置好有可能影响其他 Pod 和 Node 上其他服务的内核选项，默认禁止。如果使用，需要管理员在配置 kubelet 时开启，如 <code>kubelet --experimental-allowed-unsafe-sysctls 'kernel.msg*,net.ipv4.route.min_pmtu'</code></li>
</ul>
<p>v1.6-v1.10 示例：</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> sysctl-example
<span class="hljs-attr">  annotations:</span>
    security.alpha.kubernetes.io/sysctls: kernel.shm_rmid_forced=<span class="hljs-number">1</span>
    security.alpha.kubernetes.io/unsafe-sysctls: net.ipv4.route.min_pmtu=<span class="hljs-number">1000</span>,kernel.msgmax=<span class="hljs-number">1</span> <span class="hljs-number">2</span> <span class="hljs-number">3</span>
<span class="hljs-attr">spec:</span>
  ...
</code></pre>
<p>从 v1.11 开始，Sysctls 升级为 Beta 版本，不再区分安全和非安全 sysctl，统一通过 podSpec.securityContext.sysctls 设置，如</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> sysctl-example
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  securityContext:</span>
<span class="hljs-attr">    sysctls:</span>
<span class="hljs-attr">    - name:</span> kernel.shm_rmid_forced
<span class="hljs-attr">      value:</span> <span class="hljs-string">"0"</span>
<span class="hljs-attr">    - name:</span> net.ipv4.route.min_pmtu
<span class="hljs-attr">      value:</span> <span class="hljs-string">"552"</span>
<span class="hljs-attr">    - name:</span> kernel.msgmax
<span class="hljs-attr">      value:</span> <span class="hljs-string">"65536"</span>
  ...
</code></pre>
<h2 id="pod-时区">Pod 时区</h2>
<p>很多容器都是配置了 UTC 时区，与国内集群的 Node 所在时区有可能不一致，可以通过 HostPath 存储插件给容器配置与 Node 一样的时区：</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> sh
<span class="hljs-attr">  namespace:</span> default
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">  - image:</span> alpine
<span class="hljs-attr">    stdin:</span> <span class="hljs-literal">true</span>
<span class="hljs-attr">    tty:</span> <span class="hljs-literal">true</span>
<span class="hljs-attr">    volumeMounts:</span>
<span class="hljs-attr">    - mountPath:</span> /etc/localtime
<span class="hljs-attr">      name:</span> time
<span class="hljs-attr">      readOnly:</span> <span class="hljs-literal">true</span>
<span class="hljs-attr">  volumes:</span>
<span class="hljs-attr">  - hostPath:</span>
<span class="hljs-attr">      path:</span> /etc/localtime
<span class="hljs-attr">      type:</span> <span class="hljs-string">""</span>
<span class="hljs-attr">    name:</span> time
</code></pre>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://kubernetes.io/docs/concepts/workloads/pods/pod/" target="_blank">What is Pod?</a></li>
<li><a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/" target="_blank">Kubernetes Pod Lifecycle</a></li>
<li><a href="https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/" target="_blank">DNS Pods and Services</a></li>
<li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-capabilities-for-a-container" target="_blank">Container capabilities</a></li>
<li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/" target="_blank">Configure Liveness and Readiness Probes</a></li>
<li><a href="https://kubernetes.io/docs/concepts/workloads/pods/init-containers/" target="_blank">Init Containers</a></li>
<li><a href="http://man7.org/linux/man-pages/man7/capabilities.7.html" target="_blank">Linux Capabilities</a></li>
<li><a href="https://kubernetes.io/docs/tasks/manage-hugepages/scheduling-hugepages/" target="_blank">Manage HugePages</a></li>
<li><a href="https://github.com/kubernetes/kubernetes/issues/30039" target="_blank">Document supported docker image (Dockerfile) features</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="podpreset" class="level3">PodPreset</h1>
<p>PodPreset 用来给指定标签的 Pod 注入额外的信息，如环境变量、存储卷等。这样，Pod 模板就不需要为每个 Pod 都显式设置重复的信息。</p>
<h2 id="api-版本对照表">API 版本对照表</h2>
<table>
<thead>
<tr>
<th>Kubernetes 版本</th>
<th>API 版本</th>
<th>默认开启</th>
</tr>
</thead>
<tbody>
<tr>
<td>v1.6+</td>
<td>settings.k8s.io/v1alpha1</td>
<td>否</td>
</tr>
</tbody>
</table>
<h3 id="开启-podpreset">开启 PodPreset</h3>
<ul>
<li>开启 API <code>settings.k8s.io/v1alpha1/podpreset</code></li>
<li>开启准入控制 <code>PodPreset</code></li>
</ul>
<h2 id="podpreset-示例">PodPreset 示例</h2>
<p>增加环境变量和存储卷的 PodPreset</p>
<pre><code class="lang-yaml"><span class="hljs-attr">kind:</span> PodPreset
<span class="hljs-attr">apiVersion:</span> settings.k8s.io/v1alpha1
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> allow-database
<span class="hljs-attr">  namespace:</span> myns
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  selector:</span>
<span class="hljs-attr">    matchLabels:</span>
<span class="hljs-attr">      role:</span> frontend
<span class="hljs-attr">  env:</span>
<span class="hljs-attr">    - name:</span> DB_PORT
<span class="hljs-attr">      value:</span> <span class="hljs-string">"6379"</span>
<span class="hljs-attr">  volumeMounts:</span>
<span class="hljs-attr">    - mountPath:</span> /cache
<span class="hljs-attr">      name:</span> cache-volume
<span class="hljs-attr">  volumes:</span>
<span class="hljs-attr">    - name:</span> cache-volume
<span class="hljs-attr">      emptyDir:</span> {}
</code></pre>
<p>用户提交 Pod</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> website
<span class="hljs-attr">  labels:</span>
<span class="hljs-attr">    app:</span> website
<span class="hljs-attr">    role:</span> frontend
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">    - name:</span> website
<span class="hljs-attr">      image:</span> ecorp/website
<span class="hljs-attr">      ports:</span>
<span class="hljs-attr">        - containerPort:</span> <span class="hljs-number">80</span>
</code></pre>
<p>经过准入控制 <code>PodPreset</code> 后，Pod 会自动增加环境变量和存储卷</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> website
<span class="hljs-attr">  labels:</span>
<span class="hljs-attr">    app:</span> website
<span class="hljs-attr">    role:</span> frontend
<span class="hljs-attr">  annotations:</span>
    podpreset.admission.kubernetes.io/allow-database: <span class="hljs-string">"resource version"</span>
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">    - name:</span> website
<span class="hljs-attr">      image:</span> ecorp/website
<span class="hljs-attr">      volumeMounts:</span>
<span class="hljs-attr">        - mountPath:</span> /cache
<span class="hljs-attr">          name:</span> cache-volume
<span class="hljs-attr">      ports:</span>
<span class="hljs-attr">        - containerPort:</span> <span class="hljs-number">80</span>
<span class="hljs-attr">      env:</span>
<span class="hljs-attr">        - name:</span> DB_PORT
<span class="hljs-attr">          value:</span> <span class="hljs-string">"6379"</span>
<span class="hljs-attr">  volumes:</span>
<span class="hljs-attr">    - name:</span> cache-volume
<span class="hljs-attr">      emptyDir:</span> {}
</code></pre>
<h2 id="configmap-示例">ConfigMap 示例</h2>
<p>ConfigMap</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> ConfigMap
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> etcd-env-config
<span class="hljs-attr">data:</span>
<span class="hljs-attr">  number_of_members:</span> <span class="hljs-string">"1"</span>
<span class="hljs-attr">  initial_cluster_state:</span> new
<span class="hljs-attr">  initial_cluster_token:</span> DUMMY_ETCD_INITIAL_CLUSTER_TOKEN
<span class="hljs-attr">  discovery_token:</span> DUMMY_ETCD_DISCOVERY_TOKEN
<span class="hljs-attr">  discovery_url:</span> http://etcd_discovery:<span class="hljs-number">2379</span>
<span class="hljs-attr">  etcdctl_peers:</span> http://etcd:<span class="hljs-number">2379</span>
<span class="hljs-attr">  duplicate_key:</span> FROM_CONFIG_MAP
<span class="hljs-attr">  REPLACE_ME:</span> <span class="hljs-string">"a value"</span>
</code></pre>
<p>PodPreset</p>
<pre><code class="lang-yaml"><span class="hljs-attr">kind:</span> PodPreset
<span class="hljs-attr">apiVersion:</span> settings.k8s.io/v1alpha1
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> allow-database
<span class="hljs-attr">  namespace:</span> myns
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  selector:</span>
<span class="hljs-attr">    matchLabels:</span>
<span class="hljs-attr">      role:</span> frontend
<span class="hljs-attr">  env:</span>
<span class="hljs-attr">    - name:</span> DB_PORT
<span class="hljs-attr">      value:</span> <span class="hljs-number">6379</span>
<span class="hljs-attr">    - name:</span> duplicate_key
<span class="hljs-attr">      value:</span> FROM_ENV
<span class="hljs-attr">    - name:</span> expansion
<span class="hljs-attr">      value:</span> $(REPLACE_ME)
<span class="hljs-attr">  envFrom:</span>
<span class="hljs-attr">    - configMapRef:</span>
<span class="hljs-attr">        name:</span> etcd-env-config
<span class="hljs-attr">  volumeMounts:</span>
<span class="hljs-attr">    - mountPath:</span> /cache
<span class="hljs-attr">      name:</span> cache-volume
<span class="hljs-attr">    - mountPath:</span> /etc/app/config.json
<span class="hljs-attr">      readOnly:</span> <span class="hljs-literal">true</span>
<span class="hljs-attr">      name:</span> secret-volume
<span class="hljs-attr">  volumes:</span>
<span class="hljs-attr">    - name:</span> cache-volume
<span class="hljs-attr">      emptyDir:</span> {}
<span class="hljs-attr">    - name:</span> secret-volume
<span class="hljs-attr">      secretName:</span> config-details
</code></pre>
<p>用户提交的 Pod</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> website
<span class="hljs-attr">  labels:</span>
<span class="hljs-attr">    app:</span> website
<span class="hljs-attr">    role:</span> frontend
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">    - name:</span> website
<span class="hljs-attr">      image:</span> ecorp/website
<span class="hljs-attr">      ports:</span>
<span class="hljs-attr">        - containerPort:</span> <span class="hljs-number">80</span>
</code></pre>
<p>经过准入控制 <code>PodPreset</code> 后，Pod 会自动增加 ConfigMap 环境变量</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> website
<span class="hljs-attr">  labels:</span>
<span class="hljs-attr">    app:</span> website
<span class="hljs-attr">    role:</span> frontend
<span class="hljs-attr">  annotations:</span>
    podpreset.admission.kubernetes.io/allow-database: <span class="hljs-string">"resource version"</span>
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">    - name:</span> website
<span class="hljs-attr">      image:</span> ecorp/website
<span class="hljs-attr">      volumeMounts:</span>
<span class="hljs-attr">        - mountPath:</span> /cache
<span class="hljs-attr">          name:</span> cache-volume
<span class="hljs-attr">        - mountPath:</span> /etc/app/config.json
<span class="hljs-attr">          readOnly:</span> <span class="hljs-literal">true</span>
<span class="hljs-attr">          name:</span> secret-volume
<span class="hljs-attr">      ports:</span>
<span class="hljs-attr">        - containerPort:</span> <span class="hljs-number">80</span>
<span class="hljs-attr">      env:</span>
<span class="hljs-attr">        - name:</span> DB_PORT
<span class="hljs-attr">          value:</span> <span class="hljs-string">"6379"</span>
<span class="hljs-attr">        - name:</span> duplicate_key
<span class="hljs-attr">          value:</span> FROM_ENV
<span class="hljs-attr">        - name:</span> expansion
<span class="hljs-attr">          value:</span> $(REPLACE_ME)
<span class="hljs-attr">      envFrom:</span>
<span class="hljs-attr">        - configMapRef:</span>
<span class="hljs-attr">          name:</span> etcd-env-config
<span class="hljs-attr">  volumes:</span>
<span class="hljs-attr">    - name:</span> cache-volume
<span class="hljs-attr">      emptyDir:</span> {}
<span class="hljs-attr">    - name:</span> secret-volume
<span class="hljs-attr">      secretName:</span> config-details
</code></pre>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="replicationcontroller-和-replicaset" class="level3">ReplicaSet</h1>
<p>ReplicationController（也简称为 rc）用来确保容器应用的副本数始终保持在用户定义的副本数，即如果有容器异常退出，会自动创建新的 Pod 来替代；而异常多出来的容器也会自动回收。ReplicationController 的典型应用场景包括确保健康 Pod 的数量、弹性伸缩、滚动升级以及应用多版本发布跟踪等。</p>
<p>在新版本的 Kubernetes 中建议使用 ReplicaSet（也简称为 rs）来取代 ReplicationController。ReplicaSet 跟 ReplicationController 没有本质的不同，只是名字不一样，并且 ReplicaSet 支持集合式的 selector（ReplicationController 仅支持等式）。</p>
<p>虽然也 ReplicaSet 可以独立使用，但建议使用 Deployment 来自动管理 ReplicaSet，这样就无需担心跟其他机制的不兼容问题（比如 ReplicaSet 不支持 rolling-update 但 Deployment 支持），并且还支持版本记录、回滚、暂停升级等高级特性。Deployment 的详细介绍和使用方法见 <a href="deployment.html">这里</a>。</p>
<h2 id="api-版本对照表">API 版本对照表</h2>
<table>
<thead>
<tr>
<th>Kubernetes 版本</th>
<th>ReplicaSet API 版本</th>
<th>ReplicationController 版本</th>
</tr>
</thead>
<tbody>
<tr>
<td>v1.5-v1.7</td>
<td>extensions/v1beta1</td>
<td>core/v1</td>
</tr>
<tr>
<td>v1.8</td>
<td>apps/v1beta2</td>
<td>core/v1</td>
</tr>
<tr>
<td>v1.9</td>
<td>apps/v1</td>
<td>core/v1</td>
</tr>
</tbody>
</table>
<h2 id="replicationcontroller-示例">ReplicationController 示例</h2>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> ReplicationController
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> nginx
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  replicas:</span> <span class="hljs-number">3</span>
<span class="hljs-attr">  selector:</span>
<span class="hljs-attr">    app:</span> nginx
<span class="hljs-attr">  template:</span>
<span class="hljs-attr">    metadata:</span>
<span class="hljs-attr">      name:</span> nginx
<span class="hljs-attr">      labels:</span>
<span class="hljs-attr">        app:</span> nginx
<span class="hljs-attr">    spec:</span>
<span class="hljs-attr">      containers:</span>
<span class="hljs-attr">      - name:</span> nginx
<span class="hljs-attr">        image:</span> nginx
<span class="hljs-attr">        ports:</span>
<span class="hljs-attr">        - containerPort:</span> <span class="hljs-number">80</span>
</code></pre>
<h2 id="replicaset-示例">ReplicaSet 示例</h2>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> extensions/v1beta1
<span class="hljs-attr">kind:</span> ReplicaSet
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> frontend
  <span class="hljs-comment"># these labels can be applied automatically</span>
  <span class="hljs-comment"># from the labels in the pod template if not set</span>
  <span class="hljs-comment"># labels:</span>
    <span class="hljs-comment"># app: guestbook</span>
    <span class="hljs-comment"># tier: frontend</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-comment"># this replicas value is default</span>
  <span class="hljs-comment"># modify it according to your case</span>
<span class="hljs-attr">  replicas:</span> <span class="hljs-number">3</span>
  <span class="hljs-comment"># selector can be applied automatically</span>
  <span class="hljs-comment"># from the labels in the pod template if not set,</span>
  <span class="hljs-comment"># but we are specifying the selector here to</span>
  <span class="hljs-comment"># demonstrate its usage.</span>
<span class="hljs-attr">  selector:</span>
<span class="hljs-attr">    matchLabels:</span>
<span class="hljs-attr">      tier:</span> frontend
<span class="hljs-attr">    matchExpressions:</span>
<span class="hljs-bullet">      -</span> {key: tier, operator: In, values: [frontend]}
<span class="hljs-attr">  template:</span>
<span class="hljs-attr">    metadata:</span>
<span class="hljs-attr">      labels:</span>
<span class="hljs-attr">        app:</span> guestbook
<span class="hljs-attr">        tier:</span> frontend
<span class="hljs-attr">    spec:</span>
<span class="hljs-attr">      containers:</span>
<span class="hljs-attr">      - name:</span> php-redis
<span class="hljs-attr">        image:</span> gcr.io/google_samples/gb-frontend:v3
<span class="hljs-attr">        resources:</span>
<span class="hljs-attr">          requests:</span>
<span class="hljs-attr">            cpu:</span> <span class="hljs-number">100</span>m
<span class="hljs-attr">            memory:</span> <span class="hljs-number">100</span>Mi
<span class="hljs-attr">        env:</span>
<span class="hljs-attr">        - name:</span> GET_HOSTS_FROM
<span class="hljs-attr">          value:</span> dns
          <span class="hljs-comment"># If your cluster config does not include a dns service, then to</span>
          <span class="hljs-comment"># instead access environment variables to find service host</span>
          <span class="hljs-comment"># info, comment out the 'value: dns' line above, and uncomment the</span>
          <span class="hljs-comment"># line below.</span>
          <span class="hljs-comment"># value: env</span>
<span class="hljs-attr">        ports:</span>
<span class="hljs-attr">        - containerPort:</span> <span class="hljs-number">80</span>
</code></pre>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="resource-quotas" class="level3">ResourceQuota</h1>
<p>资源配额（Resource Quotas）是用来限制用户资源用量的一种机制。</p>
<p>它的工作原理为</p>
<ul>
<li>资源配额应用在 Namespace 上，并且每个 Namespace 最多只能有一个 <code>ResourceQuota</code> 对象</li>
<li>开启计算资源配额后，创建容器时必须配置计算资源请求或限制（也可以用 <a href="https://kubernetes.io/docs/tasks/administer-cluster/cpu-memory-limit/" target="_blank">LimitRange</a> 设置默认值）</li>
<li>用户超额后禁止创建新的资源</li>
</ul>
<h2 id="开启资源配额功能">开启资源配额功能</h2>
<ul>
<li>首先，在 API Server 启动时配置准入控制 <code>--admission-control=ResourceQuota</code></li>
<li>然后，在 namespace 中创建一个 <code>ResourceQuota</code> 对象</li>
</ul>
<h2 id="资源配额的类型">资源配额的类型</h2>
<ul>
<li>计算资源，包括 cpu 和 memory<ul>
<li>cpu, limits.cpu, requests.cpu</li>
<li>memory, limits.memory, requests.memory</li>
</ul>
</li>
<li>存储资源，包括存储资源的总量以及指定 storage class 的总量<ul>
<li>requests.storage：存储资源总量，如 500Gi</li>
<li>persistentvolumeclaims：pvc 的个数</li>
<li><storage-class-name>.storageclass.storage.k8s.io/requests.storage</storage-class-name></li>
<li><storage-class-name>.storageclass.storage.k8s.io/persistentvolumeclaims</storage-class-name></li>
<li>requests.ephemeral-storage 和 limits.ephemeral-storage （需要 v1.8+）</li>
</ul>
</li>
<li>对象数，即可创建的对象的个数<ul>
<li>pods, replicationcontrollers, configmaps, secrets</li>
<li>resourcequotas, persistentvolumeclaims</li>
<li>services, services.loadbalancers, services.nodeports</li>
</ul>
</li>
</ul>
<p>计算资源示例</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> ResourceQuota
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> compute-resources
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  hard:</span>
<span class="hljs-attr">    pods:</span> <span class="hljs-string">"4"</span>
    requests.cpu: <span class="hljs-string">"1"</span>
    requests.memory: <span class="hljs-number">1</span>Gi
    limits.cpu: <span class="hljs-string">"2"</span>
    limits.memory: <span class="hljs-number">2</span>Gi
</code></pre>
<p>对象个数示例</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> ResourceQuota
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> object-counts
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  hard:</span>
<span class="hljs-attr">    configmaps:</span> <span class="hljs-string">"10"</span>
<span class="hljs-attr">    persistentvolumeclaims:</span> <span class="hljs-string">"4"</span>
<span class="hljs-attr">    replicationcontrollers:</span> <span class="hljs-string">"20"</span>
<span class="hljs-attr">    secrets:</span> <span class="hljs-string">"10"</span>
<span class="hljs-attr">    services:</span> <span class="hljs-string">"10"</span>
    services.loadbalancers: <span class="hljs-string">"2"</span>
</code></pre>
<h2 id="limitrange">LimitRange</h2>
<p>默认情况下，Kubernetes 中所有容器都没有任何 CPU 和内存限制。LimitRange 用来给 Namespace 增加一个资源限制，包括最小、最大和默认资源。比如</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> LimitRange
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> mylimits
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  limits:</span>
<span class="hljs-attr">  - max:</span>
<span class="hljs-attr">      cpu:</span> <span class="hljs-string">"2"</span>
<span class="hljs-attr">      memory:</span> <span class="hljs-number">1</span>Gi
<span class="hljs-attr">    min:</span>
<span class="hljs-attr">      cpu:</span> <span class="hljs-number">200</span>m
<span class="hljs-attr">      memory:</span> <span class="hljs-number">6</span>Mi
<span class="hljs-attr">    type:</span> Pod
<span class="hljs-attr">  - default:</span>
<span class="hljs-attr">      cpu:</span> <span class="hljs-number">300</span>m
<span class="hljs-attr">      memory:</span> <span class="hljs-number">200</span>Mi
<span class="hljs-attr">    defaultRequest:</span>
<span class="hljs-attr">      cpu:</span> <span class="hljs-number">200</span>m
<span class="hljs-attr">      memory:</span> <span class="hljs-number">100</span>Mi
<span class="hljs-attr">    max:</span>
<span class="hljs-attr">      cpu:</span> <span class="hljs-string">"2"</span>
<span class="hljs-attr">      memory:</span> <span class="hljs-number">1</span>Gi
<span class="hljs-attr">    min:</span>
<span class="hljs-attr">      cpu:</span> <span class="hljs-number">100</span>m
<span class="hljs-attr">      memory:</span> <span class="hljs-number">3</span>Mi
<span class="hljs-attr">    type:</span> Container
</code></pre>
<pre><code class="lang-sh">$ kubectl create <span class="hljs-_">-f</span> https://k8s.io/docs/tasks/configure-pod-container/limits.yaml --namespace=<span class="hljs-built_in">limit</span>-example
limitrange <span class="hljs-string">"mylimits"</span> created
$ kubectl describe limits mylimits --namespace=<span class="hljs-built_in">limit</span>-example
Name:   mylimits
Namespace:  <span class="hljs-built_in">limit</span>-example
Type        Resource      M<span class="hljs-keyword">in</span>      Max      Default Request      Default Limit      Max Limit/Request Ratio
----        --------      ---      ---      ---------------      -------------      -----------------------
Pod         cpu           200m     2        -                    -                  -
Pod         memory        6Mi      1Gi      -                    -                  -
Container   cpu           100m     2        200m                 300m               -
Container   memory        3Mi      1Gi      100Mi                200Mi              -
</code></pre>
<h2 id="配额范围">配额范围</h2>
<p>每个配额在创建时可以指定一系列的范围</p>
<table>
<thead>
<tr>
<th>范围</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>Terminating</td>
<td>podSpec.ActiveDeadlineSeconds>=0 的 Pod</td>
</tr>
<tr>
<td>NotTerminating</td>
<td>podSpec.activeDeadlineSeconds=nil 的 Pod</td>
</tr>
<tr>
<td>BestEffort</td>
<td>所有容器的 requests 和 limits 都没有设置的 Pod（Best-Effort）</td>
</tr>
<tr>
<td>NotBestEffort</td>
<td>与 BestEffort 相反</td>
</tr>
</tbody>
</table>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="secret" class="level3">Secret</h1>
<p>Secret 解决了密码、token、密钥等敏感数据的配置问题，而不需要把这些敏感数据暴露到镜像或者 Pod Spec 中。Secret 可以以 Volume 或者环境变量的方式使用。</p>
<h2 id="secret-类型">Secret 类型</h2>
<p>Secret 有三种类型：</p>
<ul>
<li>Opaque：base64 编码格式的 Secret，用来存储密码、密钥等；但数据也通过 base64 --decode 解码得到原始数据，所有加密性很弱。</li>
<li><code>kubernetes.io/dockerconfigjson</code>：用来存储私有 docker registry 的认证信息。</li>
<li><code>kubernetes.io/service-account-token</code>： 用于被 serviceaccount 引用。serviceaccout 创建时 Kubernetes 会默认创建对应的 secret。Pod 如果使用了 serviceaccount，对应的 secret 会自动挂载到 Pod 的 <code>/run/secrets/kubernetes.io/serviceaccount</code> 目录中。</li>
</ul>
<p>备注：serviceaccount 用来使得 Pod 能够访问 Kubernetes API</p>
<h2 id="api-版本对照表">API 版本对照表</h2>
<table>
<thead>
<tr>
<th>Kubernetes 版本</th>
<th>Core API 版本</th>
</tr>
</thead>
<tbody>
<tr>
<td>v1.5+</td>
<td>core/v1</td>
</tr>
</tbody>
</table>
<h2 id="opaque-secret">Opaque Secret</h2>
<p>Opaque 类型的数据是一个 map 类型，要求 value 是 base64 编码格式：</p>
<pre><code class="lang-sh">$ <span class="hljs-built_in">echo</span> -n <span class="hljs-string">"admin"</span> | base64
YWRtaW4=
$ <span class="hljs-built_in">echo</span> -n <span class="hljs-string">"1f2d1e2e67df"</span> | base64
MWYyZDFlMmU2N2Rm
</code></pre>
<p>secrets.yml</p>
<pre><code class="lang-yml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Secret
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> mysecret
<span class="hljs-attr">type:</span> Opaque
<span class="hljs-attr">data:</span>
<span class="hljs-attr">  password:</span> MWYyZDFlMmU2N2Rm
<span class="hljs-attr">  username:</span> YWRtaW4=
</code></pre>
<p>创建 secret：<code>kubectl create -f secrets.yml</code>。</p>
<pre><code class="lang-sh"><span class="hljs-comment"># kubectl get secret</span>
NAME                  TYPE                                  DATA      AGE
default-token-cty7p   kubernetes.io/service-account-token   3         45d
mysecret              Opaque                                2         7s
</code></pre>
<p>注意：其中 default-token-cty7p 为创建集群时默认创建的 secret，被 serviceacount/default 引用。</p>
<p>如果是从文件创建 secret，则可以用更简单的 kubectl 命令，比如创建 tls 的 secret：</p>
<pre><code class="lang-sh">$ kubectl create secret generic helloworld-tls \
  --from-file=key.pem \
  --from-file=cert.pem
</code></pre>
<h2 id="opaque-secret-的使用">Opaque Secret 的使用</h2>
<p>创建好 secret 之后，有两种方式来使用它：</p>
<ul>
<li>以 Volume 方式</li>
<li>以环境变量方式</li>
</ul>
<h3 id="将-secret-挂载到-volume-中">将 Secret 挂载到 Volume 中</h3>
<pre><code class="lang-yml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  labels:</span>
<span class="hljs-attr">    name:</span> db
<span class="hljs-attr">  name:</span> db
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  volumes:</span>
<span class="hljs-attr">  - name:</span> secrets
<span class="hljs-attr">    secret:</span>
<span class="hljs-attr">      secretName:</span> mysecret
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">  - image:</span> gcr.io/my_project_id/pg:v1
<span class="hljs-attr">    name:</span> db
<span class="hljs-attr">    volumeMounts:</span>
<span class="hljs-attr">    - name:</span> secrets
<span class="hljs-attr">      mountPath:</span> <span class="hljs-string">"/etc/secrets"</span>
<span class="hljs-attr">      readOnly:</span> <span class="hljs-literal">true</span>
<span class="hljs-attr">    ports:</span>
<span class="hljs-attr">    - name:</span> cp
<span class="hljs-attr">      containerPort:</span> <span class="hljs-number">5432</span>
<span class="hljs-attr">      hostPort:</span> <span class="hljs-number">5432</span>
</code></pre>
<p>查看 Pod 中对应的信息：</p>
<pre><code class="lang-sh"><span class="hljs-comment"># ls /etc/secrets</span>
password  username
<span class="hljs-comment"># cat  /etc/secrets/username</span>
admin
<span class="hljs-comment"># cat  /etc/secrets/password</span>
1f2d1e2e67df
</code></pre>
<h3 id="将-secret-导出到环境变量中">将 Secret 导出到环境变量中</h3>
<pre><code class="lang-yml"><span class="hljs-attr">apiVersion:</span> extensions/v1beta1
<span class="hljs-attr">kind:</span> Deployment
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> wordpress-deployment
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  replicas:</span> <span class="hljs-number">2</span>
<span class="hljs-attr">  strategy:</span>
<span class="hljs-attr">      type:</span> RollingUpdate
<span class="hljs-attr">  template:</span>
<span class="hljs-attr">    metadata:</span>
<span class="hljs-attr">      labels:</span>
<span class="hljs-attr">        app:</span> wordpress
<span class="hljs-attr">        visualize:</span> <span class="hljs-string">"true"</span>
<span class="hljs-attr">    spec:</span>
<span class="hljs-attr">      containers:</span>
<span class="hljs-attr">      - name:</span> <span class="hljs-string">"wordpress"</span>
<span class="hljs-attr">        image:</span> <span class="hljs-string">"wordpress"</span>
<span class="hljs-attr">        ports:</span>
<span class="hljs-attr">        - containerPort:</span> <span class="hljs-number">80</span>
<span class="hljs-attr">        env:</span>
<span class="hljs-attr">        - name:</span> WORDPRESS_DB_USER
<span class="hljs-attr">          valueFrom:</span>
<span class="hljs-attr">            secretKeyRef:</span>
<span class="hljs-attr">              name:</span> mysecret
<span class="hljs-attr">              key:</span> username
<span class="hljs-attr">        - name:</span> WORDPRESS_DB_PASSWORD
<span class="hljs-attr">          valueFrom:</span>
<span class="hljs-attr">            secretKeyRef:</span>
<span class="hljs-attr">              name:</span> mysecret
<span class="hljs-attr">              key:</span> password
</code></pre>
<h3 id="将-secret-挂载指定的-key">将 Secret 挂载指定的 key</h3>
<pre><code class="lang-yml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  labels:</span>
<span class="hljs-attr">    name:</span> db
<span class="hljs-attr">  name:</span> db
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  volumes:</span>
<span class="hljs-attr">  - name:</span> secrets
<span class="hljs-attr">    secret:</span>
<span class="hljs-attr">      secretName:</span> mysecret
<span class="hljs-attr">      items:</span>
<span class="hljs-attr">      - key:</span> password
<span class="hljs-attr">        mode:</span> <span class="hljs-number">511</span>
<span class="hljs-attr">        path:</span> tst/psd
<span class="hljs-attr">      - key:</span> username
<span class="hljs-attr">        mode:</span> <span class="hljs-number">511</span>
<span class="hljs-attr">        path:</span> tst/usr
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">  - image:</span> nginx
<span class="hljs-attr">    name:</span> db
<span class="hljs-attr">    volumeMounts:</span>
<span class="hljs-attr">    - name:</span> secrets
<span class="hljs-attr">      mountPath:</span> <span class="hljs-string">"/etc/secrets"</span>
<span class="hljs-attr">      readOnly:</span> <span class="hljs-literal">true</span>
<span class="hljs-attr">    ports:</span>
<span class="hljs-attr">    - name:</span> cp
<span class="hljs-attr">      containerPort:</span> <span class="hljs-number">80</span>
<span class="hljs-attr">      hostPort:</span> <span class="hljs-number">5432</span>
</code></pre>
<p>创建 Pod 成功后，可以在对应的目录看到：</p>
<pre><code class="lang-sh"><span class="hljs-comment"># kubectl exec db ls /etc/secrets/tst</span>
psd
usr
</code></pre>
<p><strong> 注意 </strong>：</p>
<p>1、<code>kubernetes.io/dockerconfigjson</code> 和 <code>kubernetes.io/service-account-token</code> 类型的 secret 也同样可以被挂载成文件 (目录)。
如果使用 <code>kubernetes.io/dockerconfigjson</code> 类型的 secret 会在目录下创建一个. dockercfg 文件</p>
<pre><code class="lang-sh">root@db:/etc/secrets<span class="hljs-comment"># ls -al</span>
total 4
drwxrwxrwt  3 root root  100 Aug  5 16:06 .
drwxr-xr-x 42 root root 4096 Aug  5 16:06 ..
drwxr-xr-x  2 root root   60 Aug  5 16:06 ..8988_06_08_00_06_52.433429084
lrwxrwxrwx  1 root root   31 Aug  5 16:06 ..data -> ..8988_06_08_00_06_52.433429084
lrwxrwxrwx  1 root root   17 Aug  5 16:06 .dockercfg -> ..data/.dockercfg
</code></pre>
<p>如果使用 <code>kubernetes.io/service-account-token</code> 类型的 secret 则会创建 ca.crt，namespace，token 三个文件</p>
<pre><code class="lang-sh">root@db:/etc/secrets<span class="hljs-comment"># ls</span>
ca.crt    namespace  token
</code></pre>
<p>2、secrets 使用时被挂载到一个临时目录，Pod 被删除后 secrets 挂载时生成的文件也会被删除。</p>
<pre><code class="lang-sh">root@db:/etc/secrets<span class="hljs-comment"># df</span>
Filesystem     1K-blocks    Used Available Use% Mounted on
none           123723748 4983104 112432804   5% /
tmpfs            1957660       0   1957660   0% /dev
tmpfs            1957660       0   1957660   0% /sys/fs/cgroup
/dev/vda1       51474044 2444568  46408092   6% /etc/hosts
tmpfs            1957660      12   1957648   1% /etc/secrets
/dev/vdb       123723748 4983104 112432804   5% /etc/hostname
shm                65536       0     65536   0% /dev/shm
</code></pre>
<p>但如果在 Pod 运行的时候，在 Pod 部署的节点上还是可以看到：</p>
<pre><code class="lang-sh"><span class="hljs-comment"># 查看 Pod 中容器 Secret 的相关信息，其中 4392b02d-79f9-11e7-a70a-525400bc11f0 为 Pod 的 UUID</span>
<span class="hljs-string">"Mounts"</span>: [
  {
    <span class="hljs-string">"Source"</span>: <span class="hljs-string">"/var/lib/kubelet/pods/4392b02d-79f9-11e7-a70a-525400bc11f0/volumes/kubernetes.io~secret/secrets"</span>,
    <span class="hljs-string">"Destination"</span>: <span class="hljs-string">"/etc/secrets"</span>,
    <span class="hljs-string">"Mode"</span>: <span class="hljs-string">"ro"</span>,
    <span class="hljs-string">"RW"</span>: <span class="hljs-literal">false</span>,
    <span class="hljs-string">"Propagation"</span>: <span class="hljs-string">"rprivate"</span>
  }
]
<span class="hljs-comment">#在 Pod 部署的节点查看</span>
root@VM-0-178-ubuntu:/var/lib/kubelet/pods/4392b02d-79f9-11e7<span class="hljs-_">-a</span>70a-525400bc11f0/volumes/kubernetes.io~secret/secrets<span class="hljs-comment"># ls -al</span>
total 4
drwxrwxrwt 3 root root  140 Aug  6 00:15 .
drwxr-xr-x 3 root root 4096 Aug  6 00:15 ..
drwxr-xr-x 2 root root  100 Aug  6 00:15 ..8988_06_08_00_15_14.253276142
lrwxrwxrwx 1 root root   31 Aug  6 00:15 ..data -> ..8988_06_08_00_15_14.253276142
lrwxrwxrwx 1 root root   13 Aug  6 00:15 ca.crt -> ..data/ca.crt
lrwxrwxrwx 1 root root   16 Aug  6 00:15 namespace -> ..data/namespace
lrwxrwxrwx 1 root root   12 Aug  6 00:15 token -> ..data/token
</code></pre>
<h2 id="kubernetesiodockerconfigjson">kubernetes.io/dockerconfigjson</h2>
<p>可以直接用 kubectl 命令来创建用于 docker registry 认证的 secret：</p>
<pre><code class="lang-sh">$ kubectl create secret docker-registry myregistrykey --docker-server=DOCKER_REGISTRY_SERVER --docker-username=DOCKER_USER --docker-password=DOCKER_PASSWORD --docker-email=DOCKER_EMAIL
secret <span class="hljs-string">"myregistrykey"</span> created.
</code></pre>
<p>查看 secret 的内容：</p>
<pre><code class="lang-sh"><span class="hljs-comment"># kubectl get secret myregistrykey  -o yaml</span>
apiVersion: v1
data:
  .dockercfg: eyJjY3IuY2NzLnRlbmNlbnR5dW4uY29tL3RlbmNlbnR5dW4iOnsidXNlcm5hbWUiOiIzMzIxMzM3OTk0IiwicGFzc3dvcmQiOiIxMjM0NTYuY29tIiwiZW1haWwiOiIzMzIxMzM3OTk0QHFxLmNvbSIsImF1dGgiOiJNek15TVRNek56azVORG94TWpNME5UWXVZMjl0In19
kind: Secret
metadata:
  creationTimestamp: 2017-08-04T02:06:05Z
  name: myregistrykey
  namespace: default
  resourceVersion: <span class="hljs-string">"1374279324"</span>
  selfLink: /api/v1/namespaces/default/secrets/myregistrykey
  uid: 78f6a423-78b9-11e7<span class="hljs-_">-a</span>70a-525400bc11f0
<span class="hljs-built_in">type</span>: kubernetes.io/dockercfg
</code></pre>
<p>通过 base64 对 secret 中的内容解码：</p>
<pre><code class="lang-sh"><span class="hljs-comment"># echo "eyJjY3IuY2NzLnRlbmNlbnR5dW4uY29tL3RlbmNlbnR5dW4iOnsidXNlcm5hbWUiOiIzMzIxMzM3OTk0IiwicGFzc3dvcmQiOiIxMjM0NTYuY29tIiwiZW1haWwiOiIzMzIxMzM3OTk0QHFxLmNvbSIsImF1dGgiOiJNek15TVRNek56azVORG94TWpNME5UWXVZMjl0XXXX" | base64 --decode</span>
{<span class="hljs-string">"ccr.ccs.tencentyun.com/XXXXXXX"</span>:{<span class="hljs-string">"username"</span>:<span class="hljs-string">"3321337XXX"</span>,<span class="hljs-string">"password"</span>:<span class="hljs-string">"123456.com"</span>,<span class="hljs-string">"email"</span>:<span class="hljs-string">"3321337XXX@qq.com"</span>,<span class="hljs-string">"auth"</span>:<span class="hljs-string">"MzMyMTMzNzk5NDoxMjM0NTYuY29t"</span>}}
</code></pre>
<p>也可以直接读取 <code>~/.dockercfg</code> 的内容来创建：</p>
<pre><code class="lang-sh">$ kubectl create secret docker-registry myregistrykey \
  --from-file=<span class="hljs-string">"~/.dockercfg"</span>
</code></pre>
<p>在创建 Pod 的时候，通过 <code>imagePullSecrets</code> 来引用刚创建的 <code>myregistrykey</code>:</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> foo
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">    - name:</span> foo
<span class="hljs-attr">      image:</span> janedoe/awesomeapp:v1
<span class="hljs-attr">  imagePullSecrets:</span>
<span class="hljs-attr">    - name:</span> myregistrykey
</code></pre>
<h3 id="kubernetesioservice-account-token">kubernetes.io/service-account-token</h3>
<p><code>kubernetes.io/service-account-token</code>： 用于被 serviceaccount 引用。serviceaccout 创建时 Kubernetes 会默认创建对应的 secret。Pod 如果使用了 serviceaccount，对应的 secret 会自动挂载到 Pod 的 <code>/run/secrets/kubernetes.io/serviceaccount</code> 目录中。</p>
<pre><code class="lang-sh">$ kubectl run nginx --image nginx
deployment <span class="hljs-string">"nginx"</span> created
$ kubectl get pods
NAME                     READY     STATUS    RESTARTS   AGE
nginx-3137573019-md1u2   1/1       Running   0          13s
$ kubectl <span class="hljs-built_in">exec</span> nginx-3137573019-md1u2 ls /run/secrets/kubernetes.io/serviceaccount
ca.crt
namespace
token
</code></pre>
<h2 id="存储加密">存储加密</h2>
<p>v1.7 + 版本支持将 Secret 数据加密存储到 etcd 中，只需要在 apiserver 启动时配置 <code>--experimental-encryption-provider-config</code>。加密配置格式为</p>
<pre><code class="lang-yaml"><span class="hljs-attr">kind:</span> EncryptionConfig
<span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">resources:</span>
<span class="hljs-attr">  - resources:</span>
<span class="hljs-bullet">    -</span> secrets
<span class="hljs-attr">    providers:</span>
<span class="hljs-attr">    - aescbc:</span>
<span class="hljs-attr">        keys:</span>
<span class="hljs-attr">        - name:</span> key1
<span class="hljs-attr">          secret:</span> c2VjcmV0IGlzIHNlY3VyZQ==
<span class="hljs-attr">        - name:</span> key2
<span class="hljs-attr">          secret:</span> dGhpcyBpcyBwYXNzd29yZA==
<span class="hljs-attr">    - identity:</span> {}
<span class="hljs-attr">    - aesgcm:</span>
<span class="hljs-attr">        keys:</span>
<span class="hljs-attr">        - name:</span> key1
<span class="hljs-attr">          secret:</span> c2VjcmV0IGlzIHNlY3VyZQ==
<span class="hljs-attr">        - name:</span> key2
<span class="hljs-attr">          secret:</span> dGhpcyBpcyBwYXNzd29yZA==
<span class="hljs-attr">    - secretbox:</span>
<span class="hljs-attr">        keys:</span>
<span class="hljs-attr">        - name:</span> key1
<span class="hljs-attr">          secret:</span> YWJjZGVmZ2hpamtsbW5vcHFyc3R1dnd4eXoxMjM0NTY=
</code></pre>
<p>其中</p>
<ul>
<li>resources.resources 是 Kubernetes 的资源名</li>
<li>resources.providers 是加密方法，支持以下几种<ul>
<li>identity：不加密</li>
<li>aescbc：AES-CBC 加密</li>
<li>secretbox：XSalsa20 和 Poly1305 加密</li>
<li>aesgcm：AES-GCM 加密</li>
</ul>
</li>
</ul>
<p>Secret 是在写存储的时候加密，因而可以对已有的 secret 执行 update 操作来保证所有的 secrets 都加密</p>
<pre><code class="lang-sh">kubectl get secrets -o json | kubectl update <span class="hljs-_">-f</span> -
</code></pre>
<p>如果想取消 secret 加密的话，只需要把 <code>identity</code> 放到 providers 的第一个位置即可（aescbc 还要留着以便访问已存储的 secret）：</p>
<pre><code class="lang-yaml"><span class="hljs-attr">kind:</span> EncryptionConfig
<span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">resources:</span>
<span class="hljs-attr">  - resources:</span>
<span class="hljs-bullet">    -</span> secrets
<span class="hljs-attr">    providers:</span>
<span class="hljs-attr">    - identity:</span> {}
<span class="hljs-attr">    - aescbc:</span>
<span class="hljs-attr">        keys:</span>
<span class="hljs-attr">        - name:</span> key1
<span class="hljs-attr">          secret:</span> c2VjcmV0IGlzIHNlY3VyZQ==
<span class="hljs-attr">        - name:</span> key2
<span class="hljs-attr">          secret:</span> dGhpcyBpcyBwYXNzd29yZA==
</code></pre>
<h2 id="secret-与-configmap-对比">Secret 与 ConfigMap 对比</h2>
<p>相同点：</p>
<ul>
<li>key/value 的形式</li>
<li>属于某个特定的 namespace</li>
<li>可以导出到环境变量</li>
<li>可以通过目录 / 文件形式挂载 (支持挂载所有 key 和部分 key)</li>
</ul>
<p>不同点：</p>
<ul>
<li>Secret 可以被 ServerAccount 关联 (使用)</li>
<li>Secret 可以存储 register 的鉴权信息，用在 ImagePullSecret 参数中，用于拉取私有仓库的镜像</li>
<li>Secret 支持 Base64 加密</li>
<li>Secret 分为 Opaque，kubernetes.io/Service Account，kubernetes.io/dockerconfigjson 三种类型, Configmap 不区分类型</li>
<li>Secret 文件存储在 tmpfs 文件系统中，Pod 删除后 Secret 文件也会对应的删除。</li>
</ul>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://kubernetes.io/docs/concepts/configuration/secret/" target="_blank">Secret</a></li>
<li><a href="https://kubernetes.io/docs/concepts/configuration/secret/" target="_blank">Specifying ImagePullSecrets on a Pod</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="security-context-和-pod-security-policy" class="level3">SecurityContext</h1>
<p>Security Context 的目的是限制不可信容器的行为，保护系统和其他容器不受其影响。</p>
<p>Kubernetes 提供了三种配置 Security Context 的方法：</p>
<ul>
<li>Container-level Security Context：仅应用到指定的容器</li>
<li>Pod-level Security Context：应用到 Pod 内所有容器以及 Volume</li>
<li>Pod Security Policies（PSP）：应用到集群内部所有 Pod 以及 Volume</li>
</ul>
<h2 id="container-level-security-context">Container-level Security Context</h2>
<p><a href="https://kubernetes.io/docs/api-reference/v1.6/#securitycontext-v1-core" target="_blank">Container-level Security Context</a> 仅应用到指定的容器上，并且不会影响 Volume。比如设置容器运行在特权模式：</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> hello-world
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">    - name:</span> hello-world-container
      <span class="hljs-comment"># The container definition</span>
      <span class="hljs-comment"># ...</span>
<span class="hljs-attr">      securityContext:</span>
<span class="hljs-attr">        privileged:</span> <span class="hljs-literal">true</span>
</code></pre>
<h2 id="pod-level-security-context">Pod-level Security Context</h2>
<p><a href="https://kubernetes.io/docs/api-reference/v1.6/#podsecuritycontext-v1-core" target="_blank">Pod-level Security Context</a> 应用到 Pod 内所有容器，并且还会影响 Volume（包括 fsGroup 和 selinuxOptions）。</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> hello-world
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  containers:</span>
  <span class="hljs-comment"># specification of the pod's containers</span>
  <span class="hljs-comment"># ...</span>
<span class="hljs-attr">  securityContext:</span>
<span class="hljs-attr">    fsGroup:</span> <span class="hljs-number">1234</span>
<span class="hljs-attr">    supplementalGroups:</span> [<span class="hljs-number">5678</span>]
<span class="hljs-attr">    seLinuxOptions:</span>
<span class="hljs-attr">      level:</span> <span class="hljs-string">"s0:c123,c456"</span>
</code></pre>
<h2 id="pod-security-policies（psp）">Pod Security Policies（PSP）</h2>
<p>Pod Security Policies（PSP）是集群级的 Pod 安全策略，自动为集群内的 Pod 和 Volume 设置 Security Context。</p>
<p>使用 PSP 需要 API Server 开启 <code>extensions/v1beta1/podsecuritypolicy</code>，并且配置 <code>PodSecurityPolicy</code> admission 控制器。</p>
<h3 id="支持的控制项">支持的控制项</h3>
<table>
<thead>
<tr>
<th>控制项</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>privileged</td>
<td>运行特权容器</td>
</tr>
<tr>
<td>defaultAddCapabilities</td>
<td>可添加到容器的 Capabilities</td>
</tr>
<tr>
<td>requiredDropCapabilities</td>
<td>会从容器中删除的 Capabilities</td>
</tr>
<tr>
<td>allowedCapabilities</td>
<td>允许使用的 Capabilities 列表</td>
</tr>
<tr>
<td>volumes</td>
<td>控制容器可以使用哪些 volume</td>
</tr>
<tr>
<td>hostNetwork</td>
<td>允许使用 host 网络</td>
</tr>
<tr>
<td>hostPorts</td>
<td>允许的 host 端口列表</td>
</tr>
<tr>
<td>hostPID</td>
<td>使用 host PID namespace</td>
</tr>
<tr>
<td>hostIPC</td>
<td>使用 host IPC namespace</td>
</tr>
<tr>
<td>seLinux</td>
<td>SELinux Context</td>
</tr>
<tr>
<td>runAsUser</td>
<td>user ID</td>
</tr>
<tr>
<td>supplementalGroups</td>
<td>允许的补充用户组</td>
</tr>
<tr>
<td>fsGroup</td>
<td>volume FSGroup</td>
</tr>
<tr>
<td>readOnlyRootFilesystem</td>
<td>只读根文件系统</td>
</tr>
<tr>
<td>allowedHostPaths</td>
<td>允许 hostPath 插件使用的路径列表</td>
</tr>
<tr>
<td>allowedFlexVolumes</td>
<td>允许使用的 flexVolume 插件列表</td>
</tr>
<tr>
<td>allowPrivilegeEscalation</td>
<td>允许容器进程设置  <a href="https://www.kernel.org/doc/Documentation/prctl/no_new_privs.txt" target="_blank"><code>no_new_privs</code></a></td>
</tr>
<tr>
<td>defaultAllowPrivilegeEscalation</td>
<td>默认是否允许特权升级</td>
</tr>
</tbody>
</table>
<h3 id="示例">示例</h3>
<p>限制容器的 host 端口范围为 8000-8080：</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> extensions/v1beta1
<span class="hljs-attr">kind:</span> PodSecurityPolicy
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> permissive
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  seLinux:</span>
<span class="hljs-attr">    rule:</span> RunAsAny
<span class="hljs-attr">  supplementalGroups:</span>
<span class="hljs-attr">    rule:</span> RunAsAny
<span class="hljs-attr">  runAsUser:</span>
<span class="hljs-attr">    rule:</span> RunAsAny
<span class="hljs-attr">  fsGroup:</span>
<span class="hljs-attr">    rule:</span> RunAsAny
<span class="hljs-attr">  hostPorts:</span>
<span class="hljs-attr">  - min:</span> <span class="hljs-number">8000</span>
<span class="hljs-attr">    max:</span> <span class="hljs-number">8080</span>
<span class="hljs-attr">  volumes:</span>
<span class="hljs-bullet">  -</span> <span class="hljs-string">'*'</span>
</code></pre>
<p>限制只允许使用 lvm 和 cifs 等 flexVolume 插件：</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> extensions/v1beta1
<span class="hljs-attr">kind:</span> PodSecurityPolicy
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> allow-flex-volumes
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  fsGroup:</span>
<span class="hljs-attr">    rule:</span> RunAsAny
<span class="hljs-attr">  runAsUser:</span>
<span class="hljs-attr">    rule:</span> RunAsAny
<span class="hljs-attr">  seLinux:</span>
<span class="hljs-attr">    rule:</span> RunAsAny
<span class="hljs-attr">  supplementalGroups:</span>
<span class="hljs-attr">    rule:</span> RunAsAny
<span class="hljs-attr">  volumes:</span>
<span class="hljs-bullet">    -</span> flexVolume
<span class="hljs-attr">  allowedFlexVolumes:</span> 
<span class="hljs-attr">    - driver:</span> example/lvm
<span class="hljs-attr">    - driver:</span> example/cifs
</code></pre>
<h2 id="selinux">SELinux</h2>
<p>SELinux (Security-Enhanced Linux) 是一种强制访问控制（mandatory access control）的实现。它的作法是以最小权限原则（principle of least privilege）为基础，在 Linux 核心中使用 Linux 安全模块（Linux Security Modules）。SELinux 主要由美国国家安全局开发，并于 2000 年 12 月 22 日发行给开放源代码的开发社区。</p>
<p>可以通过 runcon 来为进程设置安全策略，ls 和 ps 的 - Z 参数可以查看文件或进程的安全策略。</p>
<h3 id="开启与关闭-selinux">开启与关闭 SELinux</h3>
<p>修改 / etc/selinux/config 文件方法：</p>
<ul>
<li>开启：SELINUX=enforcing</li>
<li>关闭：SELINUX=disabled</li>
</ul>
<p>通过命令临时修改：</p>
<ul>
<li>开启：setenforce 1</li>
<li>关闭：setenforce 0</li>
</ul>
<p>查询 SELinux 状态：</p>
<pre><code>$ getenforce
</code></pre><h3 id="示例">示例</h3>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> hello-world
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">  - image:</span> gcr.io/google_containers/busybox:<span class="hljs-number">1.24</span>
<span class="hljs-attr">    name:</span> test-container
<span class="hljs-attr">    command:</span>
<span class="hljs-bullet">    -</span> sleep
<span class="hljs-bullet">    -</span> <span class="hljs-string">"6000"</span>
<span class="hljs-attr">    volumeMounts:</span>
<span class="hljs-attr">    - mountPath:</span> /mounted_volume
<span class="hljs-attr">      name:</span> test-volume
<span class="hljs-attr">  restartPolicy:</span> Never
<span class="hljs-attr">  hostPID:</span> <span class="hljs-literal">false</span>
<span class="hljs-attr">  hostIPC:</span> <span class="hljs-literal">false</span>
<span class="hljs-attr">  securityContext:</span>
<span class="hljs-attr">    seLinuxOptions:</span>
<span class="hljs-attr">      level:</span> <span class="hljs-string">"s0:c2,c3"</span>
<span class="hljs-attr">  volumes:</span>
<span class="hljs-attr">  - name:</span> test-volume
<span class="hljs-attr">    emptyDir:</span> {}
</code></pre>
<p>这会自动给 docker 容器生成如下的 <code>HostConfig.Binds</code>:</p>
<pre><code>/var/lib/kubelet/pods/f734678c-95de-11e6-89b0-42010a8c0002/volumes/kubernetes.io~empty-dir/test-volume:/mounted_volume:Z
/var/lib/kubelet/pods/f734678c-95de-11e6-89b0-42010a8c0002/volumes/kubernetes.io~secret/default-token-88xxa:/var/run/secrets/kubernetes.io/serviceaccount:ro,Z
/var/lib/kubelet/pods/f734678c-95de-11e6-89b0-42010a8c0002/etc-hosts:/etc/hosts
</code></pre><p>对应的 volume 也都会正确设置 SELinux：</p>
<pre><code>$ ls -Z /var/lib/kubelet/pods/f734678c-95de-11e6-89b0-42010a8c0002/volumes
drwxr-xr-x. root root unconfined_u:object_r:svirt_sandbox_file_t:s0:c2,c3 kubernetes.io~empty-dir
drwxr-xr-x. root root unconfined_u:object_r:svirt_sandbox_file_t:s0:c2,c3 kubernetes.io~secret
</code></pre><h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/" target="_blank">Kubernetes Pod Security Policies</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="服务发现与负载均衡" class="level3">Service</h1>
<p>Kubernetes 在设计之初就充分考虑了针对容器的服务发现与负载均衡机制，提供了 Service 资源，并通过 kube-proxy 配合 cloud provider 来适应不同的应用场景。随着 kubernetes 用户的激增，用户场景的不断丰富，又产生了一些新的负载均衡机制。目前，kubernetes 中的负载均衡大致可以分为以下几种机制，每种机制都有其特定的应用场景：</p>
<ul>
<li>Service：直接用 Service 提供 cluster 内部的负载均衡，并借助 cloud provider 提供的 LB 提供外部访问</li>
<li>Ingress Controller：还是用 Service 提供 cluster 内部的负载均衡，但是通过自定义 Ingress Controller 提供外部访问</li>
<li>Service Load Balancer：把 load balancer 直接跑在容器中，实现 Bare Metal 的 Service Load Balancer</li>
<li>Custom Load Balancer：自定义负载均衡，并替代 kube-proxy，一般在物理部署 Kubernetes 时使用，方便接入公司已有的外部服务</li>
</ul>
<h2 id="service">Service</h2>
<p><img src="images/14735737093456.jpg" alt=""/></p>
<p>Service 是对一组提供相同功能的 Pods 的抽象，并为它们提供一个统一的入口。借助 Service，应用可以方便的实现服务发现与负载均衡，并实现应用的零宕机升级。Service 通过标签来选取服务后端，一般配合 Replication Controller 或者 Deployment 来保证后端容器的正常运行。这些匹配标签的 Pod IP 和端口列表组成 endpoints，由 kube-proxy 负责将服务 IP 负载均衡到这些 endpoints 上。</p>
<p>Service 有四种类型：</p>
<ul>
<li>ClusterIP：默认类型，自动分配一个仅 cluster 内部可以访问的虚拟 IP</li>
<li>NodePort：在 ClusterIP 基础上为 Service 在每台机器上绑定一个端口，这样就可以通过 <code><NodeIP>:NodePort</code> 来访问该服务。如果 kube-proxy 设置了 <code>--nodeport-addresses=10.240.0.0/16</code>（v1.10 支持），那么仅该 NodePort 仅对设置在范围内的 IP 有效。</li>
<li>LoadBalancer：在 NodePort 的基础上，借助 cloud provider 创建一个外部的负载均衡器，并将请求转发到 <code><NodeIP>:NodePort</code></li>
<li>ExternalName：将服务通过 DNS CNAME 记录方式转发到指定的域名（通过 <code>spec.externlName</code> 设定）。需要 kube-dns 版本在 1.7 以上。</li>
</ul>
<p>另外，也可以将已有的服务以 Service 的形式加入到 Kubernetes 集群中来，只需要在创建 Service 的时候不指定 Label selector，而是在 Service 创建好后手动为其添加 endpoint。</p>
<h3 id="service-定义">Service 定义</h3>
<p>Service 的定义也是通过 yaml 或 json，比如下面定义了一个名为 nginx 的服务，将服务的 80 端口转发到 default namespace 中带有标签 <code>run=nginx</code> 的 Pod 的 80 端口</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Service
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  labels:</span>
<span class="hljs-attr">    run:</span> nginx
<span class="hljs-attr">  name:</span> nginx
<span class="hljs-attr">  namespace:</span> default
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  ports:</span>
<span class="hljs-attr">  - port:</span> <span class="hljs-number">80</span>
<span class="hljs-attr">    protocol:</span> TCP
<span class="hljs-attr">    targetPort:</span> <span class="hljs-number">80</span>
<span class="hljs-attr">  selector:</span>
<span class="hljs-attr">    run:</span> nginx
<span class="hljs-attr">  sessionAffinity:</span> None
<span class="hljs-attr">  type:</span> ClusterIP
</code></pre>
<pre><code class="lang-sh"><span class="hljs-comment"># service 自动分配了 Cluster IP 10.0.0.108</span>
$ kubectl get service nginx
NAME      CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
nginx     10.0.0.108   <none>        80/TCP    18m
<span class="hljs-comment"># 自动创建的 endpoint</span>
$ kubectl get endpoints nginx
NAME      ENDPOINTS       AGE
nginx     172.17.0.5:80   18m
<span class="hljs-comment"># Service 自动关联 endpoint</span>
$ kubectl describe service nginx
Name:            nginx
Namespace:        default
Labels:            run=nginx
Annotations:        <none>
Selector:        run=nginx
Type:            ClusterIP
IP:            10.0.0.108
Port:            <<span class="hljs-built_in">unset</span>>    80/TCP
Endpoints:        172.17.0.5:80
Session Affinity:    None
Events:            <none>
</code></pre>
<p>当服务需要多个端口时，每个端口都必须设置一个名字</p>
<pre><code class="lang-yaml"><span class="hljs-attr">kind:</span> Service
<span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> my-service
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  selector:</span>
<span class="hljs-attr">    app:</span> MyApp
<span class="hljs-attr">  ports:</span>
<span class="hljs-attr">  - name:</span> http
<span class="hljs-attr">    protocol:</span> TCP
<span class="hljs-attr">    port:</span> <span class="hljs-number">80</span>
<span class="hljs-attr">    targetPort:</span> <span class="hljs-number">9376</span>
<span class="hljs-attr">  - name:</span> https
<span class="hljs-attr">    protocol:</span> TCP
<span class="hljs-attr">    port:</span> <span class="hljs-number">443</span>
<span class="hljs-attr">    targetPort:</span> <span class="hljs-number">9377</span>
</code></pre>
<h3 id="协议">协议</h3>
<p>Service、Endpoints 和 Pod 支持三种类型的协议：</p>
<ul>
<li>TCP（Transmission Control Protocol，传输控制协议）是一种面向连接的、可靠的、基于字节流的传输层通信协议。</li>
<li>UDP（User Datagram Protocol，用户数据报协议）是一种无连接的传输层协议，用于不可靠信息传送服务。</li>
<li>SCTP（Stream Control Transmission Protocol，流控制传输协议），用于通过IP网传输SCN（Signaling Communication Network，信令通信网）窄带信令消息。</li>
</ul>
<h3 id="api-版本对照表">API 版本对照表</h3>
<table>
<thead>
<tr>
<th>Kubernetes 版本</th>
<th>Core API 版本</th>
</tr>
</thead>
<tbody>
<tr>
<td>v1.5+</td>
<td>core/v1</td>
</tr>
</tbody>
</table>
<h3 id="不指定-selectors-的服务">不指定 Selectors 的服务</h3>
<p>在创建 Service 的时候，也可以不指定 Selectors，用来将 service 转发到 kubernetes 集群外部的服务（而不是 Pod）。目前支持两种方法</p>
<p>（1）自定义 endpoint，即创建同名的 service 和 endpoint，在 endpoint 中设置外部服务的 IP 和端口</p>
<pre><code class="lang-yaml"><span class="hljs-attr">kind:</span> Service
<span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> my-service
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  ports:</span>
<span class="hljs-attr">    - protocol:</span> TCP
<span class="hljs-attr">      port:</span> <span class="hljs-number">80</span>
<span class="hljs-attr">      targetPort:</span> <span class="hljs-number">9376</span>
<span class="hljs-meta">---</span>
<span class="hljs-attr">kind:</span> Endpoints
<span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> my-service
<span class="hljs-attr">subsets:</span>
<span class="hljs-attr">  - addresses:</span>
<span class="hljs-attr">      - ip:</span> <span class="hljs-number">1.2</span><span class="hljs-number">.3</span><span class="hljs-number">.4</span>
<span class="hljs-attr">    ports:</span>
<span class="hljs-attr">      - port:</span> <span class="hljs-number">9376</span>
</code></pre>
<p>（2）通过 DNS 转发，在 service 定义中指定 externalName。此时 DNS 服务会给 <code><service-name>.<namespace>.svc.cluster.local</code> 创建一个 CNAME 记录，其值为 <code>my.database.example.com</code>。并且，该服务不会自动分配 Cluster IP，需要通过 service 的 DNS 来访问。</p>
<pre><code class="lang-yaml"><span class="hljs-attr">kind:</span> Service
<span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> my-service
<span class="hljs-attr">  namespace:</span> default
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  type:</span> ExternalName
<span class="hljs-attr">  externalName:</span> my.database.example.com
</code></pre>
<p>注意：Endpoints 的 IP 地址不能是 127.0.0.0/8、169.254.0.0/16 和 224.0.0.0/24，也不能是 Kubernetes 中其他服务的 clusterIP。</p>
<h3 id="headless-服务">Headless 服务</h3>
<p>Headless 服务即不需要 Cluster IP 的服务，即在创建服务的时候指定 <code>spec.clusterIP=None</code>。包括两种类型</p>
<ul>
<li>不指定 Selectors，但设置 externalName，即上面的（2），通过 CNAME 记录处理</li>
<li>指定 Selectors，通过 DNS A 记录设置后端 endpoint 列表</li>
</ul>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Service
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  labels:</span>
<span class="hljs-attr">    app:</span> nginx
<span class="hljs-attr">  name:</span> nginx
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  clusterIP:</span> None
<span class="hljs-attr">  ports:</span>
<span class="hljs-attr">  - name:</span> tcp<span class="hljs-bullet">-80</span><span class="hljs-bullet">-80</span><span class="hljs-bullet">-3</span>b6tl
<span class="hljs-attr">    port:</span> <span class="hljs-number">80</span>
<span class="hljs-attr">    protocol:</span> TCP
<span class="hljs-attr">    targetPort:</span> <span class="hljs-number">80</span>
<span class="hljs-attr">  selector:</span>
<span class="hljs-attr">    app:</span> nginx
<span class="hljs-attr">  sessionAffinity:</span> None
<span class="hljs-attr">  type:</span> ClusterIP
<span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> extensions/v1beta1
<span class="hljs-attr">kind:</span> Deployment
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  labels:</span>
<span class="hljs-attr">    app:</span> nginx
<span class="hljs-attr">  name:</span> nginx
<span class="hljs-attr">  namespace:</span> default
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  replicas:</span> <span class="hljs-number">2</span>
<span class="hljs-attr">  revisionHistoryLimit:</span> <span class="hljs-number">5</span>
<span class="hljs-attr">  selector:</span>
<span class="hljs-attr">    matchLabels:</span>
<span class="hljs-attr">      app:</span> nginx
<span class="hljs-attr">  template:</span>
<span class="hljs-attr">    metadata:</span>
<span class="hljs-attr">      labels:</span>
<span class="hljs-attr">        app:</span> nginx
<span class="hljs-attr">    spec:</span>
<span class="hljs-attr">      containers:</span>
<span class="hljs-attr">      - image:</span> nginx:latest
<span class="hljs-attr">        imagePullPolicy:</span> Always
<span class="hljs-attr">        name:</span> nginx
<span class="hljs-attr">        resources:</span>
<span class="hljs-attr">          limits:</span>
<span class="hljs-attr">            memory:</span> <span class="hljs-number">128</span>Mi
<span class="hljs-attr">          requests:</span>
<span class="hljs-attr">            cpu:</span> <span class="hljs-number">200</span>m
<span class="hljs-attr">            memory:</span> <span class="hljs-number">128</span>Mi
<span class="hljs-attr">      dnsPolicy:</span> ClusterFirst
<span class="hljs-attr">      restartPolicy:</span> Always
</code></pre>
<pre><code class="lang-sh"><span class="hljs-comment"># 查询创建的 nginx 服务</span>
$ kubectl get service --all-namespaces=<span class="hljs-literal">true</span>
NAMESPACE     NAME         CLUSTER-IP      EXTERNAL-IP      PORT(S)         AGE
default       nginx        None            <none>           80/TCP          5m
kube-system   kube-dns     172.26.255.70   <none>           53/UDP,53/TCP   1d
$ kubectl get pod
NAME                       READY     STATUS    RESTARTS   AGE       IP           NODE
nginx-2204978904-6o5dg     1/1       Running   0          14s       172.26.2.5   10.0.0.2
nginx-2204978904-qyilx     1/1       Running   0          14s       172.26.1.5   10.0.0.8
$ dig @172.26.255.70  nginx.default.svc.cluster.local
;; ANSWER SECTION:
nginx.default.svc.cluster.local. 30 IN    A    172.26.1.5
nginx.default.svc.cluster.local. 30 IN    A    172.26.2.5
</code></pre>
<p>备注： 其中 dig 命令查询的信息中，部分信息省略</p>
<h2 id="保留源-ip">保留源 IP</h2>
<p>各种类型的 Service 对源 IP 的处理方法不同：</p>
<ul>
<li>ClusterIP Service：使用 iptables 模式，集群内部的源 IP 会保留（不做 SNAT）。如果 client 和 server pod 在同一个 Node 上，那源 IP 就是 client pod 的 IP 地址；如果在不同的 Node 上，源 IP 则取决于网络插件是如何处理的，比如使用 flannel 时，源 IP 是 node flannel IP 地址。</li>
<li>NodePort Service：默认情况下，源 IP 会做 SNAT，server pod 看到的源 IP 是 Node IP。为了避免这种情况，可以给 service 设置 <code>spec.ExternalTrafficPolicy=Local</code> （1.6-1.7 版本设置 Annotation <code>service.beta.kubernetes.io/external-traffic=OnlyLocal</code>），让 service 只代理本地 endpoint 的请求（如果没有本地 endpoint 则直接丢包），从而保留源 IP。</li>
<li>LoadBalancer Service：默认情况下，源 IP 会做 SNAT，server pod 看到的源 IP 是 Node IP。设置 <code>service.spec.ExternalTrafficPolicy=Local</code> 后可以自动从云平台负载均衡器中删除没有本地 endpoint 的 Node，从而保留源 IP。</li>
</ul>
<h2 id="工作原理">工作原理</h2>
<p>kube-proxy 负责将 service 负载均衡到后端 Pod 中，如下图所示</p>
<p><img src="images/service-flow.png" alt=""/></p>
<h2 id="ingress">Ingress</h2>
<p>Service 虽然解决了服务发现和负载均衡的问题，但它在使用上还是有一些限制，比如</p>
<p>－ 只支持 4 层负载均衡，没有 7 层功能
－ 对外访问的时候，NodePort 类型需要在外部搭建额外的负载均衡，而 LoadBalancer 要求 kubernetes 必须跑在支持的 cloud provider 上面</p>
<p>Ingress 就是为了解决这些限制而引入的新资源，主要用来将服务暴露到 cluster 外面，并且可以自定义服务的访问策略。比如想要通过负载均衡器实现不同子域名到不同服务的访问：</p>
<pre><code>foo.bar.com --|                 |-> foo.bar.com s1:80
              | 178.91.123.132  |
bar.foo.com --|                 |-> bar.foo.com s2:80
</code></pre><p>可以这样来定义 Ingress：</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> extensions/v1beta1
<span class="hljs-attr">kind:</span> Ingress
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> test
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  rules:</span>
<span class="hljs-attr">  - host:</span> foo.bar.com
<span class="hljs-attr">    http:</span>
<span class="hljs-attr">      paths:</span>
<span class="hljs-attr">      - backend:</span>
<span class="hljs-attr">          serviceName:</span> s1
<span class="hljs-attr">          servicePort:</span> <span class="hljs-number">80</span>
<span class="hljs-attr">  - host:</span> bar.foo.com
<span class="hljs-attr">    http:</span>
<span class="hljs-attr">      paths:</span>
<span class="hljs-attr">      - backend:</span>
<span class="hljs-attr">          serviceName:</span> s2
<span class="hljs-attr">          servicePort:</span> <span class="hljs-number">80</span>
</code></pre>
<p>注意 Ingress 本身并不会自动创建负载均衡器，cluster 中需要运行一个 ingress controller 来根据 Ingress 的定义来管理负载均衡器。目前社区提供了 nginx 和 gce 的参考实现。</p>
<p>Traefik 提供了易用的 Ingress Controller，使用方法见 <a href="https://docs.traefik.io/user-guide/kubernetes/" target="_blank">https://docs.traefik.io/user-guide/kubernetes/</a>。</p>
<p>更多 Ingress 和 Ingress Controller 的介绍参见 <a href="ingress.html">ingress</a>。</p>
<h2 id="service-load-balancer">Service Load Balancer</h2>
<p>在 Ingress 出现以前，<a href="https://github.com/kubernetes/contrib/tree/master/service-loadbalancer" target="_blank">Service Load Balancer</a> 是推荐的解决 Service 局限性的方式。Service Load Balancer 将 haproxy 跑在容器中，并监控 service 和 endpoint 的变化，通过容器 IP 对外提供 4 层和 7 层负载均衡服务。</p>
<p>社区提供的 Service Load Balancer 支持四种负载均衡协议：TCP、HTTP、HTTPS 和 SSL TERMINATION，并支持 ACL 访问控制。</p>
<blockquote>
<p>注意：Service Load Balancer 已不再推荐使用，推荐使用 <a href="ingress.html">Ingress Controller</a>。</p>
</blockquote>
<h2 id="custom-load-balancer">Custom Load Balancer</h2>
<p>虽然 Kubernetes 提供了丰富的负载均衡机制，但在实际使用的时候，还是会碰到一些复杂的场景是它不能支持的，比如</p>
<ul>
<li>接入已有的负载均衡设备</li>
<li>多租户网络情况下，容器网络和主机网络是隔离的，这样 <code>kube-proxy</code> 就不能正常工作</li>
</ul>
<p>这个时候就可以自定义组件，并代替 kube-proxy 来做负载均衡。基本的思路是监控 kubernetes 中 service 和 endpoints 的变化，并根据这些变化来配置负载均衡器。比如 weave flux、nginx plus、kube2haproxy 等。</p>
<h2 id="集群外部访问服务">集群外部访问服务</h2>
<p>Service 的 ClusterIP 是 Kubernetes 内部的虚拟 IP 地址，无法直接从外部直接访问。但如果需要从外部访问这些服务该怎么办呢，有多种方法</p>
<ul>
<li>使用 NodePort 服务在每台机器上绑定一个端口，这样就可以通过 <code><NodeIP>:NodePort</code> 来访问该服务。</li>
<li>使用 LoadBalancer 服务借助 Cloud Provider 创建一个外部的负载均衡器，并将请求转发到 <code><NodeIP>:NodePort</code>。该方法仅适用于运行在云平台之中的 Kubernetes 集群。对于物理机部署的集群，可以使用 <a href="https://github.com/google/metallb" target="_blank">MetalLB</a> 实现类似的功能。</li>
<li>使用 Ingress Controller 在 Service 之上创建 L7 负载均衡并对外开放。</li>
<li>使用 <a href="https://en.wikipedia.org/wiki/Equal-cost_multi-path_routing" target="_blank">ECMP</a> 将 Service ClusterIP 网段路由到每个 Node，这样可以直接通过 ClusterIP 来访问服务，甚至也可以直接在集群外部使用 kube-dns。这一版用在物理机部署的情况下。</li>
</ul>
<h2 id="参考资料">参考资料</h2>
<ul>
<li><a href="https://kubernetes.io/docs/concepts/services-networking/service/" target="_blank">https://kubernetes.io/docs/concepts/services-networking/service/</a></li>
<li><a href="https://kubernetes.io/docs/concepts/services-networking/ingress/" target="_blank">https://kubernetes.io/docs/concepts/services-networking/ingress/</a></li>
<li><a href="https://github.com/kubernetes/contrib/tree/master/service-loadbalancer" target="_blank">https://github.com/kubernetes/contrib/tree/master/service-loadbalancer</a></li>
<li><a href="https://www.nginx.com/blog/load-balancing-kubernetes-services-nginx-plus/" target="_blank">https://www.nginx.com/blog/load-balancing-kubernetes-services-nginx-plus/</a></li>
<li><a href="https://github.com/weaveworks/flux" target="_blank">https://github.com/weaveworks/flux</a></li>
<li><a href="https://github.com/AdoHe/kube2haproxy" target="_blank">https://github.com/AdoHe/kube2haproxy</a></li>
<li><a href="https://medium.com/@kyralak/accessing-kubernetes-services-without-ingress-nodeport-or-loadbalancer-de6061b42d72" target="_blank">Accessing Kubernetes Services Without Ingress, NodePort, or LoadBalancer</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="service-account" class="level3">ServiceAccount</h1>
<p>Service account 是为了方便 Pod 里面的进程调用 Kubernetes API 或其他外部服务而设计的。它与 User account 不同</p>
<ul>
<li>User account 是为人设计的，而 service account 则是为 Pod 中的进程调用 Kubernetes API 而设计；</li>
<li>User account 是跨 namespace 的，而 service account 则是仅局限它所在的 namespace；</li>
<li>每个 namespace 都会自动创建一个 default service account</li>
<li>Token controller 检测 service account 的创建，并为它们创建 <a href="secret.html">secret</a></li>
<li>开启 ServiceAccount Admission Controller 后<ul>
<li>每个 Pod 在创建后都会自动设置 <code>spec.serviceAccountName</code> 为 default（除非指定了其他 ServiceAccout）</li>
<li>验证 Pod 引用的 service account 已经存在，否则拒绝创建</li>
<li>如果 Pod 没有指定 ImagePullSecrets，则把 service account 的 ImagePullSecrets 加到 Pod 中</li>
<li>每个 container 启动后都会挂载该 service account 的 token 和 <code>ca.crt</code> 到 <code>/var/run/secrets/kubernetes.io/serviceaccount/</code></li>
</ul>
</li>
</ul>
<pre><code class="lang-sh">$ kubectl <span class="hljs-built_in">exec</span> nginx-3137573019-md1u2 ls /var/run/secrets/kubernetes.io/serviceaccount
ca.crt
namespace
token
</code></pre>
<h2 id="创建-service-account">创建 Service Account</h2>
<pre><code class="lang-sh">$ kubectl create serviceaccount jenkins
serviceaccount <span class="hljs-string">"jenkins"</span> created
$ kubectl get serviceaccounts jenkins -o yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  creationTimestamp: 2017-05-27T14:32:25Z
  name: jenkins
  namespace: default
  resourceVersion: <span class="hljs-string">"45559"</span>
  selfLink: /api/v1/namespaces/default/serviceaccounts/jenkins
  uid: 4d66eb4c-42e9-11e7-9860-ee7d8982865f
secrets:
- name: jenkins-token<span class="hljs-_">-l</span>9v7v
</code></pre>
<p>自动创建的 secret：</p>
<pre><code class="lang-sh">kubectl get secret jenkins-token<span class="hljs-_">-l</span>9v7v -o yaml
apiVersion: v1
data:
  ca.crt: (APISERVER CA BASE64 ENCODED)
  namespace: ZGVmYXVsdA==
  token: (BEARER TOKEN BASE64 ENCODED)
kind: Secret
metadata:
  annotations:
    kubernetes.io/service-account.name: jenkins
    kubernetes.io/service-account.uid: 4d66eb4c-42e9-11e7-9860-ee7d8982865f
  creationTimestamp: 2017-05-27T14:32:25Z
  name: jenkins-token<span class="hljs-_">-l</span>9v7v
  namespace: default
  resourceVersion: <span class="hljs-string">"45558"</span>
  selfLink: /api/v1/namespaces/default/secrets/jenkins-token<span class="hljs-_">-l</span>9v7v
  uid: 4d697992-42e9-11e7-9860-ee7d8982865f
<span class="hljs-built_in">type</span>: kubernetes.io/service-account-token
</code></pre>
<h2 id="添加-imagepullsecrets">添加 ImagePullSecrets</h2>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> ServiceAccount
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  creationTimestamp:</span> <span class="hljs-number">2015</span><span class="hljs-bullet">-08</span><span class="hljs-bullet">-07</span>T22:<span class="hljs-number">02</span>:<span class="hljs-number">39</span>Z
<span class="hljs-attr">  name:</span> default
<span class="hljs-attr">  namespace:</span> default
<span class="hljs-attr">  selfLink:</span> /api/v1/namespaces/default/serviceaccounts/default
<span class="hljs-attr">  uid:</span> <span class="hljs-number">052</span>fb0f4<span class="hljs-bullet">-3</span>d50<span class="hljs-bullet">-11e5</span>-b066<span class="hljs-bullet">-42010</span>af0d7b6
<span class="hljs-attr">secrets:</span>
<span class="hljs-attr">- name:</span> default-token-uudge
<span class="hljs-attr">imagePullSecrets:</span>
<span class="hljs-attr">- name:</span> myregistrykey
</code></pre>
<h2 id="授权">授权</h2>
<p>Service Account 为服务提供了一种方便的认证机制，但它不关心授权的问题。可以配合 <a href="https://kubernetes.io/docs/admin/authorization/#a-quick-note-on-service-accounts" target="_blank">RBAC</a> 来为 Service Account 鉴权：</p>
<ul>
<li>配置 <code>--authorization-mode=RBAC</code> 和 <code>--runtime-config=rbac.authorization.k8s.io/v1alpha1</code></li>
<li>配置 <code>--authorization-rbac-super-user=admin</code></li>
<li>定义 Role、ClusterRole、RoleBinding 或 ClusterRoleBinding</li>
</ul>
<p>比如</p>
<pre><code class="lang-yaml"><span class="hljs-comment"># This role allows to read pods in the namespace "default"</span>
<span class="hljs-attr">kind:</span> Role
<span class="hljs-attr">apiVersion:</span> rbac.authorization.k8s.io/v1alpha1
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  namespace:</span> default
<span class="hljs-attr">  name:</span> pod-reader
<span class="hljs-attr">rules:</span>
<span class="hljs-attr">  - apiGroups:</span> [<span class="hljs-string">""</span>] <span class="hljs-comment"># The API group"" indicates the core API Group.</span>
<span class="hljs-attr">    resources:</span> [<span class="hljs-string">"pods"</span>]
<span class="hljs-attr">    verbs:</span> [<span class="hljs-string">"get"</span>, <span class="hljs-string">"watch"</span>, <span class="hljs-string">"list"</span>]
<span class="hljs-attr">    nonResourceURLs:</span> []
<span class="hljs-meta">---</span>
<span class="hljs-comment"># This role binding allows "default" to read pods in the namespace "default"</span>
<span class="hljs-attr">kind:</span> RoleBinding
<span class="hljs-attr">apiVersion:</span> rbac.authorization.k8s.io/v1alpha1
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> read-pods
<span class="hljs-attr">  namespace:</span> default
<span class="hljs-attr">subjects:</span>
<span class="hljs-attr">  - kind:</span> ServiceAccount <span class="hljs-comment"># May be "User", "Group" or "ServiceAccount"</span>
<span class="hljs-attr">    name:</span> default
<span class="hljs-attr">roleRef:</span>
<span class="hljs-attr">  kind:</span> Role
<span class="hljs-attr">  name:</span> pod-reader
<span class="hljs-attr">  apiGroup:</span> rbac.authorization.k8s.io
</code></pre>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="statefulset" class="level3">StatefulSet</h1>
<p>StatefulSet 是为了解决有状态服务的问题（对应 Deployments 和 ReplicaSets 是为无状态服务而设计），其应用场景包括</p>
<ul>
<li>稳定的持久化存储，即 Pod 重新调度后还是能访问到相同的持久化数据，基于 PVC 来实现</li>
<li>稳定的网络标志，即 Pod 重新调度后其 PodName 和 HostName 不变，基于 Headless Service（即没有 Cluster IP 的 Service）来实现</li>
<li>有序部署，有序扩展，即 Pod 是有顺序的，在部署或者扩展的时候要依据定义的顺序依次依序进行（即从 0 到 N-1，在下一个 Pod 运行之前所有之前的 Pod 必须都是 Running 和 Ready 状态），基于 init containers 来实现</li>
<li>有序收缩，有序删除（即从 N-1 到 0）</li>
</ul>
<p>从上面的应用场景可以发现，StatefulSet 由以下几个部分组成：</p>
<ul>
<li>用于定义网络标志（DNS domain）的 Headless Service</li>
<li>用于创建 PersistentVolumes 的 volumeClaimTemplates</li>
<li>定义具体应用的 StatefulSet</li>
</ul>
<p>StatefulSet 中每个 Pod 的 DNS 格式为 <code>statefulSetName-{0..N-1}.serviceName.namespace.svc.cluster.local</code>，其中</p>
<ul>
<li><code>serviceName</code> 为 Headless Service 的名字</li>
<li><code>0..N-1</code> 为 Pod 所在的序号，从 0 开始到 N-1</li>
<li><code>statefulSetName</code> 为 StatefulSet 的名字</li>
<li><code>namespace</code> 为服务所在的 namespace，Headless Service 和 StatefulSet 必须在相同的 namespace</li>
<li><code>.cluster.local</code> 为 Cluster Domain</li>
</ul>
<h2 id="api-版本对照表">API 版本对照表</h2>
<table>
<thead>
<tr>
<th>Kubernetes 版本</th>
<th>Apps 版本</th>
</tr>
</thead>
<tbody>
<tr>
<td>v1.6-v1.7</td>
<td>apps/v1beta1</td>
</tr>
<tr>
<td>v1.8</td>
<td>apps/v1beta2</td>
</tr>
<tr>
<td>v1.9</td>
<td>apps/v1</td>
</tr>
</tbody>
</table>
<h2 id="简单示例">简单示例</h2>
<p>以一个简单的 nginx 服务 <a href="web.txt">web.yaml</a> 为例：</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Service
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> nginx
<span class="hljs-attr">  labels:</span>
<span class="hljs-attr">    app:</span> nginx
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  ports:</span>
<span class="hljs-attr">  - port:</span> <span class="hljs-number">80</span>
<span class="hljs-attr">    name:</span> web
<span class="hljs-attr">  clusterIP:</span> None
<span class="hljs-attr">  selector:</span>
<span class="hljs-attr">    app:</span> nginx
<span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> apps/v1
<span class="hljs-attr">kind:</span> StatefulSet
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> web
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  serviceName:</span> <span class="hljs-string">"nginx"</span>
<span class="hljs-attr">  replicas:</span> <span class="hljs-number">2</span>
<span class="hljs-attr">  selector:</span>
<span class="hljs-attr">    matchLabels:</span>
<span class="hljs-attr">      app:</span> nginx
<span class="hljs-attr">  template:</span>
<span class="hljs-attr">    metadata:</span>
<span class="hljs-attr">      labels:</span>
<span class="hljs-attr">        app:</span> nginx
<span class="hljs-attr">    spec:</span>
<span class="hljs-attr">      containers:</span>
<span class="hljs-attr">      - name:</span> nginx
<span class="hljs-attr">        image:</span> k8s.gcr.io/nginx-slim:<span class="hljs-number">0.8</span>
<span class="hljs-attr">        ports:</span>
<span class="hljs-attr">        - containerPort:</span> <span class="hljs-number">80</span>
<span class="hljs-attr">          name:</span> web
<span class="hljs-attr">        volumeMounts:</span>
<span class="hljs-attr">        - name:</span> www
<span class="hljs-attr">          mountPath:</span> /usr/share/nginx/html
<span class="hljs-attr">  volumeClaimTemplates:</span>
<span class="hljs-attr">  - metadata:</span>
<span class="hljs-attr">      name:</span> www
<span class="hljs-attr">    spec:</span>
<span class="hljs-attr">      accessModes:</span> [<span class="hljs-string">"ReadWriteOnce"</span>]
<span class="hljs-attr">      resources:</span>
<span class="hljs-attr">        requests:</span>
<span class="hljs-attr">          storage:</span> <span class="hljs-number">1</span>Gi
</code></pre>
<pre><code class="lang-sh">$ kubectl create <span class="hljs-_">-f</span> web.yaml
service <span class="hljs-string">"nginx"</span> created
statefulset <span class="hljs-string">"web"</span> created

<span class="hljs-comment"># 查看创建的 headless service 和 statefulset</span>
$ kubectl get service nginx
NAME      CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
nginx     None         <none>        80/TCP    1m
$ kubectl get statefulset web
NAME      DESIRED   CURRENT   AGE
web       2         2         2m

<span class="hljs-comment"># 根据 volumeClaimTemplates 自动创建 PVC（在 GCE 中会自动创建 kubernetes.io/gce-pd 类型的 volume）</span>
$ kubectl get pvc
NAME        STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE
www-web-0   Bound     pvc<span class="hljs-_">-d</span>064a004<span class="hljs-_">-d</span>8d4-11e6-b521-42010a800002   1Gi        RWO           16s
www-web-1   Bound     pvc<span class="hljs-_">-d</span>06a3946<span class="hljs-_">-d</span>8d4-11e6-b521-42010a800002   1Gi        RWO           16s

<span class="hljs-comment"># 查看创建的 Pod，他们都是有序的</span>
$ kubectl get pods <span class="hljs-_">-l</span> app=nginx
NAME      READY     STATUS    RESTARTS   AGE
web-0     1/1       Running   0          5m
web-1     1/1       Running   0          4m

<span class="hljs-comment"># 使用 nslookup 查看这些 Pod 的 DNS</span>
$ kubectl run -i --tty --image busybox dns-test --restart=Never --rm /bin/sh
/ <span class="hljs-comment"># nslookup web-0.nginx</span>
Server:    10.0.0.10
Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local

Name:      web-0.nginx
Address 1: 10.244.2.10
/ <span class="hljs-comment"># nslookup web-1.nginx</span>
Server:    10.0.0.10
Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local

Name:      web-1.nginx
Address 1: 10.244.3.12
/ <span class="hljs-comment"># nslookup web-0.nginx.default.svc.cluster.local</span>
Server:    10.0.0.10
Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local

Name:      web-0.nginx.default.svc.cluster.local
Address 1: 10.244.2.10
</code></pre>
<p>还可以进行其他的操作</p>
<pre><code class="lang-sh"><span class="hljs-comment"># 扩容</span>
$ kubectl scale statefulset web --replicas=5

<span class="hljs-comment"># 缩容</span>
$ kubectl patch statefulset web -p <span class="hljs-string">'{"spec":{"replicas":3}}'</span>

<span class="hljs-comment"># 镜像更新（目前还不支持直接更新 image，需要 patch 来间接实现）</span>
$ kubectl patch statefulset web --type=<span class="hljs-string">'json'</span> -p=<span class="hljs-string">'[{"op":"replace","path":"/spec/template/spec/containers/0/image","value":"gcr.io/google_containers/nginx-slim:0.7"}]'</span>

<span class="hljs-comment"># 删除 StatefulSet 和 Headless Service</span>
$ kubectl delete statefulset web
$ kubectl delete service nginx

<span class="hljs-comment"># StatefulSet 删除后 PVC 还会保留着，数据不再使用的话也需要删除</span>
$ kubectl delete pvc www-web-0 www-web-1
</code></pre>
<h2 id="更新-statefulset">更新 StatefulSet</h2>
<p>v1.7 + 支持 StatefulSet 的自动更新，通过 <code>spec.updateStrategy</code> 设置更新策略。目前支持两种策略</p>
<ul>
<li>OnDelete：当 <code>.spec.template</code> 更新时，并不立即删除旧的 Pod，而是等待用户手动删除这些旧 Pod 后自动创建新 Pod。这是默认的更新策略，兼容 v1.6 版本的行为</li>
<li>RollingUpdate：当 <code>.spec.template</code> 更新时，自动删除旧的 Pod 并创建新 Pod 替换。在更新时，这些 Pod 是按逆序的方式进行，依次删除、创建并等待 Pod 变成 Ready 状态才进行下一个 Pod 的更新。</li>
</ul>
<h3 id="partitions">Partitions</h3>
<p>RollingUpdate 还支持 Partitions，通过 <code>.spec.updateStrategy.rollingUpdate.partition</code> 来设置。当 partition 设置后，只有序号大于或等于 partition 的 Pod 会在 <code>.spec.template</code> 更新的时候滚动更新，而其余的 Pod 则保持不变（即便是删除后也是用以前的版本重新创建）。</p>
<pre><code class="lang-sh"><span class="hljs-comment"># 设置 partition 为 3</span>
$ kubectl patch statefulset web -p <span class="hljs-string">'{"spec":{"updateStrategy":{"type":"RollingUpdate","rollingUpdate":{"partition":3}}}}'</span>
statefulset <span class="hljs-string">"web"</span> patched

<span class="hljs-comment"># 更新 StatefulSet</span>
$ kubectl patch statefulset web --type=<span class="hljs-string">'json'</span> -p=<span class="hljs-string">'[{"op":"replace","path":"/spec/template/spec/containers/0/image","value":"gcr.io/google_containers/nginx-slim:0.7"}]'</span>
statefulset <span class="hljs-string">"web"</span> patched

<span class="hljs-comment"># 验证更新</span>
$ kubectl delete po web-2
pod <span class="hljs-string">"web-2"</span> deleted
$ kubectl get po -lapp=nginx -w
NAME      READY     STATUS              RESTARTS   AGE
web-0     1/1       Running             0          4m
web-1     1/1       Running             0          4m
web-2     0/1       ContainerCreating   0          11s
web-2     1/1       Running             0          18s
</code></pre>
<h2 id="pod-管理策略">Pod 管理策略</h2>
<p>v1.7 + 可以通过 <code>.spec.podManagementPolicy</code> 设置 Pod 管理策略，支持两种方式</p>
<ul>
<li>OrderedReady：默认的策略，按照 Pod 的次序依次创建每个 Pod 并等待 Ready 之后才创建后面的 Pod</li>
<li>Parallel：并行创建或删除 Pod（不等待前面的 Pod Ready 就开始创建所有的 Pod）</li>
</ul>
<h3 id="parallel-示例">Parallel 示例</h3>
<pre><code class="lang-yaml"><span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Service
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> nginx
<span class="hljs-attr">  labels:</span>
<span class="hljs-attr">    app:</span> nginx
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  ports:</span>
<span class="hljs-attr">  - port:</span> <span class="hljs-number">80</span>
<span class="hljs-attr">    name:</span> web
<span class="hljs-attr">  clusterIP:</span> None
<span class="hljs-attr">  selector:</span>
<span class="hljs-attr">    app:</span> nginx
<span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> apps/v1beta1
<span class="hljs-attr">kind:</span> StatefulSet
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> web
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  serviceName:</span> <span class="hljs-string">"nginx"</span>
<span class="hljs-attr">  podManagementPolicy:</span> <span class="hljs-string">"Parallel"</span>
<span class="hljs-attr">  replicas:</span> <span class="hljs-number">2</span>
<span class="hljs-attr">  template:</span>
<span class="hljs-attr">    metadata:</span>
<span class="hljs-attr">      labels:</span>
<span class="hljs-attr">        app:</span> nginx
<span class="hljs-attr">    spec:</span>
<span class="hljs-attr">      containers:</span>
<span class="hljs-attr">      - name:</span> nginx
<span class="hljs-attr">        image:</span> gcr.io/google_containers/nginx-slim:<span class="hljs-number">0.8</span>
<span class="hljs-attr">        ports:</span>
<span class="hljs-attr">        - containerPort:</span> <span class="hljs-number">80</span>
<span class="hljs-attr">          name:</span> web
<span class="hljs-attr">        volumeMounts:</span>
<span class="hljs-attr">        - name:</span> www
<span class="hljs-attr">          mountPath:</span> /usr/share/nginx/html
<span class="hljs-attr">  volumeClaimTemplates:</span>
<span class="hljs-attr">  - metadata:</span>
<span class="hljs-attr">      name:</span> www
<span class="hljs-attr">    spec:</span>
<span class="hljs-attr">      accessModes:</span> [<span class="hljs-string">"ReadWriteOnce"</span>]
<span class="hljs-attr">      resources:</span>
<span class="hljs-attr">        requests:</span>
<span class="hljs-attr">          storage:</span> <span class="hljs-number">1</span>Gi
</code></pre>
<p>可以看到，所有 Pod 是并行创建的</p>
<pre><code class="lang-sh">$ kubectl create <span class="hljs-_">-f</span> webp.yaml
service <span class="hljs-string">"nginx"</span> created
statefulset <span class="hljs-string">"web"</span> created

$ kubectl get po -lapp=nginx -w
NAME      READY     STATUS              RESTARTS  AGE
web-0     0/1       Pending             0         0s
web-0     0/1       Pending             0         0s
web-1     0/1       Pending             0         0s
web-1     0/1       Pending             0         0s
web-0     0/1       ContainerCreating   0         0s
web-1     0/1       ContainerCreating   0         0s
web-0     1/1       Running             0         10s
web-1     1/1       Running             0         10s
</code></pre>
<h2 id="zookeeper">zookeeper</h2>
<p>另外一个更能说明 StatefulSet 强大功能的示例为 <a href="zookeeper.txt">zookeeper.yaml</a>。</p>
<pre><code class="lang-yaml"><span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Service
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> zk-headless
<span class="hljs-attr">  labels:</span>
<span class="hljs-attr">    app:</span> zk-headless
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  ports:</span>
<span class="hljs-attr">  - port:</span> <span class="hljs-number">2888</span>
<span class="hljs-attr">    name:</span> server
<span class="hljs-attr">  - port:</span> <span class="hljs-number">3888</span>
<span class="hljs-attr">    name:</span> leader-election
<span class="hljs-attr">  clusterIP:</span> None
<span class="hljs-attr">  selector:</span>
<span class="hljs-attr">    app:</span> zk
<span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> ConfigMap
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> zk-config
<span class="hljs-attr">data:</span>
<span class="hljs-attr">  ensemble:</span> <span class="hljs-string">"zk-0;zk-1;zk-2"</span>
  jvm.heap: <span class="hljs-string">"2G"</span>
<span class="hljs-attr">  tick:</span> <span class="hljs-string">"2000"</span>
<span class="hljs-attr">  init:</span> <span class="hljs-string">"10"</span>
<span class="hljs-attr">  sync:</span> <span class="hljs-string">"5"</span>
  client.cnxns: <span class="hljs-string">"60"</span>
  snap.retain: <span class="hljs-string">"3"</span>
  purge.interval: <span class="hljs-string">"1"</span>
<span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> policy/v1beta1
<span class="hljs-attr">kind:</span> PodDisruptionBudget
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> zk-budget
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  selector:</span>
<span class="hljs-attr">    matchLabels:</span>
<span class="hljs-attr">      app:</span> zk
<span class="hljs-attr">  minAvailable:</span> <span class="hljs-number">2</span>
<span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> apps/v1beta1
<span class="hljs-attr">kind:</span> StatefulSet
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> zk
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  serviceName:</span> zk-headless
<span class="hljs-attr">  replicas:</span> <span class="hljs-number">3</span>
<span class="hljs-attr">  template:</span>
<span class="hljs-attr">    metadata:</span>
<span class="hljs-attr">      labels:</span>
<span class="hljs-attr">        app:</span> zk
<span class="hljs-attr">      annotations:</span>
        pod.alpha.kubernetes.io/initialized: <span class="hljs-string">"true"</span>
        scheduler.alpha.kubernetes.io/affinity: <span class="hljs-string">>
            {
              "podAntiAffinity": {
                "requiredDuringSchedulingRequiredDuringExecution": [{
                  "labelSelector": {
                    "matchExpressions": [{
                      "key": "app",
                      "operator": "In",
                      "values": ["zk-headless"]
                    }]
                  },
                  "topologyKey": "kubernetes.io/hostname"
                }]
              }
            }
</span><span class="hljs-attr">    spec:</span>
<span class="hljs-attr">      containers:</span>
<span class="hljs-attr">      - name:</span> k8szk
<span class="hljs-attr">        imagePullPolicy:</span> Always
<span class="hljs-attr">        image:</span> gcr.io/google_samples/k8szk:v1
<span class="hljs-attr">        resources:</span>
<span class="hljs-attr">          requests:</span>
<span class="hljs-attr">            memory:</span> <span class="hljs-string">"4Gi"</span>
<span class="hljs-attr">            cpu:</span> <span class="hljs-string">"1"</span>
<span class="hljs-attr">        ports:</span>
<span class="hljs-attr">        - containerPort:</span> <span class="hljs-number">2181</span>
<span class="hljs-attr">          name:</span> client
<span class="hljs-attr">        - containerPort:</span> <span class="hljs-number">2888</span>
<span class="hljs-attr">          name:</span> server
<span class="hljs-attr">        - containerPort:</span> <span class="hljs-number">3888</span>
<span class="hljs-attr">          name:</span> leader-election
<span class="hljs-attr">        env:</span>
<span class="hljs-bullet">        -</span> name : ZK_ENSEMBLE
<span class="hljs-attr">          valueFrom:</span>
<span class="hljs-attr">            configMapKeyRef:</span>
<span class="hljs-attr">              name:</span> zk-config
<span class="hljs-attr">              key:</span> ensemble
<span class="hljs-bullet">        -</span> name : ZK_HEAP_SIZE
<span class="hljs-attr">          valueFrom:</span>
<span class="hljs-attr">            configMapKeyRef:</span>
<span class="hljs-attr">                name:</span> zk-config
<span class="hljs-attr">                key:</span> jvm.heap
<span class="hljs-bullet">        -</span> name : ZK_TICK_TIME
<span class="hljs-attr">          valueFrom:</span>
<span class="hljs-attr">            configMapKeyRef:</span>
<span class="hljs-attr">                name:</span> zk-config
<span class="hljs-attr">                key:</span> tick
<span class="hljs-bullet">        -</span> name : ZK_INIT_LIMIT
<span class="hljs-attr">          valueFrom:</span>
<span class="hljs-attr">            configMapKeyRef:</span>
<span class="hljs-attr">                name:</span> zk-config
<span class="hljs-attr">                key:</span> init
<span class="hljs-bullet">        -</span> name : ZK_SYNC_LIMIT
<span class="hljs-attr">          valueFrom:</span>
<span class="hljs-attr">            configMapKeyRef:</span>
<span class="hljs-attr">                name:</span> zk-config
<span class="hljs-attr">                key:</span> tick
<span class="hljs-bullet">        -</span> name : ZK_MAX_CLIENT_CNXNS
<span class="hljs-attr">          valueFrom:</span>
<span class="hljs-attr">            configMapKeyRef:</span>
<span class="hljs-attr">                name:</span> zk-config
<span class="hljs-attr">                key:</span> client.cnxns
<span class="hljs-attr">        - name:</span> ZK_SNAP_RETAIN_COUNT
<span class="hljs-attr">          valueFrom:</span>
<span class="hljs-attr">            configMapKeyRef:</span>
<span class="hljs-attr">                name:</span> zk-config
<span class="hljs-attr">                key:</span> snap.retain
<span class="hljs-attr">        - name:</span> ZK_PURGE_INTERVAL
<span class="hljs-attr">          valueFrom:</span>
<span class="hljs-attr">            configMapKeyRef:</span>
<span class="hljs-attr">                name:</span> zk-config
<span class="hljs-attr">                key:</span> purge.interval
<span class="hljs-attr">        - name:</span> ZK_CLIENT_PORT
<span class="hljs-attr">          value:</span> <span class="hljs-string">"2181"</span>
<span class="hljs-attr">        - name:</span> ZK_SERVER_PORT
<span class="hljs-attr">          value:</span> <span class="hljs-string">"2888"</span>
<span class="hljs-attr">        - name:</span> ZK_ELECTION_PORT
<span class="hljs-attr">          value:</span> <span class="hljs-string">"3888"</span>
<span class="hljs-attr">        command:</span>
<span class="hljs-bullet">        -</span> sh
<span class="hljs-bullet">        -</span> -c
<span class="hljs-bullet">        -</span> zkGenConfig.sh && zkServer.sh start-foreground
<span class="hljs-attr">        readinessProbe:</span>
<span class="hljs-attr">          exec:</span>
<span class="hljs-attr">            command:</span>
<span class="hljs-bullet">            -</span> <span class="hljs-string">"zkOk.sh"</span>
<span class="hljs-attr">          initialDelaySeconds:</span> <span class="hljs-number">15</span>
<span class="hljs-attr">          timeoutSeconds:</span> <span class="hljs-number">5</span>
<span class="hljs-attr">        livenessProbe:</span>
<span class="hljs-attr">          exec:</span>
<span class="hljs-attr">            command:</span>
<span class="hljs-bullet">            -</span> <span class="hljs-string">"zkOk.sh"</span>
<span class="hljs-attr">          initialDelaySeconds:</span> <span class="hljs-number">15</span>
<span class="hljs-attr">          timeoutSeconds:</span> <span class="hljs-number">5</span>
<span class="hljs-attr">        volumeMounts:</span>
<span class="hljs-attr">        - name:</span> datadir
<span class="hljs-attr">          mountPath:</span> /var/lib/zookeeper
<span class="hljs-attr">      securityContext:</span>
<span class="hljs-attr">        runAsUser:</span> <span class="hljs-number">1000</span>
<span class="hljs-attr">        fsGroup:</span> <span class="hljs-number">1000</span>
<span class="hljs-attr">  volumeClaimTemplates:</span>
<span class="hljs-attr">  - metadata:</span>
<span class="hljs-attr">      name:</span> datadir
<span class="hljs-attr">      annotations:</span>
        volume.alpha.kubernetes.io/storage-class: anything
<span class="hljs-attr">    spec:</span>
<span class="hljs-attr">      accessModes:</span> [<span class="hljs-string">"ReadWriteOnce"</span>]
<span class="hljs-attr">      resources:</span>
<span class="hljs-attr">        requests:</span>
<span class="hljs-attr">          storage:</span> <span class="hljs-number">20</span>Gi
</code></pre>
<pre><code class="lang-sh">kubectl create <span class="hljs-_">-f</span> zookeeper.yaml
</code></pre>
<p>详细的使用说明见 <a href="https://kubernetes.io/docs/tutorials/stateful-application/zookeeper/" target="_blank">zookeeper stateful application</a>。</p>
<h2 id="statefulset-注意事项">StatefulSet 注意事项</h2>
<ol>
<li>推荐在 Kubernetes v1.9 或以后的版本中使用</li>
<li>所有 Pod 的 Volume 必须使用 PersistentVolume 或者是管理员事先创建好</li>
<li>为了保证数据安全，删除 StatefulSet 时不会删除 Volume</li>
<li>StatefulSet 需要一个 Headless Service 来定义 DNS domain，需要在 StatefulSet 之前创建好</li>
</ol>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="thirdpartyresources" class="level3">ThirdPartyResources</h1>
<p>ThirdPartyResources（TPR）是一种无需改变代码就可以扩展 Kubernetes API 的机制，可以用来管理自定义对象。每个 ThirdPartyResource 都包含以下属性</p>
<ul>
<li>metadata：跟 kubernetes metadata 一样</li>
<li>kind：自定义的资源类型，采用 <code><kind mame>.<domain></code> 的格式</li>
<li>description：资源描述</li>
<li>versions：版本列表</li>
<li>其他：还可以保护任何其他自定义的属性</li>
</ul>
<h2 id="api-版本对照表">API 版本对照表</h2>
<table>
<thead>
<tr>
<th>Kubernetes 版本</th>
<th>Extension 版本</th>
</tr>
</thead>
<tbody>
<tr>
<td>v1.5-v1.7</td>
<td>extensions/v1beta1</td>
</tr>
<tr>
<td>v1.8+</td>
<td>不再支持</td>
</tr>
</tbody>
</table>
<blockquote>
<p> <strong>ThirdPartyResources 已在 v1.8 删除</strong></p>
<p> ThirdPartyResources 已在 v1.8 版本中删除。建议从 v1.7 开始，迁移到 <a href="customresourcedefinition.html">CustomResourceDefinition（CRD）</a>。</p>
</blockquote>
<h2 id="tpr-示例">TPR 示例</h2>
<p>下面的例子会创建一个 <code>/apis/stable.example.com/v1/namespaces/<namespace>/crontabs/...</code> 的 API</p>
<pre><code class="lang-sh">$ cat resource.yaml
apiVersion: extensions/v1beta1
kind: ThirdPartyResource
metadata:
  name: cron-tab.stable.example.com
description: <span class="hljs-string">"A specification of a Pod to run on a cron style schedule"</span>
versions:
- name: v1

$ kubectl create <span class="hljs-_">-f</span> resource.yaml
thirdpartyresource <span class="hljs-string">"cron-tab.stable.example.com"</span> created
</code></pre>
<p>API 创建好后，就可以创建具体的 CronTab 对象了</p>
<pre><code class="lang-sh">$ cat my-cronjob.yaml
apiVersion: <span class="hljs-string">"stable.example.com/v1"</span>
kind: CronTab
metadata:
  name: my-new-cron-object
cronSpec: <span class="hljs-string">"* * * * /5"</span>
image: my-awesome-cron-image

$ kubectl create <span class="hljs-_">-f</span> my-crontab.yaml
crontab <span class="hljs-string">"my-new-cron-object"</span> created

$ kubectl get crontab
NAME                 KIND
my-new-cron-object   CronTab.v1.stable.example.com
</code></pre>
<h2 id="thirdpartyresources-与-rbac">ThirdPartyResources 与 RBAC</h2>
<p>注意 ThirdPartyResources 不是 namespace-scoped 的资源，在普通用户使用之前需要绑定 ClusterRole 权限。</p>
<pre><code class="lang-sh">$ cat cron-rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1alpha1
kind: ClusterRole
metadata:
  name: cron-cluster-role
rules:
- apiGroups:
  - extensions
  resources:
  - thirdpartyresources
  verbs:
  - <span class="hljs-string">'*'</span>
- apiGroups:
  - stable.example.com
  resources:
  - crontabs
  verbs:
  - <span class="hljs-string">"*"</span>

$ kubectl create <span class="hljs-_">-f</span> cron-rbac.yaml
$ kubectl create clusterrolebinding user1 --clusterrole=cron-cluster-role --user=user1 --user=user2 --group=group1
</code></pre>
<h2 id="迁移到-customresourcedefinition">迁移到 CustomResourceDefinition</h2>
<ol>
<li>首先将 TPR 资源重定义为 CRD 资源，比如下面这个 ThirdPartyResource 资源</li>
</ol>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> extensions/v1beta1
<span class="hljs-attr">kind:</span> ThirdPartyResource
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> cron-tab.stable.example.com
<span class="hljs-attr">description:</span> <span class="hljs-string">"A specification of a Pod to run on a cron style schedule"</span>
<span class="hljs-attr">versions:</span>
<span class="hljs-attr">- name:</span> v1
</code></pre>
<p>需要重新定义为</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> apiextensions.k8s.io/v1beta1
<span class="hljs-attr">kind:</span> CustomResourceDefinition
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> crontabs.stable.example.com
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  scope:</span> Namespaced
<span class="hljs-attr">  group:</span> stable.example.com
<span class="hljs-attr">  version:</span> v1
<span class="hljs-attr">  names:</span>
<span class="hljs-attr">    kind:</span> CronTab
<span class="hljs-attr">    plural:</span> crontabs
<span class="hljs-attr">    singular:</span> crontab
</code></pre>
<ol>
<li>创建 CustomResourceDefinition 定义后，等待 CRD 的 Established 条件：</li>
</ol>
<pre><code class="lang-sh">$ kubectl get crd -o <span class="hljs-string">'custom-columns=NAME:{.metadata.name},ESTABLISHED:{.status.conditions[?(@.type=="Established")].status}'</span>
NAME                          ESTABLISHED
crontabs.stable.example.com   True
</code></pre>
<ol>
<li><p>然后，停止使用 TPR 的客户端和 TPR Controller，启动新的 CRD Controller。</p>
</li>
<li><p>备份数据</p>
</li>
</ol>
<pre><code class="lang-sh">$ kubectl get crontabs --all-namespaces -o yaml > crontabs.yaml
$ kubectl get thirdpartyresource cron-tab.stable.example.com -o yaml --export > tpr.yaml
</code></pre>
<ol>
<li>删除 TPR 定义，TPR 资源会自动复制为 CRD 资源</li>
</ol>
<pre><code class="lang-sh">$ kubectl delete thirdpartyresource cron-tab.stable.example.com
</code></pre>
<ol>
<li>验证 CRD 数据是否迁移成功，如果有失败发生，可以从备份的 TPR 数据恢复</li>
</ol>
<pre><code class="lang-sh">$ kubectl create <span class="hljs-_">-f</span> tpr.yaml
</code></pre>
<ol>
<li>重启客户端和相关的控制器或监听程序，它们的数据源会自动切换到 CRD（即访问 TPR 的 API 会自动转换为对 CRD 的访问）</li>
</ol>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="kubernetes-存储卷" class="level3">Volume</h1>
<p>我们知道默认情况下容器的数据都是非持久化的，在容器消亡以后数据也跟着丢失，所以 Docker 提供了 Volume 机制以便将数据持久化存储。类似的，Kubernetes 提供了更强大的 Volume 机制和丰富的插件，解决了容器数据持久化和容器间共享数据的问题。</p>
<p>与 Docker 不同，Kubernetes Volume 的生命周期与 Pod 绑定</p>
<ul>
<li>容器挂掉后 Kubelet 再次重启容器时，Volume 的数据依然还在</li>
<li>而 Pod 删除时，Volume 才会清理。数据是否丢失取决于具体的 Volume 类型，比如 emptyDir 的数据会丢失，而 PV 的数据则不会丢</li>
</ul>
<h2 id="volume-类型">Volume 类型</h2>
<p>目前，Kubernetes 支持以下 Volume 类型：</p>
<ul>
<li>emptyDir</li>
<li>hostPath</li>
<li>gcePersistentDisk</li>
<li>awsElasticBlockStore</li>
<li>nfs</li>
<li>iscsi</li>
<li>flocker</li>
<li>glusterfs</li>
<li>rbd</li>
<li>cephfs</li>
<li>gitRepo</li>
<li>secret</li>
<li>persistentVolumeClaim</li>
<li>downwardAPI</li>
<li>azureFileVolume</li>
<li>azureDisk</li>
<li>vsphereVolume</li>
<li>Quobyte</li>
<li>PortworxVolume</li>
<li>ScaleIO</li>
<li>FlexVolume</li>
<li>StorageOS</li>
<li>local</li>
</ul>
<p>注意，这些 volume 并非全部都是持久化的，比如 emptyDir、secret、gitRepo 等，这些 volume 会随着 Pod 的消亡而消失。</p>
<h2 id="api-版本对照表">API 版本对照表</h2>
<table>
<thead>
<tr>
<th>Kubernetes 版本</th>
<th>Core API 版本</th>
</tr>
</thead>
<tbody>
<tr>
<td>v1.5+</td>
<td>core/v1</td>
</tr>
</tbody>
</table>
<h2 id="emptydir">emptyDir</h2>
<p>如果 Pod 设置了 emptyDir 类型 Volume， Pod 被分配到 Node 上时候，会创建 emptyDir，只要 Pod 运行在 Node 上，emptyDir 都会存在（容器挂掉不会导致 emptyDir 丢失数据），但是如果 Pod 从 Node 上被删除（Pod 被删除，或者 Pod 发生迁移），emptyDir 也会被删除，并且永久丢失。</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> test-pd
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">  - image:</span> gcr.io/google_containers/test-webserver
<span class="hljs-attr">    name:</span> test-container
<span class="hljs-attr">    volumeMounts:</span>
<span class="hljs-attr">    - mountPath:</span> /cache
<span class="hljs-attr">      name:</span> cache-volume
<span class="hljs-attr">  volumes:</span>
<span class="hljs-attr">  - name:</span> cache-volume
<span class="hljs-attr">    emptyDir:</span> {}
</code></pre>
<h2 id="hostpath">hostPath</h2>
<p>hostPath 允许挂载 Node 上的文件系统到 Pod 里面去。如果 Pod 需要使用 Node 上的文件，可以使用 hostPath。</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> test-pd
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">  - image:</span> gcr.io/google_containers/test-webserver
<span class="hljs-attr">    name:</span> test-container
<span class="hljs-attr">    volumeMounts:</span>
<span class="hljs-attr">    - mountPath:</span> /test-pd
<span class="hljs-attr">      name:</span> test-volume
<span class="hljs-attr">  volumes:</span>
<span class="hljs-attr">  - name:</span> test-volume
<span class="hljs-attr">    hostPath:</span>
<span class="hljs-attr">      path:</span> /data
</code></pre>
<h2 id="nfs">NFS</h2>
<p>NFS 是 Network File System 的缩写，即网络文件系统。Kubernetes 中通过简单地配置就可以挂载 NFS 到 Pod 中，而 NFS 中的数据是可以永久保存的，同时 NFS 支持同时写操作。</p>
<pre><code class="lang-yaml"><span class="hljs-attr">volumes:</span>
<span class="hljs-attr">- name:</span> nfs
<span class="hljs-attr">  nfs:</span>
    <span class="hljs-comment"># <span class="hljs-doctag">FIXME:</span> use the right hostname</span>
<span class="hljs-attr">    server:</span> <span class="hljs-number">10.254</span><span class="hljs-number">.234</span><span class="hljs-number">.223</span>
<span class="hljs-attr">    path:</span> <span class="hljs-string">"/"</span>
</code></pre>
<h2 id="gcepersistentdisk">gcePersistentDisk</h2>
<p>gcePersistentDisk 可以挂载 GCE 上的永久磁盘到容器，需要 Kubernetes 运行在 GCE 的 VM 中。</p>
<pre><code class="lang-yaml"><span class="hljs-attr">volumes:</span>
<span class="hljs-attr">  - name:</span> test-volume
    <span class="hljs-comment"># This GCE PD must already exist.</span>
<span class="hljs-attr">    gcePersistentDisk:</span>
<span class="hljs-attr">      pdName:</span> my-data-disk
<span class="hljs-attr">      fsType:</span> ext4
</code></pre>
<h2 id="awselasticblockstore">awsElasticBlockStore</h2>
<p>awsElasticBlockStore 可以挂载 AWS 上的 EBS 盘到容器，需要 Kubernetes 运行在 AWS 的 EC2 上。</p>
<pre><code class="lang-yaml"><span class="hljs-attr">volumes:</span>
<span class="hljs-attr">  - name:</span> test-volume
    <span class="hljs-comment"># This AWS EBS volume must already exist.</span>
<span class="hljs-attr">    awsElasticBlockStore:</span>
<span class="hljs-attr">      volumeID:</span> <volume-id<span class="hljs-string">>
</span><span class="hljs-attr">      fsType:</span> ext4
</code></pre>
<h2 id="gitrepo">gitRepo</h2>
<p>gitRepo volume 将 git 代码下拉到指定的容器路径中</p>
<pre><code class="lang-yaml"><span class="hljs-attr">  volumes:</span>
<span class="hljs-attr">  - name:</span> git-volume
<span class="hljs-attr">    gitRepo:</span>
<span class="hljs-attr">      repository:</span> <span class="hljs-string">"git@somewhere:me/my-git-repository.git"</span>
<span class="hljs-attr">      revision:</span> <span class="hljs-string">"22f1d8406d464b0c0874075539c1f2e96c253775"</span>
</code></pre>
<h2 id="使用-subpath">使用 subPath</h2>
<p>Pod 的多个容器使用同一个 Volume 时，subPath 非常有用</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> my-lamp-site
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">    containers:</span>
<span class="hljs-attr">    - name:</span> mysql
<span class="hljs-attr">      image:</span> mysql
<span class="hljs-attr">      volumeMounts:</span>
<span class="hljs-attr">      - mountPath:</span> /var/lib/mysql
<span class="hljs-attr">        name:</span> site-data
<span class="hljs-attr">        subPath:</span> mysql
<span class="hljs-attr">    - name:</span> php
<span class="hljs-attr">      image:</span> php
<span class="hljs-attr">      volumeMounts:</span>
<span class="hljs-attr">      - mountPath:</span> /var/www/html
<span class="hljs-attr">        name:</span> site-data
<span class="hljs-attr">        subPath:</span> html
<span class="hljs-attr">    volumes:</span>
<span class="hljs-attr">    - name:</span> site-data
<span class="hljs-attr">      persistentVolumeClaim:</span>
<span class="hljs-attr">        claimName:</span> my-lamp-site-data
</code></pre>
<h2 id="flexvolume">FlexVolume</h2>
<p>如果内置的这些 Volume 不满足要求，则可以使用 FlexVolume 实现自己的 Volume 插件。注意要把 volume plugin 放到 <code>/usr/libexec/kubernetes/kubelet-plugins/volume/exec/<vendor~driver>/<driver></code>，plugin 要实现 <code>init/attach/detach/mount/umount</code> 等命令（可参考 lvm 的 <a href="https://github.com/kubernetes/kubernetes/tree/master/examples/volumes/flexvolume" target="_blank">示例</a>）。</p>
<pre><code class="lang-yaml"><span class="hljs-attr">  - name:</span> test
<span class="hljs-attr">    flexVolume:</span>
<span class="hljs-attr">      driver:</span> <span class="hljs-string">"kubernetes.io/lvm"</span>
<span class="hljs-attr">      fsType:</span> <span class="hljs-string">"ext4"</span>
<span class="hljs-attr">      options:</span>
<span class="hljs-attr">        volumeID:</span> <span class="hljs-string">"vol1"</span>
<span class="hljs-attr">        size:</span> <span class="hljs-string">"1000m"</span>
<span class="hljs-attr">        volumegroup:</span> <span class="hljs-string">"kube_vg"</span>
</code></pre>
<h2 id="projected-volume">Projected Volume</h2>
<p>Projected volume 将多个 Volume 源映射到同一个目录中，支持 secret、downwardAPI 和 configMap。</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> volume-test
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">  - name:</span> container-test
<span class="hljs-attr">    image:</span> busybox
<span class="hljs-attr">    volumeMounts:</span>
<span class="hljs-attr">    - name:</span> all-in-one
<span class="hljs-attr">      mountPath:</span> <span class="hljs-string">"/projected-volume"</span>
<span class="hljs-attr">      readOnly:</span> <span class="hljs-literal">true</span>
<span class="hljs-attr">  volumes:</span>
<span class="hljs-attr">  - name:</span> all-in-one
<span class="hljs-attr">    projected:</span>
<span class="hljs-attr">      sources:</span>
<span class="hljs-attr">      - secret:</span>
<span class="hljs-attr">          name:</span> mysecret
<span class="hljs-attr">          items:</span>
<span class="hljs-attr">            - key:</span> username
<span class="hljs-attr">              path:</span> my-group/my-username
<span class="hljs-attr">      - downwardAPI:</span>
<span class="hljs-attr">          items:</span>
<span class="hljs-attr">            - path:</span> <span class="hljs-string">"labels"</span>
<span class="hljs-attr">              fieldRef:</span>
<span class="hljs-attr">                fieldPath:</span> metadata.labels
<span class="hljs-attr">            - path:</span> <span class="hljs-string">"cpu_limit"</span>
<span class="hljs-attr">              resourceFieldRef:</span>
<span class="hljs-attr">                containerName:</span> container-test
<span class="hljs-attr">                resource:</span> limits.cpu
<span class="hljs-attr">      - configMap:</span>
<span class="hljs-attr">          name:</span> myconfigmap
<span class="hljs-attr">          items:</span>
<span class="hljs-attr">            - key:</span> config
<span class="hljs-attr">              path:</span> my-group/my-config
</code></pre>
<h2 id="本地存储限额">本地存储限额</h2>
<p>v1.7 + 支持对基于本地存储（如 hostPath, emptyDir, gitRepo 等）的容量进行调度限额，可以通过 <code>--feature-gates=LocalStorageCapacityIsolation=true</code> 来开启这个特性。</p>
<p>为了支持这个特性，Kubernetes 将本地存储分为两类</p>
<ul>
<li><code>storage.kubernetes.io/overlay</code>，即 <code>/var/lib/docker</code> 的大小</li>
<li><code>storage.kubernetes.io/scratch</code>，即 <code>/var/lib/kubelet</code> 的大小</li>
</ul>
<p>Kubernetes 根据 <code>storage.kubernetes.io/scratch</code> 的大小来调度本地存储空间，而根据 <code>storage.kubernetes.io/overlay</code> 来调度容器的存储。比如</p>
<p>为容器请求 64MB 的可写层存储空间</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> ls1
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  restartPolicy:</span> Never
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">  - name:</span> hello
<span class="hljs-attr">    image:</span> busybox
<span class="hljs-attr">    command:</span> [<span class="hljs-string">"df"</span>]
<span class="hljs-attr">    resources:</span>
<span class="hljs-attr">      requests:</span>
        storage.kubernetes.io/overlay: <span class="hljs-number">64</span>Mi
</code></pre>
<p>为 empty 请求 64MB 的存储空间</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> ls1
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  restartPolicy:</span> Never
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">  - name:</span> hello
<span class="hljs-attr">    image:</span> busybox
<span class="hljs-attr">    command:</span> [<span class="hljs-string">"df"</span>]
<span class="hljs-attr">    volumeMounts:</span>
<span class="hljs-attr">    - name:</span> data
<span class="hljs-attr">      mountPath:</span> /data
<span class="hljs-attr">  volumes:</span>
<span class="hljs-attr">  - name:</span> data
<span class="hljs-attr">    emptyDir:</span>
<span class="hljs-attr">      sizeLimit:</span> <span class="hljs-number">64</span>Mi
</code></pre>
<h2 id="mount-传递">Mount 传递</h2>
<p>在 Kubernetes 中，Volume Mount 默认是 <a href="https://www.kernel.org/doc/Documentation/filesystems/sharedsubtree.txt" target="_blank">私有的</a>，但从 v1.8 开始，Kubernetes 支持配置 Mount 传递（mountPropagation）。它支持两种选项</p>
<ul>
<li>HostToContainer：这是开启 <code>MountPropagation=true</code> 时的默认模式，等效于 <code>rslave</code> 模式，即容器可以看到 Host 上面在该 volume 内的任何新 Mount 操作</li>
<li>Bidirectional：等效于 <code>rshared</code> 模式，即 Host 和容器都可以看到对方在该 Volume 内的任何新 Mount 操作。该模式要求容器必须运行在特权模式（即 <code>securityContext.privileged=true</code>）</li>
</ul>
<p>注意：</p>
<ul>
<li>使用 Mount 传递需要开启 <code>--feature-gates=MountPropagation=true</code></li>
<li><code>rslave</code> 和 <code>rshared</code> 的说明可以参考 <a href="https://www.kernel.org/doc/Documentation/filesystems/sharedsubtree.txt" target="_blank">内核文档</a></li>
</ul>
<h2 id="volume-快照">Volume 快照</h2>
<p>v1.8 新增了 pre-alpha 版本的 Volume 快照，但还只是一个雏形，并且其实现不在 Kubernetes 核心代码中，而是存放在 <a href="https://github.com/kubernetes-incubator/external-storage/tree/master/snapshot" target="_blank">kubernetes-incubator/external-storage</a> 中。</p>
<blockquote>
<p>TODO:  补充 Volume 快照的设计原理和示例。</p>
</blockquote>
<h2 id="windows-volume">Windows Volume</h2>
<p>Windows 容器暂时只支持 local、emptyDir、hostPath、AzureDisk、AzureFile 以及 flexvolume。注意 Volume 的路径格式需要为 <code>mountPath: "C:\\etc\\foo"</code> 或者 <code>mountPath: "C:/etc/foo"</code>。</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> hostpath-pod
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">  - name:</span> hostpath-nano
<span class="hljs-attr">    image:</span> microsoft/nanoserver:<span class="hljs-number">1709</span>
<span class="hljs-attr">    stdin:</span> <span class="hljs-literal">true</span>
<span class="hljs-attr">    tty:</span> <span class="hljs-literal">true</span>
<span class="hljs-attr">    volumeMounts:</span>
<span class="hljs-attr">    - name:</span> blah
<span class="hljs-attr">      mountPath:</span> <span class="hljs-string">"C:\\etc\\foo"</span>
<span class="hljs-attr">      readOnly:</span> <span class="hljs-literal">true</span>
<span class="hljs-attr">  nodeSelector:</span>
    beta.kubernetes.io/os: windows
<span class="hljs-attr">  volumes:</span>
<span class="hljs-attr">  - name:</span> blah
<span class="hljs-attr">    hostPath:</span>
<span class="hljs-attr">      path:</span> <span class="hljs-string">"C:\\AzureData"</span>
</code></pre>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> empty-dir-pod
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">  - image:</span> microsoft/nanoserver:<span class="hljs-number">1709</span>
<span class="hljs-attr">    name:</span> empty-dir-nano
<span class="hljs-attr">    stdin:</span> <span class="hljs-literal">true</span>
<span class="hljs-attr">    tty:</span> <span class="hljs-literal">true</span>
<span class="hljs-attr">    volumeMounts:</span>
<span class="hljs-attr">    - mountPath:</span> /cache
<span class="hljs-attr">      name:</span> cache-volume
<span class="hljs-attr">    - mountPath:</span> C:/scratch
<span class="hljs-attr">      name:</span> scratch-volume
<span class="hljs-attr">  volumes:</span>
<span class="hljs-attr">  - name:</span> cache-volume
<span class="hljs-attr">    emptyDir:</span> {}
<span class="hljs-attr">  - name:</span> scratch-volume
<span class="hljs-attr">    emptyDir:</span> {}
<span class="hljs-attr">  nodeSelector:</span>
    beta.kubernetes.io/os: windows
</code></pre>
<h2 id="挂载传播">挂载传播</h2>
<p><a href="https://www.kernel.org/doc/Documentation/filesystems/sharedsubtree.txt" target="_blank">挂载传播（MountPropagation）</a>是 v1.9 引入的新功能，并在 v1.10 中升级为 Beta 版本。挂载传播用来解决同一个 Volume 在不同的容器甚至是 Pod 之间挂载的问题。通过设置 `Container.volumeMounts.mountPropagation），可以为该存储卷设置不同的传播类型。</p>
<p>支持三种选项：</p>
<ul>
<li>None：即私有挂载（private）</li>
<li>HostToContainer：即 Host 内在该目录中的新挂载都可以在容器中看到，等价于 Linux 内核的 rslave。</li>
<li>Bidirectional：即 Host 内在该目录中的新挂载都可以在容器中看到，同样容器内在该目录中的任何新挂载也都可以在 Host 中看到，等价于 Linux 内核的 rshared。仅特权容器（privileged）可以使用 Bidirectional 类型。</li>
</ul>
<p>注意：</p>
<ul>
<li>使用前需要开启 MountPropagation 特性</li>
<li>如未设置，则 v1.9 和 v1.10 中默认为私有挂载（<code>None</code>），而 v1.11 中默认为 <code>HostToContainer</code></li>
<li>Docker 服务的 systemd 配置文件中需要设置 <code>MountFlags=shared</code></li>
</ul>
<h2 id="其他的-volume-参考示例">其他的 Volume 参考示例</h2>
<p><a href="https://github.com/kubernetes/examples/tree/master/staging/volumes/iscsi" target="_blank">https://github.com/kubernetes/examples/tree/master/staging/volumes/iscsi</a></p>
<ul>
<li><a href="https://github.com/kubernetes/examples/tree/master/staging/volumes/iscsi" target="_blank">iSCSI Volume 示例</a></li>
<li><a href="https://github.com/kubernetes/examples/tree/master/staging/volumes/cephfs" target="_blank">cephfs Volume 示例</a></li>
<li><a href="https://github.com/kubernetes/examples/tree/master/staging/volumes/flocker" target="_blank">Flocker Volume 示例</a></li>
<li><a href="https://github.com/kubernetes/examples/tree/master/staging/volumes/glusterfs" target="_blank">GlusterFS Volume 示例</a></li>
<li><a href="https://github.com/kubernetes/examples/tree/master/staging/volumes/rbd" target="_blank">RBD Volume 示例</a></li>
<li><a href="secret.html">Secret Volume 示例</a></li>
<li><a href="https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/" target="_blank">downwardAPI Volume 示例</a></li>
<li><a href="https://github.com/kubernetes/examples/tree/master/staging/volumes/azure_file" target="_blank">AzureFile Volume 示例</a></li>
<li><a href="https://github.com/kubernetes/examples/tree/master/staging/volumes/azure_disk" target="_blank">AzureDisk Volume 示例</a></li>
<li><a href="https://github.com/kubernetes/examples/tree/master/staging/volumes/quobyte" target="_blank">Quobyte Volume 示例</a></li>
<li><a href="https://github.com/kubernetes/examples/tree/master/staging/volumes/portworx" target="_blank">Portworx Volume 示例</a></li>
<li><a href="https://github.com/kubernetes/examples/tree/master/staging/volumes/scaleio" target="_blank">ScaleIO Volume 示例</a></li>
<li><a href="https://github.com/kubernetes/examples/tree/master/staging/volumes/storageos" target="_blank">StorageOS Volume 示例</a></li>
</ul>
</section>
                            
    <h1 class='level1'>部署配置</h1><section class="normal markdown-section">
                                
                                <h1 id="kubernetes-部署指南" class="level2">部署指南</h1>
<p>本章介绍创建的 Kubernetes 集群部署方法、 kubectl 客户端的安装方法以及推荐的配置。</p>
<p>其中 <a href="kubernetes-the-hard-way/">Kubernetes-The-Hard-Way</a> 介绍了在 GCE 的 Ubuntu 虚拟机中一步步部署一套 Kubernetes 高可用集群的详细步骤，这些步骤也同样适用于 CentOS 等其他系统以及 AWS、Azure 等其他公有云平台。</p>
<p>在国内部署集群时，通常还会碰到镜像无法拉取或者拉取过慢的问题。对这类问题的解决方法就是使用国内的镜像，具体可以参考<a href="../appendix/mirrors.html">国内镜像列表</a>。</p>
<p>一般部署完成后，还需要运行一系列的测试来验证部署是成功的。<a href="https://github.com/heptio/sonobuoy" target="_blank">sonobuoy</a> 可以简化这个验证的过程，它通过一系列的测试来验证集群的功能是否正常。其使用方法为</p>
<ul>
<li>通过 <a href="https://scanner.heptio.com/" target="_blank">Sonobuoy Scanner tool</a> 在线使用（需要集群公网可访问）</li>
<li>或者使用命令行工具</li>
</ul>
<pre><code class="lang-sh"><span class="hljs-comment"># Install</span>
$ go get -u -v github.com/heptio/sonobuoy

<span class="hljs-comment"># Run</span>
$ sonobuoy run
$ sonobuoy status
$ sonobuoy logs
$ sonobuoy retrieve .

<span class="hljs-comment"># Cleanup</span>
$ sonobuoy delete
</code></pre>
<h2 id="版本依赖">版本依赖</h2>
<table>
<thead>
<tr>
<th>依赖组件</th>
<th>v1.13</th>
<th>v1.12</th>
</tr>
</thead>
<tbody>
<tr>
<td>Etcd</td>
<td>v3.2.24+或v3.3.0+</td>
<td>v3.2.24+ 或 v3.3.0+ etcd2弃用</td>
</tr>
<tr>
<td>Docker</td>
<td>1.11.1, 1.12.1, 1.13.1, 17.03, 17.06, 17.09, 18.06</td>
<td>1.11.1, 1.12.1, 1.13.1, 17.03, 17.06, 17.09, 18.06</td>
</tr>
<tr>
<td>Go</td>
<td>1.11.2</td>
<td>1.10.4</td>
</tr>
<tr>
<td>CNI</td>
<td>v0.6.0</td>
<td>v0.6.0</td>
</tr>
<tr>
<td>CSI</td>
<td>1.0.0</td>
<td>0.3.0</td>
</tr>
<tr>
<td>Dashboard</td>
<td>v1.10.0</td>
<td>v1.8.3</td>
</tr>
<tr>
<td>Heapster</td>
<td>Remains v1.6.0-beta but retired</td>
<td>v1.6.0-beta</td>
</tr>
<tr>
<td>Cluster Autoscaler</td>
<td>v1.13.0</td>
<td>v1.12.0</td>
</tr>
<tr>
<td>kube-dns</td>
<td>v1.14.13</td>
<td>v1.14.13</td>
</tr>
<tr>
<td>Influxdb</td>
<td>v1.3.3</td>
<td>v1.3.3</td>
</tr>
<tr>
<td>Grafana</td>
<td>v4.4.3</td>
<td>v4.4.3</td>
</tr>
<tr>
<td>Kibana</td>
<td>v6.3.2</td>
<td>v6.3.2</td>
</tr>
<tr>
<td>cAdvisor</td>
<td>v0.32.0</td>
<td>v0.30.1</td>
</tr>
<tr>
<td>Fluentd</td>
<td>v1.2.4</td>
<td>v1.2.4</td>
</tr>
<tr>
<td>Elasticsearch</td>
<td>v6.3.2</td>
<td>v6.3.2</td>
</tr>
<tr>
<td>go-oidc</td>
<td>v2.0.0</td>
<td>v2.0.0</td>
</tr>
<tr>
<td>calico</td>
<td>v3.3.1</td>
<td>v2.6.7</td>
</tr>
<tr>
<td>crictl</td>
<td>v1.12.0</td>
<td>v1.12.0</td>
</tr>
<tr>
<td>CoreDNS</td>
<td>v1.2.6</td>
<td>v1.2.2</td>
</tr>
<tr>
<td>event-exporter</td>
<td>v0.2.3</td>
<td>v0.2.3</td>
</tr>
<tr>
<td>metrics-server</td>
<td>v0.3.1</td>
<td>v0.3.1</td>
</tr>
<tr>
<td>ingress-gce</td>
<td>v1.2.3</td>
<td>v1.2.3</td>
</tr>
<tr>
<td>ingress-nginx</td>
<td>v0.21.0</td>
<td>v0.21.0</td>
</tr>
<tr>
<td>ip-masq-agent</td>
<td>v2.1.1</td>
<td>v2.1.1</td>
</tr>
<tr>
<td>hcsshim</td>
<td>v0.6.11</td>
<td>v0.6.11</td>
</tr>
</tbody>
</table>
<h2 id="部署方法">部署方法</h2>
<ul>
<li><a href="single.html">1. 单机部署</a></li>
<li><a href="cluster.html">2. 集群部署</a><ul>
<li><a href="kubeadm.html">kubeadm</a></li>
<li><a href="kops.html">kops</a></li>
<li><a href="kubespray.html">Kubespray</a></li>
<li><a href="azure.html">Azure</a></li>
<li><a href="windows.html">Windows</a></li>
<li><a href="k8s-linuxkit.html">LinuxKit</a></li>
<li><a href="frakti/">Frakti</a></li>
<li><a href="https://github.com/gjmzj/kubeasz" target="_blank">kubeasz</a></li>
</ul>
</li>
<li><a href="kubernetes-the-hard-way/">3. Kubernetes-The-Hard-Way</a><ul>
<li><a href="kubernetes-the-hard-way/01-prerequisites.html">准备部署环境</a></li>
<li><a href="kubernetes-the-hard-way/02-client-tools.html">安装必要工具</a></li>
<li><a href="kubernetes-the-hard-way/03-compute-resources.html">创建计算资源</a></li>
<li><a href="kubernetes-the-hard-way/04-certificate-authority.html">配置创建证书</a></li>
<li><a href="kubernetes-the-hard-way/05-kubernetes-configuration-files.html">配置生成配置</a></li>
<li><a href="kubernetes-the-hard-way/06-data-encryption-keys.html">配置生成密钥</a></li>
<li><a href="kubernetes-the-hard-way/07-bootstrapping-etcd.html">部署Etcd群集</a></li>
<li><a href="kubernetes-the-hard-way/08-bootstrapping-kubernetes-controllers.html">部署控制节点</a></li>
<li><a href="kubernetes-the-hard-way/09-bootstrapping-kubernetes-workers.html">部署计算节点</a></li>
<li><a href="kubernetes-the-hard-way/10-configuring-kubectl.html">配置Kubectl</a></li>
<li><a href="kubernetes-the-hard-way/11-pod-network-routes.html">配置网络路由</a></li>
<li><a href="kubernetes-the-hard-way/12-dns-addon.html">部署DNS扩展</a></li>
<li><a href="kubernetes-the-hard-way/13-smoke-test.html">烟雾测试</a></li>
<li><a href="kubernetes-the-hard-way/14-cleanup.html">删除集群</a></li>
</ul>
</li>
<li><a href="kubectl.html">4. kubectl客户端</a></li>
<li><a href="../addons/">5. 附加组件</a><ul>
<li><a href="../addons/dashboard.html">Dashboard</a></li>
<li><a href="../addons/heapster.md">Heapster</a></li>
<li><a href="../addons/efk.md">EFK</a></li>
<li><a href="../addons/metrics.html">Metrics</a></li>
<li><a href="../addons/cluster-autoscaler.html">Cluster AutoScaler</a></li>
</ul>
</li>
<li><a href="kubernetes-configuration-best-practice.html">6. 推荐配置</a></li>
<li><a href="upgrade.html">7. 版本支持</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="kubectl-安装" class="level2">kubectl安装</h1>
<p>本章介绍 kubectl 的安装方法。</p>
<h2 id="安装方法">安装方法</h2>
<h3 id="osx">OSX</h3>
<p>可以使用 Homebrew 或者 <code>curl</code> 下载 kubectl：</p>
<pre><code class="lang-sh">brew install kubectl
</code></pre>
<p>或者</p>
<pre><code class="lang-sh">curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl <span class="hljs-_">-s</span> https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/darwin/amd64/kubectl
</code></pre>
<h3 id="linux">Linux</h3>
<pre><code class="lang-sh">curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl <span class="hljs-_">-s</span> https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl
</code></pre>
<h3 id="windows">Windows</h3>
<pre><code class="lang-sh">curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl <span class="hljs-_">-s</span> https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/windows/amd64/kubectl.exe
</code></pre>
<p>或者使用 Chocolatey 来安装：</p>
<pre><code class="lang-sh">choco install kubernetes-cli
</code></pre>
<h2 id="使用方法">使用方法</h2>
<p>kubectl 的详细使用方法请参考 <a href="../components/kubectl.html">kubectl 指南</a>。</p>
<h2 id="kubectl-插件">kubectl 插件</h2>
<p>你可以使用 krew 来管理 kubectl 插件。</p>
<p><a href="https://github.com/GoogleContainerTools/krew" target="_blank">krew</a> 是一个用来管理 kubectl 插件的工具，类似于 apt 或 yum，支持搜索、安装和管理 kubectl 插件。</p>
<h3 id="安装">安装</h3>
<pre><code class="lang-sh">(
  <span class="hljs-built_in">set</span> -x; <span class="hljs-built_in">cd</span> <span class="hljs-string">"<span class="hljs-variable">$(mktemp -d)</span>"</span> &&
  curl -fsSLO <span class="hljs-string">"https://storage.googleapis.com/krew/v0.2.1/krew.{tar.gz,yaml}"</span> &&
  tar zxvf krew.tar.gz &&
  ./krew-<span class="hljs-string">"<span class="hljs-variable">$(uname | tr '[:upper:]' '[:lower:]')</span>_amd64"</span> install \
    --manifest=krew.yaml --archive=krew.tar.gz
)
</code></pre>
<p>安装完成后，把 krew 的二进制文件加入环境变量 PATH 中：</p>
<pre><code class="lang-sh"><span class="hljs-built_in">export</span> PATH=<span class="hljs-string">"<span class="hljs-variable">${KREW_ROOT:-$HOME/.krew}</span>/bin:<span class="hljs-variable">$PATH</span>"</span>
</code></pre>
<p>最后，再执行 kubectl 命令确认安装成功：</p>
<pre><code class="lang-sh">$ kubectl plugin list
The following kubectl-compatible plugins are available:

/home/<user>/.krew/bin/kubectl-krew
</code></pre>
<h3 id="使用方法">使用方法</h3>
<p>首次使用前，请执行下面的命令更新插件索引：</p>
<pre><code class="lang-sh">kubectl krew update
</code></pre>
<p>使用示例：</p>
<pre><code class="lang-sh">kubectl krew search               <span class="hljs-comment"># show all plugins</span>
kubectl krew install ssh-jump  <span class="hljs-comment"># install a plugin named "ssh-jump"</span>
kubectl ssh-jump               <span class="hljs-comment"># use the plugin</span>
kubectl krew upgrade              <span class="hljs-comment"># upgrade installed plugins</span>
kubectl krew remove ssh-jump   <span class="hljs-comment"># uninstall a plugin</span>
</code></pre>
<p>在安装插件后，会输出插件所依赖的外部工具，这些工具需要你自己手动安装。</p>
<pre><code class="lang-sh">Installing plugin: ssh-jump
CAVEATS:
\
 |  This plugin needs the following programs:
 |  * ssh(1)
 |  * ssh-agent(1)
 |
 |  Please follow the documentation: https://github.com/yokawasa/kubectl-plugin-ssh-jump
/
Installed plugin: ssh-jump
</code></pre>
<p>最后，就可以通过 <code>kubectl <plugin-name></code> 来使用插件了：</p>
<pre><code class="lang-sh">kubectl ssh-jump <node-name> -u <username> -i ~/.ssh/id_rsa -p ~/.ssh/id_rsa.pub
</code></pre>
<h3 id="升级方法">升级方法</h3>
<pre><code class="lang-sh">kubectl krew upgrade
</code></pre>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://github.com/GoogleContainerTools/krew" target="_blank">https://github.com/GoogleContainerTools/krew</a> </li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="单机部署" class="level2">单机部署</h1>
<h2 id="minikube">minikube</h2>
<p>创建 Kubernetes cluster（单机版）最简单的方法是 <a href="https://github.com/kubernetes/minikube" target="_blank">minikube</a>。国内网络环境下也可以考虑使用 <a href="https://github.com/gjmzj/kubeasz" target="_blank">kubeasz</a> 的 AllInOne 部署。</p>
<p>首先下载 kubectl</p>
<pre><code class="lang-sh">curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl <span class="hljs-_">-s</span> https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl
chmod +x kubectl
</code></pre>
<p>安装 minikube（以 MacOS 为例）</p>
<pre><code class="lang-sh"><span class="hljs-comment"># install minikube</span>
$ brew cask install minikube
$ curl -LO https://storage.googleapis.com/minikube/releases/latest/docker-machine-driver-hyperkit
$ sudo install -o root -g wheel -m 4755 docker-machine-driver-hyperkit /usr/<span class="hljs-built_in">local</span>/bin/
</code></pre>
<p>在 Windows 上面</p>
<pre><code class="lang-sh">choco install minikube
choco install kubernetes-cli
</code></pre>
<p>最后启动 minikube</p>
<pre><code class="lang-sh"><span class="hljs-comment"># start minikube.</span>
<span class="hljs-comment"># http proxy is required in China</span>
$ minikube start --docker-env HTTP_PROXY=http://proxy-ip:port --docker-env HTTPS_PROXY=http://proxy-ip:port --vm-driver=hyperkit
</code></pre>
<h3 id="使用-calico">使用 calico</h3>
<p>minikube 支持配置使用 CNI 插件，这样可以方便的使用社区提供的各种网络插件，比如使用 calico 还可以支持 Network Policy。</p>
<p>首先使用下面的命令启动 minikube：</p>
<pre><code class="lang-sh">minikube start --docker-env HTTP_PROXY=http://proxy-ip:port \
    --docker-env HTTPS_PROXY=http://proxy-ip:port \
    --network-plugin=cni \
    --host-only-cidr 172.17.17.1/24 \
    --extra-config=kubelet.ClusterCIDR=192.168.0.0/16 \
    --extra-config=proxy.ClusterCIDR=192.168.0.0/16 \
    --extra-config=controller-manager.ClusterCIDR=192.168.0.0/16
</code></pre>
<p>安装 calico 网络插件：</p>
<pre><code class="lang-sh">kubectl apply <span class="hljs-_">-f</span> https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/rbac-kdd.yaml
curl -O -L https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calico-networking/1.7/calico.yaml
sed -i <span class="hljs-_">-e</span> <span class="hljs-string">'/nodeSelector/d'</span> calico.yaml
sed -i <span class="hljs-_">-e</span> <span class="hljs-string">'/node-role.kubernetes.io\/master:""/d'</span> calico.yaml
sed -i <span class="hljs-_">-e</span> <span class="hljs-string">'s/10\.96\.232/10.0.0/'</span> calico.yaml
kubectl apply <span class="hljs-_">-f</span> calico.yaml
</code></pre>
<h2 id="开发版">开发版</h2>
<p>minikube/localkube 只提供了正式 release 版本，而如果想要部署 master 或者开发版的话，则可以用 <code>hack/local-up-cluster.sh</code> 来启动一个本地集群：</p>
<pre><code class="lang-sh"><span class="hljs-built_in">cd</span> <span class="hljs-variable">$GOPATH</span>/src/k8s.io/kubernetes

<span class="hljs-built_in">export</span> KUBERNETES_PROVIDER=<span class="hljs-built_in">local</span>
hack/install-etcd.sh
<span class="hljs-built_in">export</span> PATH=<span class="hljs-variable">$GOPATH</span>/src/k8s.io/kubernetes/third_party/etcd:<span class="hljs-variable">$PATH</span>
hack/<span class="hljs-built_in">local</span>-up-cluster.sh
</code></pre>
<p>打开另外一个终端，配置 kubectl：</p>
<pre><code class="lang-sh"><span class="hljs-built_in">cd</span> <span class="hljs-variable">$GOPATH</span>/src/k8s.io/kubernetes
<span class="hljs-built_in">export</span> KUBECONFIG=/var/run/kubernetes/admin.kubeconfig
cluster/kubectl.sh
</code></pre>
<p>或者，使用 <a href="https://github.com/kubernetes-sigs/kind" target="_blank">kind</a>，以 Docker 容器的方式运行 Kubernetes 集群：</p>
<pre><code class="lang-sh">$ go get sigs.k8s.io/kind
<span class="hljs-comment"># ensure that Kubernetes is cloned in $(go env GOPATH)/src/k8s.io/kubernetes</span>
<span class="hljs-comment"># build a node image</span>
$ kind build node-image
<span class="hljs-comment"># create a cluster with kind build node-image</span>
$ kind create cluster --image kindest/node:latest
</code></pre>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://kubernetes.io/docs/getting-started-guides/minikube/" target="_blank">Running Kubernetes Locally via Minikube</a></li>
<li><a href="https://github.com/kubernetes-sigs/kind" target="_blank">https://github.com/kubernetes-sigs/kind</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="特性开关" class="level2">特性开关</h1>
<p>特性开关（Feature Gates）是 Kubernetes 中用来开启实验性功能的配置，可以通过选项 <code>--feature-gates</code> 来给不同的组件（如 kube-apiserver、kube-controller-manager、kube-scheduler、kubelet、kube-proxy等）开启功能特性。</p>
<table>
<thead>
<tr>
<th>Feature</th>
<th>Default</th>
<th>Stage</th>
<th>Since</th>
<th>Until</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Accelerators</code></td>
<td><code>false</code></td>
<td>Alpha</td>
<td>1.6</td>
<td>1.10</td>
</tr>
<tr>
<td><code>AdvancedAuditing</code></td>
<td><code>false</code></td>
<td>Alpha</td>
<td>1.7</td>
<td>1.7</td>
</tr>
<tr>
<td><code>AdvancedAuditing</code></td>
<td><code>true</code></td>
<td>Beta</td>
<td>1.8</td>
<td>1.11</td>
</tr>
<tr>
<td><code>AdvancedAuditing</code></td>
<td><code>true</code></td>
<td>GA</td>
<td>1.12</td>
<td>-</td>
</tr>
<tr>
<td><code>AffinityInAnnotations</code></td>
<td><code>false</code></td>
<td>Alpha</td>
<td>1.6</td>
<td>1.7</td>
</tr>
<tr>
<td><code>AllowExtTrafficLocalEndpoints</code></td>
<td><code>false</code></td>
<td>Beta</td>
<td>1.4</td>
<td>1.6</td>
</tr>
<tr>
<td><code>AllowExtTrafficLocalEndpoints</code></td>
<td><code>true</code></td>
<td>GA</td>
<td>1.7</td>
<td>-</td>
</tr>
<tr>
<td><code>APIListChunking</code></td>
<td><code>false</code></td>
<td>Alpha</td>
<td>1.8</td>
<td>1.8</td>
</tr>
<tr>
<td><code>APIListChunking</code></td>
<td><code>true</code></td>
<td>Beta</td>
<td>1.9</td>
<td/>
</tr>
<tr>
<td><code>APIResponseCompression</code></td>
<td><code>false</code></td>
<td>Alpha</td>
<td>1.7</td>
<td/>
</tr>
<tr>
<td><code>AppArmor</code></td>
<td><code>true</code></td>
<td>Beta</td>
<td>1.4</td>
<td/>
</tr>
<tr>
<td><code>AttachVolumeLimit</code></td>
<td><code>false</code></td>
<td>Alpha</td>
<td>1.11</td>
<td/>
</tr>
<tr>
<td><code>BlockVolume</code></td>
<td><code>false</code></td>
<td>Alpha</td>
<td>1.9</td>
<td/>
</tr>
<tr>
<td><code>CPUManager</code></td>
<td><code>false</code></td>
<td>Alpha</td>
<td>1.8</td>
<td>1.9</td>
</tr>
<tr>
<td><code>CPUManager</code></td>
<td><code>true</code></td>
<td>Beta</td>
<td>1.10</td>
<td/>
</tr>
<tr>
<td><code>CRIContainerLogRotation</code></td>
<td><code>false</code></td>
<td>Alpha</td>
<td>1.10</td>
<td>1.10</td>
</tr>
<tr>
<td><code>CRIContainerLogRotation</code></td>
<td><code>true</code></td>
<td>Beta</td>
<td>1.11</td>
<td/>
</tr>
<tr>
<td><code>CSIBlockVolume</code></td>
<td><code>false</code></td>
<td>Alpha</td>
<td>1.11</td>
<td>1.11</td>
</tr>
<tr>
<td><code>CSIPersistentVolume</code></td>
<td><code>false</code></td>
<td>Alpha</td>
<td>1.9</td>
<td>1.9</td>
</tr>
<tr>
<td><code>CSIPersistentVolume</code></td>
<td><code>true</code></td>
<td>Beta</td>
<td>1.10</td>
<td/>
</tr>
<tr>
<td><code>CustomPodDNS</code></td>
<td><code>false</code></td>
<td>Alpha</td>
<td>1.9</td>
<td>1.9</td>
</tr>
<tr>
<td><code>CustomPodDNS</code></td>
<td><code>true</code></td>
<td>Beta</td>
<td>1.10</td>
<td/>
</tr>
<tr>
<td><code>CustomResourceSubresources</code></td>
<td><code>false</code></td>
<td>Alpha</td>
<td>1.10</td>
<td/>
</tr>
<tr>
<td><code>CustomResourceValidation</code></td>
<td><code>false</code></td>
<td>Alpha</td>
<td>1.8</td>
<td>1.8</td>
</tr>
<tr>
<td><code>CustomResourceValidation</code></td>
<td><code>true</code></td>
<td>Beta</td>
<td>1.9</td>
<td/>
</tr>
<tr>
<td><code>DebugContainers</code></td>
<td><code>false</code></td>
<td>Alpha</td>
<td>1.10</td>
<td/>
</tr>
<tr>
<td><code>DevicePlugins</code></td>
<td><code>false</code></td>
<td>Alpha</td>
<td>1.8</td>
<td>1.9</td>
</tr>
<tr>
<td><code>DevicePlugins</code></td>
<td><code>true</code></td>
<td>Beta</td>
<td>1.10</td>
<td/>
</tr>
<tr>
<td><code>DynamicKubeletConfig</code></td>
<td><code>false</code></td>
<td>Alpha</td>
<td>1.4</td>
<td>1.10</td>
</tr>
<tr>
<td><code>DynamicKubeletConfig</code></td>
<td><code>true</code></td>
<td>Beta</td>
<td>1.11</td>
<td/>
</tr>
<tr>
<td><code>DynamicProvisioningScheduling</code></td>
<td><code>false</code></td>
<td>Alpha</td>
<td>1.11</td>
<td>1.11</td>
</tr>
<tr>
<td><code>DynamicVolumeProvisioning</code></td>
<td><code>true</code></td>
<td>Alpha</td>
<td>1.3</td>
<td>1.7</td>
</tr>
<tr>
<td><code>DynamicVolumeProvisioning</code></td>
<td><code>true</code></td>
<td>GA</td>
<td>1.8</td>
<td/>
</tr>
<tr>
<td><code>EnableEquivalenceClassCache</code></td>
<td><code>false</code></td>
<td>Alpha</td>
<td>1.8</td>
<td/>
</tr>
<tr>
<td><code>ExpandInUsePersistentVolumes</code></td>
<td><code>false</code></td>
<td>Alpha</td>
<td>1.11</td>
<td/>
</tr>
<tr>
<td><code>ExpandPersistentVolumes</code></td>
<td><code>false</code></td>
<td>Alpha</td>
<td>1.8</td>
<td>1.10</td>
</tr>
<tr>
<td><code>ExpandPersistentVolumes</code></td>
<td><code>true</code></td>
<td>Beta</td>
<td>1.11</td>
<td/>
</tr>
<tr>
<td><code>ExperimentalCriticalPodAnnotation</code></td>
<td><code>false</code></td>
<td>Alpha</td>
<td>1.5</td>
<td/>
</tr>
<tr>
<td><code>ExperimentalHostUserNamespaceDefaulting</code></td>
<td><code>false</code></td>
<td>Beta</td>
<td>1.5</td>
<td/>
</tr>
<tr>
<td><code>GCERegionalPersistentDisk</code></td>
<td><code>true</code></td>
<td>Beta</td>
<td>1.10</td>
<td/>
</tr>
<tr>
<td><code>HugePages</code></td>
<td><code>false</code></td>
<td>Alpha</td>
<td>1.8</td>
<td>1.9</td>
</tr>
<tr>
<td><code>HugePages</code></td>
<td><code>true</code></td>
<td>Beta</td>
<td>1.10</td>
<td/>
</tr>
<tr>
<td><code>HyperVContainer</code></td>
<td><code>false</code></td>
<td>Alpha</td>
<td>1.10</td>
<td/>
</tr>
<tr>
<td><code>Initializers</code></td>
<td><code>false</code></td>
<td>Alpha</td>
<td>1.7</td>
<td/>
</tr>
<tr>
<td><code>KubeletConfigFile</code></td>
<td><code>false</code></td>
<td>Alpha</td>
<td>1.8</td>
<td>1.9</td>
</tr>
<tr>
<td><code>KubeletPluginsWatcher</code></td>
<td><code>false</code></td>
<td>Alpha</td>
<td>1.11</td>
<td>1.11</td>
</tr>
<tr>
<td><code>KubeletPluginsWatcher</code></td>
<td><code>true</code></td>
<td>Beta</td>
<td>1.12</td>
<td/>
</tr>
<tr>
<td><code>LocalStorageCapacityIsolation</code></td>
<td><code>false</code></td>
<td>Alpha</td>
<td>1.7</td>
<td>1.9</td>
</tr>
<tr>
<td><code>LocalStorageCapacityIsolation</code></td>
<td><code>true</code></td>
<td>Beta</td>
<td>1.10</td>
<td/>
</tr>
<tr>
<td><code>MountContainers</code></td>
<td><code>false</code></td>
<td>Alpha</td>
<td>1.9</td>
<td/>
</tr>
<tr>
<td><code>MountPropagation</code></td>
<td><code>false</code></td>
<td>Alpha</td>
<td>1.8</td>
<td>1.9</td>
</tr>
<tr>
<td><code>MountPropagation</code></td>
<td><code>true</code></td>
<td>Beta</td>
<td>1.10</td>
<td>1.11</td>
</tr>
<tr>
<td><code>MountPropagation</code></td>
<td><code>true</code></td>
<td>GA</td>
<td>1.12</td>
<td/>
</tr>
<tr>
<td><code>PersistentLocalVolumes</code></td>
<td><code>false</code></td>
<td>Alpha</td>
<td>1.7</td>
<td>1.9</td>
</tr>
<tr>
<td><code>PersistentLocalVolumes</code></td>
<td><code>true</code></td>
<td>Beta</td>
<td>1.10</td>
<td/>
</tr>
<tr>
<td><code>PodPriority</code></td>
<td><code>false</code></td>
<td>Alpha</td>
<td>1.8</td>
<td/>
</tr>
<tr>
<td><code>PodReadinessGates</code></td>
<td><code>false</code></td>
<td>Alpha</td>
<td>1.11</td>
<td/>
</tr>
<tr>
<td><code>PodReadinessGates</code></td>
<td><code>true</code></td>
<td>Beta</td>
<td>1.12</td>
<td/>
</tr>
<tr>
<td><code>PodShareProcessNamespace</code></td>
<td><code>false</code></td>
<td>Alpha</td>
<td>1.10</td>
<td/>
</tr>
<tr>
<td><code>PodShareProcessNamespace</code></td>
<td><code>true</code></td>
<td>Beta</td>
<td>1.12</td>
<td/>
</tr>
<tr>
<td><code>PVCProtection</code></td>
<td><code>false</code></td>
<td>Alpha</td>
<td>1.9</td>
<td>1.9</td>
</tr>
<tr>
<td><code>ReadOnlyAPIDataVolumes</code></td>
<td><code>true</code></td>
<td>Deprecated</td>
<td>1.10</td>
<td/>
</tr>
<tr>
<td><code>ResourceLimitsPriorityFunction</code></td>
<td><code>false</code></td>
<td>Alpha</td>
<td>1.9</td>
<td/>
</tr>
<tr>
<td><code>RotateKubeletClientCertificate</code></td>
<td><code>true</code></td>
<td>Beta</td>
<td>1.7</td>
<td/>
</tr>
<tr>
<td><code>RotateKubeletServerCertificate</code></td>
<td><code>false</code></td>
<td>Alpha</td>
<td>1.7</td>
<td/>
</tr>
<tr>
<td><code>RunAsGroup</code></td>
<td><code>false</code></td>
<td>Alpha</td>
<td>1.10</td>
<td/>
</tr>
<tr>
<td><code>RuntimeClass</code></td>
<td><code>false</code></td>
<td>Alpha</td>
<td>1.12</td>
<td/>
</tr>
<tr>
<td><code>SCTPSupport</code></td>
<td><code>false</code></td>
<td>Alpha</td>
<td>1.12</td>
<td/>
</tr>
<tr>
<td><code>ServiceNodeExclusion</code></td>
<td><code>false</code></td>
<td>Alpha</td>
<td>1.8</td>
<td/>
</tr>
<tr>
<td><code>StorageObjectInUseProtection</code></td>
<td><code>true</code></td>
<td>Beta</td>
<td>1.10</td>
<td>1.10</td>
</tr>
<tr>
<td><code>StorageObjectInUseProtection</code></td>
<td><code>true</code></td>
<td>GA</td>
<td>1.11</td>
<td/>
</tr>
<tr>
<td><code>StreamingProxyRedirects</code></td>
<td><code>true</code></td>
<td>Beta</td>
<td>1.5</td>
<td/>
</tr>
<tr>
<td><code>SupportIPVSProxyMode</code></td>
<td><code>false</code></td>
<td>Alpha</td>
<td>1.8</td>
<td>1.8</td>
</tr>
<tr>
<td><code>SupportIPVSProxyMode</code></td>
<td><code>false</code></td>
<td>Beta</td>
<td>1.9</td>
<td>1.9</td>
</tr>
<tr>
<td><code>SupportIPVSProxyMode</code></td>
<td><code>true</code></td>
<td>Beta</td>
<td>1.10</td>
<td>1.10</td>
</tr>
<tr>
<td><code>SupportIPVSProxyMode</code></td>
<td><code>true</code></td>
<td>GA</td>
<td>1.11</td>
<td/>
</tr>
<tr>
<td><code>SupportPodPidsLimit</code></td>
<td><code>false</code></td>
<td>Alpha</td>
<td>1.10</td>
<td/>
</tr>
<tr>
<td><code>Sysctls</code></td>
<td><code>true</code></td>
<td>Beta</td>
<td>1.11</td>
<td/>
</tr>
<tr>
<td><code>TaintBasedEvictions</code></td>
<td><code>false</code></td>
<td>Alpha</td>
<td>1.6</td>
<td/>
</tr>
<tr>
<td><code>TaintNodesByCondition</code></td>
<td><code>false</code></td>
<td>Alpha</td>
<td>1.8</td>
<td/>
</tr>
<tr>
<td><code>TaintNodesByCondition</code></td>
<td><code>true</code></td>
<td>Beta</td>
<td>1.12</td>
<td/>
</tr>
<tr>
<td><code>TokenRequest</code></td>
<td><code>false</code></td>
<td>Alpha</td>
<td>1.10</td>
<td>1.11</td>
</tr>
<tr>
<td><code>TokenRequest</code></td>
<td><code>True</code></td>
<td>Beta</td>
<td>1.12</td>
<td/>
</tr>
<tr>
<td><code>TokenRequestProjection</code></td>
<td><code>false</code></td>
<td>Alpha</td>
<td>1.11</td>
<td>1.11</td>
</tr>
<tr>
<td><code>TokenRequestProjection</code></td>
<td><code>True</code></td>
<td>Beta</td>
<td>1.12</td>
<td/>
</tr>
<tr>
<td><code>TTLAfterFinished</code></td>
<td><code>false</code></td>
<td>Alpha</td>
<td>1.12</td>
<td/>
</tr>
<tr>
<td><code>VolumeScheduling</code></td>
<td><code>false</code></td>
<td>Alpha</td>
<td>1.9</td>
<td>1.9</td>
</tr>
<tr>
<td><code>VolumeScheduling</code></td>
<td><code>true</code></td>
<td>Beta</td>
<td>1.10</td>
<td/>
</tr>
<tr>
<td><code>VolumeSubpathEnvExpansion</code></td>
<td><code>false</code></td>
<td>Alpha</td>
<td>1.11</td>
<td/>
</tr>
<tr>
<td><code>ScheduleDaemonSetPods</code></td>
<td><code>true</code></td>
<td>Beta</td>
<td>1.12</td>
</tr>
</tbody>
</table>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/" target="_blank">Kubernetes Feature Gates</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="kubernetes-配置最佳实践" class="level2">最佳配置</h1>
<p>本文档旨在汇总和强调用户指南、快速开始文档和示例中的最佳实践。该文档会很很活跃并持续更新中。如果你觉得很有用的最佳实践但是本文档中没有包含，欢迎给我们提 Pull Request。</p>
<h2 id="通用配置建议">通用配置建议</h2>
<ul>
<li>定义配置文件的时候，指定最新的稳定 API 版本。</li>
<li>在部署配置文件到集群之前应该保存在版本控制系统中。这样当需要的时候能够快速回滚，必要的时候也可以快速的创建集群。</li>
<li>使用 YAML 格式而不是 JSON 格式的配置文件。在大多数场景下它们都可以互换，但是 YAML 格式比 JSON 更友好。</li>
<li>尽量将相关的对象放在同一个配置文件里，这样比分成多个文件更容易管理。参考 <a href="https://github.com/kubernetes/examples/blob/master/guestbook/all-in-one/guestbook-all-in-one.yaml" target="_blank">guestbook-all-in-one.yaml</a> 文件中的配置。</li>
<li>使用 <code>kubectl</code> 命令时指定配置文件目录。</li>
<li>不要指定不必要的默认配置，这样更容易保持配置文件简单并减少配置错误。</li>
<li>将资源对象的描述放在一个 annotation 中可以更好的内省。</li>
</ul>
<h2 id="裸奔的-pods-vs-replication-controllers-和-jobs">裸奔的 Pods vs Replication Controllers 和 Jobs</h2>
<ul>
<li>如果有其他方式替代 “裸奔的 pod”（如没有绑定到 <a href="https://kubernetes.io/docs/user-guide/replication-controller" target="_blank">replication controller </a> 上的 pod），那么就使用其他选择。</li>
<li>在 node 节点出现故障时，裸奔的 pod 不会被重新调度。</li>
<li>Replication Controller 总是会重新创建 pod，除了明确指定了 <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy" target="_blank"><code>restartPolicy: Never</code></a> 的场景。<a href="https://kubernetes.io/docs/concepts/jobs/run-to-completion-finite-workloads/" target="_blank">Job</a> 对象也适用。</li>
</ul>
<h2 id="services">Services</h2>
<ul>
<li>通常最好在创建相关的 <a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/" target="_blank">replication controllers</a> 之前先创建 <a href="https://kubernetes.io/docs/concepts/services-networking/service/" target="_blank">service</a>。这样可以保证容器在启动时就配置了该服务的环境变量。对于新的应用，推荐通过服务的 DNS 名字来访问（而不是通过环境变量）。</li>
<li>除非有必要（如运行一个 node daemon），不要使用配置 <code>hostPort</code> 的 Pod（用来指定暴露在主机上的端口号）。当你给 Pod 绑定了一个 <code>hostPort</code>，该 Pod 会因为端口冲突很难调度。如果是为了调试目的来通过端口访问的话，你可以使用 <a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/http-proxy-access-api/" target="_blank">kubectl proxy and apiserver proxy</a> 或者 <a href="https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/" target="_blank">kubectl port-forward</a>。你可使用 <a href="https://kubernetes.io/docs/concepts/services-networking/service/" target="_blank">Service</a> 来对外暴露服务。如果你确实需要将 pod 的端口暴露到主机上，考虑使用 <a href="https://kubernetes.io/docs/user-guide/services/#type-nodeport" target="_blank">NodePort</a> service。</li>
<li>跟 <code>hostPort</code> 一样的原因，避免使用 <code>hostNetwork</code>。</li>
<li>如果你不需要 kube-proxy 的负载均衡的话，可以考虑使用使用 <a href="https://kubernetes.io/docs/user-guide/services/#headless-services" target="_blank">headless services</a>（ClusterIP 为 None）。</li>
</ul>
<h2 id="使用-label">使用 Label</h2>
<ul>
<li>使用 <a href="https://kubernetes.io/docs/user-guide/labels/" target="_blank">labels</a> 来指定应用或 Deployment 的语义属性。这样可以让你能够选择合适于场景的对象组，比如 <code>app: myapp, tire: frontend, phase: test, deployment: v3</code>。</li>
<li>一个 service 可以被配置成跨越多个 deployment，只需要在它的 label selector 中简单的省略发布相关的 label。</li>
<li>注意 <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/" target="_blank">Deployment</a> 对象不需要再管理 replication controller 的版本名。Deployment 中描述了对象的期望状态，如果对 spec 的更改被应用了话，Deployment controller 会以控制的速率来更改实际状态到期望状态。</li>
<li>利用 label 做调试。因为 Kubernetes replication controller 和 service 使用 label 来匹配 pods，这允许你通过移除 pod 的相关label的方式将其从一个 controller 或者 service 中移除，而 controller 会创建一个新的 pod 来取代移除的 pod。这是一个很有用的方式，帮你在一个隔离的环境中调试之前的 “活着的” pod。</li>
</ul>
<h2 id="容器镜像">容器镜像</h2>
<ul>
<li>默认容器镜像拉取策略是 <code>IfNotPresent</code>, 当本地已存在该镜像的时候 Kubelet 不会再从镜像仓库拉取。如果你希望总是从镜像仓库中拉取镜像的话，在 yaml 文件中指定镜像拉取策略为 <code>Always</code>（ <code>imagePullPolicy: Always</code>）或者指定镜像的 tag 为 <code>:latest</code> 。</li>
<li>如果你没有将镜像标签指定为 <code>:latest</code>，例如指定为 <code>myimage:v1</code>，当该标签的镜像进行了更新，kubelet 也不会拉取该镜像。你可以在每次镜像更新后都生成一个新的 tag（例如 <code>myimage:v2</code>），在配置文件中明确指定该版本。</li>
<li>可以使用镜像的摘要（Digest）来保证容器总是使用同一版本的镜像。</li>
<li><strong> 注意：</strong> 在生产环境下部署容器应该尽量避免使用 <code>:latest</code> 标签，因为这样很难追溯到底运行的是哪个版本以及发生故障时该如何回滚。</li>
</ul>
<h2 id="使用-kubectl">使用 kubectl</h2>
<ul>
<li><p>尽量使用 <code>kubectl create -f <directory></code> 或 <code>kubectl apply -f <directory</code> 。kubeclt 会自动查找该目录下的所有后缀名为 <code>.yaml</code>、<code>.yml</code> 和 <code>.json</code> 文件并将它们传递给 <code>create</code> 或 <code>apply</code> 命令。</p>
</li>
<li><p><code>kubectl get</code> 或 <code>kubectl delete</code> 时使用标签选择器可以批量操作一组对象。</p>
</li>
<li><p>使用 <code>kubectl run</code> 和 <code>expose</code> 命令快速创建只有单个容器的 Deployment 和 Service，如</p>
<pre><code class="lang-sh">kubectl run hello-world --replicas=2 --labels=<span class="hljs-string">"run=load-balancer-example"</span> --image=gcr.io/google-samples/node-hello:1.0  --port=8080
kubectl expose deployment hello-world --type=NodePort --name=example-service
kubectl get pods --selector=<span class="hljs-string">"run=load-balancer-example"</span> --output=wide
</code></pre>
</li>
</ul>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://kubernetes.io/docs/concepts/configuration/overview/" target="_blank">Configuration Best Practices</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="kubernetes--版本支持策略" class="level2">版本支持</h1>
<h2 id="版本支持">版本支持</h2>
<p>Kubernetes 版本的格式为 <strong>x.y.z</strong>，其中 x 是主版本号，y 是次版本号，而 z 则是修订版本。版本的格式遵循 <a href="http://semver.org/" target="_blank">Semantic Versioning</a> ，即</p>
<ul>
<li>主版本号：当你做了不兼容的 API 修改，</li>
<li>次版本号：当你做了向下兼容的功能性新增，</li>
<li>修订号：当你做了向下兼容的问题修正。
Kubernetes 项目只维护最新的三个次版本，每个版本都会放到不同的发布分支中维护。上游版本发现的严重缺陷以及安全修复等都会移植到这些发布分支中，这些分支由 <a href="https://github.com/kubernetes/sig-release/blob/master/release-team/role-handbooks/patch-release-manager/README.md#release-timing" target="_blank">patch release manager</a> 来维护。
次版本一般是每三个月发布一次，所以每个发布分支一般会维护 9 个月。<h2 id="不同组件的版本支持情况">不同组件的版本支持情况</h2>
在 Kubernetes 中，不同组件的版本并不要求完全一致，但不同版本的组件混合部署时也有一些最基本的限制。<h3 id="kube-apiserver">kube-apiserver</h3>
在 <a href="https://kubernetes.io/docs/setup/independent/high-availability/" target="_blank">highly-availabile (HA) clusters</a> 集群中，kube-apiserver 的版本差不能超过一个次版本号。比如最新的 kube-apiserver 版本号为 1.13 时，其他 kube-apiserver 的版本只能是 1.13 或者 1.12。<h3 id="kubelet">kubelet</h3>
Kubelet 的版本不能高于 kube-apiserver 的版本，并且跟 kube-apiserver 相比，最多可以相差两个次版本号。比如：</li>
<li><code>kube-apiserver</code> 的版本是 <strong>1.13</strong></li>
<li>相应的  <code>kubelet</code> 的版本为 <strong>1.13</strong>, <strong>1.12</strong>, and <strong>1.11</strong>
再比如，一个高可用的集群中：</li>
<li><code>kube-apiserver</code> 版本号为 <strong>1.13</strong> and <strong>1.12</strong></li>
<li>相应的  <code>kubelet</code>  版本为 <strong>1.12</strong>, and <strong>1.11</strong> ( <strong>1.13</strong> 不支持，因为它比 kube-apiserver 的 <strong>1.12</strong> 高)<h3 id="kube-controller-manager-kube-scheduler-and-cloud-controller-manager">kube-controller-manager, kube-scheduler, and cloud-controller-manager</h3>
<code>kube-controller-manager</code>, <code>kube-scheduler</code>, 和 <code>cloud-controller-manager</code> 不能高于 kube-apiserver 的版本。通常它们的版本应该跟 kube-apiserver 一致，不过也支持相差一个次版本号同时运行。比如：</li>
<li><code>kube-apiserver</code> 版本为 <strong>1.13</strong></li>
<li>相应的 <code>kube-controller-manager</code>, <code>kube-scheduler</code>, 和 <code>cloud-controller-manager</code>  版本为 <strong>1.13</strong> and <strong>1.12</strong>
再比如，一个高可用的集群中：</li>
<li><code>kube-apiserver</code>  版本为 <strong>1.13</strong> and <strong>1.12</strong></li>
<li>相应的 <code>kube-controller-manager</code>, <code>kube-scheduler</code>, 和  <code>cloud-controller-manager</code> 版本为 <strong>1.12</strong> ( <strong>1.13</strong> 不支持，因为它比 apiserver 的<strong>1.12</strong> 高)<h3 id="kubectl">kubectl</h3>
kubectl 可以跟 kube-apiserver 相差一个次版本号，比如：</li>
<li><code>kube-apiserver</code> 版本为 <strong>1.13</strong></li>
<li>相应的  <code>kubectl</code> 版本为 <strong>1.14</strong>, <strong>1.13</strong> 和 <strong>1.12</strong><h2 id="版本升级顺序">版本升级顺序</h2>
当从 1.n 版本升级到 1.(n+1) 版本时，必须要遵循以下的升级顺序。<h3 id="kube-apiserver">kube-apiserver</h3>
前提条件：</li>
<li>单节点集群中， kube-apiserver 的版本为 1.n；HA 集群中，kube-apiserver 版本为 1.n 或者 1.(n+1)。</li>
<li><code>kube-controller-manager</code>, <code>kube-scheduler</code> 以及 <code>cloud-controller-manager</code> 的版本都是 1.n。</li>
<li>kubelet 的版本是 1.n 或者 1.(n-1)</li>
<li>已注册的注入控制 webhook 可以处理新版本的请求，比如 ValidatingWebhookConfiguration 和 MutatingWebhookConfiguration 已经更新为支持 1.(n+1) 版本中新引入的特性。</li>
</ul>
<p>接下来就可以把 kube-apiserver 升级到 1.(n+1)了，不过要注意<strong>版本升级时不可跳过次版本号</strong>。</p>
<h3 id="kube-controller-manager-kube-scheduler-and-cloud-controller-manager">kube-controller-manager, kube-scheduler, and cloud-controller-manager</h3>
<p>前提条件：</p>
<ul>
<li>kube-apiserver 已经升级到 1.(n+1) 版本。</li>
</ul>
<p>接下来就可以把  <code>kube-controller-manager</code>, <code>kube-scheduler</code> 和  <code>cloud-controller-manager</code> 都升级到 <strong>1.(n+1)</strong> 版本了。</p>
<h3 id="kubelet">kubelet</h3>
<p>前提条件：</p>
<ul>
<li>kube-apiserver 已经升级到 1.(n+1) 版本。</li>
<li>升级过程中需要保证 kubelet 跟 kube-apiserver 最多只相差一个次版本号。</li>
</ul>
<p>接下来就可以把 kubelet 升级到 1.(n+1)了。</p>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://kubernetes.io/docs/setup/version-skew-policy/" target="_blank">Kubernetes Version and Version Skew Support Policy - Kubernetes</a> </li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="集群部署" class="level2">集群部署</h1>
<h2 id="kubernetes-集群架构">Kubernetes 集群架构</h2>
<p><img src="images/ha.png" alt=""/></p>
<h3 id="etcd-集群">etcd 集群</h3>
<p>从 <code>https://discovery.etcd.io/new?size=3</code> 获取 token 后，把 <code>etcd.yaml</code> 放到每台机器的 <code>/etc/kubernetes/manifests/etcd.yaml</code>，并替换掉 <code>${DISCOVERY_TOKEN}</code>, <code>${NODE_NAME}</code> 和 <code>${NODE_IP}</code>，即可以由 kubelet 来启动一个 etcd 集群。</p>
<p>对于运行在 kubelet 外部的 etcd，可以参考 <a href="https://github.com/coreos/etcd/blob/master/Documentation/op-guide/clustering.md" target="_blank">etcd clustering guide</a> 来手动配置集群模式。</p>
<h3 id="kube-apiserver">kube-apiserver</h3>
<p>把 <code>kube-apiserver.yaml</code> 放到每台 Master 节点的 <code>/etc/kubernetes/manifests/</code>，并把相关的配置放到 <code>/srv/kubernetes/</code>，即可由 kubelet 自动创建并启动 apiserver:</p>
<ul>
<li>basic_auth.csv - basic auth user and password</li>
<li>ca.crt - Certificate Authority cert</li>
<li>known_tokens.csv - tokens that entities (e.g. the kubelet) can use to talk to the apiserver</li>
<li>kubecfg.crt - Client certificate, public key</li>
<li>kubecfg.key - Client certificate, private key</li>
<li>server.cert - Server certificate, public key</li>
<li>server.key - Server certificate, private key</li>
</ul>
<p>apiserver 启动后，还需要为它们做负载均衡，可以使用云平台的弹性负载均衡服务或者使用 haproxy/lvs/nginx 等为 master 节点配置负载均衡。</p>
<p>另外，还可以借助 Keepalived、OSPF、Pacemaker 等来保证负载均衡节点的高可用。</p>
<p>注意：</p>
<ul>
<li>大规模集群注意增加 <code>--max-requests-inflight</code>（默认 400）</li>
<li>使用 nginx 时注意增加 <code>proxy_timeout: 10m</code></li>
</ul>
<h3 id="controller-manager-和-scheduler">controller manager 和 scheduler</h3>
<p>controller manager 和 scheduler 需要保证任何时刻都只有一个实例运行，需要一个选主的过程，所以在启动时要设置 <code>--leader-elect=true</code>，比如</p>
<pre><code>kube-scheduler --master=127.0.0.1:8080 --v=2 --leader-elect=true
kube-controller-manager --master=127.0.0.1:8080 --cluster-cidr=10.245.0.0/16 --allocate-node-cidrs=true --service-account-private-key-file=/srv/kubernetes/server.key --v=2 --leader-elect=true
</code></pre><p>把 <code>kube-scheduler.yaml</code> 和 <code>kube-controller-manager</code> 放到每台 Master 节点的 <code>/etc/kubernetes/manifests/</code>，并把相关的配置放到 <code>/srv/kubernetes/</code>，即可由 kubelet 自动创建并启动 kube-scheduler 和 kube-controller-manager。</p>
<h3 id="kube-dns">kube-dns</h3>
<p>kube-dns 可以通过 Deployment 的方式来部署，默认 kubeadm 会自动创建。但在大规模集群的时候，需要放宽资源限制，比如</p>
<pre><code>dns_replicas: 6
dns_cpu_limit: 100m
dns_memory_limit: 512Mi
dns_cpu_requests 70m
dns_memory_requests: 70Mi
</code></pre><p>另外，也需要给 dnsmasq 增加资源，比如增加缓存大小到 10000，增加并发处理数量 <code>--dns-forward-max=1000</code> 等。</p>
<h3 id="数据持久化">数据持久化</h3>
<p>除了上面提到的这些配置，持久化存储也是高可用 Kubernetes 集群所必须的。</p>
<ul>
<li>对于公有云上部署的集群，可以考虑使用云平台提供的持久化存储，比如 aws ebs 或者 gce persistent disk</li>
<li>对于物理机部署的集群，可以考虑使用 iSCSI、NFS、Gluster 或者 Ceph 等网络存储，也可以使用 RAID</li>
</ul>
<h2 id="azure">Azure</h2>
<p>在 Azure 上可以使用 AKS 或者 acs-engine 来部署 Kubernetes 集群，具体部署方法参考 <a href="azure.html">这里</a>。</p>
<h2 id="gce">GCE</h2>
<p>在 GCE 上可以利用 cluster 脚本方便的部署集群：</p>
<pre><code># gce,aws,gke,azure-legacy,vsphere,openstack-heat,rackspace,libvirt-coreos
export KUBERNETES_PROVIDER=gce
curl -sS https://get.k8s.io | bash
cd kubernetes
cluster/kube-up.sh
</code></pre><h2 id="aws">AWS</h2>
<p>在 aws 上建议使用 <a href="https://kubernetes.io/docs/getting-started-guides/kops/" target="_blank">kops</a> 来部署。</p>
<h2 id="物理机或虚拟机">物理机或虚拟机</h2>
<p>在 Linux 物理机或虚拟机中，建议使用 <a href="https://kubernetes.io/docs/getting-started-guides/kubeadm/" target="_blank">kubeadm</a> 或 <a href="kubespray.html">kubespray</a> 来部署 Kubernetes 集群。</p>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="kubeadm" class="level3">kubeadm</h1>
<blockquote>
<p>Kubernetes 一键部署脚本（使用 docker 运行时）</p>
</blockquote>
<pre><code class="lang-sh"><span class="hljs-comment"># on master</span>
<span class="hljs-built_in">export</span> USE_MIRROR=<span class="hljs-literal">true</span> <span class="hljs-comment">#国内用户必须使用MIRROR</span>
git <span class="hljs-built_in">clone</span> https://github.com/feiskyer/ops
<span class="hljs-built_in">cd</span> ops
kubernetes/install-kubernetes.sh
<span class="hljs-comment"># 记住控制台输出的 TOEKN 和 MASTER 地址，在其他 Node 安装时会用到</span>

<span class="hljs-comment"># on all nodes</span>
git <span class="hljs-built_in">clone</span> https://github.com/feiskyer/ops
<span class="hljs-built_in">cd</span> ops
<span class="hljs-comment"># Setup token and CIDR first.</span>
<span class="hljs-comment"># replace this with yours.</span>
<span class="hljs-built_in">export</span> TOKEN=<span class="hljs-string">"xxxx"</span>
<span class="hljs-built_in">export</span> MASTER_IP=<span class="hljs-string">"x.x.x.x"</span>
<span class="hljs-built_in">export</span> CONTAINER_CIDR=<span class="hljs-string">"10.244.2.0/24"</span>

<span class="hljs-comment"># Setup and join the new node.</span>
./kubernetes/add-node.sh
</code></pre>
<p>以下是详细的 kubeadm 部署集群步骤。</p>
<h2 id="初始化系统">初始化系统</h2>
<p>所有机器都需要初始化 docker 和 kubelet。</p>
<h3 id="ubuntu">ubuntu</h3>
<pre><code class="lang-sh"><span class="hljs-comment"># for ubuntu 16.04</span>
apt-get update
apt-get install -y apt-transport-https ca-certificates curl software-properties-common
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add -
add-apt-repository <span class="hljs-string">"deb https://download.docker.com/linux/<span class="hljs-variable">$(. /etc/os-release; echo "$ID")</span> <span class="hljs-variable">$(lsb_release -cs)</span> stable"</span>
apt-get update && apt-get install -y docker-ce=$(apt-cache madison docker-ce | grep 17.03 | head -1 | awk <span class="hljs-string">'{print $3}'</span>)

apt-get update && apt-get install -y apt-transport-https curl
curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add -
    cat <<EOF >/etc/apt/sources.list.d/kubernetes.list
deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main
EOF
apt-get update
apt-get install -y kubelet kubeadm kubectl
</code></pre>
<h3 id="centos">CentOS</h3>
<pre><code class="lang-sh">yum install -y docker
systemctl <span class="hljs-built_in">enable</span> docker && systemctl start docker

cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
setenforce 0
sed -i <span class="hljs-string">'s/SELINUX=enforcing/SELINUX=disabled/g'</span> /etc/selinux/config
yum install -y kubelet kubeadm kubectl
systemctl <span class="hljs-built_in">enable</span> kubelet && systemctl start kubelet
</code></pre>
<h2 id="安装-master">安装 master</h2>
<pre><code class="lang-sh"><span class="hljs-comment"># --api-advertise-addresses <ip-address></span>
<span class="hljs-comment"># for flannel, setup --pod-network-cidr 10.244.0.0/16</span>
kubeadm init --pod-network-cidr 10.244.0.0/16 --kubernetes-version latest

<span class="hljs-comment"># enable schedule pods on the master</span>
<span class="hljs-built_in">export</span> KUBECONFIG=/etc/kubernetes/admin.conf
kubectl taint nodes --all node-role.kubernetes.io/master:NoSchedule-
</code></pre>
<p>如果需要修改 kubernetes 服务的配置选项，则需要创建一个 kubeadm 配置文件，其格式为</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> kubeadm.k8s.io/v1alpha3
<span class="hljs-attr">kind:</span> InitConfiguration
<span class="hljs-attr">bootstrapTokens:</span>
<span class="hljs-attr">- token:</span> <span class="hljs-string">"9a08jv.c0izixklcxtmnze7"</span>
<span class="hljs-attr">  description:</span> <span class="hljs-string">"kubeadm bootstrap token"</span>
<span class="hljs-attr">  ttl:</span> <span class="hljs-string">"24h"</span>
<span class="hljs-attr">- token:</span> <span class="hljs-string">"783bde.3f89s0fje9f38fhf"</span>
<span class="hljs-attr">  description:</span> <span class="hljs-string">"another bootstrap token"</span>
<span class="hljs-attr">  usages:</span>
<span class="hljs-bullet">  -</span> signing
<span class="hljs-attr">  groups:</span>
<span class="hljs-attr">  - system:</span>anonymous
<span class="hljs-attr">nodeRegistration:</span>
<span class="hljs-attr">  name:</span> <span class="hljs-string">"ec2-10-100-0-1"</span>
<span class="hljs-attr">  criSocket:</span> <span class="hljs-string">"/var/run/dockershim.sock"</span>
<span class="hljs-attr">  taints:</span>
<span class="hljs-attr">  - key:</span> <span class="hljs-string">"kubeadmNode"</span>
<span class="hljs-attr">    value:</span> <span class="hljs-string">"master"</span>
<span class="hljs-attr">    effect:</span> <span class="hljs-string">"NoSchedule"</span>
<span class="hljs-attr">  kubeletExtraArgs:</span>
<span class="hljs-attr">    cgroupDriver:</span> <span class="hljs-string">"cgroupfs"</span>
<span class="hljs-attr">apiEndpoint:</span>
<span class="hljs-attr">  advertiseAddress:</span> <span class="hljs-string">"10.100.0.1"</span>
<span class="hljs-attr">  bindPort:</span> <span class="hljs-number">6443</span>
<span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> kubeadm.k8s.io/v1alpha3
<span class="hljs-attr">kind:</span> ClusterConfiguration
<span class="hljs-attr">etcd:</span>
  <span class="hljs-comment"># one of local or external</span>
<span class="hljs-attr">  local:</span>
<span class="hljs-attr">    image:</span> <span class="hljs-string">"k8s.gcr.io/etcd-amd64:3.2.18"</span>
<span class="hljs-attr">    dataDir:</span> <span class="hljs-string">"/var/lib/etcd"</span>
<span class="hljs-attr">    extraArgs:</span>
<span class="hljs-attr">      listen-client-urls:</span> <span class="hljs-string">"http://10.100.0.1:2379"</span>
<span class="hljs-attr">    serverCertSANs:</span>
<span class="hljs-bullet">    -</span>  <span class="hljs-string">"ec2-10-100-0-1.compute-1.amazonaws.com"</span>
<span class="hljs-attr">    peerCertSANs:</span>
<span class="hljs-bullet">    -</span> <span class="hljs-string">"10.100.0.1"</span>
<span class="hljs-attr">  external:</span>
<span class="hljs-attr">    endpoints:</span>
<span class="hljs-bullet">    -</span> <span class="hljs-string">"10.100.0.1:2379"</span>
<span class="hljs-bullet">    -</span> <span class="hljs-string">"10.100.0.2:2379"</span>
<span class="hljs-attr">    caFile:</span> <span class="hljs-string">"/etcd/kubernetes/pki/etcd/etcd-ca.crt"</span>
<span class="hljs-attr">    certFile:</span> <span class="hljs-string">"/etcd/kubernetes/pki/etcd/etcd.crt"</span>
<span class="hljs-attr">    certKey:</span> <span class="hljs-string">"/etcd/kubernetes/pki/etcd/etcd.key"</span>
<span class="hljs-attr">networking:</span>
<span class="hljs-attr">  serviceSubnet:</span> <span class="hljs-string">"10.96.0.0/12"</span>
<span class="hljs-attr">  podSubnet:</span> <span class="hljs-string">"10.100.0.1/24"</span>
<span class="hljs-attr">  dnsDomain:</span> <span class="hljs-string">"cluster.local"</span>
<span class="hljs-attr">kubernetesVersion:</span> <span class="hljs-string">"v1.12.0"</span>
<span class="hljs-attr">controlPlaneEndpoint:</span> <span class="hljs-string">"10.100.0.1:6443"</span>
<span class="hljs-attr">apiServerExtraArgs:</span>
<span class="hljs-attr">  authorization-mode:</span> <span class="hljs-string">"Node,RBAC"</span>
<span class="hljs-attr">controlManagerExtraArgs:</span>
<span class="hljs-attr">  node-cidr-mask-size:</span> <span class="hljs-number">20</span>
<span class="hljs-attr">schedulerExtraArgs:</span>
<span class="hljs-attr">  address:</span> <span class="hljs-string">"10.100.0.1"</span>
<span class="hljs-attr">apiServerExtraVolumes:</span>
<span class="hljs-attr">- name:</span> <span class="hljs-string">"some-volume"</span>
<span class="hljs-attr">  hostPath:</span> <span class="hljs-string">"/etc/some-path"</span>
<span class="hljs-attr">  mountPath:</span> <span class="hljs-string">"/etc/some-pod-path"</span>
<span class="hljs-attr">  writable:</span> <span class="hljs-literal">true</span>
<span class="hljs-attr">  pathType:</span> File
<span class="hljs-attr">controllerManagerExtraVolumes:</span>
<span class="hljs-attr">- name:</span> <span class="hljs-string">"some-volume"</span>
<span class="hljs-attr">  hostPath:</span> <span class="hljs-string">"/etc/some-path"</span>
<span class="hljs-attr">  mountPath:</span> <span class="hljs-string">"/etc/some-pod-path"</span>
<span class="hljs-attr">  writable:</span> <span class="hljs-literal">true</span>
<span class="hljs-attr">  pathType:</span> File
<span class="hljs-attr">schedulerExtraVolumes:</span>
<span class="hljs-attr">- name:</span> <span class="hljs-string">"some-volume"</span>
<span class="hljs-attr">  hostPath:</span> <span class="hljs-string">"/etc/some-path"</span>
<span class="hljs-attr">  mountPath:</span> <span class="hljs-string">"/etc/some-pod-path"</span>
<span class="hljs-attr">  writable:</span> <span class="hljs-literal">true</span>
<span class="hljs-attr">  pathType:</span> File
<span class="hljs-attr">apiServerCertSANs:</span>
<span class="hljs-bullet">-</span> <span class="hljs-string">"10.100.1.1"</span>
<span class="hljs-bullet">-</span> <span class="hljs-string">"ec2-10-100-0-1.compute-1.amazonaws.com"</span>
<span class="hljs-attr">certificatesDir:</span> <span class="hljs-string">"/etc/kubernetes/pki"</span>
<span class="hljs-attr">imageRepository:</span> <span class="hljs-string">"k8s.gcr.io"</span>
<span class="hljs-attr">unifiedControlPlaneImage:</span> <span class="hljs-string">"k8s.gcr.io/controlplane:v1.12.0"</span>
<span class="hljs-attr">auditPolicy:</span>
  <span class="hljs-comment"># https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#audit-policy</span>
<span class="hljs-attr">  path:</span> <span class="hljs-string">"/var/log/audit/audit.json"</span>
<span class="hljs-attr">  logDir:</span> <span class="hljs-string">"/var/log/audit"</span>
<span class="hljs-attr">  logMaxAge:</span> <span class="hljs-number">7</span> <span class="hljs-comment"># in days</span>
<span class="hljs-attr">featureGates:</span>
<span class="hljs-attr">  selfhosting:</span> <span class="hljs-literal">false</span>
<span class="hljs-attr">clusterName:</span> <span class="hljs-string">"example-cluster"</span>
</code></pre>
<blockquote>
<p>注意：JoinConfiguration 重命名自 v1alpha2 API 中的 NodeConfiguration，而 InitConfiguration 重命名自 v1alpha2 API 中的 MasterConfiguration。</p>
</blockquote>
<p>然后，在初始化 master 的时候指定 kubeadm.yaml 的路径：</p>
<pre><code class="lang-sh">kubeadm init --config ./kubeadm.yaml
</code></pre>
<h2 id="配置-network-plugin">配置 Network plugin</h2>
<h3 id="cni-bridge">CNI bridge</h3>
<pre><code class="lang-sh">mkdir -p /etc/cni/net.d
cat >/etc/cni/net.d/10-mynet.conf <<-EOF
{
    <span class="hljs-string">"cniVersion"</span>: <span class="hljs-string">"0.3.0"</span>,
    <span class="hljs-string">"name"</span>: <span class="hljs-string">"mynet"</span>,
    <span class="hljs-string">"type"</span>: <span class="hljs-string">"bridge"</span>,
    <span class="hljs-string">"bridge"</span>: <span class="hljs-string">"cni0"</span>,
    <span class="hljs-string">"isGateway"</span>: <span class="hljs-literal">true</span>,
    <span class="hljs-string">"ipMasq"</span>: <span class="hljs-literal">true</span>,
    <span class="hljs-string">"ipam"</span>: {
        <span class="hljs-string">"type"</span>: <span class="hljs-string">"host-local"</span>,
        <span class="hljs-string">"subnet"</span>: <span class="hljs-string">"10.244.0.0/16"</span>,
        <span class="hljs-string">"routes"</span>: [
            {<span class="hljs-string">"dst"</span>: <span class="hljs-string">"0.0.0.0/0"</span>}
        ]
    }
}
EOF
cat >/etc/cni/net.d/99-loopback.conf <<-EOF
{
    <span class="hljs-string">"cniVersion"</span>: <span class="hljs-string">"0.3.0"</span>,
    <span class="hljs-string">"type"</span>: <span class="hljs-string">"loopback"</span>
}
EOF
</code></pre>
<h3 id="flannel">flannel</h3>
<p>注意：需要 <code>kubeadm init</code> 时设置 <code>--pod-network-cidr=10.244.0.0/16</code></p>
<pre><code class="lang-sh">kubectl apply <span class="hljs-_">-f</span> https://raw.githubusercontent.com/coreos/flannel/v0.10.0/Documentation/kube-flannel.yml
</code></pre>
<h3 id="weave">weave</h3>
<pre><code class="lang-sh">sysctl net.bridge.bridge-nf-call-iptables=1
kubectl apply <span class="hljs-_">-f</span> <span class="hljs-string">"https://cloud.weave.works/k8s/net?k8s-version=<span class="hljs-variable">$(kubectl version | base64 | tr -d '\n')</span>"</span>
</code></pre>
<h3 id="calico">calico</h3>
<p>注意：需要 <code>kubeadm init</code> 时设置 <code>--pod-network-cidr=192.168.0.0/16</code></p>
<pre><code class="lang-sh">kubectl apply <span class="hljs-_">-f</span> https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/rbac-kdd.yaml
kubectl apply <span class="hljs-_">-f</span> https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calico-networking/1.7/calico.yaml
</code></pre>
<h2 id="添加-node">添加 Node</h2>
<pre><code class="lang-sh">kubeadm join --token <token> <master-ip>:<master-port> --discovery-token-ca-cert-hash sha256:<<span class="hljs-built_in">hash</span>>
</code></pre>
<p>跟 Master 一样，添加 Node 的时候也可以自定义 Kubernetes 服务的选项，格式为</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> kubeadm.k8s.io/v1alpha2
<span class="hljs-attr">caCertPath:</span> /etc/kubernetes/pki/ca.crt
<span class="hljs-attr">clusterName:</span> kubernetes
<span class="hljs-attr">discoveryFile:</span> <span class="hljs-string">""</span>
<span class="hljs-attr">discoveryTimeout:</span> <span class="hljs-number">5</span>m0s
<span class="hljs-attr">discoveryToken:</span> abcdef<span class="hljs-number">.0123456789</span>abcdef
<span class="hljs-attr">discoveryTokenAPIServers:</span>
<span class="hljs-attr">- kube-apiserver:</span><span class="hljs-number">6443</span>
<span class="hljs-attr">discoveryTokenUnsafeSkipCAVerification:</span> <span class="hljs-literal">true</span>
<span class="hljs-attr">kind:</span> NodeConfiguration
<span class="hljs-attr">nodeRegistration:</span>
<span class="hljs-attr">  criSocket:</span> /var/run/dockershim.sock
<span class="hljs-attr">  name:</span> thegopher
<span class="hljs-attr">tlsBootstrapToken:</span> abcdef<span class="hljs-number">.0123456789</span>abcdef
<span class="hljs-attr">token:</span> abcdef<span class="hljs-number">.0123456789</span>abcdef
</code></pre>
<p>在把 Node 加入集群的时候，指定 NodeConfiguration 配置文件的路径</p>
<pre><code class="lang-sh">kubeadm join --config ./nodeconfig.yml --token <span class="hljs-variable">$token</span> <span class="hljs-variable">${master_ip}</span>
</code></pre>
<h2 id="cloud-provider">Cloud Provider</h2>
<p>默认情况下，kubeadm 不包括 Cloud Provider 的配置，在 Azure 或者 AWS 等云平台上运行时，还需要配置 Cloud Provider。如</p>
<pre><code class="lang-yaml"><span class="hljs-attr">kind:</span> MasterConfiguration
<span class="hljs-attr">apiVersion:</span> kubeadm.k8s.io/v1alpha2
<span class="hljs-attr">apiServerExtraArgs:</span>
<span class="hljs-attr">  cloud-provider:</span> <span class="hljs-string">"{cloud}"</span>
<span class="hljs-attr">  cloud-config:</span> <span class="hljs-string">"{cloud-config-path}"</span>
<span class="hljs-attr">apiServerExtraVolumes:</span>
<span class="hljs-attr">- name:</span> cloud
<span class="hljs-attr">  hostPath:</span> <span class="hljs-string">"{cloud-config-path}"</span>
<span class="hljs-attr">  mountPath:</span> <span class="hljs-string">"{cloud-config-path}"</span>
<span class="hljs-attr">controllerManagerExtraArgs:</span>
<span class="hljs-attr">  cloud-provider:</span> <span class="hljs-string">"{cloud}"</span>
<span class="hljs-attr">  cloud-config:</span> <span class="hljs-string">"{cloud-config-path}"</span>
<span class="hljs-attr">controllerManagerExtraVolumes:</span>
<span class="hljs-attr">- name:</span> cloud
<span class="hljs-attr">  hostPath:</span> <span class="hljs-string">"{cloud-config-path}"</span>
<span class="hljs-attr">  mountPath:</span> <span class="hljs-string">"{cloud-config-path}"</span>
</code></pre>
<h2 id="删除安装">删除安装</h2>
<pre><code class="lang-sh"><span class="hljs-comment"># drain and delete the node first</span>
kubectl drain <node name> --delete-local-data --force --ignore-daemonsets
kubectl delete node <node name>

<span class="hljs-comment"># then reset kubeadm</span>
kubeadm reset
</code></pre>
<h2 id="动态升级">动态升级</h2>
<p>kubeadm v1.8 开始支持动态升级，升级步骤为</p>
<ul>
<li>首先上传 kubeadm 配置，如 <code>kubeadm config upload from-flags [flags]</code>（使用命令行参数）或 <code>kubeadm config upload from-file --config [config]</code>（使用配置文件）</li>
<li>在 master 上检查新版本 <code>kubeadm upgrade plan</code>， 当有新版本（如 v1.8.0）时，执行 <code>kubeadm upgrade apply v1.8.0</code> 升级控制平面</li>
<li><strong> 手动 </strong> 升级 CNI 插件（如果有新版本的话）</li>
<li>添加自动证书回滚的 RBAC 策略 <code>kubectl create clusterrolebinding kubeadm:node-autoapprove-certificate-rotation --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeclient --group=system:nodes</code></li>
<li>最后升级 kubelet</li>
</ul>
<pre><code class="lang-sh">$ kubectl drain <span class="hljs-variable">$HOST</span> --ignore-daemonsets

<span class="hljs-comment"># 升级软件包</span>
$ apt-get update
$ apt-get upgrade
<span class="hljs-comment"># CentOS 上面执行 yum 升级</span>
<span class="hljs-comment"># $ yum update</span>

$ kubectl uncordon <span class="hljs-variable">$HOST</span>
</code></pre>
<h3 id="手动升级">手动升级</h3>
<p>kubeadm v1.7 以及以前的版本不支持动态升级，但可以手动升级。</p>
<h4 id="升级-master">升级 Master</h4>
<p>假设你已经有一个使用 kubeadm 部署的 Kubernetes v1.6 集群，那么升级到 v1.7 的方法为：</p>
<ol>
<li>升级安装包 <code>apt-get upgrade && apt-get update</code></li>
<li>重启 kubelet <code>systemctl restart kubelet</code></li>
<li>删除 kube-proxy DaemonSet <code>KUBECONFIG=/etc/kubernetes/admin.conf kubectl delete daemonset kube-proxy -n kube-system</code></li>
<li>kubeadm init --skip-preflight-checks --kubernetes-version v1.7.1</li>
<li>更新 CNI 插件</li>
</ol>
<h4 id="升级-node">升级 Node</h4>
<ol>
<li>升级安装包 <code>apt-get upgrade && apt-get update</code></li>
<li>重启 kubelet <code>systemctl restart kubelet</code></li>
</ol>
<h2 id="安全选项">安全选项</h2>
<p>默认情况下，kubeadm 会开启 Node 客户端证书的自动批准，如果不需要的话可以选择关闭，关闭方法为</p>
<pre><code class="lang-sh">$ kubectl delete clusterrole kubeadm:node-autoapprove-bootstrap
</code></pre>
<p>关闭后，增加新的 Node 时，<code>kubeadm join</code> 会阻塞等待管理员手动批准，匹配方法为</p>
<pre><code class="lang-sh">$ kubectl get csr
NAME                                                   AGE       REQUESTOR                 CONDITION
node-csr-c69HXe7aYcqkS1bKmH4faEnHAWxn6i2bHZ2mD04jZyQ   18s       system:bootstrap:878f07   Pending

$ kubectl certificate approve node-csr-c69HXe7aYcqkS1bKmH4faEnHAWxn6i2bHZ2mD04jZyQ
certificatesigningrequest <span class="hljs-string">"node-csr-c69HXe7aYcqkS1bKmH4faEnHAWxn6i2bHZ2mD04jZyQ"</span> approved

$ kubectl get csr
NAME                                                   AGE       REQUESTOR                 CONDITION
node-csr-c69HXe7aYcqkS1bKmH4faEnHAWxn6i2bHZ2mD04jZyQ   1m        system:bootstrap:878f07   Approved,Issued
</code></pre>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://kubernetes.io/docs/admin/kubeadm/" target="_blank">kubeadm 参考指南</a></li>
<li><a href="https://kubernetes.io/docs/tasks/administer-cluster/kubeadm-upgrade-1-8/" target="_blank">Upgrading kubeadm clusters from 1.7 to 1.8</a></li>
<li><a href="https://kubernetes.io/docs/tasks/administer-cluster/kubeadm-upgrade-1-7/" target="_blank">Upgrading kubeadm clusters from 1.6 to 1.7</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="kops-集群部署" class="level3">kops</h1>
<p><a href="https://github.com/kubernetes/kops" target="_blank">kops</a> 是一个生产级 Kubernetes 集群部署工具，可以在 AWS、GCE、VMWare vSphere 等平台上自动部署高可用的 Kubernetes 集群。主要功能包括</p>
<ul>
<li>自动部署高可用的 kubernetes 集群</li>
<li>支持从 <a href="https://github.com/kubernetes/kops/blob/master/docs/upgrade_from_kubeup.md" target="_blank">kube-up</a> 创建的集群升级到 kops 版本</li>
<li>dry-run 和自动幂等升级等基于状态同步模型</li>
<li>支持自动生成 AWS CloudFormation 和 Terraform 配置</li>
<li>支持自定义扩展 add-ons</li>
<li>命令行自动补全</li>
</ul>
<h2 id="安装-kops-和-kubectl">安装 kops 和 kubectl</h2>
<pre><code class="lang-sh"><span class="hljs-comment"># on macOS</span>
brew install kubectl kops

<span class="hljs-comment"># on Linux</span>
wget https://github.com/kubernetes/kops/releases/download/1.7.0/kops-linux-amd64
chmod +x kops-linux-amd64
mv kops-linux-amd64 /usr/<span class="hljs-built_in">local</span>/bin/kops
</code></pre>
<h2 id="在-aws-上面部署">在 AWS 上面部署</h2>
<p>首先需要安装 AWS CLI 并配置 IAM：</p>
<pre><code class="lang-sh"><span class="hljs-comment"># install AWS CLI</span>
pip install awscli

<span class="hljs-comment"># configure iam</span>
aws iam create-group --group-name kops
aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonEC2FullAccess --group-name kops
aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonRoute53FullAccess --group-name kops
aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess --group-name kops
aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/IAMFullAccess --group-name kops
aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonVPCFullAccess --group-name kops
aws iam create-user --user-name kops
aws iam add-user-to-group --user-name kops --group-name kops
aws iam create-access-key --user-name kops

<span class="hljs-comment"># configure the aws client to use your new IAM user</span>
aws configure           <span class="hljs-comment"># Use your new access and secret key here</span>
aws iam list-users      <span class="hljs-comment"># you should see a list of all your IAM users here</span>

<span class="hljs-comment"># Because "aws configure" doesn't export these vars for kops to use, we export them now</span>
<span class="hljs-built_in">export</span> AWS_ACCESS_KEY_ID=<access key>
<span class="hljs-built_in">export</span> AWS_SECRET_ACCESS_KEY=<secret key>
</code></pre>
<p>创建 route53 域名</p>
<pre><code class="lang-sh">aws route53 create-hosted-zone --name dev.example.com --caller-reference 1
</code></pre>
<p>创建 s3 存储 bucket</p>
<pre><code class="lang-sh">aws s3api create-bucket --bucket clusters.dev.example.com --region us-east-1
aws s3api put-bucket-versioning --bucket clusters.dev.example.com  --versioning-configuration Status=Enabled
</code></pre>
<p>部署 Kubernetes 集群</p>
<pre><code class="lang-sh"><span class="hljs-built_in">export</span> KOPS_STATE_STORE=s3://clusters.dev.example.com

kops create cluster --zones=us-east-1c useast1.dev.example.com --yes
</code></pre>
<p>当然，也可以部署一个高可用的集群</p>
<pre><code class="lang-sh">kops create cluster \
    --node-count 3 \
    --zones us-west-2a,us-west-2b,us-west-2c \
    --master-zones us-west-2a,us-west-2b,us-west-2c \
    --node-size t2.medium \
    --master-size t2.medium \
    --topology private \
    --networking kopeio-vxlan \
    hacluster.example.com
</code></pre>
<p>删除集群</p>
<pre><code class="lang-sh">kops delete cluster --name <span class="hljs-variable">${NAME}</span> --yes
</code></pre>
<h2 id="在-gce-上面部署">在 GCE 上面部署</h2>
<pre><code class="lang-sh"><span class="hljs-comment"># Create cluster in GCE.</span>
<span class="hljs-comment"># This is an alpha feature.</span>
<span class="hljs-built_in">export</span> KOPS_STATE_STORE=<span class="hljs-string">"gs://mybucket-kops"</span>
<span class="hljs-built_in">export</span> ZONES=<span class="hljs-variable">${MASTER_ZONES:-"us-east1-b,us-east1-c,us-east1-d"}</span>
<span class="hljs-built_in">export</span> KOPS_FEATURE_FLAGS=AlphaAllowGCE

kops create cluster kubernetes-k8s-gce.example.com
  --zones <span class="hljs-variable">$ZONES</span> \
  --master-zones <span class="hljs-variable">$ZONES</span> \
  --node-count 3
  --project my-gce-project \
  --image <span class="hljs-string">"ubuntu-os-cloud/ubuntu-1604-xenial-v20170202"</span> \
  --yes
</code></pre>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="kubespray-集群安裝" class="level3">Kubespray</h1>
<p><a href="https://github.com/kubernetes-incubator/kubespray" target="_blank">Kubespray</a> 是 Kubernetes incubator 中的项目，目标是提供 Production Ready Kubernetes 部署方案，该项目基础是通过 Ansible Playbook 来定义系统与 Kubernetes 集群部署的任务，具有以下几个特点：</p>
<ul>
<li>可以部署在 AWS, GCE, Azure, OpenStack 以及裸机上.</li>
<li>部署 High Available Kubernetes 集群.</li>
<li>可组合性 (Composable)，可自行选择 Network Plugin (flannel, calico, canal, weave) 来部署.</li>
<li>支持多种 Linux distributions(CoreOS, Debian Jessie, Ubuntu 16.04, CentOS/RHEL7).</li>
</ul>
<p>本篇将说明如何通过 Kubespray 部署 Kubernetes 至裸机节点，安装版本如下所示：</p>
<ul>
<li>Kubernetes v1.7.3</li>
<li>Etcd v3.2.4</li>
<li>Flannel v0.8.0</li>
<li>Docker v17.04.0-ce</li>
</ul>
<h2 id="节点资讯">节点资讯</h2>
<p>本次安装测试环境的作业系统采用 <code>Ubuntu 16.04 Server</code>，其他细节内容如下：</p>
<table>
<thead>
<tr>
<th>IP Address</th>
<th>Role</th>
<th>CPU</th>
<th>Memory</th>
</tr>
</thead>
<tbody>
<tr>
<td>192.168.121.179</td>
<td>master1 + deploy</td>
<td>2</td>
<td>4G</td>
</tr>
<tr>
<td>192.168.121.106</td>
<td>node1</td>
<td>2</td>
<td>4G</td>
</tr>
<tr>
<td>192.168.121.197</td>
<td>node2</td>
<td>2</td>
<td>4G</td>
</tr>
<tr>
<td>192.168.121.123</td>
<td>node3</td>
<td>2</td>
<td>4G</td>
</tr>
</tbody>
</table>
<blockquote>
<p>这边 master 为主要控制节点，node 为工作节点。</p>
</blockquote>
<h2 id="预先准备资讯">预先准备资讯</h2>
<ul>
<li>所有节点的网路之间可以互相通信。</li>
<li><code>部署节点 (这边为 master1)</code> 对其他节点不需要 SSH 密码即可登入。</li>
<li>所有节点都拥有 Sudoer 权限，并且不需要输入密码。</li>
<li>所有节点需要安装 <code>Python</code>。</li>
<li><p>所有节点需要设定 <code>/etc/hosts</code> 解析到所有主机。</p>
</li>
<li><p>修改所有节点的 <code>/etc/resolv.conf</code></p>
</li>
</ul>
<pre><code class="lang-sh">$ <span class="hljs-built_in">echo</span> <span class="hljs-string">"nameserver 8.8.8.8"</span> | sudo tee /etc/resolv.conf
</code></pre>
<ul>
<li><code>部署节点 (这边为 master1)</code> 安装 Ansible >= 2.3.0。</li>
</ul>
<p>Ubuntu 16.04 安装 Ansible:</p>
<pre><code class="lang-sh">$ sudo sed -i <span class="hljs-string">'s/us.archive.ubuntu.com/tw.archive.ubuntu.com/g'</span> /etc/apt/sources.list
$ sudo apt-get install -y software-properties-common
$ sudo apt-add-repository -y ppa:ansible/ansible
$ sudo apt-get update && sudo apt-get install -y ansible git cowsay python-pip python-netaddr libssl-dev
</code></pre>
<h2 id="安装-kubespray-与准备部署资讯">安装 Kubespray 与准备部署资讯</h2>
<p>首先通过 pypi 安装 kubespray-cli，虽然官方说已经改成 Go 语言版本的工具，但是根本没在更新，所以目前暂时用 pypi 版本：</p>
<pre><code class="lang-sh">$ sudo pip install -U kubespray
</code></pre>
<p>安裝完成後，新增配置檔 <code>~/.kubespray.yml</code>，並加入以下內容：</p>
<pre><code class="lang-sh">$ cat <<EOF> ~/.kubespray.yml
kubespray_git_repo: <span class="hljs-string">"https://github.com/kubernetes-incubator/kubespray.git"</span>
<span class="hljs-comment"># Logging options</span>
loglevel: <span class="hljs-string">"info"</span>
EOF
</code></pre>
<p>接着用 kubespray cli 来产生 inventory 文件：</p>
<pre><code class="lang-sh">$ kubespray prepare --masters master1 --etcds master1 --nodes node1 node2 node3
</code></pre>
<p>在 inventory.cfg，添加部分內容：</p>
<pre><code>$ vim ~/.kubespray/inventory/inventory.cfg

[all]
master1  ansible_host=192.168.121.179   ansible_user=root ip=192.168.121.179
node1    ansible_host=192.168.121.106 ansible_user=root ip=192.168.121.106
node2    ansible_host=192.168.121.197 ansible_user=root ip=192.168.121.197
node3    ansible_host=192.168.121.123 ansible_user=root ip=192.168.121.123

[kube-master]
master1

[kube-node]
node1
node2
node3

[etcd]
master1

[k8s-cluster:children]
kube-node
kube-master
</code></pre><blockquote>
<p>也可以自己新建 <code>inventory</code> 来描述部署节点。</p>
</blockquote>
<p>完成后通过以下指令进行部署 Kubernetes 集群：</p>
<pre><code class="lang-sh">$ time kubespray deploy --verbose -u root -k .ssh/id_rsa -n flannel
Run kubernetes cluster deployment with the above <span class="hljs-built_in">command</span> ? [Y/n]y
...
master1                    : ok=368  changed=89   unreachable=0    failed=0
node1                      : ok=305  changed=73   unreachable=0    failed=0
node2                      : ok=276  changed=62   unreachable=0    failed=0
node3                      : ok=276  changed=62   unreachable=0    failed=0

Kubernetes deployed successfuly
</code></pre>
<blockquote>
<p>其中 <code>-n</code> 为部署的网络插件类型，目前支持 calico、flannel、weave 与 canal。</p>
</blockquote>
<h2 id="验证集群">验证集群</h2>
<p>当 Ansible 运行完成后，若没发生错误就可以开始进行操作 Kubernetes，如取得版本资讯：</p>
<pre><code class="lang-sh">$ kubectl version
Client Version: version.Info{Major:<span class="hljs-string">"1"</span>, Minor:<span class="hljs-string">"6"</span>, GitVersion:<span class="hljs-string">"v1.7.3+coreos.0"</span>, GitCommit:<span class="hljs-string">"9212f77ed8c169a0afa02e58dce87913c6387b3e"</span>, GitTreeState:<span class="hljs-string">"clean"</span>, BuildDate:<span class="hljs-string">"2017-04-04T00:32:53Z"</span>, GoVersion:<span class="hljs-string">"go1.8.3"</span>, Compiler:<span class="hljs-string">"gc"</span>, Platform:<span class="hljs-string">"linux/amd64"</span>}
Server Version: version.Info{Major:<span class="hljs-string">"1"</span>, Minor:<span class="hljs-string">"6"</span>, GitVersion:<span class="hljs-string">"v1.7.3+coreos.0"</span>, GitCommit:<span class="hljs-string">"9212f77ed8c169a0afa02e58dce87913c6387b3e"</span>, GitTreeState:<span class="hljs-string">"clean"</span>, BuildDate:<span class="hljs-string">"2017-04-04T00:32:53Z"</span>, GoVersion:<span class="hljs-string">"go1.8.3"</span>, Compiler:<span class="hljs-string">"gc"</span>, Platform:<span class="hljs-string">"linux/amd64"</span>}
</code></pre>
<p>取得当前集群节点状态：</p>
<pre><code class="lang-sh">$ kubectl get node
NAME      STATUS                     AGE       VERSION
master1   Ready,SchedulingDisabled   11m       v1.7.3+coreos.0
node1     Ready                      11m       v1.7.3+coreos.0
node2     Ready                      11m       v1.7.3+coreos.0
node3     Ready                      11m       v1.7.3+coreos.
</code></pre>
<p>查看当前集群 Pod 状态：</p>
<pre><code class="lang-sh">$ kubectl get po -n kube-system
NAME                                  READY     STATUS    RESTARTS   AGE
dnsmasq-975202658-6jj3n               1/1       Running   0          14m
dnsmasq-975202658-h4rn9               1/1       Running   0          14m
dnsmasq-autoscaler-2349860636-kfpx0   1/1       Running   0          14m
flannel-master1                       1/1       Running   1          14m
flannel-node1                         1/1       Running   1          14m
flannel-node2                         1/1       Running   1          14m
flannel-node3                         1/1       Running   1          14m
kube-apiserver-master1                1/1       Running   0          15m
kube-controller-manager-master1       1/1       Running   0          15m
kube-proxy-master1                    1/1       Running   1          14m
kube-proxy-node1                      1/1       Running   1          14m
kube-proxy-node2                      1/1       Running   1          14m
kube-proxy-node3                      1/1       Running   1          14m
kube-scheduler-master1                1/1       Running   0          15m
kubedns-1519522227-thmrh              3/3       Running   0          14m
kubedns-autoscaler-2999057513-tx14j   1/1       Running   0          14m
nginx-proxy-node1                     1/1       Running   1          14m
nginx-proxy-node2                     1/1       Running   1          14m
nginx-proxy-node3                     1/1       Running   1          14m
</code></pre>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="kubernetes-on-azure" class="level3">Azure</h1>
<p>Azure 容器服务 (AKS) 是 Microsoft Azure 最近发布的一个托管的 Kubernetes 服务（预览版），它独立于现有的 Azure Container Service （ACS）。借助 AKS 用户无需具备容器业务流程的专业知识就可以快速、轻松的部署和管理容器化的应用程序。AKS 支持自动升级和自动故障修复，按需自动扩展或缩放资源池，消除了用户管理和维护 Kubernetes 集群的负担。并且集群管理本身是免费的，Azure 只收取容器底层的虚拟机的费用。</p>
<p>ACS 是 Microsoft Azure 在 2015 年推出的容器服务，支持 Kubernetes、DCOS 以及 Dockers Swarm 等多种容器编排工具。并且 ACS 的核心功能是开源的，用户可以通过 <a href="https://github.com/Azure/acs-engine" target="_blank">https://github.com/Azure/acs-engine</a> 来查看和下载使用。</p>
<h2 id="aks">AKS</h2>
<h3 id="基本使用">基本使用</h3>
<p>以下文档假设用户已经安装好了 Azure CLI ，如未安装可以参考 <a href="https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest" target="_blank">这里</a> 操作。</p>
<p>在创建 AKS 集群之前，首先需要开启容器服务</p>
<pre><code class="lang-sh"><span class="hljs-comment"># Enable AKS</span>
az provider register -n Microsoft.ContainerService
</code></pre>
<p>然后创建一个资源组（Resource Group）用来管理所有相关资源</p>
<pre><code class="lang-sh"><span class="hljs-comment"># Create Resource Group</span>
az group create --name group1 --location centralus
</code></pre>
<p>接下来就可以创建 AKS 集群了</p>
<pre><code class="lang-sh"><span class="hljs-comment"># Create aks</span>
az aks create --resource-group group1 --name myK8sCluster --node-count 3 --generate-ssh-keys
</code></pre>
<p>稍等一会，集群创建好后安装并配置 kubectl</p>
<pre><code class="lang-sh"><span class="hljs-comment"># Install kubectl</span>
az aks install-cli

<span class="hljs-comment"># Configure kubectl</span>
az aks get-credentials --resource-group=group1 --name=myK8sCluster
</code></pre>
<blockquote>
<p>注意使用 azure-cli 2.0.24 版本时，<code>az aks get-credentials</code> 命令可能会失败，解决方法是升级到更新版本，或回退到 2.0.23 版本。</p>
</blockquote>
<h3 id="访问-dashboard">访问 Dashboard</h3>
<pre><code class="lang-sh"><span class="hljs-comment"># Create dashboard</span>
az aks browse --resource-group group1 --name myK8SCluster
</code></pre>
<h3 id="手动扩展或收缩集群">手动扩展或收缩集群</h3>
<pre><code class="lang-sh">az aks scale --resource-group=group1 --name=myK8SCluster --agent-count 5
</code></pre>
<h3 id="升级集群">升级集群</h3>
<pre><code class="lang-sh"><span class="hljs-comment"># 查询当前集群的版本以及可升级的版本</span>
az aks get-versions --name myK8sCluster --resource-group group1 --output table

<span class="hljs-comment"># 升级到 1.11.3 版本</span>
az aks upgrade --name myK8sCluster --resource-group group1 --kubernetes-version 1.11.3
</code></pre>
<p>下图动态展示了一个部署 v1.7.7 版本集群并升级到 v1.8.1 的过程：</p>
<p><img src="https://feisky.xyz/images/aks-examples.gif" alt=""/></p>
<h3 id="使用-helm">使用 Helm</h3>
<p>当然也可以使用其他 Kubernetes 社区提供的工具和服务，比如使用 Helm 部署 Nginx Ingress 控制器</p>
<pre><code class="lang-sh">helm init --client-only
helm install stable/nginx-ingress
</code></pre>
<h3 id="删除集群">删除集群</h3>
<p>当集群不再需要时，可以删除集群</p>
<pre><code class="lang-sh">az group delete --name group1 --yes --no-wait
</code></pre>
<h2 id="acs-engine">acs-engine</h2>
<p>虽然未来 AKS 是 Azure 容器服务的下一代主打产品，但用户可能还是希望可以自己管理容器集群以保证足够的灵活性（比如自定义 master 服务等）。这时用户可以使用开源的 <a href="https://github.com/Azure/acs-engine" target="_blank">acs-engine</a> 来创建和管理自己的集群。acs-engine 其实就是 ACS 的核心部分，提供了一个部署和管理 Kubernetes、Swarm 和 DC/OS 集群的命令行工具。它通过将容器集群描述文件转化为一组 ARM（Azure Resource Manager）模板来建立容器集群。</p>
<p>在 acs-engine 中，每个集群都通过一个 json 文件来描述，比如一个 Kubernetes 集群可以描述为</p>
<pre><code class="lang-sh">{
  <span class="hljs-string">"apiVersion"</span>: <span class="hljs-string">"vlabs"</span>,
  <span class="hljs-string">"properties"</span>: {
    <span class="hljs-string">"orchestratorProfile"</span>: {
      <span class="hljs-string">"orchestratorType"</span>: <span class="hljs-string">"Kubernetes"</span>,
      <span class="hljs-string">"orchestratorRelease"</span>: <span class="hljs-string">"1.12"</span>,
      <span class="hljs-string">"kubernetesConfig"</span>: {
        <span class="hljs-string">"networkPolicy"</span>: <span class="hljs-string">""</span>,
        <span class="hljs-string">"enableRbac"</span>: <span class="hljs-literal">true</span>
      }
    },
    <span class="hljs-string">"masterProfile"</span>: {
      <span class="hljs-string">"count"</span>: 1,
      <span class="hljs-string">"dnsPrefix"</span>: <span class="hljs-string">""</span>,
      <span class="hljs-string">"vmSize"</span>: <span class="hljs-string">"Standard_D2_v2"</span>
    },
    <span class="hljs-string">"agentPoolProfiles"</span>: [
      {
        <span class="hljs-string">"name"</span>: <span class="hljs-string">"agentpool1"</span>,
        <span class="hljs-string">"count"</span>: 3,
        <span class="hljs-string">"vmSize"</span>: <span class="hljs-string">"Standard_D2_v2"</span>,
        <span class="hljs-string">"availabilityProfile"</span>: <span class="hljs-string">"AvailabilitySet"</span>
      }
    ],
    <span class="hljs-string">"linuxProfile"</span>: {
      <span class="hljs-string">"adminUsername"</span>: <span class="hljs-string">"azureuser"</span>,
      <span class="hljs-string">"ssh"</span>: {
        <span class="hljs-string">"publicKeys"</span>: [
          {
            <span class="hljs-string">"keyData"</span>: <span class="hljs-string">""</span>
          }
        ]
      }
    },
    <span class="hljs-string">"servicePrincipalProfile"</span>: {
      <span class="hljs-string">"clientId"</span>: <span class="hljs-string">""</span>,
      <span class="hljs-string">"secret"</span>: <span class="hljs-string">""</span>
    }
  }
}
</code></pre>
<p>orchestratorType 指定了部署集群的类型，目前支持三种</p>
<ul>
<li>Kubernetes</li>
<li>Swarm</li>
<li>DCOS</li>
</ul>
<p>而创建集群的步骤也很简单</p>
<pre><code class="lang-sh"><span class="hljs-comment"># create a new resource group.</span>
az group create --name myResourceGroup  --location <span class="hljs-string">"centralus"</span>

<span class="hljs-comment"># start deploy the kubernetes</span>
acs-engine deploy --resource-group myResourceGroup --subscription-id <subscription-id> --auto-suffix --api-model kubernetes.json --location centralus --dns-prefix <dns-prefix>

<span class="hljs-comment"># setup kubectl</span>
<span class="hljs-built_in">export</span> KUBECONFIG=<span class="hljs-string">"<span class="hljs-variable">$(pwd)</span>/_output/<name-with-suffix>/kubeconfig/kubeconfig.centralus.json"</span>
kubectl get node
</code></pre>
<h3 id="开启-rbac">开启 RBAC</h3>
<p>RBAC 默认是不可以开启的，可以通过设置 enableRbac 开启</p>
<pre><code class="lang-json">     <span class="hljs-string">"kubernetesConfig"</span>: {
        <span class="hljs-string">"enableRbac"</span>: <span class="hljs-literal">true</span>
      }
</code></pre>
<h3 id="自定义-kubernetes-版本">自定义 Kubernetes 版本</h3>
<p>acs-engine 基于 hyperkube 来部署 Kubernetes 服务，所以只需要使用自定义的 hyperkube 镜像即可。</p>
<pre><code class="lang-json">{
    <span class="hljs-string">"kubernetesConfig"</span>: {
        <span class="hljs-string">"customHyperkubeImage"</span>: <span class="hljs-string">"docker.io/feisky/hyperkube-amd64:v1.12.1"</span>
    }
}
</code></pre>
<p>hyperkube 镜像可以从 Kubernetes 源码编译，编译步骤为</p>
<pre><code class="lang-sh"><span class="hljs-comment"># Build Kubernetes</span>
bash build/run.sh make KUBE_FASTBUILD=<span class="hljs-literal">true</span> ARCH=amd64

<span class="hljs-comment"># Build docker image for hyperkube</span>
<span class="hljs-built_in">cd</span> cluster/images/hyperkube
make VERSION=v1.12.x-dev
<span class="hljs-built_in">cd</span> ../../..

<span class="hljs-comment"># push docker image</span>
docker tag gcr.io/google-containers/hyperkube-amd64:v1.12.x-dev feisky/hyperkube-amd64:v1.12.x-dev
docker push feisky/hyperkube-amd64:v1.12.x-dev
</code></pre>
<h3 id="添加-windows-节点">添加 Windows 节点</h3>
<p>可以通过设置 osType 来添加 Windows 节点（完整示例见 <a href="https://github.com/Azure/acs-engine/blob/master/examples/windows/kubernetes.json" target="_blank">这里</a>）</p>
<pre><code class="lang-json">    <span class="hljs-string">"agentPoolProfiles"</span>: [
      {
        <span class="hljs-string">"name"</span>: <span class="hljs-string">"windowspool2"</span>,
        <span class="hljs-string">"count"</span>: <span class="hljs-number">2</span>,
        <span class="hljs-string">"vmSize"</span>: <span class="hljs-string">"Standard_D2_v2"</span>,
        <span class="hljs-string">"availabilityProfile"</span>: <span class="hljs-string">"AvailabilitySet"</span>,
        <span class="hljs-string">"osType"</span>: <span class="hljs-string">"Windows"</span>
      }
    ],
    <span class="hljs-string">"windowsProfile"</span>: {
      <span class="hljs-string">"adminUsername"</span>: <span class="hljs-string">"azureuser"</span>,
      <span class="hljs-string">"adminPassword"</span>: <span class="hljs-string">"replacepassword1234$"</span>
    },
</code></pre>
<h3 id="使用-gpu">使用 GPU</h3>
<p>设置 vmSize 为 <code>Standard_NC*</code> 或  <code>Standard_NV*</code> 会自动配置 GPU，并自动安装所需要的 NVDIA 驱动。</p>
<h3 id="自定义网络插件">自定义网络插件</h3>
<p>acs-engine 默认使用 kubenet 网络插件，并通过用户自定义的路由以及 IP-forwarding 转发 Pod 网络。此时，Pod 网络与 Node 网络在不同的子网中，Pod 不受 VNET 管理。</p>
<p>用户还可以使用 <a href="https://github.com/Azure/azure-container-networking" target="_blank">Azure CNI plugin</a> 插件将 Pod 连接到 Azure VNET 中</p>
<pre><code class="lang-json"><span class="hljs-string">"properties"</span>: {
    <span class="hljs-string">"orchestratorProfile"</span>: {
      <span class="hljs-string">"orchestratorType"</span>: <span class="hljs-string">"Kubernetes"</span>,
      <span class="hljs-string">"kubernetesConfig"</span>: {
        <span class="hljs-string">"networkPolicy"</span>: <span class="hljs-string">"azure"</span>
      }
    }
}
</code></pre>
<p>也可以使用 calico 网络插件</p>
<pre><code class="lang-json"><span class="hljs-string">"properties"</span>: {
    <span class="hljs-string">"orchestratorProfile"</span>: {
      <span class="hljs-string">"orchestratorType"</span>: <span class="hljs-string">"Kubernetes"</span>,
      <span class="hljs-string">"kubernetesConfig"</span>: {
        <span class="hljs-string">"networkPolicy"</span>: <span class="hljs-string">"calico"</span>
      }
    }
}
</code></pre>
<h2 id="azure-container-registry">Azure Container Registry</h2>
<p>在 AKS 预览版发布的同时，Azure 还同时发布了 Azure Container Registry（ACR）服务，用于托管用户的私有镜像。</p>
<pre><code class="lang-sh"><span class="hljs-comment"># Create ACR</span>
az acr create --resource-group myResourceGroup --name <acrName> --sku Basic --admin-enabled <span class="hljs-literal">true</span>

<span class="hljs-comment"># Login</span>
az acr login --name <acrName>

<span class="hljs-comment"># Tag the image.</span>
az acr list --resource-group myResourceGroup --query <span class="hljs-string">"[].{acrLoginServer:loginServer}"</span> --output table
docker tag azure-vote-front <acrLoginServer>/azure-vote-front:redis-v1

<span class="hljs-comment"># push image</span>
docker push <acrLoginServer>/azure-vote-front:redis-v1

<span class="hljs-comment"># List images.</span>
az acr repository list --name <acrName> --output table
</code></pre>
<h2 id="virtual-kubelet">Virtual Kubelet</h2>
<p>Azure 容器实例（ACI）提供了在 Azure 中运行容器的最简捷方式，它不需要用户配置任何虚拟机或其它高级服务。ACI 适用于快速突发式增长和资源调整的业务，但其本身的功能相对比较简单。 <a href="https://github.com/virtual-kubelet/virtual-kubelet" target="_blank">Virtual Kubelet</a> 可以将 ACI 作为 Kubernetes 集群的一个无限 Node 使用，这样就无需考虑 Node 数量的问题，ACI 会根据运行容器自动管理集群资源。</p>
<p><img src="images/virtual-kubelet.png" alt=""/></p>
<p>可以使用 Helm 来部署 Virtual Kubelet：</p>
<pre><code class="lang-sh">RELEASE_NAME=virtual-kubelet
CHART_URL=https://github.com/virtual-kubelet/virtual-kubelet/raw/master/charts/virtual-kubelet-0.4.0.tgz

helm install <span class="hljs-string">"<span class="hljs-variable">$CHART_URL</span>"</span> --name <span class="hljs-string">"<span class="hljs-variable">$RELEASE_NAME</span>"</span> --namespace kube-system --set env.azureClientId=<YOUR-AZURECLIENTID-HERE>,env.azureClientKey=<YOUR-AZURECLIENTKEY-HERE>,env.azureTenantId=<YOUR-AZURETENANTID-HERE>,env.azureSubscriptionId=<YOUR-AZURESUBSCRIPTIONID-HERE>,env.aciResourceGroup=<YOUR-ACIRESOURCEGROUP-HERE>,env.nodeName=aci, env.nodeOsType=<Linux|Windows>,env.nodeTaint=azure.com/aci
</code></pre>
<p>在开启 RBAC 的集群中，还需要给 virtual-kubelet 开启对应的权限。最简单的方法是给 service account <code>kube-system:default</code> 设置 admin 权限（不推荐生产环境这么设置，应该设置具体的权限），比如</p>
<pre><code class="lang-sh">kubectl create clusterrolebinding virtual-kubelet-cluster-admin-binding --clusterrole=cluster-admin --serviceaccount=kube-system:default
</code></pre>
<p>部署成功后，会发现集群中会出现一个新的名为 <code>aci</code> 的 Node：</p>
<pre><code class="lang-sh">$ kubectl get nodes aci
NAME      STATUS    ROLES     AGE       VERSION
aci       Ready     agent     34s       v1.8.3
</code></pre>
<p>此时，就可以通过 <strong> 指定 nodeName 或者容忍 taint <code>azure.com/aci=NoSchedule</code> 调度 </strong> 到 ACI 上面。比如</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> nginx
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">  - image:</span> nginx
<span class="hljs-attr">    imagePullPolicy:</span> Always
<span class="hljs-attr">    name:</span> nginx
<span class="hljs-attr">    resources:</span>
<span class="hljs-attr">      requests:</span>
<span class="hljs-attr">        memory:</span> <span class="hljs-number">100</span>M
<span class="hljs-attr">        cpu:</span> <span class="hljs-number">1</span>
<span class="hljs-attr">    ports:</span>
<span class="hljs-attr">    - containerPort:</span> <span class="hljs-number">80</span>
<span class="hljs-attr">      name:</span> http
<span class="hljs-attr">      protocol:</span> TCP
<span class="hljs-attr">    - containerPort:</span> <span class="hljs-number">443</span>
<span class="hljs-attr">      name:</span> https
<span class="hljs-attr">  dnsPolicy:</span> ClusterFirst
<span class="hljs-attr">  nodeName:</span> aci
</code></pre>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://www.reddit.com/r/AZURE/comments/7d7diz/ama_aks_managed_kubernetes_on_azure/" target="_blank">AKS – Managed Kubernetes on Azure</a></li>
<li><a href="https://docs.microsoft.com/en-us/azure/aks/" target="_blank">Azure Container Service (AKS)</a></li>
<li><a href="https://github.com/Azure/acs-engine" target="_blank">Azure/acs-engine Github</a></li>
<li><a href="https://github.com/Azure/acs-engine/tree/master/examples" target="_blank">acs-engine/examples</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="部署-windows-节点" class="level3">Windows</h1>
<p>Kubernetes 从 v1.5 开始支持 alpha 版的 Windows 节点，并从 v1.9 开始升级为 beta 版。Windows 容器的主要特性包括</p>
<ul>
<li>Windows 容器支持 Pod（isolation=process）</li>
<li>基于 Virtual Filtering Platform (VFP) Hyper-v Switch Extension 的内核负载均衡</li>
<li>基于 Container Runtime Interface (CRI) 管理 Windows 容器</li>
<li>支持 kubeadm 命令将 Windows 节点加入到已有集群中</li>
<li>推荐使用 Windows Server Version 1803+ 和 Docker Version 17.06+</li>
</ul>
<blockquote>
<p>注意：</p>
<ol>
<li>控制平面的服务依然运行在 Linux 服务器中，而 Windows 节点上只运行 Kubelet、Kube-proxy、Docker 以及网络插件等服务。</li>
<li>推荐使用 Windows Server 1803（修复了 Windows 容器软链接的问题，从而 ServiceAccount 和 ConfigMap 可以正常使用）</li>
</ol>
</blockquote>
<h2 id="下载">下载</h2>
<p>可以从 <a href="%3Chttps:/github.com/kubernetes/kubernetes/releases"><https://github.com/kubernetes/kubernetes/releases</a> 下载已发布的用于 Windows 服务器的二进制文件，如</p>
<pre><code class="lang-sh">wget https://dl.k8s.io/v1.11.2/kubernetes-node-windows-amd64.tar.gz
</code></pre>
<p>或者从 Kubernetes 源码编译</p>
<pre><code class="lang-sh">go get -u k8s.io/kubernetes
<span class="hljs-built_in">cd</span> <span class="hljs-variable">$GOPATH</span>/src/k8s.io/kubernetes

<span class="hljs-comment"># Build the kubelet</span>
KUBE_BUILD_PLATFORMS=windows/amd64 make WHAT=cmd/kubelet

<span class="hljs-comment"># Build the kube-proxy</span>
KUBE_BUILD_PLATFORMS=windows/amd64 make WHAT=cmd/kube-proxy

<span class="hljs-comment"># You will find the output binaries under the folder _output/local/bin/windows/</span>
</code></pre>
<h2 id="网络插件">网络插件</h2>
<p>Windows Server 中支持以下几种网络插件（注意 Windows 节点上的网络插件要与 Linux 节点相同）</p>
<ol>
<li><a href="https://github.com/Microsoft/SDN/blob/master/Kubernetes/windows/cni/wincni.exe" target="_blank">wincni</a> 等 L3 路由网络插件，路由配置在 TOR 交换机、路由器或者云服务中</li>
<li><a href="https://docs.microsoft.com/en-us/virtualization/windowscontainers/kubernetes/configuring-host-gateway-mode" target="_blank">Host Gateway</a> 网络插件，跟上面类似但将 IP 路由配置到每台主机上面</li>
<li><a href="https://github.com/Azure/azure-container-networking/blob/master/docs/cni.md" target="_blank">Azure VNET CNI Plugin</a></li>
<li><a href="https://github.com/openvswitch/ovn-kubernetes/" target="_blank">Open vSwitch (OVS) & Open Virtual Network (OVN) with Overlay</a></li>
<li>Flannel v0.10.0+</li>
<li>Calico v3.0.1+</li>
<li><a href="https://github.com/containernetworking/plugins/tree/master/plugins/main/windows/win-bridge" target="_blank">win-bridge</a></li>
<li><a href="https://github.com/containernetworking/plugins/tree/master/plugins/main/windows/win-overlay" target="_blank">win-overlay</a></li>
</ol>
<h3 id="l3-路由拓扑">L3 路由拓扑</h3>
<p><img src="images/upstreamrouting.png" alt=""/></p>
<p>wincni 网络插件配置示例</p>
<pre><code class="lang-json">{
  <span class="hljs-string">"cniVersion"</span>: <span class="hljs-string">"0.2.0"</span>,
  <span class="hljs-string">"name"</span>: <span class="hljs-string">"l2bridge"</span>,
  <span class="hljs-string">"type"</span>: <span class="hljs-string">"wincni.exe"</span>,
  <span class="hljs-string">"master"</span>: <span class="hljs-string">"Ethernet"</span>,
  <span class="hljs-string">"ipam"</span>: {
    <span class="hljs-string">"environment"</span>: <span class="hljs-string">"azure"</span>,
    <span class="hljs-string">"subnet"</span>: <span class="hljs-string">"10.10.187.64/26"</span>,
    <span class="hljs-string">"routes"</span>: [{
      <span class="hljs-string">"GW"</span>: <span class="hljs-string">"10.10.187.66"</span>
    }]
  },
  <span class="hljs-string">"dns"</span>: {
    <span class="hljs-string">"Nameservers"</span>: [
      <span class="hljs-string">"11.0.0.10"</span>
    ]
  },
  <span class="hljs-string">"AdditionalArgs"</span>: [{
      <span class="hljs-string">"Name"</span>: <span class="hljs-string">"EndpointPolicy"</span>,
      <span class="hljs-string">"Value"</span>: {
        <span class="hljs-string">"Type"</span>: <span class="hljs-string">"OutBoundNAT"</span>,
        <span class="hljs-string">"ExceptionList"</span>: [
          <span class="hljs-string">"11.0.0.0/8"</span>,
          <span class="hljs-string">"10.10.0.0/16"</span>,
          <span class="hljs-string">"10.127.132.128/25"</span>
        ]
      }
    },
    {
      <span class="hljs-string">"Name"</span>: <span class="hljs-string">"EndpointPolicy"</span>,
      <span class="hljs-string">"Value"</span>: {
        <span class="hljs-string">"Type"</span>: <span class="hljs-string">"ROUTE"</span>,
        <span class="hljs-string">"DestinationPrefix"</span>: <span class="hljs-string">"11.0.0.0/8"</span>,
        <span class="hljs-string">"NeedEncap"</span>: <span class="hljs-literal">true</span>
      }
    },
    {
      <span class="hljs-string">"Name"</span>: <span class="hljs-string">"EndpointPolicy"</span>,
      <span class="hljs-string">"Value"</span>: {
        <span class="hljs-string">"Type"</span>: <span class="hljs-string">"ROUTE"</span>,
        <span class="hljs-string">"DestinationPrefix"</span>: <span class="hljs-string">"10.127.132.213/32"</span>,
        <span class="hljs-string">"NeedEncap"</span>: <span class="hljs-literal">true</span>
      }
    }
  ]
}
</code></pre>
<h3 id="ovs-网络拓扑">OVS 网络拓扑</h3>
<p><img src="images/ovn_kubernetes.png" alt=""/></p>
<h2 id="部署">部署</h2>
<h3 id="kubeadm">kubeadm</h3>
<p>如果 Master 是通过 kubeadm 来部署的，那 Windows 节点也可以使用 kubeadm 来部署：</p>
<pre><code class="lang-sh">kubeadm.exe join --token <token> <master-ip>:<master-port> --discovery-token-ca-cert-hash sha256:<<span class="hljs-built_in">hash</span>>
</code></pre>
<h3 id="azure">Azure</h3>
<p>在 Azure 上面推荐使用 <a href="azure.html#Windows">acs-engine</a> 自动部署 Master 和 Windows 节点。</p>
<p>首先创建一个包含 Windows 的 Kubernetes 集群配置文件 <code>windows.json</code></p>
<pre><code class="lang-json">{
    <span class="hljs-string">"apiVersion"</span>: <span class="hljs-string">"vlabs"</span>,
    <span class="hljs-string">"properties"</span>: {
        <span class="hljs-string">"orchestratorProfile"</span>: {
            <span class="hljs-string">"orchestratorType"</span>: <span class="hljs-string">"Kubernetes"</span>,
            <span class="hljs-string">"orchestratorVersion"</span>: <span class="hljs-string">"1.11.1"</span>,
            <span class="hljs-string">"kubernetesConfig"</span>: {
                <span class="hljs-string">"networkPolicy"</span>: <span class="hljs-string">"none"</span>,
                <span class="hljs-string">"enableAggregatedAPIs"</span>: <span class="hljs-literal">true</span>,
                <span class="hljs-string">"enableRbac"</span>: <span class="hljs-literal">true</span>
            }
        },
        <span class="hljs-string">"masterProfile"</span>: {
            <span class="hljs-string">"count"</span>: <span class="hljs-number">3</span>,
            <span class="hljs-string">"dnsPrefix"</span>: <span class="hljs-string">"kubernetes-windows"</span>,
            <span class="hljs-string">"vmSize"</span>: <span class="hljs-string">"Standard_D2_v3"</span>
        },
        <span class="hljs-string">"agentPoolProfiles"</span>: [
            {
                <span class="hljs-string">"name"</span>: <span class="hljs-string">"windowspool1"</span>,
                <span class="hljs-string">"count"</span>: <span class="hljs-number">3</span>,
                <span class="hljs-string">"vmSize"</span>: <span class="hljs-string">"Standard_D2_v3"</span>,
                <span class="hljs-string">"availabilityProfile"</span>: <span class="hljs-string">"AvailabilitySet"</span>,
                <span class="hljs-string">"osType"</span>: <span class="hljs-string">"Windows"</span>
            }
        ],
        <span class="hljs-string">"windowsProfile"</span>: {
            <span class="hljs-string">"adminUsername"</span>: <span class="hljs-string">"<your-username>"</span>,
            <span class="hljs-string">"adminPassword"</span>: <span class="hljs-string">"<your-password>"</span>
        },
        <span class="hljs-string">"linuxProfile"</span>: {
            <span class="hljs-string">"adminUsername"</span>: <span class="hljs-string">"azure"</span>,
            <span class="hljs-string">"ssh"</span>: {
                <span class="hljs-string">"publicKeys"</span>: [
                    {
                        <span class="hljs-string">"keyData"</span>: <span class="hljs-string">"<your-ssh-public-key>"</span>
                    }
                ]
            }
        },
        <span class="hljs-string">"servicePrincipalProfile"</span>: {
            <span class="hljs-string">"clientId"</span>: <span class="hljs-string">""</span>,
            <span class="hljs-string">"secret"</span>: <span class="hljs-string">""</span>
        }
    }
}
</code></pre>
<p>然后使用 acs-engine 部署：</p>
<pre><code class="lang-sh"><span class="hljs-comment"># create a new resource group.</span>
az group create --name myResourceGroup  --location <span class="hljs-string">"centralus"</span>

<span class="hljs-comment"># start deploy the kubernetes</span>
acs-engine deploy --resource-group myResourceGroup --subscription-id <subscription-id> --auto-suffix --api-model windows.json --location centralus --dns-prefix <dns-prefix>

<span class="hljs-comment"># setup kubectl</span>
<span class="hljs-built_in">export</span> KUBECONFIG=<span class="hljs-string">"<span class="hljs-variable">$(pwd)</span>/_output/<name-with-suffix>/kubeconfig/kubeconfig.centralus.json"</span>
kubectl get node
</code></pre>
<h3 id="手动部署">手动部署</h3>
<p>(1) 在 Windows Server 中 <a href="https://docs.microsoft.com/en-us/virtualization/windowscontainers/quick-start/quick-start-windows-server" target="_blank">安装 Docker</a></p>
<pre><code class="lang-powershell">Install-Module -Name DockerMsftProvider -Repository PSGallery -Force
Install-Package -Name Docker -ProviderName DockerMsftProvider
Restart-Computer -Force
</code></pre>
<p>(2) 根据前面的下载部分下载 kubelet.exe 和 kube-proxy.exe</p>
<p>(3) 从 Master 节点上面拷贝 Node spec file (kube config)</p>
<p>(4) 配置 CNI 网络插件和基础镜像</p>
<pre><code class="lang-powershell">[Net.ServicePointManager]::SecurityProtocol = [Net.SecurityProtocolType]::Tls12
wget https://github.com/Microsoft/SDN/archive/master.zip -o master.zip
Expand-Archive master.zip -DestinationPath master
mkdir C:/k/
mv master/SDN-master/Kubernetes/windows/* C:/k/
rm -recurse -force master,master.zip
</code></pre>
<pre><code class="lang-powershell">docker pull microsoft/windowsservercore:<span class="hljs-number">1709</span>
docker tag microsoft/windowsservercore:<span class="hljs-number">1709</span> microsoft/windowsservercore:latest
cd C:/k/
docker build -t kubeletwin/pause .
</code></pre>
<p>(5) 使用 <a href="https://github.com/Microsoft/SDN/blob/master/Kubernetes/windows/start-kubelet.ps1" target="_blank">start-kubelet.ps1</a> 启动 kubelet.exe，并使用 <a href="https://github.com/Microsoft/SDN/blob/master/Kubernetes/windows/start-kubeproxy.ps1" target="_blank">start-kubeproxy.ps1</a> 启动 kube-proxy.exe</p>
<pre><code class="lang-sh">[Environment]::SetEnvironmentVariable(<span class="hljs-string">"KUBECONFIG"</span>, <span class="hljs-string">"C:\k\config"</span>, [EnvironmentVariableTarget]::User)
./start-kubelet.ps1 -ClusterCidr 192.168.0.0/16
./start-kubeproxy.ps1
</code></pre>
<p>(6) 如果使用 Host-Gateway 网络插件，还需要使用 <a href="https://github.com/Microsoft/SDN/blob/master/Kubernetes/windows/AddRoutes.ps1" target="_blank">AddRoutes.ps1</a> 添加静态路由</p>
<p>详细的操作步骤可以参考 <a href="https://github.com/MicrosoftDocs/Virtualization-Documentation/blob/live/virtualization/windowscontainers/kubernetes/getting-started-kubernetes-windows.md" target="_blank">这里</a>。</p>
<h2 id="运行-windows-容器">运行 Windows 容器</h2>
<p>使用 NodeSelector  <code>beta.kubernetes.io/os: windows</code> 将容器调度到 Windows 节点上，比如</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> extensions/v1beta1
<span class="hljs-attr">kind:</span> Deployment
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> iis
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  replicas:</span> <span class="hljs-number">1</span>
<span class="hljs-attr">  template:</span>
<span class="hljs-attr">    metadata:</span>
<span class="hljs-attr">      labels:</span>
<span class="hljs-attr">        app:</span> iis
<span class="hljs-attr">    spec:</span>
<span class="hljs-attr">      nodeSelector:</span>
        beta.kubernetes.io/os: windows
<span class="hljs-attr">      containers:</span>
<span class="hljs-attr">      - name:</span> iis
<span class="hljs-attr">        image:</span> microsoft/iis
<span class="hljs-attr">        resources:</span>
<span class="hljs-attr">          limits:</span>
<span class="hljs-attr">            memory:</span> <span class="hljs-string">"128Mi"</span>
<span class="hljs-attr">            cpu:</span> <span class="hljs-number">2</span>
<span class="hljs-attr">        ports:</span>
<span class="hljs-attr">        - containerPort:</span> <span class="hljs-number">80</span>
<span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Service
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  labels:</span>
<span class="hljs-attr">    app:</span> iis
<span class="hljs-attr">  name:</span> iis
<span class="hljs-attr">  namespace:</span> default
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  ports:</span>
<span class="hljs-attr">  - port:</span> <span class="hljs-number">80</span>
<span class="hljs-attr">    protocol:</span> TCP
<span class="hljs-attr">    targetPort:</span> <span class="hljs-number">80</span>
<span class="hljs-attr">  selector:</span>
<span class="hljs-attr">    app:</span> iis
<span class="hljs-attr">  type:</span> NodePort
</code></pre>
<p>运行 DaemonSet</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> extensions/v1beta1
<span class="hljs-attr">kind:</span> DaemonSet
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> my-DaemonSet
<span class="hljs-attr">  labels:</span>
<span class="hljs-attr">    app:</span> foo
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  template:</span>
<span class="hljs-attr">    metadata:</span>
<span class="hljs-attr">      labels:</span>
<span class="hljs-attr">        app:</span> foo
<span class="hljs-attr">    spec:</span>
<span class="hljs-attr">      containers:</span>
<span class="hljs-attr">      - name:</span> foo
<span class="hljs-attr">        image:</span> microsoft/windowsservercore:<span class="hljs-number">1709</span>
<span class="hljs-attr">      nodeSelector:</span>
        beta.kubernetes.io/os: windows
</code></pre>
<h2 id="已知问题">已知问题</h2>
<h3 id="secrets-和-configmaps-只能以环境变量的方式使用">Secrets 和 ConfigMaps 只能以环境变量的方式使用</h3>
<p>1709和更早版本有这个问题，升级到 1803 即可解决。</p>
<h3 id="volume-支持情况">Volume 支持情况</h3>
<p>Windows 容器暂时只支持 local、emptyDir、hostPath、AzureDisk、AzureFile 以及 flexvolume。注意 Volume 的路径格式需要为 <code>mountPath: "C:\\etc\\foo"</code> 或者 <code>mountPath: "C:/etc/foo"</code>。</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> hostpath-pod
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">  - name:</span> hostpath-nano
<span class="hljs-attr">    image:</span> microsoft/nanoserver:<span class="hljs-number">1709</span>
<span class="hljs-attr">    stdin:</span> <span class="hljs-literal">true</span>
<span class="hljs-attr">    tty:</span> <span class="hljs-literal">true</span>
<span class="hljs-attr">    volumeMounts:</span>
<span class="hljs-attr">    - name:</span> blah
<span class="hljs-attr">      mountPath:</span> <span class="hljs-string">"C:\\etc\\foo"</span>
<span class="hljs-attr">      readOnly:</span> <span class="hljs-literal">true</span>
<span class="hljs-attr">  nodeSelector:</span>
    beta.kubernetes.io/os: windows
<span class="hljs-attr">  volumes:</span>
<span class="hljs-attr">  - name:</span> blah
<span class="hljs-attr">    hostPath:</span>
<span class="hljs-attr">      path:</span> <span class="hljs-string">"C:\\AzureData"</span>
</code></pre>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> empty-dir-pod
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">  - image:</span> microsoft/nanoserver:<span class="hljs-number">1709</span>
<span class="hljs-attr">    name:</span> empty-dir-nano
<span class="hljs-attr">    stdin:</span> <span class="hljs-literal">true</span>
<span class="hljs-attr">    tty:</span> <span class="hljs-literal">true</span>
<span class="hljs-attr">    volumeMounts:</span>
<span class="hljs-attr">    - mountPath:</span> /cache
<span class="hljs-attr">      name:</span> cache-volume
<span class="hljs-attr">    - mountPath:</span> C:/scratch
<span class="hljs-attr">      name:</span> scratch-volume
<span class="hljs-attr">  volumes:</span>
<span class="hljs-attr">  - name:</span> cache-volume
<span class="hljs-attr">    emptyDir:</span> {}
<span class="hljs-attr">  - name:</span> scratch-volume
<span class="hljs-attr">    emptyDir:</span> {}
<span class="hljs-attr">  nodeSelector:</span>
    beta.kubernetes.io/os: windows
</code></pre>
<h3 id="镜像版本匹配问题">镜像版本匹配问题</h3>
<p>在 <code>Windows Server version 1709</code> 中必须使用带有 1709 标签的镜像，如</p>
<ul>
<li>microsoft/aspnet:4.7.1-windowsservercore-1709</li>
<li>microsoft/windowsservercore:1709</li>
<li>microsoft/iis:windowsservercore-1709</li>
</ul>
<p>同样，在 <code>Windows Server version 1803</code> 中必须使用带有 1803 标签的镜像。而在 <code>Windows Server 2016</code> 上需要使用带有 ltsc2016 标签的镜像，如 <code>microsoft/windowsservercore:ltsc2016</code>。</p>
<h2 id="设置-cpu-和内存">设置 CPU 和内存</h2>
<p>从 v1.10 开始，Kubernetes 支持给 Windows 容器设置 CPU 和内存：</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> apps/v1
<span class="hljs-attr">kind:</span> Deployment
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> iis
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  replicas:</span> <span class="hljs-number">3</span>
<span class="hljs-attr">  template:</span>
<span class="hljs-attr">    metadata:</span>
<span class="hljs-attr">      labels:</span>
<span class="hljs-attr">        app:</span> iis
<span class="hljs-attr">    spec:</span>
<span class="hljs-attr">      containers:</span>
<span class="hljs-attr">      - name:</span> iis
<span class="hljs-attr">        image:</span> microsoft/iis
<span class="hljs-attr">        resources:</span>
<span class="hljs-attr">          limits:</span>
<span class="hljs-attr">            memory:</span> <span class="hljs-string">"128Mi"</span>
<span class="hljs-attr">            cpu:</span> <span class="hljs-number">2</span>
<span class="hljs-attr">        ports:</span>
<span class="hljs-attr">        - containerPort:</span> <span class="hljs-number">80</span>
</code></pre>
<h2 id="hyper-v-容器">Hyper-V 容器</h2>
<p>从 v1.10 开始支持 Hyper-V 隔离的容器（Alpha）。 在使用之前，需要配置 kubelet 开启 <code>HyperVContainer</code> 特性开关。然后使用 Annotation <code>experimental.windows.kubernetes.io/isolation-type=hyperv</code> 来指定容器使用 Hyper-V 隔离:</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> apps/v1
<span class="hljs-attr">kind:</span> Deployment
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> iis
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  replicas:</span> <span class="hljs-number">3</span>
<span class="hljs-attr">  template:</span>
<span class="hljs-attr">    metadata:</span>
<span class="hljs-attr">      labels:</span>
<span class="hljs-attr">        app:</span> iis
<span class="hljs-attr">      annotations:</span>
        experimental.windows.kubernetes.io/isolation-type: hyperv
<span class="hljs-attr">    spec:</span>
<span class="hljs-attr">      containers:</span>
<span class="hljs-attr">      - name:</span> iis
<span class="hljs-attr">        image:</span> microsoft/iis
<span class="hljs-attr">        ports:</span>
<span class="hljs-attr">        - containerPort:</span> <span class="hljs-number">80</span>
</code></pre>
<h3 id="其他已知问题">其他已知问题</h3>
<ul>
<li>仅  Windows Server 1709 或更新的版本才支持在 Pod 内运行多个容器（仅支持 Process 隔离）</li>
<li>暂不支持 StatefulSet</li>
<li>暂不支持 Windows Server Container Pods 的自动扩展（Horizontal Pod Autoscaling）</li>
<li>Windows 容器的 OS 版本需要与 Host OS 版本匹配，否则容器无法启动</li>
<li>使用 L3 或者 Host GW 网络时，无法从 Windows Node 中直接访问 Kubernetes Services（使用 OVS/OVN 时没有这个问题）</li>
<li>在 VMWare Fusion 的 Window Server 中 kubelet.exe 可能会无法启动（已在 <a href="https://github.com/kubernetes/kubernetes/pull/57124" target="_blank">#57124</a> 中修复）</li>
<li>暂不支持 Weave 网络插件</li>
<li>Calico 网络插件仅支持 Policy-Only 模式</li>
<li>对于需要使用 <code>:</code> 作为环境变量的 .NET 容器，可以将环境变量中的 <code>:</code> 替换为 <code>__</code>（参考 <a href="https://docs.microsoft.com/en-us/aspnet/core/fundamentals/configuration/?tabs=basicconfiguration#configuration-by-environment" target="_blank">这里</a>）</li>
</ul>
<h2 id="附录：docker-ee-安装方法">附录：Docker EE 安装方法</h2>
<p>安装 Docker EE 稳定版本</p>
<pre><code class="lang-powershell">Install-Module -Name DockerMsftProvider -Repository PSGallery -Force
Install-Package -Name docker -ProviderName DockerMsftProvider
Restart-Computer -Force
</code></pre>
<p>安装 Docker EE 预览版本</p>
<pre><code class="lang-powershell">Install-Module DockerProvider
Install-Package -Name Docker -ProviderName DockerProvider -RequiredVersion preview
</code></pre>
<p>升级 Docker EE 版本</p>
<pre><code class="lang-powershell"><span class="hljs-comment"># Check the installed version</span>
Get-Package -Name Docker -ProviderName DockerMsftProvider

<span class="hljs-comment"># Find the current version</span>
Find-Package -Name Docker -ProviderName DockerMsftProvider

<span class="hljs-comment"># Upgrade Docker EE</span>
Install-Package -Name Docker -ProviderName DockerMsftProvider -Update -Force
<span class="hljs-built_in">Start-Service</span> Docker
</code></pre>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://kubernetes.io/docs/getting-started-guides/windows/" target="_blank">Using Windows Server Containers in Kubernetes</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="利用-linuxkit-部署-kubernetes-集群" class="level3">LinuxKit</h1>
<p>LinuxKit 是以 Container 来建立最小、不可变的 Linux 系统框架，可以参考 <a href="https://github.com/linuxkit/linuxkit" target="_blank">LinuxKit</a> 简单介绍。本着则将利用 LinuxKit 来建立 Kubernetes 的映像档，并部署简单的 Kubernetes 集群。</p>
<p><img src="images/moby+kubernetes.png" alt=""/></p>
<p>本着教学会在 <code>Mac OS X</code> 系统上进行，部署的环境资讯如下：</p>
<ul>
<li>Kubernetes v1.7.2</li>
<li>Etcd v3</li>
<li>Weave</li>
<li>Docker v17.06.0-ce</li>
</ul>
<h2 id="预先准备资讯">预先准备资讯</h2>
<ul>
<li>主机已安装与启动 <code>Docker</code> 工具。</li>
<li>主机已安装 <code>Git</code> 工具。</li>
<li>主机以下载 LinuxKit 项目，并建构了 Moby 与 LinuxKit 工具。</li>
</ul>
<p>建构 Moby 与 LinuxKit 方法如以下操作：</p>
<pre><code class="lang-sh">$ git <span class="hljs-built_in">clone</span> https://github.com/linuxkit/linuxkit.git
$ <span class="hljs-built_in">cd</span> linuxkit
$ make
$ ./bin/moby version
moby version 0.0
commit: c2b081ed8a9f690820cc0c0568238e641848f58f

$ ./bin/linuxkit version
linuxkit version 0.0
commit: 0e3ca695d07d1c9870eca71fb7dd9ede31a38380
</code></pre>
<h2 id="建构-kubernetes-系统映像档">建构 Kubernetes 系统映像档</h2>
<p>首先要建立一个打包好 Kubernetes 的 Linux 系统，而官方已经有做好范例，利用以下方式即可建构：</p>
<pre><code class="lang-sh">$ <span class="hljs-built_in">cd</span> linuxkit/projects/kubernetes/
$ make build-vm-images
...
Create outputs:
  kube-node-kernel kube-node-initrd.img kube-node-cmdline
</code></pre>
<h2 id="部署-kubernetes-cluster">部署 Kubernetes cluster</h2>
<p>完成建构映像档后，就可以透过以下指令来启动 Master OS，然后获取节点 IP：</p>
<pre><code class="lang-sh">$ ./boot.sh

(ns: getty) linuxkit-025000000002:~\<span class="hljs-comment"># ip addr show dev eth0</span>
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP qlen 1000
    link/ether 02:50:00:00:00:02 brd ff:ff:ff:ff:ff:ff
    inet 192.168.65.3/24 brd 192.168.65.255 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::abf0:9fa4:d0f4:8da2/64 scope link
       valid_lft forever preferred_lft forever
</code></pre>
<p>启动后，开启新的 Console 来 SSH 进入 Master，来利用 kubeadm 初始化 Master：</p>
<pre><code class="lang-sh">$ <span class="hljs-built_in">cd</span> linuxkit/projects/kubernetes/
$ ./ssh_into_kubelet.sh 192.168.65.3
linuxkit-025000000002:/\<span class="hljs-comment"># kubeadm-init.sh</span>
...
kubeadm join --token 4236d3.29f61af661c49dbf 192.168.65.3:6443
</code></pre>
<p>一旦 kubeadm 完成后，就会看到 Token，这时请记住 Token 资讯。接着开启新 Console，然后执行以下指令来启动 Node：</p>
<pre><code class="lang-sh">console1>$ ./boot.sh 1 --token 4236d3.29f61af661c49dbf 192.168.65.3:6443
</code></pre>
<blockquote>
<p>P.S. 开启节点格式为 <code>./boot.sh <n> [<join_args> ...]</code>。</p>
</blockquote>
<p>接着分别在开两个 Console 来加入集群：</p>
<pre><code class="lang-sh">console2> $ ./boot.sh 2 --token 4236d3.29f61af661c49dbf 192.168.65.3:6443
console3> $ ./boot.sh 3 --token 4236d3.29f61af661c49dbf 192.168.65.3:6443
</code></pre>
<p>完成后回到 Master 节点上，执行以下指令来查看节点状况：</p>
<pre><code class="lang-sh">$ kubectl get no
NAME                    STATUS    AGE       VERSION
linuxkit-025000000002   Ready     16m       v1.7.2
linuxkit-025000000003   Ready     6m        v1.7.2
linuxkit-025000000004   Ready     1m        v1.7.2
linuxkit-025000000005   Ready     1m        v1.7.2
</code></pre>
<h2 id="简单部署-nginx-服务">简单部署 Nginx 服务</h2>
<p>Kubernetes 可以选择使用指令直接建立应用程式与服务，或者撰写 YAML 与 JSON 档案来描述部署应用的配置，以下将建立一个简单的 Nginx 服务：</p>
<pre><code class="lang-sh">$ kubectl run nginx --image=nginx --replicas=1 --port=80
$ kubectl get pods -o wide
NAME                     READY     STATUS    RESTARTS   AGE       IP          NODE
nginx-1423793266-v0hpb   1/1       Running   0          38s       10.42.0.1   linuxkit-025000000004
</code></pre>
<p>完成后要接着建立 svc(Service)，来提供外部网络存取应用：</p>
<pre><code class="lang-sh">$ kubectl expose deploy nginx --port=80 --type=NodePort
$ kubectl get svc
NAME         CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
kubernetes   10.96.0.1       <none>        443/TCP        19m
nginx        10.108.41.230   <nodes>       80:31773/TCP   5s
</code></pre>
<p>由于不是使用物理机器部署，因此网络使用 Docker namespace 网络，故需透过 <code>ubuntu-desktop-lxde-vnc</code> 来浏览 Nginx 应用：</p>
<pre><code class="lang-sh">$ docker run -it --rm -p 6080:80 dorowu/ubuntu-desktop-lxde-vnc
</code></pre>
<blockquote>
<p>完成后透过浏览器连接 <a href="localhost:6080" target="_blank">HTML VNC</a>。</p>
</blockquote>
<p><img src="images/docker-desktop.png" alt=""/></p>
<p>最后关闭节点只需要执行以下即可：</p>
<pre><code class="lang-sh">$ halt
[1503.034689] reboot: Power down
</code></pre>
</section>
                            
    <h1 class='level3'>LinuxKit</h1><section class="normal markdown-section">
                                
                                <h1 id="附加组件" class="level2">附加组件</h1>
<p>部署 Kubernetes 集群后，还需要部署一系列的附加组件（addons），这些组件通常是保证集群功能正常运行必不可少的。</p>
<p>通常使用 <a href="addon-manager.html">addon-manager</a> 来管理集群中的附加组件。它运行在 Kubernetes 集群 Master 节点中，管理着 <code>$ADDON_PATH</code>（默认是 <code>/etc/kubernetes/addons/</code>）目录中的所有扩展，保证它们始终运行在期望状态。</p>
<p>常见的组件包括：</p>
<ul>
<li><a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/addon-manager" target="_blank">addon-manager</a></li>
<li><a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/cluster-loadbalancing" target="_blank">cluster-loadbalancing</a></li>
<li><a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/cluster-monitoring" target="_blank">cluster-monitoring</a></li>
<li><a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dashboard" target="_blank">dashboard</a></li>
<li><a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/device-plugins/nvidia-gpu" target="_blank">device-plugins/nvidia-gpu</a></li>
<li><a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dns-horizontal-autoscaler" target="_blank">dns-horizontal-autoscaler</a></li>
<li><a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dns" target="_blank">dns</a></li>
<li><a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/fluentd-elasticsearch" target="_blank">fluentd-elasticsearch</a></li>
<li><a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/ip-masq-agent" target="_blank">ip-masq-agent</a></li>
<li><a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/istio" target="_blank">istio</a></li>
<li><a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/kube-proxy" target="_blank">kube-proxy</a></li>
<li><a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/metrics-server" target="_blank">metrics-server</a></li>
<li><a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/node-problem-detector" target="_blank">node-problem-detector</a></li>
<li><a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/prometheus" target="_blank">prometheus</a></li>
<li><a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/storage-class" target="_blank">storage-class</a></li>
</ul>
<p>更多的扩展组件可以参考 <a href="https://kubernetes.io/docs/concepts/cluster-administration/addons/" target="_blank">Installing Addons</a> 和 <a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons" target="_blank">Legacy Addons</a>。</p>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="addon-manager" class="level3">Addon-manager</h1>
<p>附加组件管理器（Addon-manager）是运行在 Kubernetes 集群 Master 节点、用来管理附加组件（Addons）的服务。它管理着 <code>$ADDON_PATH</code>（默认是 <code>/etc/kubernetes/addons/</code>）目录中的所有扩展，保证它们始终运行在期望状态。</p>
<p>Addon-manager 支持两种标签</p>
<ul>
<li>对于带有 <code>addonmanager.kubernetes.io/mode=Reconcile</code> 标签的扩展，无法通过 API 来修改，即<ul>
<li>如果通过 API 修改了，则会自动回滚到 <code>/etc/kubernetes/addons/</code> 中的配置</li>
<li>如果通过 API 删除了，则会通过 <code>/etc/kubernetes/addons/</code> 中的配置自动重新创建</li>
<li>如果从 <code>/etc/kubernetes/addons/</code> 中删除配置，则 Kubernetes 资源也会删除</li>
<li>也就是说只能通过修改 <code>/etc/kubernetes/addons/</code> 中的配置来修改</li>
</ul>
</li>
<li>对于带有 <code>addonmanager.kubernetes.io/mode=EnsureExists</code> 标签到扩展，仅检查扩展是否存在而不检查配置是否更改，即<ul>
<li>可以通过 API 来修改配置，不会自动回滚</li>
<li>如果通过 API 删除了，则会通过 <code>/etc/kubernetes/addons/</code> 中的配置自动重新创建</li>
<li>如果从 <code>/etc/kubernetes/addons/</code> 中删除配置，则 Kubernetes 资源不会删除</li>
</ul>
</li>
</ul>
<h2 id="部署方法">部署方法</h2>
<p>将下面的 YAML 存入所有 Master 节点的 <code>/etc/kubernetes/manifests/kube-addon-manager.yaml</code> 文件中：</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> kube-addon-manager
<span class="hljs-attr">  namespace:</span> kube-system
<span class="hljs-attr">  annotations:</span>
    scheduler.alpha.kubernetes.io/critical-pod: <span class="hljs-string">''</span>
    seccomp.security.alpha.kubernetes.io/pod: <span class="hljs-string">'docker/default'</span>
<span class="hljs-attr">  labels:</span>
<span class="hljs-attr">    component:</span> kube-addon-manager
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  hostNetwork:</span> <span class="hljs-literal">true</span>
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">  - name:</span> kube-addon-manager
    <span class="hljs-comment"># When updating version also bump it in:</span>
    <span class="hljs-comment"># - test/kubemark/resources/manifests/kube-addon-manager.yaml</span>
<span class="hljs-attr">    image:</span> k8s.gcr.io/kube-addon-manager:v8<span class="hljs-number">.7</span>
<span class="hljs-attr">    command:</span>
<span class="hljs-bullet">    -</span> /bin/bash
<span class="hljs-bullet">    -</span> -c
<span class="hljs-bullet">    -</span> exec /opt/kube-addons.sh <span class="hljs-number">1</span>>>/var/log/kube-addon-manager.log <span class="hljs-number">2</span>>&<span class="hljs-number">1</span>
<span class="hljs-attr">    resources:</span>
<span class="hljs-attr">      requests:</span>
<span class="hljs-attr">        cpu:</span> <span class="hljs-number">3</span>m
<span class="hljs-attr">        memory:</span> <span class="hljs-number">50</span>Mi
<span class="hljs-attr">    volumeMounts:</span>
<span class="hljs-attr">    - mountPath:</span> /etc/kubernetes/
<span class="hljs-attr">      name:</span> addons
<span class="hljs-attr">      readOnly:</span> <span class="hljs-literal">true</span>
<span class="hljs-attr">    - mountPath:</span> /var/log
<span class="hljs-attr">      name:</span> varlog
<span class="hljs-attr">      readOnly:</span> <span class="hljs-literal">false</span>
<span class="hljs-attr">    env:</span>
<span class="hljs-attr">    - name:</span> KUBECTL_EXTRA_PRUNE_WHITELIST
<span class="hljs-attr">      value:</span> {{kubectl_extra_prune_whitelist}}
<span class="hljs-attr">  volumes:</span>
<span class="hljs-attr">  - hostPath:</span>
<span class="hljs-attr">      path:</span> /etc/kubernetes/
<span class="hljs-attr">    name:</span> addons
<span class="hljs-attr">  - hostPath:</span>
<span class="hljs-attr">      path:</span> /var/log
<span class="hljs-attr">    name:</span> varlog
</code></pre>
<h2 id="源码">源码</h2>
<p>Addon-manager 的源码维护在 <a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/addon-manager" target="_blank">https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/addon-manager</a>。</p>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="kubernetes-dashboard" class="level3">Dashboard</h1>
<p>Kubernetes Dashboard 的部署非常简单，只需要运行</p>
<pre><code class="lang-sh">kubectl apply <span class="hljs-_">-f</span> https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml
</code></pre>
<p>稍等一会，dashborad 就会创建好</p>
<pre><code class="lang-sh">$ kubectl -n kube-system get service kubernetes-dashboard
NAME                   CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
kubernetes-dashboard   10.101.211.212   <nodes>       80:32729/TCP   1m
$ kubectl -n kube-system describe service kubernetes-dashboard
Name:            kubernetes-dashboard
Namespace:        kube-system
Labels:            app=kubernetes-dashboard
Annotations:        <none>
Selector:        app=kubernetes-dashboard
Type:            NodePort
IP:            10.101.211.212
Port:            <<span class="hljs-built_in">unset</span>>    80/TCP
NodePort:        <<span class="hljs-built_in">unset</span>>    32729/TCP
Endpoints:        10.244.1.3:9090
Session Affinity:    None
Events:            <none>
</code></pre>
<p>然后就可以通过 <code>http://nodeIP:32729</code> 来访问了。</p>
<h2 id="登录认证">登录认证</h2>
<h3 id="导入证书登录">导入证书登录</h3>
<p>在 v1.7 之前的版本中，Dashboard 并不提供登陆的功能。而通常情况下，Dashboard 服务都是以 https 的方式运行，所以可以在访问它之前将证书导入系统中:</p>
<pre><code class="lang-sh">openssl pkcs12 -export -in apiserver-kubelet-client.crt -inkey apiserver-kubelet-client.key -out kube.p12
curl <span class="hljs-_">-s</span>SL -E ./kube.p12:password -k https://nodeIP:6443/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard
</code></pre>
<p>将 kube.p12 导入系统就可以用浏览器来访问了。注意，如果 nodeIP 不在证书 CN 里面，则需要做个 hosts 映射。</p>
<h3 id="使用-kubeconfig-配置文件登录">使用 kubeconfig 配置文件登录</h3>
<p>从 v1.7.0 版本开始，Dashboard 支持以 kubeconfig 配置文件的方式登录。打开 Dashboard 页面会自动跳转到登录的界面，选择 Kubeconfig 方式，并选择本地的 kubeconfig 配置文件即可。</p>
<p><img src="https://user-images.githubusercontent.com/2285385/30416718-8ee657d8-992d-11e7-84c8-9ba5f4c78bb2.png" alt=""/></p>
<h3 id="使用-token-登录">使用 Token 登录</h3>
<p>从 v1.7.0 版本开始，Dashboard 支持以 Token 的方式登录。注意从 Kubernetes 中取得的 Token 需要以 Base64 解码后才可以用来登录。</p>
<p>下面是一个在开启 RBAC 时创建一个只可以访问 demo namespace 的 service account token 示例：</p>
<pre><code class="lang-sh"><span class="hljs-comment"># 创建 demo namespace</span>
kubectl create namespace demo

<span class="hljs-comment"># 创建并限制只可以访问 demo namespace</span>
cat <<EOF | kubectl apply <span class="hljs-_">-f</span> -
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: demo
  name: default-role
rules:
  - apiGroups:
    - <span class="hljs-string">'*'</span>
    resources:
    - <span class="hljs-string">'*'</span>
    verbs:
    - <span class="hljs-string">'*'</span>
EOF
kubectl create rolebinding default-rolebinding --serviceaccount=demo:default --namespace=demo --role=default-role

<span class="hljs-comment"># 获取 token</span>
secret=$(kubectl -n demo get sa default -o jsonpath=<span class="hljs-string">'{.secrets[0].name}'</span>)
kubectl -n demo get secret <span class="hljs-variable">$secret</span> -o jsonpath=<span class="hljs-string">'{.data.token}'</span> | base64 <span class="hljs-_">-d</span>
</code></pre>
<p>注意，由于使用该 token 仅可以访问 demo namespace，故而需要登录后将访问 URL 中的 default 改成 demo。</p>
<h2 id="其他用户界面">其他用户界面</h2>
<p>除了 Kubernetes 社区提供的 Dashboard，还可以使用下列用户界面来管理 Kubernetes 集群</p>
<ul>
<li><a href="https://github.com/bitnami-labs/cabin" target="_blank">Cabin</a>：Android/iOS App，用于在移动端管理 Kubernetes</li>
<li><a href="http://kubernetic.com/" target="_blank">Kubernetic</a>：Kubernetes 桌面客户端</li>
<li><a href="https://github.com/smpio/kubernator" target="_blank">Kubernator</a>：低级（low-level） Web 界面，用于直接管理 Kubernetes 的资源对象（即 YAML 配置）</li>
</ul>
<p><img src="images/kubernator.png" alt="kubernator"/></p>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="kubernetes-监控" class="level3">监控</h1>
<p>Kubernetes 社区提供了一些列的工具来监控容器和集群的状态，并借助 Prometheus 提供告警的功能。</p>
<ul>
<li>cAdvisor 负责单节点内部的容器和节点资源使用统计，内置在 Kubelet 内部，并通过 Kubelet <code>/metrics/cadvisor</code> 对外提供 API</li>
<li><a href="https://www.influxdata.com/time-series-platform/influxdb/" target="_blank">InfluxDB</a> 是一个开源分布式时序、事件和指标数据库；而 <a href="http://grafana.org/" target="_blank">Grafana</a> 则是 InfluxDB 的 Dashboard，提供了强大的图表展示功能。它们常被组合使用展示图表化的监控数据。</li>
<li><a href="metrics.html">metrics-server</a> 提供了整个集群的资源监控数据，但要注意<ul>
<li>Metrics API 只可以查询当前的度量数据，并不保存历史数据</li>
<li>Metrics API URI 为 <code>/apis/metrics.k8s.io/</code>，在 <a href="https://github.com/kubernetes/metrics" target="_blank">k8s.io/metrics</a> 维护</li>
<li>必须部署 <code>metrics-server</code> 才能使用该 API，metrics-server 通过调用 Kubelet Summary API 获取数据</li>
</ul>
</li>
<li><a href="https://github.com/kubernetes/kube-state-metrics" target="_blank">kube-state-metrics</a> 提供了 Kubernetes 资源对象（如 DaemonSet、Deployments 等）的度量。</li>
<li><a href="https://prometheus.io" target="_blank">Prometheus</a> 是另外一个监控和时间序列数据库，还提供了告警的功能。</li>
<li><a href="https://github.com/kubernetes/node-problem-detector" target="_blank">Node Problem Detector</a> 监测 Node 本身的硬件、内核或者运行时等问题。</li>
<li><del><a href="https://github.com/kubernetes/heapster" target="_blank">Heapster</a> 提供了整个集群的资源监控，并支持持久化数据存储到 InfluxDB 等后端存储中（已弃用）</del></li>
</ul>
<h2 id="cadvisor">cAdvisor</h2>
<p><a href="https://github.com/google/cadvisor" target="_blank">cAdvisor</a> 是一个来自 Google 的容器监控工具，也是 Kubelet 内置的容器资源收集工具。它会自动收集本机容器 CPU、内存、网络和文件系统的资源占用情况，并对外提供 cAdvisor 原生的 API（默认端口为 <code>--cadvisor-port=4194</code>）。</p>
<p><img src="images/14842107270881.png" alt=""/></p>
<p>从 v1.7 开始，Kubelet metrics API 不再包含 cadvisor metrics，而是提供了一个独立的 API 接口：</p>
<ul>
<li>Kubelet metrics: <code>http://127.0.0.1:8001/api/v1/proxy/nodes/<node-name>/metrics</code></li>
<li>Cadvisor metrics: <code>http://127.0.0.1:8001/api/v1/proxy/nodes/<node-name>/metrics/cadvisor</code></li>
</ul>
<p>这样，在 Prometheus 等工具中需要使用新的 Metrics API 来获取这些数据，比如下面的 Prometheus 自动配置了 cadvisor metrics API：</p>
<pre><code class="lang-sh">helm install stable/prometheus --set rbac.create=<span class="hljs-literal">true</span> --name prometheus --namespace monitoring
</code></pre>
<p>注意：cadvisor 监听的端口将在 v1.12 中删除，建议所有外部工具使用 Kubelet Metrics API 替代。</p>
<h2 id="influxdb-和-grafana">InfluxDB 和 Grafana</h2>
<p><a href="https://www.influxdata.com/time-series-platform/influxdb/" target="_blank">InfluxDB</a> 是一个开源分布式时序、事件和指标数据库；而 <a href="http://grafana.org/" target="_blank">Grafana</a> 则是 InfluxDB 的 Dashboard，提供了强大的图表展示功能。它们常被组合使用展示图表化的监控数据。</p>
<p><img src="images/14842114123604.jpg" alt=""/></p>
<h2 id="heapster">Heapster</h2>
<p>Kubelet 内置的 cAdvisor 只提供了单机的容器资源占用情况，而 <a href="https://github.com/kubernetes/heapster" target="_blank">Heapster</a> 则提供了整个集群的资源监控，并支持持久化数据存储到 InfluxDB、Google Cloud Monitoring 或者 <a href="https://github.com/kubernetes/heapster" target="_blank">其他的存储后端</a>。注意：</p>
<ul>
<li>仅 Kubernetes v1.7.X 或者更老的集群推荐使用 Heapster。</li>
<li>从 Kubernetes v1.8 开始，资源使用情况的度量（如容器的 CPU 和内存使用）就已经通过 Metrics API 获取，并且 HPA 也从 metrics-server 查询必要的数据。</li>
<li><strong>Heapster 已在 v1.11 中弃用，推荐 v1.8 及以上版本部署 <a href="metrics.html">metrics-server</a> 替代 Heapster</strong></li>
</ul>
<p>Heapster 首先从 Kubernetes apiserver 查询所有 Node 的信息，然后再从 kubelet 提供的 API 采集节点和容器的资源占用，同时在 <code>/metrics</code> API 提供了 Prometheus 格式的数据。Heapster 采集到的数据可以推送到各种持久化的后端存储中，如 InfluxDB、Google Cloud Monitoring、OpenTSDB 等。</p>
<p><img src="images/14842118198998.png" alt=""/></p>
<h3 id="部署-heapster、influxdb-和-grafana">部署 Heapster、InfluxDB 和 Grafana</h3>
<p>在 Kubernetes 部署成功后，dashboard、DNS 和监控的服务也会默认部署好，比如通过 <code>cluster/kube-up.sh</code> 部署的集群默认会开启以下服务：</p>
<pre><code class="lang-sh">$ kubectl cluster-info
Kubernetes master is running at https://kubernetes-master
Heapster is running at https://kubernetes-master/api/v1/proxy/namespaces/kube-system/services/heapster
KubeDNS is running at https://kubernetes-master/api/v1/proxy/namespaces/kube-system/services/kube-dns
kubernetes-dashboard is running at https://kubernetes-master/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard
Grafana is running at https://kubernetes-master/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana
InfluxDB is running at https://kubernetes-master/api/v1/proxy/namespaces/kube-system/services/monitoring-influxdb
</code></pre>
<p>如果这些服务没有自动部署的话，可以参考 <a href="https://github.com/kubernetes/heapster/tree/master/deploy/kube-config" target="_blank">kubernetes/heapster</a> 来部署这些服务：</p>
<pre><code class="lang-sh">git <span class="hljs-built_in">clone</span> https://github.com/kubernetes/heapster
<span class="hljs-built_in">cd</span> heapster
kubectl create <span class="hljs-_">-f</span> deploy/kube-config/influxdb/
kubectl create <span class="hljs-_">-f</span> deploy/kube-config/rbac/heapster-rbac.yaml
</code></pre>
<p>注意在访问这些服务时，需要先在浏览器中导入 apiserver 证书才可以认证。为了简化访问过程，也可以使用 kubectl 代理来访问（不需要导入证书）：</p>
<pre><code class="lang-sh"><span class="hljs-comment"># 启动代理</span>
kubectl proxy --address=<span class="hljs-string">'0.0.0.0'</span> --port=8080 --accept-hosts=<span class="hljs-string">'^*$'</span> &
</code></pre>
<p>然后打开 <code>http://<master-ip>:8080/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana</code> 就可以访问 Grafana。</p>
<p><img src="images/grafana.png" alt=""/></p>
<h2 id="prometheus">Prometheus</h2>
<p><a href="https://prometheus.io" target="_blank">Prometheus</a> 是另外一个监控和时间序列数据库，并且还提供了告警的功能。它提供了强大的查询语言和 HTTP 接口，也支持将数据导出到 Grafana 中展示。</p>
<p><img src="images/prometheus.png" alt="prometheus"/></p>
<p>使用 Prometheus 监控 Kubernetes 需要配置好数据源，一个简单的示例是 <a href="prometheus.txt">prometheus.yml</a>。</p>
<p>推荐使用 <a href="https://github.com/coreos/prometheus-operator" target="_blank">Prometheus Operator</a> 或 <a href="https://github.com/kubernetes/charts/tree/master/stable/prometheus" target="_blank">Prometheus Chart</a> 来部署和管理 Prometheus，比如</p>
<pre><code class="lang-sh"><span class="hljs-comment"># 使用 prometheus operator</span>
helm repo add coreos https://s3-eu-west-1.amazonaws.com/coreos-charts/stable/
helm install coreos/prometheus-operator --name prometheus-operator --namespace monitoring
helm install coreos/kube-prometheus --name kube-prometheus --namespace monitoring
</code></pre>
<p>使用端口转发的方式访问 Prometheus，如 <code>kubectl --namespace monitoring port-forward service/kube-prometheus-prometheus :9090</code></p>
<p><img src="images/14842125295113.jpg" alt="prometheus-web"/></p>
<p>如果发现 exporter-kubelets 功能不正常，比如报 <code>server returned HTTP status 401 Unauthorized</code> 错误，则需要给 Kubelet 配置 webhook 认证：</p>
<pre><code class="lang-sh">kubelet --authentication-token-webhook=<span class="hljs-literal">true</span> --authorization-mode=Webhook
</code></pre>
<p>如果发现 K8SControllerManagerDown 和 K8SSchedulerDown 告警，则说明 kube-controller-manager 和 kube-scheduler 是以 Pod 的形式运行在集群中的，并且 prometheus 部署的监控服务与它们的标签不一致。可通过修改服务标签的方法解决，如</p>
<pre><code class="lang-sh">kubectl -n kube-system <span class="hljs-built_in">set</span> selector service kube-prometheus-exporter-kube-controller-manager  component=kube-controller-manager
kubectl -n kube-system <span class="hljs-built_in">set</span> selector service kube-prometheus-exporter-kube-scheduler  component=kube-scheduler
</code></pre>
<p>查询 Grafana 的管理员密码</p>
<pre><code class="lang-sh">kubectl get secret --namespace monitoring kube-prometheus-grafana -o jsonpath=<span class="hljs-string">"{.data.user}"</span> | base64 --decode ; <span class="hljs-built_in">echo</span>
kubectl get secret --namespace monitoring kube-prometheus-grafana -o jsonpath=<span class="hljs-string">"{.data.password}"</span> | base64 --decode ; <span class="hljs-built_in">echo</span>
</code></pre>
<p>然后，以端口转发的方式访问 Grafana 界面</p>
<pre><code class="lang-sh">kubectl port-forward -n monitoring service/kube-prometheus-grafana :80
</code></pre>
<p>添加 Prometheus 类型的 Data Source，填入原地址 <code>http://prometheus-prometheus-server.monitoring</code>。</p>
<h2 id="node-problem-detector">Node Problem Detector</h2>
<p>Kubernetes node 有可能会出现各种硬件、内核或者运行时等问题，这些问题有可能导致服务异常。而 Node Problem Detector（NPD）就是用来监测这些异常的服务。NPD 以 DaemonSet 的方式运行在每台 Node 上面，并在异常发生时更新 NodeCondition（比如 KernelDaedlock、DockerHung、BadDisk 等）或者 Node Event（比如 OOM Kill 等）。</p>
<p>可以参考 <a href="https://github.com/kubernetes/node-problem-detector#start-daemonset" target="_blank">kubernetes/node-problem-detector</a> 来部署 NPD，或者也可以使用 Helm 来部署：</p>
<pre><code class="lang-sh"><span class="hljs-comment"># add repo</span>
helm repo add feisky https://feisky.xyz/kubernetes-charts
helm update

<span class="hljs-comment"># install packages</span>
helm install feisky/node-problem-detector --namespace kube-system --name npd
</code></pre>
<h2 id="node-重启守护进程">Node 重启守护进程</h2>
<p>Kubernetres 集群中的节点通常会开启自动安全更新，这样有助于尽可能避免因系统漏洞带来的损失。但一般来说，涉及到内核的更新需要重启系统才可生效。此时，就需要手动或自动的方法来重启节点。</p>
<p><a href="https://github.com/weaveworks/kured" target="_blank">Kured (KUbernetes REboot Daemon)</a> 就是这样一个守护进程，它会</p>
<ul>
<li>监控 <code>/var/run/reboot-required</code> 信号后重启节点</li>
<li>通过 DaemonSet Annotation 的方式每次仅重启一台节点</li>
<li>重启前驱逐节点，重启后恢复调度</li>
<li>根据 Prometheus 告警 (<code>--alert-filter-regexp=^(RebootRequired|AnotherBenignAlert|...$</code>) 取消重启</li>
<li>Slack 通知</li>
</ul>
<p>部署方法</p>
<pre><code class="lang-sh">kubectl apply <span class="hljs-_">-f</span> https://github.com/weaveworks/kured/releases/download/1.0.0/kured-ds.yaml
</code></pre>
<h2 id="其他容器监控系统">其他容器监控系统</h2>
<p>除了以上监控工具，还有很多其他的开源或商业系统可用来辅助监控，如</p>
<ul>
<li><a href="http://blog.kubernetes.io/2015/11/monitoring-Kubernetes-with-Sysdig.html" target="_blank">Sysdig</a></li>
<li><a href="https://www.weave.works/docs/scope/latest/features/" target="_blank">Weave scope</a></li>
<li><a href="https://www.coscale.com/" target="_blank">CoScale</a></li>
<li><a href="https://www.datadoghq.com/" target="_blank">Datadog</a></li>
<li><a href="https://sematext.com/" target="_blank">Sematext</a></li>
</ul>
<h3 id="sysdig">sysdig</h3>
<p>sysdig 是一个容器排错工具，提供了开源和商业版本。对于常规排错来说，使用开源版本即可。</p>
<p>除了 sysdig，还可以使用其他两个辅助工具</p>
<ul>
<li>csysdig：与 sysdig 一起自动安装，提供了一个命令行界面</li>
<li><a href="https://github.com/draios/sysdig-inspect" target="_blank">sysdig-inspect</a>：为 sysdig 保存的跟踪文件（如 <code>sudo sysdig -w filename.scap</code>）提供了一个图形界面（非实时）</li>
</ul>
<h4 id="安装-sysdig">安装 sysdig</h4>
<pre><code class="lang-sh"><span class="hljs-comment"># on Linux</span>
curl <span class="hljs-_">-s</span> https://s3.amazonaws.com/download.draios.com/stable/install-sysdig | sudo bash

<span class="hljs-comment"># on MacOS</span>
brew install sysdig
</code></pre>
<p>使用示例</p>
<pre><code class="lang-sh"><span class="hljs-comment"># Refer https://www.sysdig.org/wiki/sysdig-examples/.</span>
<span class="hljs-comment"># View the top network connections for a single container</span>
sysdig -pc -c topconns

<span class="hljs-comment"># Show the network data exchanged with the host 192.168.0.1</span>
sysdig <span class="hljs-_">-s</span>2000 -A -c <span class="hljs-built_in">echo</span>_fds fd.cip=192.168.0.1

<span class="hljs-comment"># List all the incoming connections that are not served by apache.</span>
sysdig -p<span class="hljs-string">"%proc.name %fd.name"</span> <span class="hljs-string">"evt.type=accept and proc.name!=httpd"</span>

<span class="hljs-comment"># View the CPU/Network/IO usage of the processes running inside the container.</span>
sysdig -pc -c topprocs_cpu container.id=2e854c4525b8
sysdig -pc -c topprocs_net container.id=2e854c4525b8
sysdig -pc -c topfiles_bytes container.id=2e854c4525b8

<span class="hljs-comment"># See the files where apache spends the most time doing I/O</span>
sysdig -c topfiles_time proc.name=httpd

<span class="hljs-comment"># Show all the interactive commands executed inside a given container.</span>
sysdig -pc -c spy_users

<span class="hljs-comment"># Show every time a file is opened under /etc.</span>
sysdig evt.type=open and fd.name
</code></pre>
<h3 id="weave-scope">Weave Scope</h3>
<p>Weave Scope 是另外一款可视化容器监控和排错工具。与 sysdig 相比，它没有强大的命令行工具，但提供了一个简单易用的交互界面，自动描绘了整个集群的拓扑，并可以通过插件扩展其功能。从其官网的介绍来看，其提供的功能包括</p>
<ul>
<li><a href="https://www.weave.works/docs/scope/latest/features/#topology-mapping" target="_blank">交互式拓扑界面</a></li>
<li><a href="https://www.weave.works/docs/scope/latest/features/#mode" target="_blank">图形模式和表格模式</a></li>
<li><a href="https://www.weave.works/docs/scope/latest/features/#flexible-filtering" target="_blank">过滤功能</a></li>
<li><a href="https://www.weave.works/docs/scope/latest/features/#powerful-search" target="_blank">搜索功能</a></li>
<li><a href="https://www.weave.works/docs/scope/latest/features/#real-time-app-and-container-metrics" target="_blank">实时度量</a></li>
<li><a href="https://www.weave.works/docs/scope/latest/features/#interact-with-and-manage-containers" target="_blank">容器排错</a></li>
<li><a href="https://www.weave.works/docs/scope/latest/features/#custom-plugins" target="_blank">插件扩展</a></li>
</ul>
<p>Weave Scope 由 <a href="https://www.weave.works/docs/scope/latest/how-it-works" target="_blank">App 和 Probe 两部分</a>组成，它们</p>
<ul>
<li>Probe 负责收集容器和宿主的信息，并发送给 App</li>
<li>App 负责处理这些信息，并生成相应的报告，并以交互界面的形式展示</li>
</ul>
<pre><code class="lang-sh">                    +--Docker host----------+      +--Docker host----------+
.---------------.   |  +--Container------+  |      |  +--Container------+  |
| Browser       |   |  |                 |  |      |  |                 |  |
|---------------|   |  |  +-----------+  |  |      |  |  +-----------+  |  |
|               |----->|  | scope-app |<-----.    .----->| scope-app |  |  |
|               |   |  |  +-----------+  |  | \  / |  |  +-----------+  |  |
|               |   |  |        ^        |  |  \/  |  |        ^        |  |
<span class="hljs-string">'---------------'</span>   |  |        |        |  |  /\  |  |        |        |  |
                    |  | +-------------+ |  | /  \ |  | +-------------+ |  |
                    |  | | scope-probe |-----<span class="hljs-string">'    '</span>-----| scope-probe | |  |
                    |  | +-------------+ |  |      |  | +-------------+ |  |
                    |  |                 |  |      |  |                 |  |
                    |  +-----------------+  |      |  +-----------------+  |
                    +-----------------------+      +-----------------------+
</code></pre>
<h4 id="安装-weave-scope">安装 Weave scope</h4>
<pre><code class="lang-sh">kubectl apply <span class="hljs-_">-f</span> <span class="hljs-string">"https://cloud.weave.works/k8s/scope.yaml?k8s-version=<span class="hljs-variable">$(kubectl version | base64 | tr -d '\n')</span>&k8s-service-type=LoadBalancer"</span>
</code></pre>
<p>安装完成后，可以通过 weave-scope-app 来访问交互界面</p>
<pre><code class="lang-sh">kubectl -n weave get service weave-scope-app
</code></pre>
<p><img src="images/weave-scope.png" alt=""/></p>
<p>点击 Pod，还可以查看该 Pod 所有容器的实时状态和度量数据：</p>
<p><img src="images/scope-pod.png" alt=""/></p>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://github.com/kubernetes/heapster" target="_blank">Kubernetes Heapster</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="kubernetes-日志" class="level3">日志</h1>
<p>ELK 可谓是容器日志收集、处理和搜索的黄金搭档:</p>
<ul>
<li>Logstash（或者 Fluentd）负责收集日志</li>
<li>Elasticsearch 存储日志并提供搜索</li>
<li>Kibana 负责日志查询和展示</li>
</ul>
<p>注意：Kubernetes 默认使用 fluentd（以 DaemonSet 的方式启动）来收集日志，并将收集的日志发送给 elasticsearch。</p>
<p><strong>小提示</strong></p>
<p>在使用 <code>cluster/kube-up.sh</code> 部署集群的时候，可以设置 <code>KUBE_LOGGING_DESTINATION</code> 环境变量自动部署 Elasticsearch 和 Kibana，并使用 fluentd 收集日志 (配置参考 <a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/fluentd-elasticsearch" target="_blank">addons/fluentd-elasticsearch</a>)：</p>
<pre><code class="lang-sh">KUBE_LOGGING_DESTINATION=elasticsearch
KUBE_ENABLE_NODE_LOGGING=<span class="hljs-literal">true</span>
cluster/kube-up.sh
</code></pre>
<p>如果使用 GCE 或者 GKE 的话，还可以 <a href="https://kubernetes.io/docs/user-guide/logging/stackdriver/" target="_blank">将日志发送给 Google Cloud Logging</a>，并可以集成 Google Cloud Storage 和 BigQuery。</p>
<p>如果需要集成其他的日志方案，还可以自定义 docker 的 log driver，将日志发送到 splunk 或者 awslogs 等。</p>
<h2 id="部署方法">部署方法</h2>
<p>由于 Fluentd daemonset 只会调度到带有标签 <code>kubectl label nodes --all beta.kubernetes.io/fluentd-ds-ready=true</code> 的 Node 上，需要给 Node 设置标签</p>
<pre><code class="lang-sh">kubectl label nodes --all beta.kubernetes.io/fluentd-ds-ready=<span class="hljs-literal">true</span>
</code></pre>
<p>然后下载 manifest 部署：</p>
<pre><code class="lang-sh">$ git <span class="hljs-built_in">clone</span> https://github.com/kubernetes/kubernetes
$ <span class="hljs-built_in">cd</span> cluster/addons/fluentd-elasticsearch
$ kubectl apply <span class="hljs-_">-f</span> .
clusterrole <span class="hljs-string">"elasticsearch-logging"</span> configured
clusterrolebinding <span class="hljs-string">"elasticsearch-logging"</span> configured
replicationcontroller <span class="hljs-string">"elasticsearch-logging-v1"</span> configured
service <span class="hljs-string">"elasticsearch-logging"</span> configured
serviceaccount <span class="hljs-string">"elasticsearch-logging"</span> configured
clusterrole <span class="hljs-string">"fluentd-es"</span> configured
clusterrolebinding <span class="hljs-string">"fluentd-es"</span> configured
daemonset <span class="hljs-string">"fluentd-es-v1.24"</span> configured
serviceaccount <span class="hljs-string">"fluentd-es"</span> configured
deployment <span class="hljs-string">"kibana-logging"</span> configured
service <span class="hljs-string">"kibana-logging"</span> configured
</code></pre>
<p>注意：Kibana 容器第一次启动的时候会用较长的时间（Optimizing and caching bundles for kibana and statusPage. This may take a few minutes），可以通过日志观察初始化的情况</p>
<pre><code class="lang-sh">$ kubectl -n kube-system logs kibana-logging-1237565573-p88lm <span class="hljs-_">-f</span>
</code></pre>
<h2 id="访问-kibana">访问 Kibana</h2>
<p>可以从 <code>kubectl cluster-info</code> 的输出中找到 Kibana 服务的访问地址，注意需要在浏览器中导入 apiserver 证书才可以认证：</p>
<pre><code class="lang-sh">$ kubectl cluster-info | grep Kibana
Kibana is running at https://10.0.4.3:6443/api/v1/namespaces/kube-system/services/kibana-logging/proxy
</code></pre>
<p>这里采用另外一种方式，使用 kubectl 代理来访问（不需要导入证书）：</p>
<pre><code class="lang-sh"><span class="hljs-comment"># 启动代理</span>
kubectl proxy --address=<span class="hljs-string">'0.0.0.0'</span> --port=8080 --accept-hosts=<span class="hljs-string">'^*$'</span> &
</code></pre>
<p>然后打开 <code>http://<master-ip>:8080/api/v1/proxy/namespaces/kube-system/services/kibana-logging/app/kibana#</code>。在 Settings -> Indices 页面创建一个 index，选中 Index contains time-based events，使用默认的 <code>logstash-*</code> pattern，点击 Create。</p>
<p><img src="images/kibana.png" alt=""/></p>
<h2 id="filebeat">Filebeat</h2>
<p>除了 Fluentd 和 Logstash，还可以使用 <a href="https://www.elastic.co/products/beats/filebeat" target="_blank">Filebeat</a> 来收集日志:</p>
<pre><code class="lang-sh">kubectl apply <span class="hljs-_">-f</span> https://raw.githubusercontent.com/elastic/beats/master/deploy/kubernetes/filebeat-kubernetes.yaml
</code></pre>
<p>注意，默认假设 Elasticsearch 可通过 <code>elasticsearch:9200</code> 访问，如果不同的话，需要先修改再部署</p>
<pre><code class="lang-sh">- name: ELASTICSEARCH_HOST
  value: elasticsearch
- name: ELASTICSEARCH_PORT
  value: <span class="hljs-string">"9200"</span>
- name: ELASTICSEARCH_USERNAME
  value: elastic
- name: ELASTICSEARCH_PASSWORD
  value: changeme
</code></pre>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/fluentd-elasticsearch" target="_blank">Logging Agent For Elasticsearch</a></li>
<li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/logging-elasticsearch-kibana/" target="_blank">Logging Using Elasticsearch and Kibana</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="metrics" class="level3">Metrics</h1>
<p>从 v1.8 开始，资源使用情况的度量（如容器的 CPU 和内存使用）可以通过 Metrics API 获取。注意</p>
<ul>
<li>Metrics API 只可以查询当前的度量数据，并不保存历史数据</li>
<li>Metrics API URI 为 <code>/apis/metrics.k8s.io/</code>，在 <a href="https://github.com/kubernetes/metrics" target="_blank">k8s.io/metrics</a> 维护</li>
<li>必须部署 <code>metrics-server</code> 才能使用该 API，metrics-server 通过调用 Kubelet Summary API 获取数据</li>
</ul>
<h2 id="kubernetes-监控架构">Kubernetes 监控架构</h2>
<p><a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/instrumentation/monitoring_architecture.md" target="_blank">Kubernetes 监控架构</a>由以下两部分组成：</p>
<ul>
<li>核心度量流程（下图黑色部分）：这是 Kubernetes 正常工作所需要的核心度量，从 Kubelet、cAdvisor 等获取度量数据，再由 metrics-server 提供给 Dashboard、HPA 控制器等使用。</li>
<li>监控流程（下图蓝色部分）：基于核心度量构建的监控流程，比如 Prometheus 可以从 metrics-server 获取核心度量，从其他数据源（如 Node Exporter 等）获取非核心度量，再基于它们构建监控告警系统。</li>
</ul>
<p><img src="images/monitoring_architecture.png" alt=""/></p>
<h2 id="开启api-aggregation">开启API Aggregation</h2>
<p>在部署 metrics-server 之前，需要在 kube-apiserver 中开启 API Aggregation，即增加以下配置</p>
<pre><code class="lang-sh">--requestheader-client-ca-file=/etc/kubernetes/certs/proxy-ca.crt
--proxy-client-cert-file=/etc/kubernetes/certs/proxy.crt
--proxy-client-key-file=/etc/kubernetes/certs/proxy.key
--requestheader-allowed-names=aggregator
--requestheader-extra-headers-prefix=X-Remote-Extra-
--requestheader-group-headers=X-Remote-Group
--requestheader-username-headers=X-Remote-User
</code></pre>
<p>如果kube-proxy没有在Master上面运行，还需要配置</p>
<pre><code class="lang-sh">--enable-aggregator-routing=<span class="hljs-literal">true</span>
</code></pre>
<h2 id="部署-metrics-server">部署 metrics-server</h2>
<pre><code class="lang-sh">$ git <span class="hljs-built_in">clone</span> https://github.com/kubernetes-incubator/metrics-server
$ <span class="hljs-built_in">cd</span> metrics-server
$ kubectl create <span class="hljs-_">-f</span> deploy/1.8+/
</code></pre>
<p>稍后就可以看到 metrics-server 运行起来：</p>
<pre><code class="lang-sh">kubectl -n kube-system get pods <span class="hljs-_">-l</span> k8s-app=metrics-server
</code></pre>
<h2 id="metrics-api">Metrics API</h2>
<p>可以通过 <code>kubectl proxy</code> 来访问 <a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/instrumentation/resource-metrics-api.md" target="_blank">Metrics API</a>：</p>
<ul>
<li><code>http://127.0.0.1:8001/apis/metrics.k8s.io/v1beta1/nodes</code></li>
<li><code>http://127.0.0.1:8001/apis/metrics.k8s.io/v1beta1/nodes/<node-name></code></li>
<li><code>http://127.0.0.1:8001/apis/metrics.k8s.io/v1beta1/pods</code></li>
<li><code>http://127.0.0.1:8001/apis/metrics.k8s.io/v1beta1/namespace/<namespace-name>/pods/<pod-name></code></li>
</ul>
<p>也可以直接通过 kubectl 命令来访问这些 API，比如</p>
<ul>
<li><code>kubectl get --raw /apis/metrics.k8s.io/v1beta1/nodes</code></li>
<li><code>kubectl get --raw /apis/metrics.k8s.io/v1beta1/pods</code></li>
<li><code>kubectl get --raw /apis/metrics.k8s.io/v1beta1/nodes/<node-name></code></li>
<li><code>kubectl get --raw /apis/metrics.k8s.io/v1beta1/namespace/<namespace-name>/pods/<pod-name></code></li>
</ul>
<h2 id="排错">排错</h2>
<p>如果发现 metrics-server Pod 无法正常启动，比如处于 CrashLoopBackOff 状态，并且 restartCount 在不停增加，则很有可能是其跟 kube-apiserver 通信有问题。查看该 Pod 的日志，可以发现</p>
<pre><code class="lang-sh">dial tcp 10.96.0.1:443: i/o timeout
</code></pre>
<p>解决方法是：</p>
<pre><code class="lang-sh"><span class="hljs-built_in">echo</span> <span class="hljs-string">"ExecStartPost=/sbin/iptables -P FORWARD ACCEPT"</span> >> /etc/systemd/system/docker.service.d/<span class="hljs-built_in">exec</span>_start.conf
systemctl daemon-reload
systemctl restart docker
</code></pre>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/core-metrics-pipeline/" target="_blank">Core metrics pipeline</a></li>
<li><a href="https://github.com/kubernetes-incubator/metrics-server" target="_blank">metrics-server</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="gpu" class="level3">GPU</h1>
<p>Kubernetes 支持容器请求 GPU 资源（目前仅支持 NVIDIA GPU），在深度学习等场景中有大量应用。</p>
<h2 id="使用方法">使用方法</h2>
<h3 id="kubernetes-v18-及更新版本">Kubernetes v1.8 及更新版本</h3>
<p>从 Kubernetes v1.8 开始，GPU 开始以 DevicePlugin 的形式实现。在使用之前需要配置</p>
<ul>
<li>kubelet/kube-apiserver/kube-controller-manager: <code>--feature-gates="DevicePlugins=true"</code></li>
<li>在所有的 Node 上安装 Nvidia 驱动，包括 NVIDIA Cuda Toolkit 和 cuDNN 等</li>
<li>Kubelet 配置使用 docker 容器引擎（默认就是 docker），其他容器引擎暂不支持该特性</li>
</ul>
<h4 id="nvidia-插件">NVIDIA 插件</h4>
<p>NVIDIA 需要 nvidia-docker。</p>
<p>安装 nvidia-docker</p>
<pre><code class="lang-sh"><span class="hljs-comment"># Install docker-ce</span>
sudo apt-get install \
    apt-transport-https \
    ca-certificates \
    curl \
    software-properties-common
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
sudo add-apt-repository \
   <span class="hljs-string">"deb [arch=amd64] https://download.docker.com/linux/ubuntu \
   <span class="hljs-variable">$(lsb_release -cs)</span> \
   stable"</span>
sudo apt-get update
sudo apt-get install docker-ce

<span class="hljs-comment"># Add the package repositories</span>
curl <span class="hljs-_">-s</span> -L https://nvidia.github.io/nvidia-docker/gpgkey | \
  sudo apt-key add -
curl <span class="hljs-_">-s</span> -L https://nvidia.github.io/nvidia-docker/ubuntu16.04/amd64/nvidia-docker.list | \
  sudo tee /etc/apt/sources.list.d/nvidia-docker.list
sudo apt-get update

<span class="hljs-comment"># Install nvidia-docker2 and reload the Docker daemon configuration</span>
sudo apt-get install -y nvidia-docker2
sudo pkill -SIGHUP dockerd

<span class="hljs-comment"># Test nvidia-smi with the latest official CUDA image</span>
docker run --runtime=nvidia --rm nvidia/cuda nvidia-smi
</code></pre>
<p>设置 Docker 默认运行时为 nvidia</p>
<pre><code class="lang-sh"><span class="hljs-comment"># cat /etc/docker/daemon.json</span>
{
    <span class="hljs-string">"default-runtime"</span>: <span class="hljs-string">"nvidia"</span>,
    <span class="hljs-string">"runtimes"</span>: {
        <span class="hljs-string">"nvidia"</span>: {
            <span class="hljs-string">"path"</span>: <span class="hljs-string">"/usr/bin/nvidia-container-runtime"</span>,
            <span class="hljs-string">"runtimeArgs"</span>: []
        }
    }
}
</code></pre>
<p>部署 NVDIA 设备插件</p>
<pre><code class="lang-sh"><span class="hljs-comment"># For Kubernetes v1.8</span>
kubectl create <span class="hljs-_">-f</span> https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v1.8/nvidia-device-plugin.yml

<span class="hljs-comment"># For Kubernetes v1.9</span>
kubectl create <span class="hljs-_">-f</span> https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v1.9/nvidia-device-plugin.yml
</code></pre>
<h4 id="gcegke-gpu-插件">GCE/GKE GPU 插件</h4>
<p>该插件不需要 nvidia-docker，并且也支持 CRI 容器运行时。</p>
<pre><code class="lang-sh"><span class="hljs-comment"># Install NVIDIA drivers on Container-Optimized OS:</span>
kubectl create <span class="hljs-_">-f</span> https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/k8s-1.9/daemonset.yaml

<span class="hljs-comment"># Install NVIDIA drivers on Ubuntu (experimental):</span>
kubectl create <span class="hljs-_">-f</span> https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/k8s-1.9/nvidia-driver-installer/ubuntu/daemonset.yaml

<span class="hljs-comment"># Install the device plugin:</span>
kubectl create <span class="hljs-_">-f</span> https://raw.githubusercontent.com/kubernetes/kubernetes/release-1.9/cluster/addons/device-plugins/nvidia-gpu/daemonset.yaml
</code></pre>
<h4 id="请求-nvidiacomgpu-资源示例">请求 <code>nvidia.com/gpu</code> 资源示例</h4>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> cuda-vector-add
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  restartPolicy:</span> OnFailure
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">    - name:</span> cuda-vector-add
      <span class="hljs-comment"># https://github.com/kubernetes/kubernetes/blob/v1.7.11/test/images/nvidia-cuda/Dockerfile</span>
<span class="hljs-attr">      image:</span> <span class="hljs-string">"k8s.gcr.io/cuda-vector-add:v0.1"</span>
<span class="hljs-attr">      resources:</span>
<span class="hljs-attr">        limits:</span>
          nvidia.com/gpu: <span class="hljs-number">1</span> <span class="hljs-comment"># requesting 1 GPU</span>
</code></pre>
<h3 id="kubernetes-v16-和-v17">Kubernetes v1.6 和 v1.7</h3>
<blockquote>
<p><code>alpha.kubernetes.io/nvidia-gpu</code> 已在 v1.10 中删除，新版本请使用 <code>nvidia.com/gpu</code>。</p>
</blockquote>
<p>在 Kubernetes v1.6 和 v1.7 中使用 GPU 需要预先配置</p>
<ul>
<li>在所有的 Node 上安装 Nvidia 驱动，包括 NVIDIA Cuda Toolkit 和 cuDNN 等</li>
<li>在 apiserver 和 kubelet 上开启 <code>--feature-gates="Accelerators=true"</code></li>
<li>Kubelet 配置使用 docker 容器引擎（默认就是 docker），其他容器引擎暂不支持该特性</li>
</ul>
<p>使用资源名 <code>alpha.kubernetes.io/nvidia-gpu</code> 指定请求 GPU 的个数，如</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> tensorflow
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  restartPolicy:</span> Never
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">  - image:</span> gcr.io/tensorflow/tensorflow:latest-gpu
<span class="hljs-attr">    name:</span> gpu-container<span class="hljs-bullet">-1</span>
<span class="hljs-attr">    command:</span> [<span class="hljs-string">"python"</span>]
<span class="hljs-attr">    env:</span>
<span class="hljs-attr">    - name:</span> LD_LIBRARY_PATH
<span class="hljs-attr">      value:</span> /usr/lib/nvidia
<span class="hljs-attr">    args:</span>
<span class="hljs-bullet">    -</span> -u
<span class="hljs-bullet">    -</span> -c
<span class="hljs-bullet">    -</span> from tensorflow.python.client import device_lib; print device_lib.list_local_devices()
<span class="hljs-attr">    resources:</span>
<span class="hljs-attr">      limits:</span>
        alpha.kubernetes.io/nvidia-gpu: <span class="hljs-number">1</span> <span class="hljs-comment"># requests one GPU</span>
<span class="hljs-attr">    volumeMounts:</span>
<span class="hljs-attr">    - mountPath:</span> /usr/local/nvidia/bin
<span class="hljs-attr">      name:</span> bin
<span class="hljs-attr">    - mountPath:</span> /usr/lib/nvidia
<span class="hljs-attr">      name:</span> lib
<span class="hljs-attr">    - mountPath:</span> /usr/lib/x86_64-linux-gnu/libcuda.so
<span class="hljs-attr">      name:</span> libcuda-so
<span class="hljs-attr">    - mountPath:</span> /usr/lib/x86_64-linux-gnu/libcuda.so<span class="hljs-number">.1</span>
<span class="hljs-attr">      name:</span> libcuda-so<span class="hljs-bullet">-1</span>
<span class="hljs-attr">    - mountPath:</span> /usr/lib/x86_64-linux-gnu/libcuda.so<span class="hljs-number">.375</span><span class="hljs-number">.66</span>
<span class="hljs-attr">      name:</span> libcuda-so<span class="hljs-bullet">-375</span><span class="hljs-bullet">-66</span>
<span class="hljs-attr">  volumes:</span>
<span class="hljs-attr">    - name:</span> bin
<span class="hljs-attr">      hostPath:</span>
<span class="hljs-attr">        path:</span> /usr/lib/nvidia<span class="hljs-bullet">-375</span>/bin
<span class="hljs-attr">    - name:</span> lib
<span class="hljs-attr">      hostPath:</span>
<span class="hljs-attr">        path:</span> /usr/lib/nvidia<span class="hljs-bullet">-375</span>
<span class="hljs-attr">    - name:</span> libcuda-so
<span class="hljs-attr">      hostPath:</span>
<span class="hljs-attr">        path:</span> /usr/lib/x86_64-linux-gnu/libcuda.so
<span class="hljs-attr">    - name:</span> libcuda-so<span class="hljs-bullet">-1</span>
<span class="hljs-attr">      hostPath:</span>
<span class="hljs-attr">        path:</span> /usr/lib/x86_64-linux-gnu/libcuda.so<span class="hljs-number">.1</span>
<span class="hljs-attr">    - name:</span> libcuda-so<span class="hljs-bullet">-375</span><span class="hljs-bullet">-66</span>
<span class="hljs-attr">      hostPath:</span>
<span class="hljs-attr">        path:</span> /usr/lib/x86_64-linux-gnu/libcuda.so<span class="hljs-number">.375</span><span class="hljs-number">.66</span>
</code></pre>
<pre><code class="lang-sh">$ kubectl create <span class="hljs-_">-f</span> pod.yaml
pod <span class="hljs-string">"tensorflow"</span> created

$ kubectl logs tensorflow
...
[name: <span class="hljs-string">"/cpu:0"</span>
device_<span class="hljs-built_in">type</span>: <span class="hljs-string">"CPU"</span>
memory_<span class="hljs-built_in">limit</span>: 268435456
locality {
}
incarnation: 9675741273569321173
, name: <span class="hljs-string">"/gpu:0"</span>
device_<span class="hljs-built_in">type</span>: <span class="hljs-string">"GPU"</span>
memory_<span class="hljs-built_in">limit</span>: 11332668621
locality {
  bus_id: 1
}
incarnation: 7807115828340118187
physical_device_desc: <span class="hljs-string">"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0"</span>
]
</code></pre>
<p>注意</p>
<ul>
<li>GPU 资源必须在 <code>resources.limits</code> 中请求，<code>resources.requests</code> 中无效</li>
<li>容器可以请求 1 个或多个 GPU，不能只请求一部分</li>
<li>多个容器之间不能共享 GPU</li>
<li>默认假设所有 Node 安装了相同型号的 GPU</li>
</ul>
<h2 id="多种型号的-gpu">多种型号的 GPU</h2>
<p>如果集群 Node 中安装了多种型号的 GPU，则可以使用 Node Affinity 来调度 Pod 到指定 GPU 型号的 Node 上。</p>
<p>首先，在集群初始化时，需要给 Node 打上 GPU 型号的标签</p>
<pre><code class="lang-sh"><span class="hljs-comment"># Label your nodes with the accelerator type they have.</span>
kubectl label nodes <node-with-k80> accelerator=nvidia-tesla-k80
kubectl label nodes <node-with-p100> accelerator=nvidia-tesla-p100
</code></pre>
<p>然后，在创建 Pod 时设置 Node Affinity：</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> cuda-vector-add
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  restartPolicy:</span> OnFailure
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">    - name:</span> cuda-vector-add
      <span class="hljs-comment"># https://github.com/kubernetes/kubernetes/blob/v1.7.11/test/images/nvidia-cuda/Dockerfile</span>
<span class="hljs-attr">      image:</span> <span class="hljs-string">"k8s.gcr.io/cuda-vector-add:v0.1"</span>
<span class="hljs-attr">      resources:</span>
<span class="hljs-attr">        limits:</span>
          nvidia.com/gpu: <span class="hljs-number">1</span>
<span class="hljs-attr">  nodeSelector:</span>
<span class="hljs-attr">    accelerator:</span> nvidia-tesla-p100 <span class="hljs-comment"># or nvidia-tesla-k80 etc.</span>
</code></pre>
<h2 id="使用-cuda-库">使用 CUDA 库</h2>
<p>NVIDIA Cuda Toolkit 和 cuDNN 等需要预先安装在所有 Node 上。为了访问 <code>/usr/lib/nvidia-375</code>，需要将 CUDA 库以 hostPath volume 的形式传给容器：</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> batch/v1
<span class="hljs-attr">kind:</span> Job
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> nvidia-smi
<span class="hljs-attr">  labels:</span>
<span class="hljs-attr">    name:</span> nvidia-smi
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  template:</span>
<span class="hljs-attr">    metadata:</span>
<span class="hljs-attr">      labels:</span>
<span class="hljs-attr">        name:</span> nvidia-smi
<span class="hljs-attr">    spec:</span>
<span class="hljs-attr">      containers:</span>
<span class="hljs-attr">      - name:</span> nvidia-smi
<span class="hljs-attr">        image:</span> nvidia/cuda
<span class="hljs-attr">        command:</span> [<span class="hljs-string">"nvidia-smi"</span>]
<span class="hljs-attr">        imagePullPolicy:</span> IfNotPresent
<span class="hljs-attr">        resources:</span>
<span class="hljs-attr">          limits:</span>
            alpha.kubernetes.io/nvidia-gpu: <span class="hljs-number">1</span>
<span class="hljs-attr">        volumeMounts:</span>
<span class="hljs-attr">        - mountPath:</span> /usr/local/nvidia/bin
<span class="hljs-attr">          name:</span> bin
<span class="hljs-attr">        - mountPath:</span> /usr/lib/nvidia
<span class="hljs-attr">          name:</span> lib
<span class="hljs-attr">      volumes:</span>
<span class="hljs-attr">        - name:</span> bin
<span class="hljs-attr">          hostPath:</span>
<span class="hljs-attr">            path:</span> /usr/lib/nvidia<span class="hljs-bullet">-375</span>/bin
<span class="hljs-attr">        - name:</span> lib
<span class="hljs-attr">          hostPath:</span>
<span class="hljs-attr">            path:</span> /usr/lib/nvidia<span class="hljs-bullet">-375</span>
<span class="hljs-attr">      restartPolicy:</span> Never
</code></pre>
<pre><code class="lang-sh">$ kubectl create <span class="hljs-_">-f</span> job.yaml
job <span class="hljs-string">"nvidia-smi"</span> created

$ kubectl get job
NAME         DESIRED   SUCCESSFUL   AGE
nvidia-smi   1         1            14m

$ kubectl get pod <span class="hljs-_">-a</span>
NAME               READY     STATUS      RESTARTS   AGE
nvidia-smi-kwd2m   0/1       Completed   0          14m

$ kubectl logs nvidia-smi-kwd2m
Fri Jun 16 19:49:53 2017
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 375.66                 Driver Version: 375.66                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla K80           Off  | 0000:00:04.0     Off |                    0 |
| N/A   74C    P0    80W / 149W |      0MiB / 11439MiB |    100%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
</code></pre>
<h2 id="附录：cuda-安装方法">附录：CUDA 安装方法</h2>
<p>安装 CUDA：</p>
<pre><code class="lang-sh"><span class="hljs-comment"># Check for CUDA and try to install.</span>
<span class="hljs-keyword">if</span> ! dpkg-query -W cuda; <span class="hljs-keyword">then</span>
  <span class="hljs-comment"># The 16.04 installer works with 16.10.</span>
  curl -O http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/cuda-repo-ubuntu1604_8.0.61-1_amd64.deb
  dpkg -i ./cuda-repo-ubuntu1604_8.0.61-1_amd64.deb
  apt-get update
  apt-get install cuda -y
<span class="hljs-keyword">fi</span>
</code></pre>
<p>安装 cuDNN：</p>
<p>首先到网站 <a href="https://developer.nvidia.com/cudnn" target="_blank">https://developer.nvidia.com/cudnn</a> 注册，并下载 cuDNN v5.1，然后运行命令安装</p>
<pre><code class="lang-sh">tar zxvf cudnn-8.0-linux-x64-v5.1.tgz
ln <span class="hljs-_">-s</span> /usr/<span class="hljs-built_in">local</span>/cuda-8.0 /usr/<span class="hljs-built_in">local</span>/cuda
sudo cp -P cuda/include/cudnn.h /usr/<span class="hljs-built_in">local</span>/cuda/include
sudo cp -P cuda/lib64/libcudnn* /usr/<span class="hljs-built_in">local</span>/cuda/lib64
sudo chmod a+r /usr/<span class="hljs-built_in">local</span>/cuda/include/cudnn.h /usr/<span class="hljs-built_in">local</span>/cuda/lib64/libcudnn*
</code></pre>
<p>安装完成后，可以运行 nvidia-smi 查看 GPU 设备的状态</p>
<pre><code class="lang-sh">$ nvidia-smi
Fri Jun 16 19:33:35 2017
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 375.66                 Driver Version: 375.66                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla K80           Off  | 0000:00:04.0     Off |                    0 |
| N/A   74C    P0    80W / 149W |      0MiB / 11439MiB |    100%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
</code></pre>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://github.com/NVIDIA/k8s-device-plugin" target="_blank">NVIDIA/k8s-device-plugin</a></li>
<li><a href="https://kubernetes.io/docs/tasks/manage-gpus/scheduling-gpus/" target="_blank">Schedule GPUs on Kubernetes</a></li>
<li><a href="https://github.com/GoogleCloudPlatform/container-engine-accelerators" target="_blank">GoogleCloudPlatform/container-engine-accelerators</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="cluster-autoscaler" class="level3">ClusterAutoscaler</h1>
<p>Cluster AutoScaler 是一个自动扩展和收缩 Kubernetes 集群 Node 的扩展。当集群容量不足时，它会自动去 Cloud Provider （支持 GCE、GKE、Azure、AKS、AWS 等）创建新的 Node，而在 Node 长时间（超过 10 分钟）资源利用率很低时（低于 50%）自动将其删除以节省开支。</p>
<p>Cluster AutoScaler 独立于 Kubernetes 主代码库，维护在 <a href="https://github.com/kubernetes/autoscaler" target="_blank">https://github.com/kubernetes/autoscaler</a>。</p>
<h2 id="部署">部署</h2>
<p>Cluster AutoScaler v1.0+ 可以基于 Docker 镜像 <code>gcr.io/google_containers/cluster-autoscaler:v1.3.0</code> 来部署，详细的部署步骤可以参考</p>
<ul>
<li>GCE: <a href="https://kubernetes.io/docs/concepts/cluster-administration/cluster-management/" target="_blank">https://kubernetes.io/docs/concepts/cluster-administration/cluster-management/</a></li>
<li>GKE: <a href="https://cloud.google.com/container-engine/docs/cluster-autoscaler" target="_blank">https://cloud.google.com/container-engine/docs/cluster-autoscaler</a></li>
<li>AWS: <a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md" target="_blank">https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md</a></li>
<li>Azure: <a href="https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler/cloudprovider/azure" target="_blank">https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler/cloudprovider/azure</a></li>
</ul>
<p>注意，在开启 RBAC 的集群中创建 <a href="https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/rbac/cluster-autoscaler/cluster-autoscaler-rbac.yaml" target="_blank">cluster-autoscaler ClusterRole</a>。</p>
<h2 id="工作原理">工作原理</h2>
<p>Cluster AutoScaler 定期（默认间隔 10s）检测是否有充足的资源来调度新创建的 Pod，当资源不足时会调用 Cloud Provider 创建新的 Node。</p>
<p><img src="images/15084813044270.png" alt=""/></p>
<p>为了自动创建和初始化 Node，Cluster Autoscaler 要求 Node 必须属于某个 Node Group，比如</p>
<ul>
<li>GCE/GKE 中的 Managed instance groups（MIG）</li>
<li>AWS 中的 Autoscaling Groups</li>
<li>Azure 中的 Scale Sets 和 Availability Sets</li>
</ul>
<p>当集群中有多个 Node Group 时，可以通过 <code>--expander=<option></code> 选项配置选择 Node Group 的策咯，支持如下四种方式</p>
<ul>
<li>random：随机选择</li>
<li>most-pods：选择容量最大（可以创建最多 Pod）的 Node Group</li>
<li>least-waste：以最小浪费原则选择，即选择有最少可用资源的 Node Group</li>
<li>price：选择最便宜的 Node Group（仅支持 GCE 和 GKE）</li>
</ul>
<p>目前，Cluster Autoscaler 可以保证</p>
<ul>
<li>小集群（小于 100 个 Node）可以在不超过 30 秒内完成扩展（平均 5 秒）</li>
<li>大集群（100-1000 个 Node）可以在不超过 60 秒内完成扩展（平均 15 秒）</li>
</ul>
<p>Cluster AutoScaler 也会定期（默认间隔 10s）自动监测 Node 的资源使用情况，当一个 Node 长时间（超过 10 分钟其期间没有执行任何扩展操作）资源利用率都很低时（低于 50%）自动将其所在虚拟机从云服务商中删除（注意删除时会有 1 分钟的 graceful termination 时间）。此时，原来的 Pod 会自动调度到其他 Node 上面（通过 Deployment、StatefulSet 等控制器）。</p>
<p><img src="images/15084813160226.png" alt=""/></p>
<p>注意，Cluster Autoscaler 仅根据 Pod 的调度情况和 Node 的整体资源使用清空来增删 Node，跟 Pod 或 Node 的资源度量（metrics）没有直接关系。</p>
<p>用户在启动 Cluster AutoScaler 时可以配置 Node 数量的范围（包括最大 Node 数和最小 Node 数）。</p>
<p>在使用 Cluster AutoScaler 时需要注意：</p>
<ul>
<li>由于在删除 Node 时会发生 Pod 重新调度的情况，所以应用必须可以容忍重新调度和短时的中断（比如使用多副本的 Deployment）</li>
<li>当 Node 上面的 <a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#what-types-of-pods-can-prevent-ca-from-removing-a-node" target="_blank">Pods 满足下面的条件之一</a> 时，Node 不会删除<ul>
<li>Pod 配置了 PodDisruptionBudget (PDB)</li>
<li>kube-system Pod 默认不在 Node 上运行或者未配置 PDB</li>
<li>Pod 不是通过 deployment, replica set, job, stateful set 等控制器创建的</li>
<li>Pod 使用了本地存储</li>
<li>其他原因导致的 Pod 无法重新调度，如资源不足，其他 Node 无法满足 NodeSelector 或 Affinity 等</li>
</ul>
</li>
</ul>
<h2 id="最佳实践">最佳实践</h2>
<ul>
<li>Cluster AutoScaler 可以和 Horizontal Pod Autoscaler（HPA）配合使用</li>
<li>不要手动修改 Node 配置，保证集群内的所有 Node 有相同的配置并属于同一个 Node 组</li>
<li>运行 Pod 时指定资源请求</li>
<li>必要时使用 PodDisruptionBudgets 阻止 Pod 被误删除</li>
<li>确保云服务商的配额充足</li>
<li>Cluster AutoScaler <strong>与云服务商提供的 Node 自动扩展功能以及基于 CPU 利用率的 Node 自动扩展机制冲突，不要同时启用</strong></li>
</ul>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://github.com/kubernetes/autoscaler" target="_blank">Kubernetes Autoscaler</a></li>
<li><a href="http://blog.spotinst.com/2017/06/14/k8-autoscaler-support/" target="_blank">Kubernetes Cluster AutoScaler Support</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="ip-masq-agent" class="level3">ip-masq-agent</h1>
<p><a href="https://github.com/kubernetes-incubator/ip-masq-agent" target="_blank">ip-masq-agent</a> 是一个用来管理 IP 伪装的扩展，即管理节点中 IP 网段的 SNAT 规则。</p>
<p>ip-masq-agent 配置 iptables 规则，以便将流量发送到集群节点之外的目标时处理 IP 伪装。默认情况下，RFC 1918 定一个的三个私有 IP 范围是非伪装网段，即 10.0.0.0/8、172.16.0.0/12 和 192.168.0.0/16。另外，链接本地地址（169.254.0.0/16）也被视为非伪装网段。</p>
<p><img src="assets/image-20181014212528267.png" alt="image-20181014212528267"/></p>
<h2 id="部署方法">部署方法</h2>
<p>首先，标记要运行 ip-masq-agent 的 Node</p>
<pre><code class="lang-sh">kubectl label nodes my-node beta.kubernetes.io/masq-agent-ds-ready=<span class="hljs-literal">true</span>
</code></pre>
<p>然后部署 ip-masq-agent：</p>
<pre><code class="lang-sh">kubectl create <span class="hljs-_">-f</span> https://raw.githubusercontent.com/kubernetes-incubator/ip-masq-agent/master/ip-masq-agent.yaml
</code></pre>
<p>部署好，查看 iptables 规则，可以发现</p>
<pre><code class="lang-sh">iptables -t nat -L IP-MASQ-AGENT
RETURN     all  --  anywhere             169.254.0.0/16       /* ip-masq-agent: cluster-local traffic should not be subject to MASQUERADE */ ADDRTYPE match dst-type !LOCAL
RETURN     all  --  anywhere             10.0.0.0/8           /* ip-masq-agent: cluster-local traffic should not be subject to MASQUERADE */ ADDRTYPE match dst-type !LOCAL
RETURN     all  --  anywhere             172.16.0.0/12        /* ip-masq-agent: cluster-local traffic should not be subject to MASQUERADE */ ADDRTYPE match dst-type !LOCAL
RETURN     all  --  anywhere             192.168.0.0/16       /* ip-masq-agent: cluster-local traffic should not be subject to MASQUERADE */ ADDRTYPE match dst-type !LOCAL
MASQUERADE  all  --  anywhere             anywhere             /* ip-masq-agent: outbound traffic should be subject to MASQUERADE (this match must come after cluster-local CIDR matches) */ ADDRTYPE match dst-type !LOCAL
</code></pre>
<h2 id="使用方法">使用方法</h2>
<p>自定义 SNAT 网段的方法：</p>
<pre><code class="lang-sh">cat >config <<EOF
nonMasqueradeCIDRs:
  - 10.0.0.0/8
resyncInterval: 60s
EOF

kubectl create configmap ip-masq-agent --from-file=config --namespace=kube-system
</code></pre>
<p>这样，查看 iptables 规则可以发现</p>
<pre><code class="lang-sh">$ iptables -t nat -L IP-MASQ-AGENT
Chain IP-MASQ-AGENT (1 references)
target     prot opt <span class="hljs-built_in">source</span>               destination         
RETURN     all  --  anywhere             169.254.0.0/16       /* ip-masq-agent: cluster-local traffic should not be subject to MASQUERADE */ ADDRTYPE match dst-type !LOCAL
RETURN     all  --  anywhere             10.0.0.0/8           /* ip-masq-agent: cluster-local
MASQUERADE  all  --  anywhere             anywhere             /* ip-masq-agent: outbound traffic should be subject to MASQUERADE (this match must come after cluster-local CIDR matches) */ ADDRTYPE match dst-type !LOCAL
</code></pre>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="kubernetes-the-hard-way" class="level2">Kubernetes-The-Hard-Way</h1>
<p>翻译注：本部分翻译自 <a href="https://github.com/kelseyhightower/kubernetes-the-hard-way" target="_blank">Kubernetes The Hard Way</a>，译者 <a href="https://github.com/kweisamx" target="_blank">@kweisamx</a> 和 <a href="https://github.com/feiskyer" target="_blank">@feiskyer</a>。该教程指引用户在 <a href="https://cloud.google.com" target="_blank">Google Cloud Platform</a> 上面一步步搭建一个高可用的 Kubernetes 集群。</p>
<p>如果你正在使用 <a href="https://azure.microsoft.com" target="_blank">Microsoft Azure</a>，那么请参考 <a href="https://github.com/ivanfioravanti/kubernetes-the-hard-way-on-azure" target="_blank">kubernetes-the-hard-way-on-azure</a> 在 Azure 上面搭建 Kubernetes 集群。</p>
<p>如有翻译不好的地方或文字上的错误, 欢迎提出 <a href="https://github.com/feiskyer/kubernetes-handbook" target="_blank">Issue</a> 或是 <a href="https://github.com/feiskyer/kubernetes-handbook" target="_blank">PR</a>。</p>
<hr/>
<p>本教程将带领你一步步配置和部署一套高可用的 Kubernetes 集群。它不适用于想要一键自动化部署 Kubernetes 集群的人。如果你想要一键自动化部署，请参考 <a href="https://cloud.google.com/container-engine" target="_blank">Google Container Engine</a> 或 <a href="http://kubernetes.io/docs/getting-started-guides/" target="_blank">Getting Started Guides</a>。</p>
<p>Kubernetes The Hard Way 的主要目的是学习, 也就是说它会花很多时间来保障读者可以真正理解搭建 Kubernetes 的每个步骤。</p>
<blockquote>
<p>使用该教程部署的集群不应该直接视为生产环境可用，并且也可能无法获得 Kubernetes 社区的许多支持，但这都不影响你想真正了解 Kubernetes 的决心！</p>
</blockquote>
<hr/>
<h2 id="目标读者">目标读者</h2>
<p>该教程的目标是给那些计划要将 Kubernetes 应用到生产环境的人, 并想了解每个有关 Kubernetes 的环节以及他们如何运作的。</p>
<h2 id="集群版本">集群版本</h2>
<p>Kubernetes The Hard Way 将引导你建立高可用的 Kubernetes 集群, 包括每个组件之间的加密以及 RBAC 认证</p>
<ul>
<li><a href="https://github.com/kubernetes/kubernetes" target="_blank">Kubernetes</a> 1.12.0</li>
<li><a href="https://github.com/containerd/containerd" target="_blank">Containerd Container Runtime</a> 1.2.0-rc0</li>
<li><a href="https://github.com/containernetworking/cni" target="_blank">CNI Container Networking</a> 0.6.0</li>
<li><a href="https://github.com/google/gvisor" target="_blank">gVisor</a> 50c283b9f56bb7200938d9e207355f05f79f0d17</li>
<li><a href="https://github.com/coreos/etcd" target="_blank">etcd</a> 3.3.9</li>
<li><a href="https://github.com/coredns/coredns" target="_blank">CoreDNS</a> v1.2.2</li>
</ul>
<h2 id="实验步骤">实验步骤</h2>
<p>这份教程假设你已经创建并配置好了 <a href="https://cloud.google.com" target="_blank">Google Cloud Platform</a> 账户。该教程只是将 GCP 作为最基础的架构，教程的内容也同样适用于其他的平台。</p>
<ul>
<li><a href="01-prerequisites.html">准备部署环境</a></li>
<li><a href="02-client-tools.html">安装必要工具</a></li>
<li><a href="03-compute-resources.html">创建计算资源</a></li>
<li><a href="04-certificate-authority.html">配置创建证书</a></li>
<li><a href="05-kubernetes-configuration-files.html">配置生成配置</a></li>
<li><a href="06-data-encryption-keys.html">配置生成密钥</a></li>
<li><a href="07-bootstrapping-etcd.html">部署Etcd群集</a></li>
<li><a href="08-bootstrapping-kubernetes-controllers.html">部署控制节点</a></li>
<li><a href="09-bootstrapping-kubernetes-workers.html">部署计算节点</a></li>
<li><a href="10-configuring-kubectl.html">配置Kubectl</a></li>
<li><a href="11-pod-network-routes.html">配置网络路由</a></li>
<li><a href="12-dns-addon.html">部署DNS扩展</a></li>
<li><a href="13-smoke-test.html">烟雾测试</a></li>
<li><a href="14-cleanup.html">删除集群</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="准备环境" class="level3">准备部署环境</h1>
<h2 id="google-cloud-platform">Google Cloud Platform</h2>
<p>该指南使用 <a href="https://cloud.google.com/" target="_blank">Google Cloud Platform</a> 作为 kubernetes 集群的环境平台。<a href="https://cloud.google.com/free/" target="_blank">注册</a> 即可获得 300 美元的试用金。</p>
<p><a href="https://cloud.google.com/products/calculator/#id=78df6ced-9c50-48f8-a670-bc5003f2ddaa" target="_blank">估计</a> 完成教学的花费金额: 每小时 0.22 美元 (每天 5.39 美元).</p>
<blockquote>
<p>注意：该教程所需要的计算资源会超出 GCP 免费额度。</p>
</blockquote>
<h2 id="google-cloud-platform-sdk">Google Cloud Platform SDK</h2>
<h3 id="安装-google-cloud-sdk">安装 Google Cloud SDK</h3>
<p>按照 Google Cloud SDK <a href="https://cloud.google.com/sdk/" target="_blank">文档</a> 的步骤去安装并设置 gcloud` 命令。验证 Google Cloud SDK 版本为 218.0.0 或更高:</p>
<pre><code class="lang-sh">gcloud version
</code></pre>
<h3 id="设置默认-region-和-zone">设置默认 Region 和 Zone</h3>
<p>本指南假设你的默认 Region 和 Zone 已经设置好了。如果你第一次使用 <code>gcloud</code> 指令工具, <code>init</code> 是一个最简单的设定方式:</p>
<pre><code class="lang-sh">gcloud init
</code></pre>
<p>或者，执行下面的命令手动设定 default compute region:</p>
<pre><code class="lang-sh">gcloud config <span class="hljs-built_in">set</span> compute/region us-west1
</code></pre>
<p>手动设定 compute zone</p>
<pre><code class="lang-sh">gcloud config <span class="hljs-built_in">set</span> compute/zone us-west1-c
</code></pre>
<blockquote>
<p>使用 <code>gcloud compute zones list</code> 指令来查询其他的 region 和 zone。</p>
</blockquote>
<h2 id="使用-tmux-并行执行命令">使用 tmux 并行执行命令</h2>
<p><a href="https://github.com/tmux/tmux/wiki" target="_blank">tmux</a> 可以用来在多个虚拟机中并行执行命令。该教程中的某些步骤需要在多台虚拟机中操作，此时可以考虑使用 tmux 来加速执行过程。</p>
<blockquote>
<p>tmux 是可选的，不是该教程的必要工具。</p>
</blockquote>
<p><img src="images/tmux-screenshot.png" alt=""/></p>
<blockquote>
<p>开启 tmux 同步的方法：按下 <code>ctrb+b</code> 和 <code>shift</code>，接着输入 <code>set synchronize-panes on</code>。关闭同步可以输入 <code>set synchronize-panes off</code>。</p>
</blockquote>
<p>下一步: <a href="02-client-tools.html">安装命令行工具</a></p>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="安装命令行工具" class="level3">安装必要工具</h1>
<p>本次实验你将会安装一些实用的命令行工具, 用来完成这份指南，这包括 <a href="https://github.com/cloudflare/cfssl" target="_blank">cfssl</a>、<a href="https://github.com/cloudflare/cfssl" target="_blank">cfssljson</a> 以及 <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl" target="_blank">kubectl</a>。</p>
<h2 id="安装-cfssl">安装 CFSSL</h2>
<p>从 <a href="https://pkg.cfssl.org" target="_blank">cfssl 网站</a> 下载 <code>cfssl</code> 和 <code>cfssljson</code> 并安装：</p>
<h3 id="os-x">OS X</h3>
<pre><code class="lang-sh">curl -o cfssl https://pkg.cfssl.org/R1.2/cfssl_darwin-amd64
curl -o cfssljson https://pkg.cfssl.org/R1.2/cfssljson_darwin-amd64
chmod +x cfssl cfssljson
sudo mv cfssl cfssljson /usr/<span class="hljs-built_in">local</span>/bin/
</code></pre>
<p>或者使用 Homebrew 来安装</p>
<pre><code class="lang-sh">brew install cfssl
</code></pre>
<h3 id="linux">Linux</h3>
<pre><code class="lang-sh">wget -q --show-progress --https-only --timestamping \
  https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 \
  https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64
chmod +x cfssl_linux-amd64 cfssljson_linux-amd64
sudo mv cfssl_linux-amd64 /usr/<span class="hljs-built_in">local</span>/bin/cfssl
sudo mv cfssljson_linux-amd64 /usr/<span class="hljs-built_in">local</span>/bin/cfssljson
</code></pre>
<h3 id="验证">验证</h3>
<p>验证 <code>cfssl</code> 的版本为 1.2.0 或是更高</p>
<pre><code class="lang-sh">cfssl version
</code></pre>
<blockquote>
<p>输出为</p>
</blockquote>
<pre><code class="lang-sh">Version: 1.2.0
Revision: dev
Runtime: go1.6
</code></pre>
<blockquote>
<p>注意：cfssljson 命令行工具没有提供查询版本的方法。</p>
</blockquote>
<h2 id="安装-kubectl">安装 kubectl</h2>
<p><code>kubectl</code> 命令行工具用来与 Kubernetes API Server 交互，可以在 Kubernetes 官方网站下载并安装 <code>kubectl</code>。</p>
<h3 id="os-x">OS X</h3>
<pre><code class="lang-sh">curl -o kubectl https://storage.googleapis.com/kubernetes-release/release/v1.12.0/bin/darwin/amd64/kubectl
chmod +x kubectl
sudo mv kubectl /usr/<span class="hljs-built_in">local</span>/bin/
</code></pre>
<h3 id="linux">Linux</h3>
<pre><code class="lang-sh">wget https://storage.googleapis.com/kubernetes-release/release/v1.12.0/bin/linux/amd64/kubectl
chmod +x kubectl
sudo mv kubectl /usr/<span class="hljs-built_in">local</span>/bin/
</code></pre>
<h3 id="验证">验证</h3>
<p>验证 <code>kubectl</code> 的安装版本为 1.12.0 或是更高</p>
<pre><code class="lang-sh">kubectl version --client
</code></pre>
<blockquote>
<p>输出为</p>
</blockquote>
<pre><code class="lang-sh">Client Version: version.Info{Major:<span class="hljs-string">"1"</span>, Minor:<span class="hljs-string">"12"</span>, GitVersion:<span class="hljs-string">"v1.12.0"</span>, GitCommit:<span class="hljs-string">"0ed33881dc4355495f623c6f22e7dd0b7632b7c0"</span>, GitTreeState:<span class="hljs-string">"clean"</span>, BuildDate:<span class="hljs-string">"2018-09-27T17:05:32Z"</span>, GoVersion:<span class="hljs-string">"go1.10.4"</span>, Compiler:<span class="hljs-string">"gc"</span>, Platform:<span class="hljs-string">"linux/amd64"</span>}
</code></pre>
<p>下一步： <a href="03-compute-resources.html">准备计算资源</a></p>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="准备计算资源" class="level3">创建计算资源</h1>
<p>Kubernetes 需要一些机器去搭建管理 Kubernetes 的控制平台, 也需要一些工作节点（work node）来运行容器。在这个实验中你将会创建一些虚拟机，并利用 GCE <a href="https://cloud.google.com/compute/docs/regions-zones/regions-zones" target="_blank">Compute Zone</a> 来运行安全且高可用的 Kubernetes 集群。</p>
<blockquote>
<p>请确定默认 Compute Zone 和 Region 已按照 <a href="01-prerequisites.html#set-a-default-compute-region-and-zone">事前准备</a> 的设定步骤完成。</p>
</blockquote>
<h2 id="网络">网络</h2>
<p>Kubernetes <a href="https://kubernetes.io/docs/concepts/cluster-administration/networking/#kubernetes-model" target="_blank">网络模型</a> 假设使用扁平网路能让每个容器与节点都可以相互通信。 在这里我们先忽略用于控制容器网络隔离的 Network policies（Network Policies 不在本指南的范围内）。</p>
<h3 id="虚拟私有网络（vpc）">虚拟私有网络（VPC）</h3>
<p>本节将会创建一个专用的 <a href="https://cloud.google.com/compute/docs/networks-and-firewalls#networks" target="_blank">Virtual Private Cloud</a> (VPC) 网络来搭建我们的 Kubernetes 集群。</p>
<p>首先创建一个名为 kubernetes-the-hard-way 的 VPC 网络：</p>
<pre><code class="lang-sh">gcloud compute networks create kubernetes-the-hard-way --mode custom
</code></pre>
<p>为了给 Kubernetes 集群的每个节点分配私有 IP 地址，需要创建一个含有足够大 IP 地址池的子网。 在 <code>kubernetes-the-hard-way</code> VPC 网络中创建 <code>kubernetes</code> 子网：</p>
<pre><code class="lang-sh">gcloud compute networks subnets create kubernetes \
  --network kubernetes-the-hard-way \
  --range 10.240.0.0/24
</code></pre>
<blockquote>
<p><code>10.240.0.0/24</code> IP 地址范围, 可以分配 254 个计算节点。</p>
</blockquote>
<h3 id="防火墙规则">防火墙规则</h3>
<p>创建一个防火墙规则允许内部网路通过所有协议进行通信：</p>
<pre><code class="lang-sh">gcloud compute firewall-rules create kubernetes-the-hard-way-allow-internal \
  --allow tcp,udp,icmp \
  --network kubernetes-the-hard-way \
  --source-ranges 10.240.0.0/24,10.200.0.0/16
</code></pre>
<p>创建一个防火墙规则允许外部 SSH、ICMP 以及 HTTPS 等通信：</p>
<pre><code class="lang-sh">gcloud compute firewall-rules create kubernetes-the-hard-way-allow-external \
  --allow tcp:22,tcp:6443,icmp \
  --network kubernetes-the-hard-way \
  --source-ranges 0.0.0.0/0
</code></pre>
<blockquote>
<p> <a href="https://cloud.google.com/compute/docs/load-balancing/network/" target="_blank">外部负载均衡器</a> 被用来暴露 Kubernetes API Servers 给远端客户端。</p>
</blockquote>
<p>列出在 <code>kubernetes-the-hard-way</code> VPC 网络中的防火墙规则：</p>
<pre><code class="lang-sh">gcloud compute firewall-rules list --filter=<span class="hljs-string">"network:kubernetes-the-hard-way"</span>
</code></pre>
<blockquote>
<p>输出为</p>
</blockquote>
<pre><code class="lang-sh">NAME                                    NETWORK                  DIRECTION  PRIORITY  ALLOW                 DENY
kubernetes-the-hard-way-allow-external  kubernetes-the-hard-way  INGRESS    1000      tcp:22,tcp:6443,icmp
kubernetes-the-hard-way-allow-internal  kubernetes-the-hard-way  INGRESS    1000      tcp,udp,icmp
</code></pre>
<h3 id="kubernetes-公网-ip-地址">Kubernetes 公网 IP 地址</h3>
<p>分配固定的 IP 地址, 被用来连接外部的负载平衡器至 Kubernetes API Servers:</p>
<pre><code class="lang-sh">gcloud compute addresses create kubernetes-the-hard-way \
  --region $(gcloud config get-value compute/region)
</code></pre>
<p>验证 <code>kubernetes-the-hard-way</code> 固定 IP 地址已经在默认的 Compute Region 中创建出来：</p>
<pre><code class="lang-sh">gcloud compute addresses list --filter=<span class="hljs-string">"name=('kubernetes-the-hard-way')"</span>
</code></pre>
<blockquote>
<p>输出为</p>
</blockquote>
<pre><code class="lang-sh">NAME                     REGION    ADDRESS        STATUS
kubernetes-the-hard-way  us-west1  XX.XXX.XXX.XX  RESERVED
</code></pre>
<h2 id="计算实例">计算实例</h2>
<p>本节将会创建基于 <a href="https://www.ubuntu.com/server" target="_blank">Ubuntu Server 18.04</a> 的计算实例，原因是它对 <a href="https://github.com/containerd/containerd" target="_blank">containerd</a> 容器引擎有很好的支持。每个虚拟机将会分配一个私有 IP 地址用以简化 Kubernetes 的设置。</p>
<h3 id="kubernetes-控制节点">Kubernetes 控制节点</h3>
<p>建立三个计算节点用以配置 Kubernetes 控制平面：</p>
<pre><code class="lang-sh"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> 0 1 2; <span class="hljs-keyword">do</span>
  gcloud compute instances create controller-<span class="hljs-variable">${i}</span> \
    --async \
    --boot-disk-size 200GB \
    --can-ip-forward \
    --image-family ubuntu-1804-lts \
    --image-project ubuntu-os-cloud \
    --machine-type n1-standard-1 \
    --private-network-ip 10.240.0.1<span class="hljs-variable">${i}</span> \
    --scopes compute-rw,storage-ro,service-management,service-control,logging-write,monitoring \
    --subnet kubernetes \
    --tags kubernetes-the-hard-way,controller
<span class="hljs-keyword">done</span>
</code></pre>
<h3 id="kubernetes-工作节点">Kubernetes 工作节点</h3>
<p>每台 worker 节点都需要从 Kubernetes 集群 CIDR 范围中分配一个 Pod 子网。 Pod 子网分配将会在之后的容器网路章节做练习。在 worker 节点内部可以通过 <code>pod-cidr</code> 元数据来获得 Pod 子网的分配结果。</p>
<blockquote>
<p>Kubernetes 集群 CIDR 的范围可以通过 Controller Manager 的 <code>--cluster-cidr</code> 参数来设定。在本次教学中我们会设置为 <code>10.200.0.0/16</code>，它支持 254 个子网。</p>
</blockquote>
<p>创建三个计算节点用来作为 Kubernetes Worker 节点：</p>
<pre><code class="lang-sh"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> 0 1 2; <span class="hljs-keyword">do</span>
  gcloud compute instances create worker-<span class="hljs-variable">${i}</span> \
    --async \
    --boot-disk-size 200GB \
    --can-ip-forward \
    --image-family ubuntu-1804-lts \
    --image-project ubuntu-os-cloud \
    --machine-type n1-standard-1 \
    --metadata pod-cidr=10.200.<span class="hljs-variable">${i}</span>.0/24 \
    --private-network-ip 10.240.0.2<span class="hljs-variable">${i}</span> \
    --scopes compute-rw,storage-ro,service-management,service-control,logging-write,monitoring \
    --subnet kubernetes \
    --tags kubernetes-the-hard-way,worker
<span class="hljs-keyword">done</span>
</code></pre>
<h3 id="验证">验证</h3>
<p>列出所有在默认 Compute Zone 的计算节点：</p>
<pre><code class="lang-sh">gcloud compute instances list
</code></pre>
<p>输出为：</p>
<pre><code class="lang-sh">NAME          ZONE        MACHINE_TYPE   PREEMPTIBLE  INTERNAL_IP  EXTERNAL_IP     STATUS
controller-0  us-west1-c  n1-standard-1               10.240.0.10  XX.XXX.XXX.XXX  RUNNING
controller-1  us-west1-c  n1-standard-1               10.240.0.11  XX.XXX.X.XX     RUNNING
controller-2  us-west1-c  n1-standard-1               10.240.0.12  XX.XXX.XXX.XX   RUNNING
worker-0      us-west1-c  n1-standard-1               10.240.0.20  XXX.XXX.XXX.XX  RUNNING
worker-1      us-west1-c  n1-standard-1               10.240.0.21  XX.XXX.XX.XXX   RUNNING
worker-2      us-west1-c  n1-standard-1               10.240.0.22  XXX.XXX.XX.XX   RUNNING
</code></pre>
<h2 id="配置-ssh">配置 SSH</h2>
<p>本教程使用 SSH 来配置控制节点和工作节点。当通过 <code>gcloud compute ssh</code> 第一次连接计算实例时，会自动生成 SSH 证书，并<a href="https://cloud.google.com/compute/docs/instances/connecting-to-instance" target="_blank">保存在项目或者实例的元数据中</a>。</p>
<p>验证 <code>controller-0</code> 的 SSH 访问</p>
<pre><code class="lang-sh">gcloud compute ssh controller-0
</code></pre>
<p>因为这是第一次访问，此时会生成 SSH 证书。按照提示操作</p>
<pre><code class="lang-sh">WARNING: The public SSH key file <span class="hljs-keyword">for</span> gcloud does not exist.
WARNING: The private SSH key file <span class="hljs-keyword">for</span> gcloud does not exist.
WARNING: You <span class="hljs-keyword">do</span> not have an SSH key <span class="hljs-keyword">for</span> gcloud.
WARNING: SSH keygen will be executed to generate a key.
Generating public/private rsa key pair.
Enter passphrase (empty <span class="hljs-keyword">for</span> no passphrase):
Enter same passphrase again:
</code></pre>
<p>此时，SSH 证书回保存在你的项目中：</p>
<pre><code class="lang-sh">Your identification has been saved <span class="hljs-keyword">in</span> /home/<span class="hljs-variable">$USER</span>/.ssh/google_compute_engine.
Your public key has been saved <span class="hljs-keyword">in</span> /home/<span class="hljs-variable">$USER</span>/.ssh/google_compute_engine.pub.
The key fingerprint is:
SHA256:nz1i8jHmgQuGt+WscqP5SeIaSy5wyIJeL71MuV+QruE <span class="hljs-variable">$USER</span>@<span class="hljs-variable">$HOSTNAME</span>
The key<span class="hljs-string">'s randomart image is:
+---[RSA 2048]----+
|                 |
|                 |
|                 |
|        .        |
|o.     oS        |
|=... .o .o o     |
|+.+ =+=.+.X o    |
|.+ ==O*B.B = .   |
| .+.=EB++ o      |
+----[SHA256]-----+
Updating project ssh metadata...-Updated [https://www.googleapis.com/compute/v1/projects/$PROJECT_ID].
Updating project ssh metadata...done.
Waiting for SSH key to propagate.
</span></code></pre>
<p>SSH 证书更新后，你就可以登录到 <code>controller-0</code> 实例中了：</p>
<pre><code class="lang-sh">Welcome to Ubuntu 18.04 LTS (GNU/Linux 4.15.0-1006-gcp x86_64)

...

Last login: Sun May 13 14:34:27 2018 from XX.XXX.XXX.XX
</code></pre>
<p>下一步：<a href="04-certificate-authority.html">配置 CA 和创建 TLS 证书</a></p>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="配置-ca-并创建-tls-证书" class="level3">配置创建证书</h1>
<p>我们将使用 CloudFlare's PKI 工具 <a href="https://github.com/cloudflare/cfssl" target="_blank">cfssl</a> 来配置 <a href="https://en.wikipedia.org/wiki/Public_key_infrastructure" target="_blank">PKI Infrastructure</a>，然后使用它去创建 Certificate Authority（CA）， 并为 etcd、kube-apiserver、kubelet 以及 kube-proxy 创建 TLS 证书。</p>
<h2 id="certificate-authority">Certificate Authority</h2>
<p>本节创建用于生成其他 TLS 证书的 Certificate Authority。</p>
<p>新建 CA 配置文件</p>
<pre><code class="lang-sh">cat > ca-config.json <<EOF
{
  <span class="hljs-string">"signing"</span>: {
    <span class="hljs-string">"default"</span>: {
      <span class="hljs-string">"expiry"</span>: <span class="hljs-string">"8760h"</span>
    },
    <span class="hljs-string">"profiles"</span>: {
      <span class="hljs-string">"kubernetes"</span>: {
        <span class="hljs-string">"usages"</span>: [<span class="hljs-string">"signing"</span>, <span class="hljs-string">"key encipherment"</span>, <span class="hljs-string">"server auth"</span>, <span class="hljs-string">"client auth"</span>],
        <span class="hljs-string">"expiry"</span>: <span class="hljs-string">"8760h"</span>
      }
    }
  }
}
EOF
</code></pre>
<p>新建 CA 凭证簽发请求文件:</p>
<pre><code class="lang-sh">cat > ca-csr.json <<EOF
{
  <span class="hljs-string">"CN"</span>: <span class="hljs-string">"Kubernetes"</span>,
  <span class="hljs-string">"key"</span>: {
    <span class="hljs-string">"algo"</span>: <span class="hljs-string">"rsa"</span>,
    <span class="hljs-string">"size"</span>: 2048
  },
  <span class="hljs-string">"names"</span>: [
    {
      <span class="hljs-string">"C"</span>: <span class="hljs-string">"US"</span>,
      <span class="hljs-string">"L"</span>: <span class="hljs-string">"Portland"</span>,
      <span class="hljs-string">"O"</span>: <span class="hljs-string">"Kubernetes"</span>,
      <span class="hljs-string">"OU"</span>: <span class="hljs-string">"CA"</span>,
      <span class="hljs-string">"ST"</span>: <span class="hljs-string">"Oregon"</span>
    }
  ]
}
EOF
</code></pre>
<p>生成 CA 凭证和私钥:</p>
<pre><code class="lang-sh">cfssl gencert -initca ca-csr.json | cfssljson -bare ca
</code></pre>
<p>结果将生成以下两个文件：</p>
<pre><code class="lang-sh">ca-key.pem
ca.pem
</code></pre>
<h2 id="client-与-server-凭证">client 与 server 凭证</h2>
<p>本节将创建用于 Kubernetes 组件的 client 与 server 凭证，以及一个用于 Kubernetes admin 用户的 client 凭证。</p>
<h3 id="admin-客户端凭证">Admin 客户端凭证</h3>
<p>创建 <code>admin</code> client 凭证簽发请求文件:</p>
<pre><code class="lang-sh">cat > admin-csr.json <<EOF
{
  <span class="hljs-string">"CN"</span>: <span class="hljs-string">"admin"</span>,
  <span class="hljs-string">"key"</span>: {
    <span class="hljs-string">"algo"</span>: <span class="hljs-string">"rsa"</span>,
    <span class="hljs-string">"size"</span>: 2048
  },
  <span class="hljs-string">"names"</span>: [
    {
      <span class="hljs-string">"C"</span>: <span class="hljs-string">"US"</span>,
      <span class="hljs-string">"L"</span>: <span class="hljs-string">"Portland"</span>,
      <span class="hljs-string">"O"</span>: <span class="hljs-string">"system:masters"</span>,
      <span class="hljs-string">"OU"</span>: <span class="hljs-string">"Kubernetes The Hard Way"</span>,
      <span class="hljs-string">"ST"</span>: <span class="hljs-string">"Oregon"</span>
    }
  ]
}
EOF
</code></pre>
<p>创建 <code>admin</code> client 凭证和私钥:</p>
<pre><code class="lang-sh">cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  admin-csr.json | cfssljson -bare admin
</code></pre>
<p>结果将生成以下两个文件</p>
<pre><code class="lang-sh">admin-key.pem
admin.pem
</code></pre>
<h3 id="kubelet-客户端凭证">Kubelet 客户端凭证</h3>
<p>Kubernetes 使用 <a href="https://kubernetes.io/docs/admin/authorization/node/" target="_blank">special-purpose authorization mode</a>（被称作 Node Authorizer）授权来自 <a href="https://kubernetes.io/docs/concepts/overview/components/#kubelet" target="_blank">Kubelet</a>
的 API 请求。为了通过 Node Authorizer 的授权, Kubelet 必须使用一个署名为 <code>system:node:<nodeName></code> 的凭证来证明它属于 <code>system:nodes</code> 用户组。本节将会给每台 worker 节点创建符合 Node Authorizer 要求的凭证。</p>
<p>给每台 worker 节点创建凭证和私钥：</p>
<pre><code class="lang-sh"><span class="hljs-keyword">for</span> instance <span class="hljs-keyword">in</span> worker-0 worker-1 worker-2; <span class="hljs-keyword">do</span>
cat > <span class="hljs-variable">${instance}</span>-csr.json <<EOF
{
  <span class="hljs-string">"CN"</span>: <span class="hljs-string">"system:node:<span class="hljs-variable">${instance}</span>"</span>,
  <span class="hljs-string">"key"</span>: {
    <span class="hljs-string">"algo"</span>: <span class="hljs-string">"rsa"</span>,
    <span class="hljs-string">"size"</span>: 2048
  },
  <span class="hljs-string">"names"</span>: [
    {
      <span class="hljs-string">"C"</span>: <span class="hljs-string">"US"</span>,
      <span class="hljs-string">"L"</span>: <span class="hljs-string">"Portland"</span>,
      <span class="hljs-string">"O"</span>: <span class="hljs-string">"system:nodes"</span>,
      <span class="hljs-string">"OU"</span>: <span class="hljs-string">"Kubernetes The Hard Way"</span>,
      <span class="hljs-string">"ST"</span>: <span class="hljs-string">"Oregon"</span>
    }
  ]
}
EOF

EXTERNAL_IP=$(gcloud compute instances describe <span class="hljs-variable">${instance}</span> \
  --format <span class="hljs-string">'value(networkInterfaces[0].accessConfigs[0].natIP)'</span>)

INTERNAL_IP=$(gcloud compute instances describe <span class="hljs-variable">${instance}</span> \
  --format <span class="hljs-string">'value(networkInterfaces[0].networkIP)'</span>)

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -hostname=<span class="hljs-variable">${instance}</span>,<span class="hljs-variable">${EXTERNAL_IP}</span>,<span class="hljs-variable">${INTERNAL_IP}</span> \
  -profile=kubernetes \
  <span class="hljs-variable">${instance}</span>-csr.json | cfssljson -bare <span class="hljs-variable">${instance}</span>
<span class="hljs-keyword">done</span>
</code></pre>
<p>结果将产生以下几个文件：</p>
<pre><code class="lang-sh">worker-0-key.pem
worker-0.pem
worker-1-key.pem
worker-1.pem
worker-2-key.pem
worker-2.pem
</code></pre>
<h3 id="kube-controller-manager-客户端凭证">Kube-controller-manager 客户端凭证</h3>
<pre><code class="lang-sh">cat > kube-controller-manager-csr.json <<EOF
{
  <span class="hljs-string">"CN"</span>: <span class="hljs-string">"system:kube-controller-manager"</span>,
  <span class="hljs-string">"key"</span>: {
    <span class="hljs-string">"algo"</span>: <span class="hljs-string">"rsa"</span>,
    <span class="hljs-string">"size"</span>: 2048
  },
  <span class="hljs-string">"names"</span>: [
    {
      <span class="hljs-string">"C"</span>: <span class="hljs-string">"US"</span>,
      <span class="hljs-string">"L"</span>: <span class="hljs-string">"Portland"</span>,
      <span class="hljs-string">"O"</span>: <span class="hljs-string">"system:kube-controller-manager"</span>,
      <span class="hljs-string">"OU"</span>: <span class="hljs-string">"Kubernetes The Hard Way"</span>,
      <span class="hljs-string">"ST"</span>: <span class="hljs-string">"Oregon"</span>
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager
</code></pre>
<p>结果将产生以下几个文件：</p>
<pre><code class="lang-sh">kube-controller-manager-key.pem
kube-controller-manager.pem
</code></pre>
<h3 id="kube-proxy-客户端凭证">Kube-proxy 客户端凭证</h3>
<pre><code class="lang-sh">cat > kube-proxy-csr.json <<EOF
{
  <span class="hljs-string">"CN"</span>: <span class="hljs-string">"system:kube-proxy"</span>,
  <span class="hljs-string">"key"</span>: {
    <span class="hljs-string">"algo"</span>: <span class="hljs-string">"rsa"</span>,
    <span class="hljs-string">"size"</span>: 2048
  },
  <span class="hljs-string">"names"</span>: [
    {
      <span class="hljs-string">"C"</span>: <span class="hljs-string">"US"</span>,
      <span class="hljs-string">"L"</span>: <span class="hljs-string">"Portland"</span>,
      <span class="hljs-string">"O"</span>: <span class="hljs-string">"system:node-proxier"</span>,
      <span class="hljs-string">"OU"</span>: <span class="hljs-string">"Kubernetes The Hard Way"</span>,
      <span class="hljs-string">"ST"</span>: <span class="hljs-string">"Oregon"</span>
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  kube-proxy-csr.json | cfssljson -bare kube-proxy
</code></pre>
<p>结果将产生以下两个文件：</p>
<pre><code class="lang-sh">kube-proxy-key.pem
kube-proxy.pem
</code></pre>
<h3 id="kube-scheduler-证书">kube-scheduler 证书</h3>
<pre><code class="lang-sh">cat > kube-scheduler-csr.json <<EOF
{
  <span class="hljs-string">"CN"</span>: <span class="hljs-string">"system:kube-scheduler"</span>,
  <span class="hljs-string">"key"</span>: {
    <span class="hljs-string">"algo"</span>: <span class="hljs-string">"rsa"</span>,
    <span class="hljs-string">"size"</span>: 2048
  },
  <span class="hljs-string">"names"</span>: [
    {
      <span class="hljs-string">"C"</span>: <span class="hljs-string">"US"</span>,
      <span class="hljs-string">"L"</span>: <span class="hljs-string">"Portland"</span>,
      <span class="hljs-string">"O"</span>: <span class="hljs-string">"system:kube-scheduler"</span>,
      <span class="hljs-string">"OU"</span>: <span class="hljs-string">"Kubernetes The Hard Way"</span>,
      <span class="hljs-string">"ST"</span>: <span class="hljs-string">"Oregon"</span>
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  kube-scheduler-csr.json | cfssljson -bare kube-scheduler
</code></pre>
<p>结果将产生以下两个文件：</p>
<pre><code class="lang-sh">kube-scheduler-key.pem
kube-scheduler.pem
</code></pre>
<h3 id="kubernetes-api-server-证书">Kubernetes API Server 证书</h3>
<p>为了保证客户端与 Kubernetes API 的认证，Kubernetes API Server 凭证 中必需包含 <code>kubernetes-the-hard-way</code> 的静态 IP 地址。</p>
<p>首先查询 <code>kubernetes-the-hard-way</code> 的静态 IP 地址:</p>
<pre><code class="lang-sh">KUBERNETES_PUBLIC_ADDRESS=$(gcloud compute addresses describe kubernetes-the-hard-way \
  --region $(gcloud config get-value compute/region) \
  --format <span class="hljs-string">'value(address)'</span>)
</code></pre>
<p>创建 Kubernetes API Server 凭证簽发请求文件:</p>
<pre><code class="lang-sh">cat > kubernetes-csr.json <<EOF
{
  <span class="hljs-string">"CN"</span>: <span class="hljs-string">"kubernetes"</span>,
  <span class="hljs-string">"key"</span>: {
    <span class="hljs-string">"algo"</span>: <span class="hljs-string">"rsa"</span>,
    <span class="hljs-string">"size"</span>: 2048
  },
  <span class="hljs-string">"names"</span>: [
    {
      <span class="hljs-string">"C"</span>: <span class="hljs-string">"US"</span>,
      <span class="hljs-string">"L"</span>: <span class="hljs-string">"Portland"</span>,
      <span class="hljs-string">"O"</span>: <span class="hljs-string">"Kubernetes"</span>,
      <span class="hljs-string">"OU"</span>: <span class="hljs-string">"Kubernetes The Hard Way"</span>,
      <span class="hljs-string">"ST"</span>: <span class="hljs-string">"Oregon"</span>
    }
  ]
}
EOF
</code></pre>
<p>创建 Kubernetes API Server 凭证与私钥:</p>
<pre><code class="lang-sh">cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -hostname=10.32.0.1,10.240.0.10,10.240.0.11,10.240.0.12,<span class="hljs-variable">${KUBERNETES_PUBLIC_ADDRESS}</span>,127.0.0.1,kubernetes.default \
  -profile=kubernetes \
  kubernetes-csr.json | cfssljson -bare kubernetes
</code></pre>
<p>结果产生以下两个文件:</p>
<pre><code class="lang-sh">kubernetes-key.pem
kubernetes.pem
</code></pre>
<h3 id="service-account-证书">Service Account 证书</h3>
<pre><code class="lang-sh">at > service-account-csr.json <<EOF
{
  <span class="hljs-string">"CN"</span>: <span class="hljs-string">"service-accounts"</span>,
  <span class="hljs-string">"key"</span>: {
    <span class="hljs-string">"algo"</span>: <span class="hljs-string">"rsa"</span>,
    <span class="hljs-string">"size"</span>: 2048
  },
  <span class="hljs-string">"names"</span>: [
    {
      <span class="hljs-string">"C"</span>: <span class="hljs-string">"US"</span>,
      <span class="hljs-string">"L"</span>: <span class="hljs-string">"Portland"</span>,
      <span class="hljs-string">"O"</span>: <span class="hljs-string">"Kubernetes"</span>,
      <span class="hljs-string">"OU"</span>: <span class="hljs-string">"Kubernetes The Hard Way"</span>,
      <span class="hljs-string">"ST"</span>: <span class="hljs-string">"Oregon"</span>
    }
  ]
}
EOF

cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -profile=kubernetes \
  service-account-csr.json | cfssljson -bare service-account
</code></pre>
<p>结果将生成以下两个文件</p>
<pre><code class="lang-sh">service-account-key.pem
service-account.pem
</code></pre>
<h2 id="分发客户端和服务器证书">分发客户端和服务器证书</h2>
<p>将客户端凭证以及私钥复制到每个工作节点上:</p>
<pre><code class="lang-sh"><span class="hljs-keyword">for</span> instance <span class="hljs-keyword">in</span> worker-0 worker-1 worker-2; <span class="hljs-keyword">do</span>
  gcloud compute scp ca.pem <span class="hljs-variable">${instance}</span>-key.pem <span class="hljs-variable">${instance}</span>.pem <span class="hljs-variable">${instance}</span>:~/
<span class="hljs-keyword">done</span>
</code></pre>
<p>将服务器凭证以及私钥复制到每个控制节点上:</p>
<pre><code class="lang-sh"><span class="hljs-keyword">for</span> instance <span class="hljs-keyword">in</span> controller-0 controller-1 controller-2; <span class="hljs-keyword">do</span>
  gcloud compute scp ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem \
    service-account-key.pem service-account.pem <span class="hljs-variable">${instance}</span>:~/
<span class="hljs-keyword">done</span>
</code></pre>
<blockquote>
<p><code>kube-proxy</code>、<code>kube-controller-manager</code>、<code>kube-scheduler</code> 和 <code>kubelet</code> 客户端凭证将会在下一节中用来创建客户端簽发请求文件。</p>
</blockquote>
<p>下一步：<a href="05-kubernetes-configuration-files.html">配置和生成 Kubernetes 配置文件</a>。</p>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="配置和生成-kubernetes-配置文件" class="level3">配置生成配置</h1>
<p>本部分内容将会创建 <a href="https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/" target="_blank">kubeconfig 配置文件</a>，它们是 Kubernetes 客户端与 API Server 认证与鉴权的保证。</p>
<h2 id="客户端认证配置">客户端认证配置</h2>
<p>本节将会创建用于 <code>kube-proxy</code>、<code>kube-controller-manager</code>、<code>kube-scheduler</code> 和 <code>kubelet</code> 的 kubeconfig 文件。</p>
<h3 id="kubernetes-公有-ip-地址">Kubernetes 公有 IP 地址</h3>
<p>每一个 kubeconfig 文件都需要一个 Kuberntes API Server 的 IP 地址。为了保证高可用性，我们将该 IP 分配给 API Server 之前的外部负载均衡器。</p>
<p>查询 <code>kubernetes-the-hard-way</code> 的静态 IP 地址：</p>
<pre><code class="lang-sh">KUBERNETES_PUBLIC_ADDRESS=$(gcloud compute addresses describe kubernetes-the-hard-way \
  --region $(gcloud config get-value compute/region) \
  --format <span class="hljs-string">'value(address)'</span>)
</code></pre>
<h3 id="kubelet-配置文件">kubelet 配置文件</h3>
<p>为了确保 <a href="https://kubernetes.io/docs/admin/authorization/node/" target="_blank">Node Authorizer</a> 授权，Kubelet 配置文件中的客户端证书必需匹配 Node 名字。</p>
<p>为每个 worker 节点创建 kubeconfig 配置：</p>
<pre><code class="lang-sh"><span class="hljs-keyword">for</span> instance <span class="hljs-keyword">in</span> worker-0 worker-1 worker-2; <span class="hljs-keyword">do</span>
  kubectl config <span class="hljs-built_in">set</span>-cluster kubernetes-the-hard-way \
    --certificate-authority=ca.pem \
    --embed-certs=<span class="hljs-literal">true</span> \
    --server=https://<span class="hljs-variable">${KUBERNETES_PUBLIC_ADDRESS}</span>:6443 \
    --kubeconfig=<span class="hljs-variable">${instance}</span>.kubeconfig

  kubectl config <span class="hljs-built_in">set</span>-credentials system:node:<span class="hljs-variable">${instance}</span> \
    --client-certificate=<span class="hljs-variable">${instance}</span>.pem \
    --client-key=<span class="hljs-variable">${instance}</span>-key.pem \
    --embed-certs=<span class="hljs-literal">true</span> \
    --kubeconfig=<span class="hljs-variable">${instance}</span>.kubeconfig

  kubectl config <span class="hljs-built_in">set</span>-context default \
    --cluster=kubernetes-the-hard-way \
    --user=system:node:<span class="hljs-variable">${instance}</span> \
    --kubeconfig=<span class="hljs-variable">${instance}</span>.kubeconfig

  kubectl config use-context default --kubeconfig=<span class="hljs-variable">${instance}</span>.kubeconfig
<span class="hljs-keyword">done</span>
</code></pre>
<p>结果将会生成以下 3 个文件：</p>
<pre><code class="lang-sh">worker-0.kubeconfig
worker-1.kubeconfig
worker-2.kubeconfig
</code></pre>
<h3 id="kube-proxy-配置文件">kube-proxy 配置文件</h3>
<p>为 kube-proxy 服务生成 kubeconfig 配置文件：</p>
<pre><code class="lang-sh">{
  kubectl config <span class="hljs-built_in">set</span>-cluster kubernetes-the-hard-way \
    --certificate-authority=ca.pem \
    --embed-certs=<span class="hljs-literal">true</span> \
    --server=https://<span class="hljs-variable">${KUBERNETES_PUBLIC_ADDRESS}</span>:6443 \
    --kubeconfig=kube-proxy.kubeconfig

  kubectl config <span class="hljs-built_in">set</span>-credentials system:kube-proxy \
    --client-certificate=kube-proxy.pem \
    --client-key=kube-proxy-key.pem \
    --embed-certs=<span class="hljs-literal">true</span> \
    --kubeconfig=kube-proxy.kubeconfig

  kubectl config <span class="hljs-built_in">set</span>-context default \
    --cluster=kubernetes-the-hard-way \
    --user=system:kube-proxy \
    --kubeconfig=kube-proxy.kubeconfig

  kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig
}
</code></pre>
<h3 id="kube-controller-manager-配置文件">kube-controller-manager 配置文件</h3>
<pre><code class="lang-sh">{
  kubectl config <span class="hljs-built_in">set</span>-cluster kubernetes-the-hard-way \
    --certificate-authority=ca.pem \
    --embed-certs=<span class="hljs-literal">true</span> \
    --server=https://127.0.0.1:6443 \
    --kubeconfig=kube-controller-manager.kubeconfig

  kubectl config <span class="hljs-built_in">set</span>-credentials system:kube-controller-manager \
    --client-certificate=kube-controller-manager.pem \
    --client-key=kube-controller-manager-key.pem \
    --embed-certs=<span class="hljs-literal">true</span> \
    --kubeconfig=kube-controller-manager.kubeconfig

  kubectl config <span class="hljs-built_in">set</span>-context default \
    --cluster=kubernetes-the-hard-way \
    --user=system:kube-controller-manager \
    --kubeconfig=kube-controller-manager.kubeconfig

  kubectl config use-context default --kubeconfig=kube-controller-manager.kubeconfig
}
</code></pre>
<h3 id="kube-scheduler-配置文件">kube-scheduler 配置文件</h3>
<pre><code class="lang-sh">{
  kubectl config <span class="hljs-built_in">set</span>-cluster kubernetes-the-hard-way \
    --certificate-authority=ca.pem \
    --embed-certs=<span class="hljs-literal">true</span> \
    --server=https://127.0.0.1:6443 \
    --kubeconfig=kube-scheduler.kubeconfig

  kubectl config <span class="hljs-built_in">set</span>-credentials system:kube-scheduler \
    --client-certificate=kube-scheduler.pem \
    --client-key=kube-scheduler-key.pem \
    --embed-certs=<span class="hljs-literal">true</span> \
    --kubeconfig=kube-scheduler.kubeconfig

  kubectl config <span class="hljs-built_in">set</span>-context default \
    --cluster=kubernetes-the-hard-way \
    --user=system:kube-scheduler \
    --kubeconfig=kube-scheduler.kubeconfig

  kubectl config use-context default --kubeconfig=kube-scheduler.kubeconfig
}
</code></pre>
<h3 id="admin-配置文件">Admin 配置文件</h3>
<pre><code class="lang-sh">{
  kubectl config <span class="hljs-built_in">set</span>-cluster kubernetes-the-hard-way \
    --certificate-authority=ca.pem \
    --embed-certs=<span class="hljs-literal">true</span> \
    --server=https://127.0.0.1:6443 \
    --kubeconfig=admin.kubeconfig

  kubectl config <span class="hljs-built_in">set</span>-credentials admin \
    --client-certificate=admin.pem \
    --client-key=admin-key.pem \
    --embed-certs=<span class="hljs-literal">true</span> \
    --kubeconfig=admin.kubeconfig

  kubectl config <span class="hljs-built_in">set</span>-context default \
    --cluster=kubernetes-the-hard-way \
    --user=admin \
    --kubeconfig=admin.kubeconfig

  kubectl config use-context default --kubeconfig=admin.kubeconfig
}
</code></pre>
<h3 id="分发配置文件">分发配置文件</h3>
<p>将 <code>kubelet</code> 与 <code>kube-proxy</code> kubeconfig 配置文件复制到每个 worker 节点上：</p>
<pre><code class="lang-sh"><span class="hljs-keyword">for</span> instance <span class="hljs-keyword">in</span> worker-0 worker-1 worker-2; <span class="hljs-keyword">do</span>
  gcloud compute scp <span class="hljs-variable">${instance}</span>.kubeconfig kube-proxy.kubeconfig <span class="hljs-variable">${instance}</span>:~/
<span class="hljs-keyword">done</span>
</code></pre>
<p>将 <code>admin</code>、<code>kube-controller-manager</code> 与 <code>kube-scheduler</code> kubeconfig 配置文件复制到每个 controller 节点上：</p>
<pre><code class="lang-sh"><span class="hljs-keyword">for</span> instance <span class="hljs-keyword">in</span> controller-0 controller-1 controller-2; <span class="hljs-keyword">do</span>
  gcloud compute scp admin.kubeconfig kube-controller-manager.kubeconfig kube-scheduler.kubeconfig <span class="hljs-variable">${instance}</span>:~/
<span class="hljs-keyword">done</span>
</code></pre>
<p>下一步：<a href="06-data-encryption-keys.html">配置和生成密钥</a>。</p>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="配置和生成密钥" class="level3">配置生成密钥</h1>
<p>Kubernetes 存储了集群状态、应用配置和密钥等很多不同的数据。而 Kubernetes 也支持集群数据的加密存储。</p>
<p>本部分将会创建加密密钥以及一个用于加密 Kubernetes Secrets 的 <a href="https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/#understanding-the-encryption-at-rest-configuration" target="_blank">加密配置文件</a>。</p>
<h2 id="加密密钥">加密密钥</h2>
<p>建立加密密钥:</p>
<pre><code class="lang-sh">ENCRYPTION_KEY=$(head -c 32 /dev/urandom | base64)
</code></pre>
<h2 id="加密配置文件">加密配置文件</h2>
<p>生成名为 <code>encryption-config.yaml</code> 的加密配置文件：</p>
<pre><code class="lang-sh">cat > encryption-config.yaml <<EOF
kind: EncryptionConfig
apiVersion: v1
resources:
  - resources:
      - secrets
    providers:
      - aescbc:
          keys:
            - name: key1
              secret: <span class="hljs-variable">${ENCRYPTION_KEY}</span>
      - identity: {}
EOF
</code></pre>
<p>将 <code>encryption-config.yaml</code> 复制到每个控制节点上：</p>
<pre><code class="lang-sh"><span class="hljs-keyword">for</span> instance <span class="hljs-keyword">in</span> controller-0 controller-1 controller-2; <span class="hljs-keyword">do</span>
  gcloud compute scp encryption-config.yaml <span class="hljs-variable">${instance}</span>:~/
<span class="hljs-keyword">done</span>
</code></pre>
<p>下一步：<a href="07-bootstrapping-etcd.html">部署 etcd 群集</a>。</p>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="部署-etcd-群集" class="level3">部署Etcd群集</h1>
<p>Kubernetes 组件都是无状态的，所有的群集状态都储存在 <a href="https://github.com/coreos/etcd" target="_blank">etcd</a> 集群中。</p>
<p>本部分内容将部署一套三节点的 etcd 群集，并配置高可用以及远程加密访问。</p>
<h2 id="事前准备">事前准备</h2>
<p>本部分的命令需要在每个控制节点上都运行以便，包括 <code>controller-0</code>、<code>controller-1</code> 和 <code>controller-2</code>。可以使用 <code>gcloud</code> 命令登录每个控制节点，比如</p>
<pre><code class="lang-sh">gcloud compute ssh controller-0
</code></pre>
<p>可以使用 tmux 同时登录到三点控制节点上，加快部署步骤。</p>
<h2 id="部署-etcd-集群成员">部署 etcd 集群成员</h2>
<h3 id="下载并安装-etcd-二进制文件">下载并安装 etcd 二进制文件</h3>
<p>从 <a href="https://github.com/coreos/etcd" target="_blank">coreos/etcd</a> GitHub 中下载 etcd 发布文件：</p>
<pre><code class="lang-sh">wget -q --show-progress --https-only --timestamping \
  <span class="hljs-string">"https://github.com/coreos/etcd/releases/download/v3.3.9/etcd-v3.3.9-linux-amd64.tar.gz"</span>
</code></pre>
<p>解压缩并安装 <code>etcd</code> 服务与 <code>etcdctl</code> 命令行工具：</p>
<pre><code class="lang-sh">  tar -xvf etcd-v3.3.9-linux-amd64.tar.gz
  sudo mv etcd-v3.3.9-linux-amd64/etcd* /usr/<span class="hljs-built_in">local</span>/bin/
</code></pre>
<h3 id="配置-etcd-server">配置 etcd Server</h3>
<pre><code class="lang-sh">  sudo mkdir -p /etc/etcd /var/lib/etcd
  sudo cp ca.pem kubernetes-key.pem kubernetes.pem /etc/etcd/
</code></pre>
<p>使用虚拟机的内网 IP 地址来作为 etcd 集群的服务地址。查询当前节点的内网 IP 地址：</p>
<pre><code class="lang-sh">INTERNAL_IP=$(curl <span class="hljs-_">-s</span> -H <span class="hljs-string">"Metadata-Flavor: Google"</span> \
  http://metadata.google.internal/computeMetadata/v1/instance/network-interfaces/0/ip)
</code></pre>
<p>每个 etcd 成员必须有一个整集群中唯一的名字，使用 hostname 作为 etcd name：</p>
<pre><code class="lang-sh">ETCD_NAME=$(hostname <span class="hljs-_">-s</span>)
</code></pre>
<p>生成 <code>etcd.service</code> 的 systemd 配置文件</p>
<pre><code class="lang-sh">cat <<EOF | sudo tee /etc/systemd/system/etcd.service
[Unit]
Description=etcd
Documentation=https://github.com/coreos

[Service]
ExecStart=/usr/<span class="hljs-built_in">local</span>/bin/etcd \\
  --name <span class="hljs-variable">${ETCD_NAME}</span> \\
  --cert-file=/etc/etcd/kubernetes.pem \\
  --key-file=/etc/etcd/kubernetes-key.pem \\
  --peer-cert-file=/etc/etcd/kubernetes.pem \\
  --peer-key-file=/etc/etcd/kubernetes-key.pem \\
  --trusted-ca-file=/etc/etcd/ca.pem \\
  --peer-trusted-ca-file=/etc/etcd/ca.pem \\
  --peer-client-cert-auth \\
  --client-cert-auth \\
  --initial-advertise-peer-urls https://<span class="hljs-variable">${INTERNAL_IP}</span>:2380 \\
  --listen-peer-urls https://<span class="hljs-variable">${INTERNAL_IP}</span>:2380 \\
  --listen-client-urls https://<span class="hljs-variable">${INTERNAL_IP}</span>:2379,https://127.0.0.1:2379 \\
  --advertise-client-urls https://<span class="hljs-variable">${INTERNAL_IP}</span>:2379 \\
  --initial-cluster-token etcd-cluster-0 \\
  --initial-cluster controller-0=https://10.240.0.10:2380,controller-1=https://10.240.0.11:2380,controller-2=https://10.240.0.12:2380 \\
  --initial-cluster-state new \\
  --data-dir=/var/lib/etcd
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
</code></pre>
<h3 id="启动-etcd-server">启动 etcd Server</h3>
<pre><code class="lang-sh">sudo systemctl daemon-reload
sudo systemctl <span class="hljs-built_in">enable</span> etcd
sudo systemctl start etcd
</code></pre>
<blockquote>
<p>不要忘记在所有控制节点上都运行上述命令，包括 <code>controller-0</code>、<code>controller-1</code> 和 <code>controller-2</code> 等。</p>
</blockquote>
<h2 id="验证">验证</h2>
<p>列出 etcd 的群集成员:</p>
<pre><code class="lang-sh">sudo ETCDCTL_API=3 etcdctl member list \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/etcd/ca.pem \
  --cert=/etc/etcd/kubernetes.pem \
  --key=/etc/etcd/kubernetes-key.pem
</code></pre>
<blockquote>
<p>输出</p>
</blockquote>
<pre><code class="lang-sh">3a57933972cb5131, started, controller-2, https://10.240.0.12:2380, https://10.240.0.12:2379
f98dc20bce6225a0, started, controller-0, https://10.240.0.10:2380, https://10.240.0.10:2379
ffed16798470cab5, started, controller-1, https://10.240.0.11:2380, https://10.240.0.11:2379
</code></pre>
<p>下一步：<a href="08-bootstrapping-kubernetes-controllers.html">部署 Kubernetes 控制节点</a>。</p>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="部署-kubernetes-控制节点" class="level3">部署控制节点</h1>
<p>本部分将会在三台控制节点上部署 Kubernetes 控制服务，并配置高可用的集群架构。并且还会创建一个用于外部访问的负载均衡器。每个控制节点上需要部署的服务包括：Kubernetes API Server、Scheduler 以及 Controller Manager 等。</p>
<h2 id="事前准备">事前准备</h2>
<p>以下命令需要在每台控制节点上面都运行一遍，包括 <code>controller-0</code>、<code>controller-1</code> 和 <code>controller-2</code>。可以使用 <code>gcloud</code> 命令登录每个控制节点。例如:</p>
<pre><code class="lang-sh">gcloud compute ssh controller-0
</code></pre>
<p>可以使用 tmux 同时登录到三点控制节点上，加快部署步骤。</p>
<h2 id="部署-kubernetes-控制平面">部署 Kubernetes 控制平面</h2>
<p>创建 Kubernetes 配置目录</p>
<pre><code class="lang-sh">sudo mkdir -p /etc/kubernetes/config
</code></pre>
<h3 id="下载并安装-kubernetes-controller-二进制文件">下载并安装 Kubernetes Controller 二进制文件</h3>
<pre><code class="lang-sh">wget -q --show-progress --https-only --timestamping \
  <span class="hljs-string">"https://storage.googleapis.com/kubernetes-release/release/v1.12.0/bin/linux/amd64/kube-apiserver"</span> \
  <span class="hljs-string">"https://storage.googleapis.com/kubernetes-release/release/v1.12.0/bin/linux/amd64/kube-controller-manager"</span> \
  <span class="hljs-string">"https://storage.googleapis.com/kubernetes-release/release/v1.12.0/bin/linux/amd64/kube-scheduler"</span> \
  <span class="hljs-string">"https://storage.googleapis.com/kubernetes-release/release/v1.12.0/bin/linux/amd64/kubectl"</span>

chmod +x kube-apiserver kube-controller-manager kube-scheduler kubectl
sudo mv kube-apiserver kube-controller-manager kube-scheduler kubectl /usr/<span class="hljs-built_in">local</span>/bin/
</code></pre>
<h3 id="配置-kubernetes-api-server">配置 Kubernetes API Server</h3>
<pre><code class="lang-sh">  sudo mkdir -p /var/lib/kubernetes/

  sudo mv ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem \
    service-account-key.pem service-account.pem \
    encryption-config.yaml /var/lib/kubernetes/
</code></pre>
<p>使用节点的内网 IP 地址作为 API server 与集群内部成员的广播地址。首先查询当前节点的内网 IP 地址：</p>
<pre><code class="lang-sh">INTERNAL_IP=$(curl <span class="hljs-_">-s</span> -H <span class="hljs-string">"Metadata-Flavor: Google"</span> \
  http://metadata.google.internal/computeMetadata/v1/instance/network-interfaces/0/ip)
</code></pre>
<p>生成 <code>kube-apiserver.service</code> systemd 配置文件：</p>
<pre><code class="lang-sh">cat <<EOF | sudo tee /etc/systemd/system/kube-apiserver.service
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/<span class="hljs-built_in">local</span>/bin/kube-apiserver \\
  --advertise-address=<span class="hljs-variable">${INTERNAL_IP}</span> \\
  --allow-privileged=<span class="hljs-literal">true</span> \\
  --apiserver-count=3 \\
  --audit-log-maxage=30 \\
  --audit-log-maxbackup=3 \\
  --audit-log-maxsize=100 \\
  --audit-log-path=/var/<span class="hljs-built_in">log</span>/audit.log \\
  --authorization-mode=Node,RBAC \\
  --bind-address=0.0.0.0 \\
  --client-ca-file=/var/lib/kubernetes/ca.pem \\
  --enable-admission-plugins=Initializers,NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \\
  --enable-swagger-ui=<span class="hljs-literal">true</span> \\
  --etcd-cafile=/var/lib/kubernetes/ca.pem \\
  --etcd-certfile=/var/lib/kubernetes/kubernetes.pem \\
  --etcd-keyfile=/var/lib/kubernetes/kubernetes-key.pem \\
  --etcd-servers=https://10.240.0.10:2379,https://10.240.0.11:2379,https://10.240.0.12:2379 \\
  --event-ttl=1h \\
  --experimental-encryption-provider-config=/var/lib/kubernetes/encryption-config.yaml \\
  --kubelet-certificate-authority=/var/lib/kubernetes/ca.pem \\
  --kubelet-client-certificate=/var/lib/kubernetes/kubernetes.pem \\
  --kubelet-client-key=/var/lib/kubernetes/kubernetes-key.pem \\
  --kubelet-https=<span class="hljs-literal">true</span> \\
  --runtime-config=api/all \\
  --service-account-key-file=/var/lib/kubernetes/service-account.pem \\
  --service-cluster-ip-range=10.32.0.0/24 \\
  --service-node-port-range=30000-32767 \\
  --tls-cert-file=/var/lib/kubernetes/kubernetes.pem \\
  --tls-private-key-file=/var/lib/kubernetes/kubernetes-key.pem \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
</code></pre>
<h3 id="配置-kubernetes-controller-manager">配置 Kubernetes Controller Manager</h3>
<p>生成 <code>kube-controller-manager.service</code> systemd 配置文件：</p>
<pre><code class="lang-sh">sudo mv kube-controller-manager.kubeconfig /var/lib/kubernetes/

cat <<EOF | sudo tee /etc/systemd/system/kube-controller-manager.service
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/<span class="hljs-built_in">local</span>/bin/kube-controller-manager \\
  --address=0.0.0.0 \\
  --cluster-cidr=10.200.0.0/16 \\
  --cluster-name=kubernetes \\
  --cluster-signing-cert-file=/var/lib/kubernetes/ca.pem \\
  --cluster-signing-key-file=/var/lib/kubernetes/ca-key.pem \\
  --kubeconfig=/var/lib/kubernetes/kube-controller-manager.kubeconfig \\
  --leader-elect=<span class="hljs-literal">true</span> \\
  --root-ca-file=/var/lib/kubernetes/ca.pem \\
  --service-account-private-key-file=/var/lib/kubernetes/service-account-key.pem \\
  --service-cluster-ip-range=10.32.0.0/24 \\
  --use-service-account-credentials=<span class="hljs-literal">true</span> \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
</code></pre>
<h3 id="配置-kubernetes-scheduler">配置 Kubernetes Scheduler</h3>
<p>生成 <code>kube-scheduler.service</code> systemd 配置文件：</p>
<pre><code class="lang-sh">sudo mv kube-scheduler.kubeconfig /var/lib/kubernetes/

cat <<EOF | sudo tee /etc/kubernetes/config/kube-scheduler.yaml
apiVersion: componentconfig/v1alpha1
kind: KubeSchedulerConfiguration
clientConnection:
  kubeconfig: <span class="hljs-string">"/var/lib/kubernetes/kube-scheduler.kubeconfig"</span>
leaderElection:
  leaderElect: <span class="hljs-literal">true</span>
EOF

cat <<EOF | sudo tee /etc/systemd/system/kube-scheduler.service
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/<span class="hljs-built_in">local</span>/bin/kube-scheduler \\
  --config=/etc/kubernetes/config/kube-scheduler.yaml \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
</code></pre>
<h3 id="启动控制器服务">启动控制器服务</h3>
<pre><code class="lang-sh">  sudo systemctl daemon-reload
  sudo systemctl <span class="hljs-built_in">enable</span> kube-apiserver kube-controller-manager kube-scheduler
  sudo systemctl start kube-apiserver kube-controller-manager kube-scheduler
</code></pre>
<blockquote>
<p>请等待 10 秒以便 Kubernetes API Server 初始化。</p>
</blockquote>
<h3 id="开启-http-健康检查">开启 HTTP 健康检查</h3>
<p><a href="https://cloud.google.com/compute/docs/load-balancing/network" target="_blank">Google Network Load Balancer</a> 将用在在三个 API Server 之前作负载均衡，并可以终止 TLS 并验证客户端证书。但是该负载均衡仅支持 HTTP 健康检查，因而这里部署 nginx 来代理 API Server 的 <code>/healthz</code> 连接。</p>
<blockquote>
<p><code>/healthz</code> API 默认不需要认证。</p>
</blockquote>
<pre><code class="lang-sh">sudo apt-get install -y nginx

cat > kubernetes.default.svc.cluster.local <<EOF
server {
  listen      80;
  server_name kubernetes.default.svc.cluster.local;

  location /healthz {
     proxy_pass                    https://127.0.0.1:6443/healthz;
     proxy_ssl_trusted_certificate /var/lib/kubernetes/ca.pem;
  }
}
EOF

sudo mv kubernetes.default.svc.cluster.local \
    /etc/nginx/sites-available/kubernetes.default.svc.cluster.local
sudo ln <span class="hljs-_">-s</span> /etc/nginx/sites-available/kubernetes.default.svc.cluster.local /etc/nginx/sites-enabled/

sudo systemctl restart nginx
sudo systemctl <span class="hljs-built_in">enable</span> nginx
</code></pre>
<h3 id="验证">验证</h3>
<pre><code class="lang-sh">kubectl get componentstatuses --kubeconfig admin.kubeconfig
</code></pre>
<p>将输出结果</p>
<pre><code class="lang-sh">NAME                 STATUS    MESSAGE              ERROR
controller-manager   Healthy   ok
scheduler            Healthy   ok
etcd-2               Healthy   {<span class="hljs-string">"health"</span>: <span class="hljs-string">"true"</span>}
etcd-0               Healthy   {<span class="hljs-string">"health"</span>: <span class="hljs-string">"true"</span>}
etcd-1               Healthy   {<span class="hljs-string">"health"</span>: <span class="hljs-string">"true"</span>}
</code></pre>
<p>验证 Nginx HTTP 健康检查</p>
<pre><code class="lang-sh">curl -H <span class="hljs-string">"Host: kubernetes.default.svc.cluster.local"</span> -i http://127.0.0.1/healthz
</code></pre>
<p>将输出</p>
<pre><code class="lang-sh">HTTP/1.1 200 OK
Server: nginx/1.14.0 (Ubuntu)
Date: Mon, 14 May 2018 13:45:39 GMT
Content-Type: text/plain; charset=utf-8
Content-Length: 2
Connection: keep-alive

ok
</code></pre>
<blockquote>
<p>记得在每台控制节点上面都运行一遍，包括 <code>controller-0</code>、<code>controller-1</code> 和 <code>controller-2</code>。</p>
</blockquote>
<h2 id="kubelet-rbac-授权">Kubelet RBAC 授权</h2>
<p>本节将会配置 API Server 访问 Kubelet API 的 RBAC 授权。访问 Kubelet API 是获取 metrics、日志以及执行容器命令所必需的。</p>
<blockquote>
<p>这里设置 Kubeket <code>--authorization-mode</code> 为 <code>Webhook</code> 模式。Webhook 模式使用 <a href="https://kubernetes.io/docs/admin/authorization/#checking-api-access" target="_blank">SubjectAccessReview</a> API 来决定授权。</p>
</blockquote>
<pre><code class="lang-sh">gcloud compute ssh controller-0
</code></pre>
<p>创建 <code>system:kube-apiserver-to-kubelet</code> <a href="https://kubernetes.io/docs/admin/authorization/rbac/#role-and-clusterrole" target="_blank">ClusterRole</a> 以允许请求 Kubelet API 和执行许用来管理 Pods 的任务:</p>
<pre><code class="lang-sh">cat <<EOF | kubectl apply --kubeconfig admin.kubeconfig <span class="hljs-_">-f</span> -
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: <span class="hljs-string">"true"</span>
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:kube-apiserver-to-kubelet
rules:
  - apiGroups:
      - <span class="hljs-string">""</span>
    resources:
      - nodes/proxy
      - nodes/stats
      - nodes/<span class="hljs-built_in">log</span>
      - nodes/spec
      - nodes/metrics
    verbs:
      - <span class="hljs-string">"*"</span>
EOF
</code></pre>
<p>Kubernetes API Server 使用客户端凭证授权 Kubelet 为 <code>kubernetes</code> 用户，此凭证用 <code>--kubelet-client-certificate</code> flag 来定义。</p>
<p>绑定 <code>system:kube-apiserver-to-kubelet</code> ClusterRole 到 <code>kubernetes</code> 用户:</p>
<pre><code class="lang-sh">cat <<EOF | kubectl apply --kubeconfig admin.kubeconfig <span class="hljs-_">-f</span> -
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: system:kube-apiserver
  namespace: <span class="hljs-string">""</span>
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:kube-apiserver-to-kubelet
subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: User
    name: kubernetes
EOF
</code></pre>
<h2 id="kubernetes-前端负载均衡器">Kubernetes 前端负载均衡器</h2>
<p>本节将会建立一个位于 Kubernetes API Servers 前端的外部负载均衡器。 <code>kubernetes-the-hard-way</code> 静态 IP 地址将会配置在这个负载均衡器上。</p>
<blockquote>
<p>本指南创建的虚拟机内部并没有操作负载均衡器的权限，需要到创建这些虚拟机的那台机器上去做下面的操作。</p>
</blockquote>
<p>创建外部负载均衡器网络资源：</p>
<pre><code class="lang-sh">  KUBERNETES_PUBLIC_ADDRESS=$(gcloud compute addresses describe kubernetes-the-hard-way \
    --region $(gcloud config get-value compute/region) \
    --format <span class="hljs-string">'value(address)'</span>)

  gcloud compute http-health-checks create kubernetes \
    --description <span class="hljs-string">"Kubernetes Health Check"</span> \
    --host <span class="hljs-string">"kubernetes.default.svc.cluster.local"</span> \
    --request-path <span class="hljs-string">"/healthz"</span>

  gcloud compute firewall-rules create kubernetes-the-hard-way-allow-health-check \
    --network kubernetes-the-hard-way \
    --source-ranges 209.85.152.0/22,209.85.204.0/22,35.191.0.0/16 \
    --allow tcp

  gcloud compute target-pools create kubernetes-target-pool \
    --http-health-check kubernetes

  gcloud compute target-pools add-instances kubernetes-target-pool \
   --instances controller-0,controller-1,controller-2

  gcloud compute forwarding-rules create kubernetes-forwarding-rule \
    --address <span class="hljs-variable">${KUBERNETES_PUBLIC_ADDRESS}</span> \
    --ports 6443 \
    --region $(gcloud config get-value compute/region) \
    --target-pool kubernetes-target-pool
</code></pre>
<h3 id="验证">验证</h3>
<p>查询 <code>kubernetes-the-hard-way</code> 静态 IP 地址:</p>
<pre><code class="lang-sh">KUBERNETES_PUBLIC_ADDRESS=$(gcloud compute addresses describe kubernetes-the-hard-way \
  --region $(gcloud config get-value compute/region) \
  --format <span class="hljs-string">'value(address)'</span>)
</code></pre>
<p>发送一个查询 Kubernetes 版本信息的 HTTP 请求</p>
<pre><code class="lang-sh">curl --cacert ca.pem https://<span class="hljs-variable">${KUBERNETES_PUBLIC_ADDRESS}</span>:6443/version
</code></pre>
<p>结果为</p>
<pre><code class="lang-json">{
  <span class="hljs-string">"major"</span>: <span class="hljs-string">"1"</span>,
  <span class="hljs-string">"minor"</span>: <span class="hljs-string">"12"</span>,
  <span class="hljs-string">"gitVersion"</span>: <span class="hljs-string">"v1.12.0"</span>,
  <span class="hljs-string">"gitCommit"</span>: <span class="hljs-string">"0ed33881dc4355495f623c6f22e7dd0b7632b7c0"</span>,
  <span class="hljs-string">"gitTreeState"</span>: <span class="hljs-string">"clean"</span>,
  <span class="hljs-string">"buildDate"</span>: <span class="hljs-string">"2018-09-27T16:55:41Z"</span>,
  <span class="hljs-string">"goVersion"</span>: <span class="hljs-string">"go1.10.4"</span>,
  <span class="hljs-string">"compiler"</span>: <span class="hljs-string">"gc"</span>,
  <span class="hljs-string">"platform"</span>: <span class="hljs-string">"linux/amd64"</span>
}
</code></pre>
<p>下一步：<a href="09-bootstrapping-kubernetes-workers.html">部署 Kubernetes Worker 节点</a>。</p>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="部署-kubernetes-workers-节点" class="level3">部署计算节点</h1>
<p>本部分将会部署三个 Kubernetes Worker 节点。每个节点上将会安装以下服务：<a href="https://github.com/opencontainers/runc" target="_blank">runc</a>, <a href="https://github.com/google/gvisor" target="_blank">gVisor</a>, <a href="https://github.com/containernetworking/cni" target="_blank">container networking plugins</a>, <a href="https://github.com/containerd/containerd" target="_blank">containerd</a>, <a href="https://kubernetes.io/docs/admin/kubelet" target="_blank">kubelet</a>, 和 <a href="https://kubernetes.io/docs/concepts/cluster-administration/proxies" target="_blank">kube-proxy</a>。</p>
<h2 id="事前准备">事前准备</h2>
<p>以下命令需要在所有 worker 节点上面都运行一遍，包括 <code>worker-0</code>, <code>worker-1</code> 和 <code>worker-2</code>。可以使用 <code>gcloud</code> 命令登录到 worker 节点上，比如</p>
<pre><code class="lang-sh">gcloud compute ssh worker-0
</code></pre>
<p>可以使用 tmux 同时登录到三个 Worker 节点上，加快部署步骤。</p>
<h2 id="部署-kubernetes-worker-节点">部署 Kubernetes Worker 节点</h2>
<p>安装 OS 依赖组件：</p>
<pre><code class="lang-sh">sudo apt-get update
sudo apt-get -y install socat conntrack ipset
</code></pre>
<blockquote>
<p>socat 命令用于支持 <code>kubectl port-forward</code> 命令。</p>
</blockquote>
<h3 id="下载并安装-worker-二进制文件">下载并安装 worker 二进制文件</h3>
<pre><code class="lang-sh">wget -q --show-progress --https-only --timestamping \
  https://github.com/kubernetes-sigs/cri-tools/releases/download/v1.12.0/crictl-v1.12.0-linux-amd64.tar.gz \
  https://storage.googleapis.com/kubernetes-the-hard-way/runsc-50c283b9f56bb7200938d9e207355f05f79f0d17 \
  https://github.com/opencontainers/runc/releases/download/v1.0.0-rc5/runc.amd64 \
  https://github.com/containernetworking/plugins/releases/download/v0.6.0/cni-plugins-amd64-v0.6.0.tgz \
  https://github.com/containerd/containerd/releases/download/v1.2.0-rc.0/containerd-1.2.0-rc.0.linux-amd64.tar.gz \
  https://storage.googleapis.com/kubernetes-release/release/v1.12.0/bin/linux/amd64/kubectl \
  https://storage.googleapis.com/kubernetes-release/release/v1.12.0/bin/linux/amd64/kube-proxy \
  https://storage.googleapis.com/kubernetes-release/release/v1.12.0/bin/linux/amd64/kubelet
</code></pre>
<p>创建安装目录：</p>
<pre><code class="lang-sh">sudo mkdir -p \
  /etc/cni/net.d \
  /opt/cni/bin \
  /var/lib/kubelet \
  /var/lib/kube-proxy \
  /var/lib/kubernetes \
  /var/run/kubernetes
</code></pre>
<p>安装 worker 二进制文件</p>
<pre><code class="lang-sh">  sudo mv runsc-50c283b9f56bb7200938d9e207355f05f79f0d17 runsc
  sudo mv runc.amd64 runc
  chmod +x kubectl kube-proxy kubelet runc runsc
  sudo mv kubectl kube-proxy kubelet runc runsc /usr/<span class="hljs-built_in">local</span>/bin/
  sudo tar -xvf crictl-v1.12.0-linux-amd64.tar.gz -C /usr/<span class="hljs-built_in">local</span>/bin/
  sudo tar -xvf cni-plugins-amd64-v0.6.0.tgz -C /opt/cni/bin/
  sudo tar -xvf containerd-1.2.0-rc.0.linux-amd64.tar.gz -C /
</code></pre>
<h3 id="配置-cni-网路">配置 CNI 网路</h3>
<p>查询当前计算节点的 Pod CIDR 范围：</p>
<pre><code class="lang-sh">POD_CIDR=$(curl <span class="hljs-_">-s</span> -H <span class="hljs-string">"Metadata-Flavor: Google"</span> \
  http://metadata.google.internal/computeMetadata/v1/instance/attributes/pod-cidr)
</code></pre>
<p>生成 <code>bridge</code> 网络插件配置文件</p>
<pre><code class="lang-sh">cat <<EOF | sudo tee /etc/cni/net.d/10-bridge.conf
{
    <span class="hljs-string">"cniVersion"</span>: <span class="hljs-string">"0.3.1"</span>,
    <span class="hljs-string">"name"</span>: <span class="hljs-string">"bridge"</span>,
    <span class="hljs-string">"type"</span>: <span class="hljs-string">"bridge"</span>,
    <span class="hljs-string">"bridge"</span>: <span class="hljs-string">"cnio0"</span>,
    <span class="hljs-string">"isGateway"</span>: <span class="hljs-literal">true</span>,
    <span class="hljs-string">"ipMasq"</span>: <span class="hljs-literal">true</span>,
    <span class="hljs-string">"ipam"</span>: {
        <span class="hljs-string">"type"</span>: <span class="hljs-string">"host-local"</span>,
        <span class="hljs-string">"ranges"</span>: [
          [{<span class="hljs-string">"subnet"</span>: <span class="hljs-string">"<span class="hljs-variable">${POD_CIDR}</span>"</span>}]
        ],
        <span class="hljs-string">"routes"</span>: [{<span class="hljs-string">"dst"</span>: <span class="hljs-string">"0.0.0.0/0"</span>}]
    }
}
EOF
</code></pre>
<p>生成 <code>loopback</code> 网络插件配置文件</p>
<pre><code class="lang-sh">cat <<EOF | sudo tee /etc/cni/net.d/99-loopback.conf
{
    <span class="hljs-string">"cniVersion"</span>: <span class="hljs-string">"0.3.1"</span>,
    <span class="hljs-string">"type"</span>: <span class="hljs-string">"loopback"</span>
}
EOF
</code></pre>
<h3 id="配置-containerd">配置 containerd</h3>
<pre><code class="lang-sh">sudo mkdir -p /etc/containerd/

<span class="hljs-comment"># Untrusted workloads will be run using the gVisor (runsc) runtime.</span>
cat << EOF | sudo tee /etc/containerd/config.toml
[plugins]
  [plugins.cri.containerd]
    snapshotter = <span class="hljs-string">"overlayfs"</span>
    [plugins.cri.containerd.default_runtime]
      runtime_<span class="hljs-built_in">type</span> = <span class="hljs-string">"io.containerd.runtime.v1.linux"</span>
      runtime_engine = <span class="hljs-string">"/usr/local/bin/runc"</span>
      runtime_root = <span class="hljs-string">""</span>
    [plugins.cri.containerd.untrusted_workload_runtime]
      runtime_<span class="hljs-built_in">type</span> = <span class="hljs-string">"io.containerd.runtime.v1.linux"</span>
      runtime_engine = <span class="hljs-string">"/usr/local/bin/runsc"</span>
      runtime_root = <span class="hljs-string">"/run/containerd/runsc"</span>
    [plugins.cri.containerd.gvisor]
      runtime_<span class="hljs-built_in">type</span> = <span class="hljs-string">"io.containerd.runtime.v1.linux"</span>
      runtime_engine = <span class="hljs-string">"/usr/local/bin/runsc"</span>
      runtime_root = <span class="hljs-string">"/run/containerd/runsc"</span>
EOF

<span class="hljs-comment"># Create the containerd.service systemd unit file</span>
cat <<EOF | sudo tee /etc/systemd/system/containerd.service
[Unit]
Description=containerd container runtime
Documentation=https://containerd.io
After=network.target

[Service]
ExecStartPre=/sbin/modprobe overlay
ExecStart=/bin/containerd
Restart=always
RestartSec=5
Delegate=yes
KillMode=process
OOMScoreAdjust=-999
LimitNOFILE=1048576
LimitNPROC=infinity
LimitCORE=infinity

[Install]
WantedBy=multi-user.target
EOF
</code></pre>
<h3 id="配置-kubelet">配置 Kubelet</h3>
<pre><code class="lang-sh">  sudo mv <span class="hljs-variable">${HOSTNAME}</span>-key.pem <span class="hljs-variable">${HOSTNAME}</span>.pem /var/lib/kubelet/
  sudo mv <span class="hljs-variable">${HOSTNAME}</span>.kubeconfig /var/lib/kubelet/kubeconfig
  sudo mv ca.pem /var/lib/kubernetes/
</code></pre>
<p>生成 <code>kubelet.service</code> systemd 配置文件：</p>
<pre><code class="lang-sh"><span class="hljs-comment"># The resolvConf configuration is used to avoid loops</span>
<span class="hljs-comment"># when using CoreDNS for service discovery on systems running systemd-resolved.</span>
cat <<EOF | sudo tee /var/lib/kubelet/kubelet-config.yaml
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: <span class="hljs-literal">false</span>
  webhook:
    enabled: <span class="hljs-literal">true</span>
  x509:
    clientCAFile: <span class="hljs-string">"/var/lib/kubernetes/ca.pem"</span>
authorization:
  mode: Webhook
clusterDomain: <span class="hljs-string">"cluster.local"</span>
clusterDNS:
  - <span class="hljs-string">"10.32.0.10"</span>
podCIDR: <span class="hljs-string">"<span class="hljs-variable">${POD_CIDR}</span>"</span>
resolvConf: <span class="hljs-string">"/run/systemd/resolve/resolv.conf"</span>
runtimeRequestTimeout: <span class="hljs-string">"15m"</span>
tlsCertFile: <span class="hljs-string">"/var/lib/kubelet/<span class="hljs-variable">${HOSTNAME}</span>.pem"</span>
tlsPrivateKeyFile: <span class="hljs-string">"/var/lib/kubelet/<span class="hljs-variable">${HOSTNAME}</span>-key.pem"</span>
EOF

cat <<EOF | sudo tee /etc/systemd/system/kubelet.service
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/kubernetes/kubernetes
After=containerd.service
Requires=containerd.service

[Service]
ExecStart=/usr/<span class="hljs-built_in">local</span>/bin/kubelet \\
  --config=/var/lib/kubelet/kubelet-config.yaml \\
  --container-runtime=remote \\
  --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \\
  --image-pull-progress-deadline=2m \\
  --kubeconfig=/var/lib/kubelet/kubeconfig \\
  --network-plugin=cni \\
  --register-node=<span class="hljs-literal">true</span> \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
</code></pre>
<h3 id="配置-kube-proxy">配置 Kube-Proxy</h3>
<pre><code class="lang-sh">sudo mv kube-proxy.kubeconfig /var/lib/kube-proxy/kubeconfig
</code></pre>
<p>生成 <code>kube-proxy.service</code> systemd 配置文件：</p>
<pre><code class="lang-sh">cat <<EOF | sudo tee /var/lib/kube-proxy/kube-proxy-config.yaml
kind: KubeProxyConfiguration
apiVersion: kubeproxy.config.k8s.io/v1alpha1
clientConnection:
  kubeconfig: <span class="hljs-string">"/var/lib/kube-proxy/kubeconfig"</span>
mode: <span class="hljs-string">"iptables"</span>
clusterCIDR: <span class="hljs-string">"10.200.0.0/16"</span>
EOF

cat <<EOF | sudo tee /etc/systemd/system/kube-proxy.service
[Unit]
Description=Kubernetes Kube Proxy
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/<span class="hljs-built_in">local</span>/bin/kube-proxy \\
  --config=/var/lib/kube-proxy/kube-proxy-config.yaml
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
</code></pre>
<h3 id="启动-worker-服务">启动 worker 服务</h3>
<pre><code class="lang-sh">  sudo systemctl daemon-reload
  sudo systemctl <span class="hljs-built_in">enable</span> containerd kubelet kube-proxy
  sudo systemctl start containerd kubelet kube-proxy
</code></pre>
<blockquote>
<p>记得在所有 worker 节点上面都运行一遍，包括 <code>worker-0</code>, <code>worker-1</code> 和 <code>worker-2</code>。</p>
</blockquote>
<h2 id="验证">验证</h2>
<p>登入任意一台控制节点查询 Nodes 列表</p>
<pre><code class="lang-sh">gcloud compute ssh controller-0 \
  --command <span class="hljs-string">"kubectl get nodes --kubeconfig admin.kubeconfig"</span>
</code></pre>
<p>输出为</p>
<pre><code class="lang-sh">NAME       STATUS   ROLES    AGE   VERSION
worker-0   Ready    <none>   35s   v1.12.0
worker-1   Ready    <none>   36s   v1.12.0
worker-2   Ready    <none>   36s   v1.12.0
</code></pre>
<p>下一步：<a href="10-configuring-kubectl.html">配置 Kubectl</a>。</p>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="配置-kubectl" class="level3">配置Kubectl</h1>
<p>本部分将生成一个用于 admin 用户的 kubeconfig 文件。</p>
<blockquote>
<p>注意：在生成 admin 客户端证书的目录来运行本部分的指令。</p>
</blockquote>
<h2 id="admin-kubeconfig">admin kubeconfig</h2>
<p>每一个 kubeconfig 都需要一个 Kuberntes API Server 地址。为了保证高可用，这里将使用 API Servers 前端外部负载均衡器的 IP 地址。</p>
<p>查询 <code>kubernetes-the-hard-way</code> 的静态 IP 地址：</p>
<pre><code class="lang-sh">KUBERNETES_PUBLIC_ADDRESS=$(gcloud compute addresses describe kubernetes-the-hard-way \
    --region $(gcloud config get-value compute/region) \
    --format <span class="hljs-string">'value(address)'</span>)
</code></pre>
<p>为 <code>admin</code> 用户生成 kubeconfig 文件：</p>
<pre><code class="lang-sh">  kubectl config <span class="hljs-built_in">set</span>-cluster kubernetes-the-hard-way \
    --certificate-authority=ca.pem \
    --embed-certs=<span class="hljs-literal">true</span> \
    --server=https://<span class="hljs-variable">${KUBERNETES_PUBLIC_ADDRESS}</span>:6443

  kubectl config <span class="hljs-built_in">set</span>-credentials admin \
    --client-certificate=admin.pem \
    --client-key=admin-key.pem

  kubectl config <span class="hljs-built_in">set</span>-context kubernetes-the-hard-way \
    --cluster=kubernetes-the-hard-way \
    --user=admin

  kubectl config use-context kubernetes-the-hard-way
</code></pre>
<h2 id="验证">验证</h2>
<p>检查远端 Kubernetes 群集的健康状况:</p>
<pre><code class="lang-sh">kubectl get componentstatuses
</code></pre>
<p>输出为</p>
<pre><code class="lang-sh">NAME                 STATUS    MESSAGE              ERROR
controller-manager   Healthy   ok
scheduler            Healthy   ok
etcd-2               Healthy   {<span class="hljs-string">"health"</span>: <span class="hljs-string">"true"</span>}
etcd-0               Healthy   {<span class="hljs-string">"health"</span>: <span class="hljs-string">"true"</span>}
etcd-1               Healthy   {<span class="hljs-string">"health"</span>: <span class="hljs-string">"true"</span>}
</code></pre>
<p>列出远端 kubernetes cluster 的节点:</p>
<pre><code class="lang-sh">kubectl get nodes
</code></pre>
<p>输出为</p>
<pre><code class="lang-sh">NAME       STATUS   ROLES    AGE    VERSION
worker-0   Ready    <none>   117s   v1.12.0
worker-1   Ready    <none>   118s   v1.12.0
worker-2   Ready    <none>   118s   v1.12.0
</code></pre>
<p>下一步：<a href="11-pod-network-routes.html">配置 Pod 网络路由</a>。</p>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="配置-pod-网络路由" class="level3">配置网络路由</h1>
<p>每个 Pod 都会从所在 Node 的 Pod CIDR 中分配一个 IP 地址。由于网络 <a href="https://cloud.google.com/compute/docs/vpc/routes" target="_blank">路由</a> 还没有配置，跨节点的 Pod 之间还无法通信。</p>
<p>本部分将为每个 worker 节点创建一条路由，将匹配 Pod CIDR 的网络请求路由到 Node 的内网 IP 地址上。</p>
<blockquote>
<p>也可以选择 <a href="https://kubernetes.io/docs/concepts/cluster-administration/networking/#how-to-achieve-this" target="_blank">其他方法</a> 来实现 Kubernetes 网络模型。</p>
</blockquote>
<h2 id="路由表">路由表</h2>
<p>本节将为创建 <code>kubernetes-the-hard-way</code> VPC 路由收集必要的信息。</p>
<p>列出每个 worker 节点的内部 IP 地址和 Pod CIDR 范围:</p>
<pre><code class="lang-sh"><span class="hljs-keyword">for</span> instance <span class="hljs-keyword">in</span> worker-0 worker-1 worker-2; <span class="hljs-keyword">do</span>
  gcloud compute instances describe <span class="hljs-variable">${instance}</span> \
    --format <span class="hljs-string">'value[separator=" "](networkInterfaces[0].networkIP,metadata.items[0].value)'</span>
<span class="hljs-keyword">done</span>
</code></pre>
<p>输出为</p>
<pre><code class="lang-sh">10.240.0.20 10.200.0.0/24
10.240.0.21 10.200.1.0/24
10.240.0.22 10.200.2.0/24
</code></pre>
<h2 id="路由">路由</h2>
<p>为每个 worker 节点创建网络路由:</p>
<pre><code class="lang-sh"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> 0 1 2; <span class="hljs-keyword">do</span>
  gcloud compute routes create kubernetes-route-10-200-<span class="hljs-variable">${i}</span>-0-24 \
    --network kubernetes-the-hard-way \
    --next-hop-address 10.240.0.2<span class="hljs-variable">${i}</span> \
    --destination-range 10.200.<span class="hljs-variable">${i}</span>.0/24
<span class="hljs-keyword">done</span>
</code></pre>
<p>列出 <code>kubernetes-the-hard-way</code> VPC 网络的路由表:</p>
<pre><code class="lang-sh">gcloud compute routes list --filter <span class="hljs-string">"network: kubernetes-the-hard-way"</span>
</code></pre>
<p>输出为</p>
<pre><code class="lang-sh">NAME                            NETWORK                  DEST_RANGE     NEXT_HOP                  PRIORITY
default-route-081879136902de56  kubernetes-the-hard-way  10.240.0.0/24  kubernetes-the-hard-way   1000
default-route-55199a5aa126d7aa  kubernetes-the-hard-way  0.0.0.0/0      default-internet-gateway  1000
kubernetes-route-10-200-0-0-24  kubernetes-the-hard-way  10.200.0.0/24  10.240.0.20               1000
kubernetes-route-10-200-1-0-24  kubernetes-the-hard-way  10.200.1.0/24  10.240.0.21               1000
kubernetes-route-10-200-2-0-24  kubernetes-the-hard-way  10.200.2.0/24  10.240.0.22               1000
</code></pre>
<p>下一步：<a href="12-dns-addon.html">部署 DNS 扩展</a>。</p>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="部署-dns-扩展" class="level3">部署DNS扩展</h1>
<p>本部分将部署 <a href="https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/" target="_blank">DNS 扩展</a>，用于为集群内的应用提供服务发现。</p>
<h2 id="dns-扩展">DNS 扩展</h2>
<p>部属 <code>kube-dns</code> 群集扩展:</p>
<pre><code class="lang-sh">kubectl apply <span class="hljs-_">-f</span> https://storage.googleapis.com/kubernetes-the-hard-way/coredns.yaml
</code></pre>
<p>输出为</p>
<pre><code class="lang-sh">serviceaccount/coredns created
clusterrole.rbac.authorization.k8s.io/system:coredns created
clusterrolebinding.rbac.authorization.k8s.io/system:coredns created
configmap/coredns created
deployment.extensions/coredns created
service/kube-dns created
</code></pre>
<p>列出 <code>kube-dns</code> 部署的 Pod 列表:</p>
<pre><code class="lang-sh">kubectl get pods <span class="hljs-_">-l</span> k8s-app=kube-dns -n kube-system
</code></pre>
<p>输出为</p>
<pre><code class="lang-sh">NAME                       READY   STATUS    RESTARTS   AGE
coredns-699f8ddd77-94qv9   1/1     Running   0          20s
coredns-699f8ddd77-gtcgb   1/1     Running   0          20s
</code></pre>
<h2 id="验证">验证</h2>
<p>建立一个 <code>busybox</code> 部署:</p>
<pre><code class="lang-sh">kubectl run busybox --image=busybox --command -- sleep 3600
</code></pre>
<p>列出 <code>busybox</code> 部署的 Pod：</p>
<pre><code class="lang-sh">kubectl get pods <span class="hljs-_">-l</span> run=busybox
</code></pre>
<p>输出为</p>
<pre><code class="lang-sh">NAME                       READY     STATUS    RESTARTS   AGE
busybox-2125412808-mt2vb   1/1       Running   0          15s
</code></pre>
<p>查询 <code>busybox</code> Pod 的全名:</p>
<pre><code class="lang-sh">POD_NAME=$(kubectl get pods <span class="hljs-_">-l</span> run=busybox -o jsonpath=<span class="hljs-string">"{.items[0].metadata.name}"</span>)
</code></pre>
<p>在 <code>busybox</code> Pod 中查询 DNS：</p>
<pre><code class="lang-sh">kubectl <span class="hljs-built_in">exec</span> -ti <span class="hljs-variable">$POD_NAME</span> -- nslookup kubernetes
</code></pre>
<p>输出为</p>
<pre><code class="lang-sh">Server:    10.32.0.10
Address 1: 10.32.0.10 kube-dns.kube-system.svc.cluster.local

Name:      kubernetes
Address 1: 10.32.0.1 kubernetes.default.svc.cluster.local
</code></pre>
<p>下一步：<a href="13-smoke-test.html">烟雾测试</a>。</p>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="烟雾测试" class="level3">烟雾测试</h1>
<p>本部分将会运行一系列的测试来验证 Kubernetes 集群的功能正常。</p>
<h2 id="数据加密">数据加密</h2>
<p>本节将会验证 <a href="https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/#verifying-that-data-is-encrypted" target="_blank">encrypt secret data at rest</a> 的功能。</p>
<p>创建一个 Secret:</p>
<pre><code class="lang-sh">kubectl create secret generic kubernetes-the-hard-way \
  --from-literal=<span class="hljs-string">"mykey=mydata"</span>
</code></pre>
<p>查询存在 etcd 里 16 进位编码的 <code>kubernetes-the-hard-way</code> secret:</p>
<pre><code class="lang-sh">gcloud compute ssh controller-0 \
  --command <span class="hljs-string">"sudo ETCDCTL_API=3 etcdctl get \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/etcd/ca.pem \
  --cert=/etc/etcd/kubernetes.pem \
  --key=/etc/etcd/kubernetes-key.pem\
  /registry/secrets/default/kubernetes-the-hard-way | hexdump -C"</span>
</code></pre>
<p>输出为</p>
<pre><code class="lang-sh">00000000  2f 72 65 67 69 73 74 72  79 2f 73 65 63 72 65 74  |/registry/secret|
00000010  73 2f 64 65 66 61 75 6c  74 2f 6b 75 62 65 72 6e  |s/default/kubern|
00000020  65 74 65 73 2d 74 68 65  2d 68 61 72 64 2d 77 61  |etes-the-hard-wa|
00000030  79 0a 6b 38 73 3a 65 6e  63 3a 61 65 73 63 62 63  |y.k8s:enc:aescbc|
00000040  3a 76 31 3a 6b 65 79 31  3a ea 7c 76 32 43 62 6f  |:v1:key1:.|v2Cbo|
00000050  44 02 02 8c b7 ca fe 95  a5 33 f6 a1 18 6c 3d 53  |D........3...l=S|
00000060  e7 9c 51 ee 32 f6 e4 17  ea bb 11 d5 2f e2 40 00  |..Q.2......./.@.|
00000070  ae cf d9 e7 ba 7f 68 18  d3 c1 10 10 93 43 35 bd  |......h......C5.|
00000080  24 dd 66 b4 f8 f9 82 77  4a d5 78 03 19 41 1e bc  |$.f....wJ.x..A..|
00000090  94 3f 17 41 ad cc 8c ba  9f 8f 8e 56 97 7e 96 fb  |.?.A.......V.~..|
000000a0  8f 2e 6a a5 bf 08 1f 0b  c3 4b 2b 93 d1 ec f8 70  |..j......K+....p|
000000b0  c1 e4 1d 1a d2 0d f8 74  3a a1 4f 3c e0 c9 6d 3f  |.......t:.O<..m?|
000000c0  de a3 f5 fd 76 aa 5e bc  27 d9 3c 6b 8f 54 97 45  |....v.^.<span class="hljs-string">'.<k.T.E|
000000d0  31 25 ff 23 90 a4 2a f2  db 78 b1 3b ca 21 f3 6b  |1%.#..*..x.;.!.k|
000000e0  dd fb 8e 53 c6 23 0d 35  c8 0a                    |...S.#.5..|
000000ea
</span></code></pre>
<p>Etcd 的密钥以 <code>k8s:enc:aescbc:v1:key1</code> 为前缀, 表示使用密钥为 <code>key1</code> 的 <code>aescbc</code> 加密数据。</p>
<h2 id="部署">部署</h2>
<p>本节将会验证建立与管理 <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/" target="_blank">Deployments</a> 的功能。</p>
<p>创建一个 Deployment 用来搭建 <a href="https://nginx.org/en/" target="_blank">nginx</a> Web 服务：</p>
<pre><code class="lang-sh">kubectl run nginx --image=nginx
</code></pre>
<p>列出 <code>nginx</code> deployment 的 pods:</p>
<pre><code class="lang-sh">kubectl get pods <span class="hljs-_">-l</span> run=nginx
</code></pre>
<p>输出为</p>
<pre><code class="lang-sh">NAME                     READY     STATUS    RESTARTS   AGE
nginx-4217019353-b5gzn   1/1       Running   0          15s
</code></pre>
<h3 id="端口转发">端口转发</h3>
<p>本节将会验证使用 <a href="https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/" target="_blank">port forwarding</a> 从远端进入容器的功能。</p>
<p>查询 <code>nginx</code> pod 的全名:</p>
<pre><code class="lang-sh">POD_NAME=$(kubectl get pods <span class="hljs-_">-l</span> run=nginx -o jsonpath=<span class="hljs-string">"{.items[0].metadata.name}"</span>)
</code></pre>
<p>将本地机器的 8080 端口转发到 nginx pod 的 80 端口：</p>
<pre><code class="lang-sh">kubectl port-forward <span class="hljs-variable">$POD_NAME</span> 8080:80
</code></pre>
<p>输出为</p>
<pre><code class="lang-sh">Forwarding from 127.0.0.1:8080 -> 80
Forwarding from [::1]:8080 -> 80
</code></pre>
<p>开一个新的终端来做 HTTP 请求测试:</p>
<pre><code class="lang-sh">curl --head http://127.0.0.1:8080
</code></pre>
<p>输出为</p>
<pre><code class="lang-sh">HTTP/1.1 200 OK
Server: nginx/1.13.5
Date: Mon, 02 Oct 2017 01:04:20 GMT
Content-Type: text/html
Content-Length: 612
Last-Modified: Tue, 08 Aug 2017 15:25:00 GMT
Connection: keep-alive
ETag: <span class="hljs-string">"5989d7cc-264"</span>
Accept-Ranges: bytes
</code></pre>
<p>回到前面的终端并按下 <code>Ctrl + C</code> 停止 port forwarding 命令：</p>
<pre><code class="lang-sh">Forwarding from 127.0.0.1:8080 -> 80
Forwarding from [::1]:8080 -> 80
Handling connection <span class="hljs-keyword">for</span> 8080
^C
</code></pre>
<h3 id="容器日志">容器日志</h3>
<p>本节会验证 <a href="https://kubernetes.io/docs/concepts/cluster-administration/logging/" target="_blank">获取容器日志</a> 的功能。</p>
<p>输出 nginx Pod 的容器日志：</p>
<pre><code class="lang-sh">kubectl logs <span class="hljs-variable">$POD_NAME</span>
</code></pre>
<p>输出为</p>
<pre><code class="lang-sh">127.0.0.1 - - [02/Oct/2017:01:04:20 +0000] <span class="hljs-string">"HEAD / HTTP/1.1"</span> 200 0 <span class="hljs-string">"-"</span> <span class="hljs-string">"curl/7.54.0"</span> <span class="hljs-string">"-"</span>
</code></pre>
<h3 id="执行容器命令">执行容器命令</h3>
<p>本节将验证 <a href="https://kubernetes.io/docs/tasks/debug-application-cluster/get-shell-running-container/#running-individual-commands-in-a-container" target="_blank">在容器里执行命令</a> 的功能。</p>
<p>使用 <code>nginx -v</code> 命令在 <code>nginx</code> Pod 中输出 nginx 的版本：</p>
<pre><code class="lang-sh">kubectl <span class="hljs-built_in">exec</span> -ti <span class="hljs-variable">$POD_NAME</span> -- nginx -v
</code></pre>
<p>输出为</p>
<pre><code class="lang-sh">nginx version: nginx/1.13.7
</code></pre>
<h2 id="服务（service）">服务（Service）</h2>
<p>本节将验证 Kubernetes <a href="https://kubernetes.io/docs/concepts/services-networking/service/" target="_blank">Service</a>。</p>
<p>将 <code>nginx</code> 部署导出为 <a href="https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport" target="_blank">NodePort</a> 类型的 Service：</p>
<pre><code class="lang-sh">kubectl expose deployment nginx --port 80 --type NodePort
</code></pre>
<blockquote>
<p>LoadBalancer 类型的 Service 不能使用是因为没有设置 <a href="https://kubernetes.io/docs/getting-started-guides/scratch/#cloud-provider" target="_blank">cloud provider 集成</a>。 设定 cloud provider 不在本教程范围之内。</p>
</blockquote>
<p>查询 <code>nginx</code> 服务分配的 Node Port：</p>
<pre><code class="lang-sh">NODE_PORT=$(kubectl get svc nginx \
  --output=jsonpath=<span class="hljs-string">'{range .spec.ports[0]}{.nodePort}'</span>)
</code></pre>
<p>建立防火墙规则允许外网访问该 Node 端口：</p>
<pre><code class="lang-sh">gcloud compute firewall-rules create kubernetes-the-hard-way-allow-nginx-service \
  --allow=tcp:<span class="hljs-variable">${NODE_PORT}</span> \
  --network kubernetes-the-hard-way
</code></pre>
<p>查询 worker 节点的外网 IP 地址：</p>
<pre><code class="lang-sh">EXTERNAL_IP=$(gcloud compute instances describe worker-0 \
  --format <span class="hljs-string">'value(networkInterfaces[0].accessConfigs[0].natIP)'</span>)
</code></pre>
<p>对得到的外网 IP 地址 + nginx 服务的 Node Port 做 HTTP 请求测试：</p>
<pre><code class="lang-sh">curl -I http://<span class="hljs-variable">${EXTERNAL_IP}</span>:<span class="hljs-variable">${NODE_PORT}</span>
</code></pre>
<p>输出为</p>
<pre><code class="lang-sh">HTTP/1.1 200 OK
Server: nginx/1.13.7
Date: Mon, 18 Dec 2017 14:52:09 GMT
Content-Type: text/html
Content-Length: 612
Last-Modified: Tue, 21 Nov 2017 14:28:04 GMT
Connection: keep-alive
ETag: <span class="hljs-string">"5a1437f4-264"</span>
Accept-Ranges: bytes
</code></pre>
<h2 id="非可信应用">非可信应用</h2>
<p>非可信应用可以运行在 <a href="https://github.com/google/gvisor" target="_blank">gVisor</a> 容器引擎之中。</p>
<pre><code class="lang-sh">cat <<EOF | kubectl apply <span class="hljs-_">-f</span> -
apiVersion: v1
kind: Pod
metadata:
  name: untrusted
  annotations:
    io.kubernetes.cri.untrusted-workload: <span class="hljs-string">"true"</span>
spec:
  containers:
    - name: webserver
      image: gcr.io/hightowerlabs/helloworld:2.0.0
EOF
</code></pre>
<p>验证</p>
<pre><code class="lang-sh">kubectl get pods -o wide
</code></pre>
<pre><code>NAME                       READY     STATUS    RESTARTS   AGE       IP           NODE
busybox-68654f944b-djjjb   1/1       Running   0          5m        10.200.0.2   worker-0
nginx-65899c769f-xkfcn     1/1       Running   0          4m        10.200.1.2   worker-1
untrusted                  1/1       Running   0          10s       10.200.0.3   worker-0
</code></pre><p>查看 untrusted Pod 运行信息</p>
<pre><code class="lang-sh"><span class="hljs-comment"># SSH to the node</span>
INSTANCE_NAME=$(kubectl get pod untrusted --output=jsonpath=<span class="hljs-string">'{.spec.nodeName}'</span>)
gcloud compute ssh <span class="hljs-variable">${INSTANCE_NAME}</span>

<span class="hljs-comment"># List the containers running under gVisor</span>
sudo runsc --root  /run/containerd/runsc/k8s.io list
</code></pre>
<p>输出</p>
<pre><code class="lang-sh">I0514 14:03:56.108368   14988 x:0] ***************************
I0514 14:03:56.108548   14988 x:0] Args: [runsc --root /run/containerd/runsc/k8s.io list]
I0514 14:03:56.108730   14988 x:0] Git Revision: 08879266fef3a67fac1a77f1ea133c3ac75759dd
I0514 14:03:56.108787   14988 x:0] PID: 14988
I0514 14:03:56.108838   14988 x:0] UID: 0, GID: 0
I0514 14:03:56.108877   14988 x:0] Configuration:
I0514 14:03:56.108912   14988 x:0]              RootDir: /run/containerd/runsc/k8s.io
I0514 14:03:56.109000   14988 x:0]              Platform: ptrace
I0514 14:03:56.109080   14988 x:0]              FileAccess: proxy, overlay: <span class="hljs-literal">false</span>
I0514 14:03:56.109159   14988 x:0]              Network: sandbox, logging: <span class="hljs-literal">false</span>
I0514 14:03:56.109238   14988 x:0]              Strace: <span class="hljs-literal">false</span>, max size: 1024, syscalls: []
I0514 14:03:56.109315   14988 x:0] ***************************
ID                                                                 PID         STATUS      BUNDLE                                                           CREATED                          OWNER
3528c6b270c76858e15e10ede61bd1100b77519e7c9972d51b370d6a3c60adbb   14766       running     /run/containerd/io.containerd.runtime.v1.linux/k8s.io/3528c6b270c76858e15e10ede61bd1100b77519e7c9972d51b370d6a3c60adbb   2018-05-14T14:02:34.302378996Z
7ff747c919c2dcf31e64d7673340885138317c91c7c51ec6302527df680ba981   14716       running     /run/containerd/io.containerd.runtime.v1.linux/k8s.io/7ff747c919c2dcf31e64d7673340885138317c91c7c51ec6302527df680ba981   2018-05-14T14:02:32.159552044Z
I0514 14:03:56.111287   14988 x:0] Exiting with status: 0
</code></pre>
<p>查询容器中的进程</p>
<pre><code class="lang-sh">POD_ID=$(sudo crictl -r unix:///var/run/containerd/containerd.sock \
  pods --name untrusted -q)

CONTAINER_ID=$(sudo crictl -r unix:///var/run/containerd/containerd.sock \
  ps -p <span class="hljs-variable">${POD_ID}</span> -q)

sudo runsc --root /run/containerd/runsc/k8s.io ps <span class="hljs-variable">${CONTAINER_ID}</span>
</code></pre>
<p>输出</p>
<pre><code>I0514 14:05:16.499237   15096 x:0] ***************************
I0514 14:05:16.499542   15096 x:0] Args: [runsc --root /run/containerd/runsc/k8s.io ps 3528c6b270c76858e15e10ede61bd1100b77519e7c9972d51b370d6a3c60adbb]
I0514 14:05:16.499597   15096 x:0] Git Revision: 08879266fef3a67fac1a77f1ea133c3ac75759dd
I0514 14:05:16.499644   15096 x:0] PID: 15096
I0514 14:05:16.499695   15096 x:0] UID: 0, GID: 0
I0514 14:05:16.499734   15096 x:0] Configuration:
I0514 14:05:16.499769   15096 x:0]              RootDir: /run/containerd/runsc/k8s.io
I0514 14:05:16.499880   15096 x:0]              Platform: ptrace
I0514 14:05:16.499962   15096 x:0]              FileAccess: proxy, overlay: false
I0514 14:05:16.500042   15096 x:0]              Network: sandbox, logging: false
I0514 14:05:16.500120   15096 x:0]              Strace: false, max size: 1024, syscalls: []
I0514 14:05:16.500197   15096 x:0] ***************************
UID       PID       PPID      C         STIME     TIME      CMD
0         1         0         0         14:02     40ms      app
I0514 14:05:16.501354   15096 x:0] Exiting with status: 0
</code></pre><p>下一步：<a href="14-cleanup.html">删除集群</a>。</p>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="删除集群" class="level3">删除集群</h1>
<p>本部分将删除该教程所创建的全部计算资源。</p>
<h2 id="计算节点">计算节点</h2>
<p>删除所有的控制节点和 worker 节点:</p>
<pre><code class="lang-sh">gcloud -q compute instances delete \
  controller-0 controller-1 controller-2 \
  worker-0 worker-1 worker-2
</code></pre>
<h2 id="网路">网路</h2>
<p>删除外部负载均衡器以及网络资源:</p>
<pre><code class="lang-sh">gcloud -q compute forwarding-rules delete kubernetes-forwarding-rule \
    --region $(gcloud config get-value compute/region)
gcloud -q compute target-pools delete kubernetes-target-pool
gcloud -q compute http-health-checks delete kubernetes
gcloud -q compute addresses delete kubernetes-the-hard-way
</code></pre>
<p>删除 <code>kubernetes-the-hard-way</code> 防火墙规则:</p>
<pre><code class="lang-sh">gcloud -q compute firewall-rules delete \
  kubernetes-the-hard-way-allow-nginx-service \
  kubernetes-the-hard-way-allow-internal \
  kubernetes-the-hard-way-allow-external \
  kubernetes-the-hard-way-allow-health-check
</code></pre>
<p>删除 Pod 网络路由:</p>
<pre><code class="lang-sh">gcloud -q compute routes delete \
    kubernetes-route-10-200-0-0-24 \
    kubernetes-route-10-200-1-0-24 \
    kubernetes-route-10-200-2-0-24
</code></pre>
<p>删除 <code>kubernetes</code> 子网:</p>
<pre><code class="lang-sh">gcloud -q compute networks subnets delete kubernetes
</code></pre>
<p>删除 <code>kubernetes-the-hard-way</code> 网络 VPC:</p>
<pre><code class="lang-sh">gcloud -q compute networks delete kubernetes-the-hard-way
</code></pre>
</section>
                            
    <h1 class='level1'>插件扩展</h1><section class="normal markdown-section">
                                
                                <h1 id="api-扩展" class="level2">API扩展</h1>
<p>Kubernetes 的架构非常灵活，提供了从 API、认证授权、准入控制、网络、存储、运行时以及云平台等一系列的<a href="https://kubernetes.io/docs/concepts/extend-kubernetes/extend-cluster/" target="_blank">扩展机制</a>，方便用户无侵入的扩展集群的功能。</p>
<p>从 API 的角度来说，可以通过 Aggregation 和 CustomResourceDefinition（CRD） 等扩展 Kubernetes API。</p>
<ul>
<li>API Aggregation 允许在不修改 Kubernetes 核心代码的同时将第三方服务注册到 Kubernetes API 中，这样就可以通过 Kubernetes API 来访问外部服务。</li>
<li>CustomResourceDefinition 则可以在集群中新增资源对象，并可以与已有资源对象（如 Pod、Deployment 等）相同的方式来管理它们。</li>
</ul>
<p>CRD 相比 Aggregation 更易用，两者对比如下</p>
<table>
<thead>
<tr>
<th>CRDs</th>
<th>Aggregated API</th>
</tr>
</thead>
<tbody>
<tr>
<td>无需编程即可使用 CRD 管理资源</td>
<td>需要使用 Go 来构建 Aggregated APIserver</td>
</tr>
<tr>
<td>不需要运行额外服务，但一般需要一个 CRD 控制器同步和管理这些资源</td>
<td>需要独立的第三方服务</td>
</tr>
<tr>
<td>任何缺陷都会在 Kubernetes 核心中修复</td>
<td>可能需要定期从 Kubernetes 社区同步缺陷修复方法并重新构建 Aggregated APIserver.</td>
</tr>
<tr>
<td>无需额外管理版本</td>
<td>需要第三方服务来管理版本</td>
</tr>
</tbody>
</table>
<p>更多的特性对比</p>
<table>
<thead>
<tr>
<th>Feature</th>
<th>Description</th>
<th>CRDs</th>
<th>Aggregated API</th>
</tr>
</thead>
<tbody>
<tr>
<td>Validation</td>
<td>Help users prevent errors and allow you to evolve your API independently of your clients. These features are most useful when there are many clients who can’t all update at the same time.</td>
<td>Yes. Most validation can be specified in the CRD using <a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/extend-api-custom-resource-definitions/#validation" target="_blank">OpenAPI v3.0 validation</a>. Any other validations supported by addition of a Validating Webhook.</td>
<td>Yes, arbitrary validation checks</td>
</tr>
<tr>
<td>Defaulting</td>
<td>See above</td>
<td>Yes, via a Mutating Webhook; Planned, via CRD OpenAPI schema.</td>
<td>Yes</td>
</tr>
<tr>
<td>Multi-versioning</td>
<td>Allows serving the same object through two API versions. Can help ease API changes like renaming fields. Less important if you control your client versions.</td>
<td>No, but planned</td>
<td>Yes</td>
</tr>
<tr>
<td>Custom Storage</td>
<td>If you need storage with a different performance mode (for example, time-series database instead of key-value store) or isolation for security (for example, encryption secrets or different</td>
<td>No</td>
<td>Yes</td>
</tr>
<tr>
<td>Custom Business Logic</td>
<td>Perform arbitrary checks or actions when creating, reading, updating or deleting an object</td>
<td>Yes, using Webhooks.</td>
<td>Yes</td>
</tr>
<tr>
<td>Scale Subresource</td>
<td>Allows systems like HorizontalPodAutoscaler and PodDisruptionBudget interact with your new resource</td>
<td><a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/extend-api-custom-resource-definitions/#scale-subresource" target="_blank">Yes</a></td>
<td>Yes</td>
</tr>
<tr>
<td>Status Subresource</td>
<td>Finer-grained access control: user writes spec section, controller writes status section.Allows incrementing object Generation on custom resource data mutation (requires separate spec and status sections in the resource)</td>
<td><a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/extend-api-custom-resource-definitions/#status-subresource" target="_blank">Yes</a></td>
<td>Yes</td>
</tr>
<tr>
<td>Other Subresources</td>
<td>Add operations other than CRUD, such as “logs” or “exec”.</td>
<td>No</td>
<td>Yes</td>
</tr>
<tr>
<td>strategic-merge-patch</td>
<td>The new endpoints support PATCH with <code>Content-Type: application/strategic-merge-patch+json</code>. Useful for updating objects that may be modified both locally, and by the server. For more information, see <a href="https://kubernetes.io/docs/tasks/run-application/update-api-object-kubectl-patch/" target="_blank">“Update API Objects in Place Using kubectl patch”</a></td>
<td>No, but similar functionality planned</td>
<td>Yes</td>
</tr>
<tr>
<td>Protocol Buffers</td>
<td>The new resource supports clients that want to use Protocol Buffers</td>
<td>No</td>
<td>Yes</td>
</tr>
<tr>
<td>OpenAPI Schema</td>
<td>Is there an OpenAPI (swagger) schema for the types that can be dynamically fetched from the server? Is the user protected from misspelling field names by ensuring only allowed fields are set? Are types enforced (in other words, don’t put an <code>int</code> in a <code>string</code> field?)</td>
<td>No, but planned</td>
<td>Yes</td>
</tr>
</tbody>
</table>
<h2 id="使用方法">使用方法</h2>
<p>详细的使用方法请参考</p>
<ul>
<li><a href="aggregation.html">Aggregation</a></li>
<li><a href="../concepts/customresourcedefinition.html">CustomResourceDefinition</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="aggregation-layer" class="level3">Aggregation</h1>
<p>API Aggregation 允许在不修改 Kubernetes 核心代码的同时扩展 Kubernetes API，即将第三方服务注册到 Kubernetes API 中，这样就可以通过 Kubernetes API 来访问外部服务。</p>
<blockquote>
<p>备注：另外一种扩展 Kubernetes API 的方法是使用 <a href="../concepts/customresourcedefinition.html">CustomResourceDefinition (CRD)</a>。</p>
</blockquote>
<h2 id="何时使用-aggregation">何时使用 Aggregation</h2>
<table>
<thead>
<tr>
<th>满足以下条件时使用 API Aggregation</th>
<th>满足以下条件时使用独立 API</th>
</tr>
</thead>
<tbody>
<tr>
<td>Your API is <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/#declarative-apis" target="_blank">Declarative</a>.</td>
<td>Your API does not fit the <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/#declarative-apis" target="_blank">Declarative</a> model.</td>
</tr>
<tr>
<td>You want your new types to be readable and writable using <code>kubectl</code>.</td>
<td><code>kubectl</code> support is not required</td>
</tr>
<tr>
<td>You want to view your new types in a Kubernetes UI, such as dashboard, alongside built-in types.</td>
<td>Kubernetes UI support is not required.</td>
</tr>
<tr>
<td>You are developing a new API.</td>
<td>You already have a program that serves your API and works well.</td>
</tr>
<tr>
<td>You are willing to accept the format restriction that Kubernetes puts on REST resource paths, such as API Groups and Namespaces. (See the <a href="https://kubernetes.io/docs/concepts/overview/kubernetes-api/" target="_blank">API Overview</a>.)</td>
<td>You need to have specific REST paths to be compatible with an already defined REST API.</td>
</tr>
<tr>
<td>Your resources are naturally scoped to a cluster or to namespaces of a cluster.</td>
<td>Cluster or namespace scoped resources are a poor fit; you need control over the specifics of resource paths.</td>
</tr>
<tr>
<td>You want to reuse <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/#common-features" target="_blank">Kubernetes API support features</a>.</td>
<td>You don’t need those features.</td>
</tr>
</tbody>
</table>
<h2 id="开启-api-aggregation">开启 API Aggregation</h2>
<p>kube-apiserver 增加以下配置</p>
<pre><code class="lang-sh">--requestheader-client-ca-file=<path to aggregator CA cert>
--requestheader-allowed-names=aggregator
--requestheader-extra-headers-prefix=X-Remote-Extra-
--requestheader-group-headers=X-Remote-Group
--requestheader-username-headers=X-Remote-User
--proxy-client-cert-file=<path to aggregator proxy cert>
--proxy-client-key-file=<path to aggregator proxy key>
</code></pre>
<p>如果 <code>kube-proxy</code> 没有在 Master 上面运行，还需要配置</p>
<pre><code class="lang-sh">--enable-aggregator-routing=<span class="hljs-literal">true</span>
</code></pre>
<h2 id="创建扩展-api">创建扩展 API</h2>
<ol>
<li>确保开启 APIService API（默认开启，可用 <code>kubectl get apiservice</code> 命令验证）</li>
<li>创建 RBAC 规则</li>
<li>创建一个 namespace，用来运行扩展的 API 服务</li>
<li>创建 CA 和证书，用于 https</li>
<li>创建一个存储证书的 secret</li>
<li>创建一个部署扩展 API 服务的 deployment，并使用上一步的 secret 配置证书，开启 https 服务</li>
<li>创建一个 ClusterRole 和 ClusterRoleBinding</li>
<li>创建一个非 namespace 的 apiservice，注意设置 <code>spec.caBundle</code></li>
<li>运行 <code>kubectl get <resource-name></code>，正常应该返回 <code>No resources found.</code></li>
</ol>
<p>可以使用 <a href="https://github.com/kubernetes-incubator/apiserver-builder" target="_blank">apiserver-builder</a> 工具自动化上面的步骤。</p>
<pre><code class="lang-sh"><span class="hljs-comment"># 初始化项目</span>
$ <span class="hljs-built_in">cd</span> GOPATH/src/github.com/my-org/my-project
$ apiserver-boot init repo --domain <your-domain>
$ apiserver-boot init glide

<span class="hljs-comment"># 创建资源</span>
$ apiserver-boot create group version resource --group <group> --version <version> --kind <Kind>

<span class="hljs-comment"># 编译</span>
$ apiserver-boot build executables
$ apiserver-boot build docs

<span class="hljs-comment"># 本地运行</span>
$ apiserver-boot run <span class="hljs-built_in">local</span>

<span class="hljs-comment"># 集群运行</span>
$ apiserver-boot run <span class="hljs-keyword">in</span>-cluster --name nameofservicetorun --namespace default --image gcr.io/myrepo/myimage:mytag
$ kubectl create <span class="hljs-_">-f</span> sample/<<span class="hljs-built_in">type</span>>.yaml
</code></pre>
<h2 id="示例">示例</h2>
<p>见 <a href="https://github.com/kubernetes/sample-apiserver" target="_blank">sample-apiserver</a> 和 <a href="https://github.com/kubernetes-incubator/apiserver-builder/tree/master/example" target="_blank">apiserver-builder/example</a>。</p>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="访问控制" class="level2">访问控制</h1>
<p>Kubernetes 对 API 访问提供了三种安全访问控制措施：认证、授权和 Admission Control。认证解决用户是谁的问题，授权解决用户能做什么的问题，Admission Control 则是资源管理方面的作用。通过合理的权限管理，能够保证系统的安全可靠。</p>
<p>Kubernetes 集群的所有操作基本上都是通过 kube-apiserver 这个组件进行的，它提供 HTTP RESTful 形式的 API 供集群内外客户端调用。需要注意的是：认证授权过程只存在 HTTPS 形式的 API 中。也就是说，如果客户端使用 HTTP 连接到 kube-apiserver，那么是不会进行认证授权的。所以说，可以这么设置，在集群内部组件间通信使用 HTTP，集群外部就使用 HTTPS，这样既增加了安全性，也不至于太复杂。</p>
<p>下图是 API 访问要经过的三个步骤，前面两个是认证和授权，第三个是 Admission Control。</p>
<p><img src="images/authentication.png" alt=""/></p>
<h2 id="认证">认证</h2>
<p>开启 TLS 时，所有的请求都需要首先认证。Kubernetes 支持多种认证机制，并支持同时开启多个认证插件（只要有一个认证通过即可）。如果认证成功，则用户的 <code>username</code> 会传入授权模块做进一步授权验证；而对于认证失败的请求则返回 HTTP 401。</p>
<blockquote>
<p><strong>Kubernetes 不直接管理用户</strong></p>
<p>虽然 Kubernetes 认证和授权用到了 username，但 Kubernetes 并不直接管理用户，不能创建 <code>user</code> 对象，
也不存储 username。但是 Kubernetes 提供了 Service Account，用来与 API 交互。</p>
</blockquote>
<p>目前，Kubernetes 支持以下认证插件：</p>
<ul>
<li>X509 证书</li>
<li>静态 Token 文件</li>
<li>引导 Token</li>
<li>静态密码文件</li>
<li>Service Account</li>
<li>OpenID</li>
<li>Webhook</li>
<li>认证代理</li>
<li>OpenStack Keystone 密码</li>
</ul>
<p>详细使用方法请参考<a href="authentication.html">这里</a></p>
<h2 id="授权">授权</h2>
<p>授权主要是用于对集群资源的访问控制，通过检查请求包含的相关属性值，与相对应的访问策略相比较，API 请求必须满足某些策略才能被处理。跟认证类似，Kubernetes 也支持多种授权机制，并支持同时开启多个授权插件（只要有一个验证通过即可）。如果授权成功，则用户的请求会发送到准入控制模块做进一步的请求验证；对于授权失败的请求则返回 HTTP 403。</p>
<p>Kubernetes 授权仅处理以下的请求属性：</p>
<ul>
<li>user, group, extra</li>
<li>API、请求方法（如 get、post、update、patch 和 delete）和请求路径（如 <code>/api</code>）</li>
<li>请求资源和子资源</li>
<li>Namespace</li>
<li>API Group</li>
</ul>
<p>目前，Kubernetes 支持以下授权插件：</p>
<ul>
<li>ABAC</li>
<li>RBAC</li>
<li>Webhook</li>
<li>Node</li>
</ul>
<blockquote>
<p><strong>AlwaysDeny 和 AlwaysAllow</strong></p>
<p>Kubernetes 还支持 AlwaysDeny 和 AlwaysAllow 模式，其中 AlwaysDeny 仅用来测试，而 AlwaysAllow 则
允许所有请求（会覆盖其他模式）。</p>
</blockquote>
<h3 id="abac-授权">ABAC 授权</h3>
<p>使用 ABAC 授权需要 API Server 配置 <code>--authorization-policy-file=SOME_FILENAME</code>，文件格式为每行一个 json 对象，比如</p>
<pre><code class="lang-json">{
    <span class="hljs-string">"apiVersion"</span>: <span class="hljs-string">"abac.authorization.kubernetes.io/v1beta1"</span>,
    <span class="hljs-string">"kind"</span>: <span class="hljs-string">"Policy"</span>,
    <span class="hljs-string">"spec"</span>: {
        <span class="hljs-string">"group"</span>: <span class="hljs-string">"system:authenticated"</span>,
        <span class="hljs-string">"nonResourcePath"</span>: <span class="hljs-string">"*"</span>,
        <span class="hljs-string">"readonly"</span>: <span class="hljs-literal">true</span>
    }
}
{
    <span class="hljs-string">"apiVersion"</span>: <span class="hljs-string">"abac.authorization.kubernetes.io/v1beta1"</span>,
    <span class="hljs-string">"kind"</span>: <span class="hljs-string">"Policy"</span>,
    <span class="hljs-string">"spec"</span>: {
        <span class="hljs-string">"group"</span>: <span class="hljs-string">"system:unauthenticated"</span>,
        <span class="hljs-string">"nonResourcePath"</span>: <span class="hljs-string">"*"</span>,
        <span class="hljs-string">"readonly"</span>: <span class="hljs-literal">true</span>
    }
}
{
    <span class="hljs-string">"apiVersion"</span>: <span class="hljs-string">"abac.authorization.kubernetes.io/v1beta1"</span>,
    <span class="hljs-string">"kind"</span>: <span class="hljs-string">"Policy"</span>,
    <span class="hljs-string">"spec"</span>: {
        <span class="hljs-string">"user"</span>: <span class="hljs-string">"admin"</span>,
        <span class="hljs-string">"namespace"</span>: <span class="hljs-string">"*"</span>,
        <span class="hljs-string">"resource"</span>: <span class="hljs-string">"*"</span>,
        <span class="hljs-string">"apiGroup"</span>: <span class="hljs-string">"*"</span>
    }
}
</code></pre>
<h3 id="rbac-授权">RBAC 授权</h3>
<p>见 <a href="rbac.html">RBAC 授权</a>。</p>
<h3 id="webhook-授权">WebHook 授权</h3>
<p>使用 WebHook 授权需要 API Server 配置 <code>--authorization-webhook-config-file=SOME_FILENAME</code> 和 <code>--runtime-config=authorization.k8s.io/v1beta1=true</code>，配置文件格式同 kubeconfig，如</p>
<pre><code class="lang-yaml"><span class="hljs-comment"># clusters refers to the remote service.</span>
<span class="hljs-attr">clusters:</span>
<span class="hljs-attr">  - name:</span> name-of-remote-authz-service
<span class="hljs-attr">    cluster:</span>
      <span class="hljs-comment"># CA for verifying the remote service.</span>
<span class="hljs-attr">      certificate-authority:</span> /path/to/ca.pem
      <span class="hljs-comment"># URL of remote service to query. Must use 'https'.</span>
<span class="hljs-attr">      server:</span> https://authz.example.com/authorize

<span class="hljs-comment"># users refers to the API Server's webhook configuration.</span>
<span class="hljs-attr">users:</span>
<span class="hljs-attr">  - name:</span> name-of-api-server
<span class="hljs-attr">    user:</span>
      <span class="hljs-comment"># cert for the webhook plugin to use</span>
<span class="hljs-attr">      client-certificate:</span> /path/to/cert.pem
       <span class="hljs-comment"># key matching the cert</span>
<span class="hljs-attr">      client-key:</span> /path/to/key.pem

<span class="hljs-comment"># kubeconfig files require a context. Provide one for the API Server.</span>
<span class="hljs-attr">current-context:</span> webhook
<span class="hljs-attr">contexts:</span>
<span class="hljs-attr">- context:</span>
<span class="hljs-attr">    cluster:</span> name-of-remote-authz-service
<span class="hljs-attr">    user:</span> name-of-api-server
<span class="hljs-attr">  name:</span> webhook
</code></pre>
<p>API Server 请求 Webhook server 的格式为</p>
<pre><code class="lang-json">{
  <span class="hljs-string">"apiVersion"</span>: <span class="hljs-string">"authorization.k8s.io/v1beta1"</span>,
  <span class="hljs-string">"kind"</span>: <span class="hljs-string">"SubjectAccessReview"</span>,
  <span class="hljs-string">"spec"</span>: {
    <span class="hljs-string">"resourceAttributes"</span>: {
      <span class="hljs-string">"namespace"</span>: <span class="hljs-string">"kittensandponies"</span>,
      <span class="hljs-string">"verb"</span>: <span class="hljs-string">"get"</span>,
      <span class="hljs-string">"group"</span>: <span class="hljs-string">"unicorn.example.org"</span>,
      <span class="hljs-string">"resource"</span>: <span class="hljs-string">"pods"</span>
    },
    <span class="hljs-string">"user"</span>: <span class="hljs-string">"jane"</span>,
    <span class="hljs-string">"group"</span>: [
      <span class="hljs-string">"group1"</span>,
      <span class="hljs-string">"group2"</span>
    ]
  }
}
</code></pre>
<p>而 Webhook server 需要返回授权的结果，允许 (allowed=true) 或拒绝(allowed=false)：</p>
<pre><code class="lang-json">{
  <span class="hljs-string">"apiVersion"</span>: <span class="hljs-string">"authorization.k8s.io/v1beta1"</span>,
  <span class="hljs-string">"kind"</span>: <span class="hljs-string">"SubjectAccessReview"</span>,
  <span class="hljs-string">"status"</span>: {
    <span class="hljs-string">"allowed"</span>: <span class="hljs-literal">true</span>
  }
}
</code></pre>
<h3 id="node-授权">Node 授权</h3>
<p>v1.7 + 支持 Node 授权，配合 <code>NodeRestriction</code> 准入控制来限制 kubelet 仅可访问 node、endpoint、pod、service 以及 secret、configmap、PV 和 PVC 等相关的资源，配置方法为</p>
<p><code>--authorization-mode=Node,RBAC --admission-control=...,NodeRestriction,...</code></p>
<p>注意，kubelet 认证需要使用 <code>system:nodes</code> 组，并使用用户名 <code>system:node:<nodeName></code>。</p>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://kubernetes.io/docs/admin/authentication/" target="_blank">Authenticating</a></li>
<li><a href="https://kubernetes.io/docs/admin/authorization/" target="_blank">Authorization</a></li>
<li><a href="https://kubernetes.io/docs/admin/bootstrap-tokens/" target="_blank">Bootstrap Tokens</a></li>
<li><a href="https://kubernetes.io/docs/admin/service-accounts-admin/" target="_blank">Managing Service Accounts</a></li>
<li><a href="https://kubernetes.io/docs/admin/authorization/abac/" target="_blank">ABAC Mode</a></li>
<li><a href="https://kubernetes.io/docs/admin/authorization/webhook/" target="_blank">Webhook Mode</a></li>
<li><a href="https://kubernetes.io/docs/admin/authorization/node/" target="_blank">Node Authorization</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="kubernetes-认证" class="level3">认证</h1>
<p>开启 TLS 时，所有的请求都需要首先认证。Kubernetes 支持多种认证机制，并支持同时开启多个认证插件（只要有一个认证通过即可）。如果认证成功，则用户的 <code>username</code> 会传入授权模块做进一步授权验证；而对于认证失败的请求则返回 HTTP 401。</p>
<blockquote>
<p><strong>Kubernetes 不直接管理用户</strong></p>
<p>虽然 Kubernetes 认证和授权用到了 username，但 Kubernetes 并不直接管理用户，不能创建 <code>user</code> 对象，
也不存储 username。但是 Kubernetes 提供了 Service Account，用来与 API 交互。</p>
</blockquote>
<p>目前，Kubernetes 支持以下认证插件：</p>
<ul>
<li>X509 证书</li>
<li>静态 Token 文件</li>
<li>引导 Token</li>
<li>静态密码文件</li>
<li>Service Account</li>
<li>OpenID</li>
<li>Webhook</li>
<li>认证代理</li>
<li>OpenStack Keystone 密码</li>
</ul>
<h2 id="x509-证书">X509 证书</h2>
<p>使用 X509 客户端证书只需要 API Server 启动时配置 <code>--client-ca-file=SOMEFILE</code>。在证书认证时，其 CN 域用作用户名，而组织机构域则用作 group 名。</p>
<p>创建一个客户端证书的方法为：</p>
<pre><code class="lang-sh">openssl req -new -key jbeda.pem -out jbeda-csr.pem -subj <span class="hljs-string">"/CN=jbeda/O=app1/O=app2"</span>
</code></pre>
<h2 id="静态-token-文件">静态 Token 文件</h2>
<p>使用静态 Token 文件认证只需要 API Server 启动时配置 <code>--token-auth-file=SOMEFILE</code>。该文件为 csv 格式，每行至少包括三列 <code>token,username,user id</code>，后面是可选的 group 名，比如</p>
<pre><code>token,user,uid,"group1,group2,group3"
</code></pre><p>客户端在使用 token 认证时，需要在请求头中加入 Bearer Authorization 头，比如</p>
<pre><code>Authorization: Bearer 31ada4fd-adec-460c-809a-9e56ceb75269
</code></pre><h2 id="引导-token">引导 Token</h2>
<p>引导 Token 是动态生成的，存储在 kube-system namespace 的 Secret 中，用来部署新的 Kubernetes 集群。</p>
<p>使用引导 Token 需要 API Server 启动时配置 <code>--experimental-bootstrap-token-auth</code>，并且 Controller Manager 开启 TokenCleaner <code>--controllers=*,tokencleaner,bootstrapsigner</code>。</p>
<p>在使用 kubeadm 部署 Kubernetes 时，kubeadm 会自动创建默认 token，可通过 <code>kubeadm token list</code> 命令查询。</p>
<h2 id="静态密码文件">静态密码文件</h2>
<p>需要 API Server 启动时配置 <code>--basic-auth-file=SOMEFILE</code>，文件格式为 csv，每行至少三列 <code>password, user, uid</code>，后面是可选的 group 名，如</p>
<pre><code>password,user,uid,"group1,group2,group3"
</code></pre><p>客户端在使用密码认证时，需要在请求头重加入 Basic Authorization 头，如</p>
<pre><code>Authorization: Basic BASE64ENCODED(USER:PASSWORD)
</code></pre><h2 id="service-account">Service Account</h2>
<p>ServiceAccount 是 Kubernetes 自动生成的，并会自动挂载到容器的 <code>/var/run/secrets/kubernetes.io/serviceaccount</code> 目录中。</p>
<p>在认证时，ServiceAccount 的用户名格式为 <code>system:serviceaccount:(NAMESPACE):(SERVICEACCOUNT)</code>，并从属于两个 group：<code>system:serviceaccounts</code> 和 <code>system:serviceaccounts:(NAMESPACE)</code>。</p>
<h2 id="openid">OpenID</h2>
<p>OpenID 提供了 OAuth2 的认证机制，是很多云服务商（如 GCE、Azure 等）的首选认证方法。</p>
<p><img src="images/oidc.png" alt=""/></p>
<p>使用 OpenID 认证，API Server 需要配置</p>
<ul>
<li><code>--oidc-issuer-url</code>，如 <code>https://accounts.google.com</code></li>
<li><code>--oidc-client-id</code>，如 <code>kubernetes</code></li>
<li><code>--oidc-username-claim</code>，如 <code>sub</code></li>
<li><code>--oidc-groups-claim</code>，如 <code>groups</code></li>
<li><code>--oidc-ca-file</code>，如 <code>/etc/kubernetes/ssl/kc-ca.pem</code></li>
</ul>
<h2 id="webhook">Webhook</h2>
<p>API Server 需要配置</p>
<pre><code class="lang-sh"><span class="hljs-comment"># 配置如何访问 webhook server</span>
--authentication-token-webhook-config-file
<span class="hljs-comment"># 默认 2 分钟</span>
--authentication-token-webhook-cache-ttl
</code></pre>
<p>配置文件格式为</p>
<pre><code class="lang-yaml"><span class="hljs-comment"># clusters refers to the remote service.</span>
<span class="hljs-attr">clusters:</span>
<span class="hljs-attr">  - name:</span> name-of-remote-authn-service
<span class="hljs-attr">    cluster:</span>
      <span class="hljs-comment"># CA for verifying the remote service.</span>
<span class="hljs-attr">      certificate-authority:</span> /path/to/ca.pem
      <span class="hljs-comment"># URL of remote service to query. Must use 'https'.</span>
<span class="hljs-attr">      server:</span> https://authn.example.com/authenticate

<span class="hljs-comment"># users refers to the API server's webhook configuration.</span>
<span class="hljs-attr">users:</span>
<span class="hljs-attr">  - name:</span> name-of-api-server
<span class="hljs-attr">    user:</span>
      <span class="hljs-comment"># cert for the webhook plugin to use</span>
<span class="hljs-attr">      client-certificate:</span> /path/to/cert.pem
       <span class="hljs-comment"># key matching the cert</span>
<span class="hljs-attr">      client-key:</span> /path/to/key.pem

<span class="hljs-comment"># kubeconfig files require a context. Provide one for the API server.</span>
<span class="hljs-attr">current-context:</span> webhook
<span class="hljs-attr">contexts:</span>
<span class="hljs-attr">- context:</span>
<span class="hljs-attr">    cluster:</span> name-of-remote-authn-service
<span class="hljs-attr">    user:</span> name-of-api-sever
<span class="hljs-attr">  name:</span> webhook
</code></pre>
<p>Kubernetes 发给 webhook server 的请求格式为</p>
<pre><code class="lang-json">{
  <span class="hljs-string">"apiVersion"</span>: <span class="hljs-string">"authentication.k8s.io/v1beta1"</span>,
  <span class="hljs-string">"kind"</span>: <span class="hljs-string">"TokenReview"</span>,
  <span class="hljs-string">"spec"</span>: {
    <span class="hljs-string">"token"</span>: <span class="hljs-string">"(BEARERTOKEN)"</span>
  }
}
</code></pre>
<p>示例：<a href="https://github.com/oursky/kubernetes-github-authn" target="_blank">kubernetes-github-authn</a> 实现了一个基于 WebHook 的 github 认证。</p>
<h2 id="认证代理">认证代理</h2>
<p>API Server 需要配置</p>
<pre><code class="lang-sh">--requestheader-username-headers=X-Remote-User
--requestheader-group-headers=X-Remote-Group
--requestheader-extra-headers-prefix=X-Remote-Extra-
<span class="hljs-comment"># 为了防止头部欺骗，证书是必选项</span>
--requestheader-client-ca-file
<span class="hljs-comment"># 设置允许的 CN 列表。可选。</span>
--requestheader-allowed-names
</code></pre>
<h2 id="openstack-keystone-密码">OpenStack Keystone 密码</h2>
<p>需要 API Server 在启动时指定 <code>--experimental-keystone-url=<AuthURL></code>，而 https 时还需要设置 <code>--experimental-keystone-ca-file=SOMEFILE</code>。</p>
<blockquote>
<p><strong>不支持 Keystone v3</strong></p>
<p>目前只支持 keystone v2.0，不支持 v3（无法传入 domain）。</p>
</blockquote>
<h2 id="匿名请求">匿名请求</h2>
<p>如果使用 AlwaysAllow 以外的认证模式，则匿名请求默认开启，但可用 <code>--anonymous-auth=false</code> 禁止匿名请求。</p>
<p>匿名请求的用户名格式为 <code>system:anonymous</code>，而 group 则为 <code>system:unauthenticated</code>。</p>
<h2 id="credential-plugin">Credential Plugin</h2>
<p>从 v1.11 开始支持 Credential Plugin（Beta），通过调用外部插件来获取用户的访问凭证。这是一种客户端认证插件，用来支持不在 Kubernetes 中内置的认证协议，如 LDAP、OAuth2、SAML 等。它通常与 <a href="#webhook">Webhook</a> 配合使用。</p>
<p>Credential Plugin 可以在 kubectl 的配置文件中设置，比如</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Config
<span class="hljs-attr">users:</span>
<span class="hljs-attr">- name:</span> my-user
<span class="hljs-attr">  user:</span>
<span class="hljs-attr">    exec:</span>
      <span class="hljs-comment"># Command to execute. Required.</span>
<span class="hljs-attr">      command:</span> <span class="hljs-string">"example-client-go-exec-plugin"</span>

      <span class="hljs-comment"># API version to use when decoding the ExecCredentials resource. Required.</span>
      <span class="hljs-comment">#</span>
      <span class="hljs-comment"># The API version returned by the plugin MUST match the version listed here.</span>
      <span class="hljs-comment">#</span>
      <span class="hljs-comment"># To integrate with tools that support multiple versions (such as client.authentication.k8s.io/v1alpha1),</span>
      <span class="hljs-comment"># set an environment variable or pass an argument to the tool that indicates which version the exec plugin expects.</span>
<span class="hljs-attr">      apiVersion:</span> <span class="hljs-string">"client.authentication.k8s.io/v1beta1"</span>

      <span class="hljs-comment"># Environment variables to set when executing the plugin. Optional.</span>
<span class="hljs-attr">      env:</span>
<span class="hljs-attr">      - name:</span> <span class="hljs-string">"FOO"</span>
<span class="hljs-attr">        value:</span> <span class="hljs-string">"bar"</span>

      <span class="hljs-comment"># Arguments to pass when executing the plugin. Optional.</span>
<span class="hljs-attr">      args:</span>
<span class="hljs-bullet">      -</span> <span class="hljs-string">"arg1"</span>
<span class="hljs-bullet">      -</span> <span class="hljs-string">"arg2"</span>
<span class="hljs-attr">clusters:</span>
<span class="hljs-attr">- name:</span> my-cluster
<span class="hljs-attr">  cluster:</span>
<span class="hljs-attr">    server:</span> <span class="hljs-string">"https://172.17.4.100:6443"</span>
<span class="hljs-attr">    certificate-authority:</span> <span class="hljs-string">"/etc/kubernetes/ca.pem"</span>
<span class="hljs-attr">contexts:</span>
<span class="hljs-attr">- name:</span> my-cluster
<span class="hljs-attr">  context:</span>
<span class="hljs-attr">    cluster:</span> my-cluster
<span class="hljs-attr">    user:</span> my-user
<span class="hljs-attr">current-context:</span> my-cluster
</code></pre>
<p>具体的插件开发及使用方法请参考 <a href="https://github.com/kubernetes/client-go/tree/master/plugin/pkg/client/auth" target="_blank">kubernetes/client-go</a>。</p>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="rbac" class="level3">RBAC授权</h1>
<p>Kubernetes 从 1.6 开始支持基于角色的访问控制机制（Role-Based Access，RBAC），集群管理员可以对用户或服务账号的角色进行更精确的资源访问控制。在 RBAC 中，权限与角色相关联，用户通过成为适当角色的成员而得到这些角色的权限。这就极大地简化了权限的管理。在一个组织中，角色是为了完成各种工作而创造，用户则依据它的责任和资格来被指派相应的角色，用户可以很容易地从一个角色被指派到另一个角色。</p>
<h2 id="前言">前言</h2>
<p><a href="http://blog.kubernetes.io/2017/03/kubernetes-1.6-multi-user-multi-workloads-at-scale.html" target="_blank">Kubernetes 1.6</a> 中的一个亮点时 RBAC 访问控制机制升级到了 beta 版本（版本为 <code>rbac.authorization.k8s.io/v1beta1</code> ）。RBAC，基于角色的访问控制机制，是用来管理 kubernetes 集群中资源访问权限的机制。使用 RBAC 可以很方便的更新访问授权策略而不用重启集群。</p>
<p>从 Kubernetes 1.8 开始，RBAC 进入稳定版，其 API 为 <code>rbac.authorization.k8s.io/v1</code>。</p>
<p>在使用 RBAC 时，只需要在启动 kube-apiserver 时配置 <code>--authorization-mode=RBAC</code> 即可。</p>
<h2 id="rbac-vs-abac">RBAC vs ABAC</h2>
<p>目前 kubernetes 中已经有一系列 l <a href="https://kubernetes.io/docs/admin/authorization/" target="_blank">鉴权机制</a>。鉴权的作用是，决定一个用户是否有权使用 Kubernetes API 做某些事情。它除了会影响 kubectl 等组件之外，还会对一些运行在集群内部并对集群进行操作的软件产生作用，例如使用了 Kubernetes 插件的 Jenkins，或者是利用 Kubernetes API 进行软件部署的 Helm。ABAC 和 RBAC 都能够对访问策略进行配置。</p>
<p>ABAC（Attribute Based Access Control）本来是不错的概念，但是在 Kubernetes 中的实现比较难于管理和理解，而且需要对 Master 所在节点的 SSH 和文件系统权限，而且要使得对授权的变更成功生效，还需要重新启动 API Server。</p>
<p>而 RBAC 的授权策略可以利用 kubectl 或者 Kubernetes API 直接进行配置。<strong>RBAC 可以授权给用户，让用户有权进行授权管理，这样就可以无需接触节点，直接进行授权管理。</strong>RBAC 在 Kubernetes 中被映射为 API 资源和操作。</p>
<p>因为 Kubernetes 社区的投入和偏好，相对于 ABAC 而言，RBAC 是更好的选择。</p>
<h2 id="基础概念">基础概念</h2>
<p>需要理解 RBAC 一些基础的概念和思路，RBAC 是让用户能够访问 <a href="https://kubernetes.io/docs/api-reference/v1.6/" target="_blank">Kubernetes API 资源</a> 的授权方式。</p>
<p><img src="images/rbac1.png" alt="RBAC 架构图 1"/></p>
<p>在 RBAC 中定义了两个对象，用于描述在用户和资源之间的连接权限。</p>
<h3 id="role-与-clusterrole">Role 与 ClusterRole</h3>
<p>Role（角色）是一系列权限的集合，例如一个角色可以包含读取 Pod 的权限和列出 Pod 的权限。Role 只能用来给某个特定 namespace 中的资源作鉴权，对多 namespace 和集群级的资源或者是非资源类的 API（如 <code>/healthz</code>）使用 ClusterRole。</p>
<pre><code class="lang-yaml"><span class="hljs-comment"># Role 示例</span>
<span class="hljs-attr">kind:</span> Role
<span class="hljs-attr">apiVersion:</span> rbac.authorization.k8s.io/v1
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  namespace:</span> default
<span class="hljs-attr">  name:</span> pod-reader
<span class="hljs-attr">rules:</span>
<span class="hljs-attr">- apiGroups:</span> [<span class="hljs-string">""</span>] <span class="hljs-comment">#"" indicates the core API group</span>
<span class="hljs-attr">  resources:</span> [<span class="hljs-string">"pods"</span>]
<span class="hljs-attr">  verbs:</span> [<span class="hljs-string">"get"</span>, <span class="hljs-string">"watch"</span>, <span class="hljs-string">"list"</span>]
</code></pre>
<pre><code class="lang-yaml"><span class="hljs-comment"># ClusterRole 示例</span>
<span class="hljs-attr">kind:</span> ClusterRole
<span class="hljs-attr">apiVersion:</span> rbac.authorization.k8s.io/v1
<span class="hljs-attr">metadata:</span>
  <span class="hljs-comment"># "namespace" omitted since ClusterRoles are not namespaced</span>
<span class="hljs-attr">  name:</span> secret-reader
<span class="hljs-attr">rules:</span>
<span class="hljs-attr">- apiGroups:</span> [<span class="hljs-string">""</span>]
<span class="hljs-attr">  resources:</span> [<span class="hljs-string">"secrets"</span>]
<span class="hljs-attr">  verbs:</span> [<span class="hljs-string">"get"</span>, <span class="hljs-string">"watch"</span>, <span class="hljs-string">"list"</span>]
</code></pre>
<h3 id="rolebinding-和-clusterrolebinding">RoleBinding 和 ClusterRoleBinding</h3>
<p>RoleBinding 把角色（Role 或 ClusterRole）的权限映射到用户或者用户组，从而让这些用户继承角色在 namespace 中的权限。ClusterRoleBinding 让用户继承 ClusterRole 在整个集群中的权限。</p>
<p>注意 ServiceAccount 的用户名格式为 <code>system:serviceaccount:<service-account-name></code>，并且都属于 <code>system:serviceaccounts:</code> 用户组。</p>
<pre><code class="lang-yaml"><span class="hljs-comment"># RoleBinding 示例（引用 Role）</span>
<span class="hljs-comment"># This role binding allows "jane" to read pods in the "default" namespace.</span>
<span class="hljs-attr">kind:</span> RoleBinding
<span class="hljs-attr">apiVersion:</span> rbac.authorization.k8s.io/v1
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> read-pods
<span class="hljs-attr">  namespace:</span> default
<span class="hljs-attr">subjects:</span>
<span class="hljs-attr">- kind:</span> User
<span class="hljs-attr">  name:</span> jane
<span class="hljs-attr">  apiGroup:</span> rbac.authorization.k8s.io
<span class="hljs-attr">roleRef:</span>
<span class="hljs-attr">  kind:</span> Role
<span class="hljs-attr">  name:</span> pod-reader
<span class="hljs-attr">  apiGroup:</span> rbac.authorization.k8s.io
</code></pre>
<p><img src="images/rbac2.png" alt="RBAC 架构图 2"/></p>
<pre><code class="lang-yaml"><span class="hljs-comment"># RoleBinding 示例（引用 ClusterRole）</span>
<span class="hljs-comment"># This role binding allows "dave" to read secrets in the "development" namespace.</span>
<span class="hljs-attr">kind:</span> RoleBinding
<span class="hljs-attr">apiVersion:</span> rbac.authorization.k8s.io/v1
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> read-secrets
<span class="hljs-attr">  namespace:</span> development <span class="hljs-comment"># This only grants permissions within the "development" namespace.</span>
<span class="hljs-attr">subjects:</span>
<span class="hljs-attr">- kind:</span> User
<span class="hljs-attr">  name:</span> dave
<span class="hljs-attr">  apiGroup:</span> rbac.authorization.k8s.io
<span class="hljs-attr">roleRef:</span>
<span class="hljs-attr">  kind:</span> ClusterRole
<span class="hljs-attr">  name:</span> secret-reader
<span class="hljs-attr">  apiGroup:</span> rbac.authorization.k8s.io
</code></pre>
<h3 id="clusterrole-聚合">ClusterRole 聚合</h3>
<p>从 v1.9 开始，在 ClusterRole 中可以通过 <code>aggregationRule</code> 来与其他 ClusterRole 聚合使用（该特性在 v1.11 GA）。</p>
<p>比如</p>
<pre><code class="lang-yaml"><span class="hljs-attr">kind:</span> ClusterRole
<span class="hljs-attr">apiVersion:</span> rbac.authorization.k8s.io/v1
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> monitoring
<span class="hljs-attr">aggregationRule:</span>
<span class="hljs-attr">  clusterRoleSelectors:</span>
<span class="hljs-attr">  - matchLabels:</span>
      rbac.example.com/aggregate-to-monitoring: <span class="hljs-string">"true"</span>
<span class="hljs-attr">rules:</span> [] <span class="hljs-comment"># Rules are automatically filled in by the controller manager.</span>
<span class="hljs-meta">---</span>
<span class="hljs-attr">kind:</span> ClusterRole
<span class="hljs-attr">apiVersion:</span> rbac.authorization.k8s.io/v1
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> monitoring-endpoints
<span class="hljs-attr">  labels:</span>
    rbac.example.com/aggregate-to-monitoring: <span class="hljs-string">"true"</span>
<span class="hljs-comment"># These rules will be added to the "monitoring" role.</span>
<span class="hljs-attr">rules:</span>
<span class="hljs-attr">- apiGroups:</span> [<span class="hljs-string">""</span>]
<span class="hljs-attr">  resources:</span> [<span class="hljs-string">"services"</span>, <span class="hljs-string">"endpoints"</span>, <span class="hljs-string">"pods"</span>]
<span class="hljs-attr">  verbs:</span> [<span class="hljs-string">"get"</span>, <span class="hljs-string">"list"</span>, <span class="hljs-string">"watch"</span>]
</code></pre>
<h3 id="默认-clusterrole">默认 ClusterRole</h3>
<p>RBAC 现在被 Kubernetes 深度集成，并使用它给系统组件进行授权。<a href="https://kubernetes.io/docs/admin/authorization/rbac/#default-roles-and-role-bindings" target="_blank">System Roles</a> 一般具有前缀 <code>system:</code>，很容易识别：</p>
<pre><code class="lang-bash">$ kubectl get clusterroles --namespace=kube-system
NAME                                           AGE
admin                                          10d
cluster-admin                                  10d
edit                                           10d
system:auth-delegator                          10d
system:basic-user                              10d
system:controller:attachdetach-controller      10d
system:controller:certificate-controller       10d
system:controller:cronjob-controller           10d
system:controller:daemon-set-controller        10d
system:controller:deployment-controller        10d
system:controller:disruption-controller        10d
system:controller:endpoint-controller          10d
system:controller:generic-garbage-collector    10d
system:controller:horizontal-pod-autoscaler    10d
system:controller:job-controller               10d
system:controller:namespace-controller         10d
system:controller:node-controller              10d
system:controller:persistent-volume-binder     10d
system:controller:pod-garbage-collector        10d
system:controller:replicaset-controller        10d
system:controller:replication-controller       10d
system:controller:resourcequota-controller     10d
system:controller:route-controller             10d
system:controller:service-account-controller   10d
system:controller:service-controller           10d
system:controller:statefulset-controller       10d
system:controller:ttl-controller               10d
system:discovery                               10d
system:heapster                                10d
system:kube-aggregator                         10d
system:kube-controller-manager                 10d
system:kube-dns                                10d
system:kube-scheduler                          10d
system:node                                    10d
system:node-bootstrapper                       10d
system:node-problem-detector                   10d
system:node-proxier                            10d
system:persistent-volume-provisioner           10d
view                                           10d
</code></pre>
<p>其他的内置角色可以参考 <a href="https://kubernetes.io/docs/admin/authorization/rbac/#default-roles-and-role-bindings" target="_blank">default-roles-and-role-bindings</a>。</p>
<p>RBAC 系统角色已经完成足够的覆盖，让集群可以完全在 RBAC 的管理下运行。</p>
<h2 id="从-abac-迁移到-rbac">从 ABAC 迁移到 RBAC</h2>
<p>在 ABAC 到 RBAC 进行迁移的过程中，有些在 ABAC 集群中缺省开放的权限，在 RBAC 中会被视为不必要的授权，会对其进行 <a href="https://kubernetes.io/docs/admin/authorization/rbac/#upgrading-from-15" target="_blank">降级</a>。这种情况会影响到使用 Service Account 的应用。ABAC 配置中，从 Pod 中发出的请求会使用 Pod Token，API Server 会为其授予较高权限。例如下面的命令在 ABAC 集群中会返回 JSON 结果，而在 RBAC 的情况下则会返回错误。</p>
<pre><code class="lang-bash">$ kubectl run nginx --image=nginx:latest
$ kubectl <span class="hljs-built_in">exec</span> -it $(kubectl get pods -o jsonpath=<span class="hljs-string">'{.items[0].metadata.name}'</span>) bash
$ apt-get update && apt-get install -y curl
$ curl -ik \
  -H <span class="hljs-string">"Authorization: Bearer <span class="hljs-variable">$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)</span>"</span> \
  https://kubernetes/api/v1/namespaces/default/pods
</code></pre>
<p>所有在 Kubernetes 集群中运行的应用，一旦和 API Server 进行通信，都会有可能受到迁移的影响。</p>
<p>要平滑的从 ABAC 升级到 RBAC，在创建 1.6 集群的时候，可以同时启用 <a href="https://kubernetes.io/docs/admin/authorization/rbac/#parallel-authorizers" target="_blank">ABAC 和 RBAC</a>。当他们同时启用的时候，对一个资源的权限请求，在任何一方获得放行都会获得批准。然而在这种配置下的权限太过粗放，很可能无法在单纯的 RBAC 环境下工作。</p>
<p>目前 RBAC 已经进入稳定版，ABAC 可能会被弃用。在可见的未来 ABAC 依然会保留在 kubernetes 中，不过开发的重心已经转移到了 RBAC。</p>
<h2 id="permissive-rbac">Permissive RBAC</h2>
<p>所谓 Permissive RBAC 是指授权给所有的 Service Accounts 管理员权限。注意，这是一个不推荐的配置。</p>
<pre><code class="lang-sh">kubectl create clusterrolebinding permissive-binding \
  --clusterrole=cluster-admin \
  --user=admin \
  --user=kubelet \
  --group=system:serviceaccounts
</code></pre>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://kubernetes.io/docs/admin/authorization/rbac/" target="_blank">RBAC documentation</a></li>
<li><a href="https://www.youtube.com/watch?v=Cd4JU7qzYbE#t=8m01s" target="_blank">Google Cloud Next talks 1</a></li>
<li><a href="https://www.youtube.com/watch?v=18P7cFc6nTU#t=41m06s" target="_blank">Google Cloud Next talks 2</a></li>
<li><a href="http://tonybai.com/2017/03/03/access-api-server-from-a-pod-through-serviceaccount/" target="_blank">在 Kubernetes Pod 中使用 Service Account 访问 API Server</a></li>
<li>部分翻译自 <a href="http://blog.kubernetes.io/2017/04/rbac-support-in-kubernetes.html" target="_blank">RBAC Support in Kubernetes</a>（转载自<a href="https://www.kubernetes.org.cn/1879.html" target="_blank">kubernetes 中文社区</a>，译者催总，<a href="http://rootsongjc.github.com/about" target="_blank">Jimmy Song</a> 做了稍许修改）</li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="准入控制" class="level3">准入控制</h1>
<p>准入控制（Admission Control）在授权后对请求做进一步的验证或添加默认参数。不同于授权和认证只关心请求的用户和操作，准入控制还处理请求的内容，并且仅对创建、更新、删除或连接（如代理）等有效，而对读操作无效。</p>
<p>准入控制支持同时开启多个插件，它们依次调用，只有全部插件都通过的请求才可以放过进入系统。</p>
<p>Kubernetes 目前提供了以下几种准入控制插件</p>
<ul>
<li>AlwaysAdmit: 接受所有请求。</li>
<li>AlwaysPullImages: 总是拉取最新镜像。在多租户场景下非常有用。</li>
<li>DenyEscalatingExec: 禁止特权容器的 exec 和 attach 操作。</li>
<li>ImagePolicyWebhook: 通过 webhook 决定 image 策略，需要同时配置 <code>--admission-control-config-file</code>，配置文件格式见 <a href="https://kubernetes.io/docs/admin/admission-controllers/#configuration-file-format" target="_blank">这里</a>。</li>
<li>ServiceAccount：自动创建默认 ServiceAccount，并确保 Pod 引用的 ServiceAccount 已经存在</li>
<li>SecurityContextDeny：拒绝包含非法 SecurityContext 配置的容器</li>
<li>ResourceQuota：限制 Pod 的请求不会超过配额，需要在 namespace 中创建一个 ResourceQuota 对象</li>
<li>LimitRanger：为 Pod 设置默认资源请求和限制，需要在 namespace 中创建一个 LimitRange 对象</li>
<li>InitialResources：根据镜像的历史使用记录，为容器设置默认资源请求和限制</li>
<li>NamespaceLifecycle：确保处于 termination 状态的 namespace 不再接收新的对象创建请求，并拒绝请求不存在的 namespace</li>
<li>DefaultStorageClass：为 PVC 设置默认 StorageClass（见 <a href="../concepts/persistent-volume.html#StorageClass">这里</a>）</li>
<li>DefaultTolerationSeconds：设置 Pod 的默认 forgiveness toleration 为 5 分钟</li>
<li>PodSecurityPolicy：使用 Pod Security Policies 时必须开启</li>
<li>NodeRestriction：限制 kubelet 仅可访问 node、endpoint、pod、service 以及 secret、configmap、PV 和 PVC 等相关的资源（仅适用于 v1.7+）</li>
<li>EventRateLimit：限制事件请求数量（仅适用于 v1.9）</li>
<li>ExtendedResourceToleration：为使用扩展资源（如 GPU 和 FPGA 等）的 Pod 自动添加 tolerations</li>
<li>StorageProtection：自动给新创建的 PVC 增加 <code>kubernetes.io/pvc-protection</code> finalizer（v1.9 及以前版本为 <code>PVCProtection</code>，v.11 GA）</li>
<li>PersistentVolumeClaimResize：允许设置 <code>allowVolumeExpansion=true</code> 的 StorageClass 调整 PVC 大小（v1.11 Beta）</li>
<li>PodNodeSelector：限制一个 Namespace 中可以使用的 Node 选择标签</li>
<li>ValidatingAdmissionWebhook：使用 Webhook 验证请求，这些 Webhook 并行调用，并且任何一个调用拒绝都会导致请求失败</li>
<li>MutatingAdmissionWebhook：使用 Webhook 修改请求，这些 Webhook 依次顺序调用</li>
</ul>
<p>Kubernetes v1.7 + 还支持 Initializers 和 GenericAdmissionWebhook，可以用来方便地扩展准入控制。</p>
<h2 id="initializers">Initializers</h2>
<p>Initializers 可以用来给资源执行策略或者配置默认选项，包括 Initializers 控制器和用户定义的 Initializer 任务，控制器负责执行用户提交的任务，并完成后将任务从 <code>metadata.initializers</code> 列表中删除。</p>
<p>Initializers 的开启方法为</p>
<ul>
<li>kube-apiserver 配置 <code>--admission-control=...,Initializers</code></li>
<li>kube-apiserver 开启 <code>admissionregistration.k8s.io/v1alpha1</code> API，即配置 <code>--runtime-config=admissionregistration.k8s.io/v1alpha1</code></li>
<li>部署 Initializers 控制器</li>
</ul>
<p>另外，可以使用 <code>initializerconfigurations</code> 来自定义哪些资源开启 Initializer 功能</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> admissionregistration.k8s.io/v1alpha1
<span class="hljs-attr">kind:</span> InitializerConfiguration
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> example-config
<span class="hljs-attr">initializers:</span>
  <span class="hljs-comment"># the name needs to be fully qualified, i.e., containing at least two "."</span>
<span class="hljs-attr">  - name:</span> podimage.example.com
<span class="hljs-attr">    rules:</span>
      <span class="hljs-comment"># apiGroups, apiVersion, resources all support wildcard "*".</span>
      <span class="hljs-comment"># "*" cannot be mixed with non-wildcard.</span>
<span class="hljs-attr">      - apiGroups:</span>
<span class="hljs-bullet">          -</span> <span class="hljs-string">""</span>
<span class="hljs-attr">        apiVersions:</span>
<span class="hljs-bullet">          -</span> v1
<span class="hljs-attr">        resources:</span>
<span class="hljs-bullet">          -</span> pods
</code></pre>
<p>Initializers 可以用来</p>
<ul>
<li>修改资源的配置，比如自动给 Pod 添加一个 sidecar 容器或者存储卷</li>
<li>如果不需要修改对象的话，建议使用性能更好的 GenericAdmissionWebhook。</li>
</ul>
<p>如何开发 Initializers</p>
<ul>
<li>参考 <a href="https://github.com/kelseyhightower/kubernetes-initializer-tutorial" target="_blank">Kubernetes Initializer Tutorial</a> 开发 Initializer</li>
<li>Initializer 必须有一个全局唯一的名字，比如 <code>initializer.vaultproject.io</code></li>
<li>Initializer 有可能收到信息不全的资源（比如还未调度的 Pod 没有 nodeName 和 status），在实现时需要考虑这种情况</li>
<li>对于 Initializer 自身的部署，可以使用 Deployment，但需要手动设置 initializers 列表为空，以避免无法启动的问题，如</li>
</ul>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> apps/v1beta1
<span class="hljs-attr">kind:</span> Deployment
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  initializers:</span>
<span class="hljs-attr">    pending:</span> []
</code></pre>
<h2 id="genericadmissionwebhook">GenericAdmissionWebhook</h2>
<p>GenericAdmissionWebhook 提供了一种 Webhook 方式的准入控制机制，它不会改变请求对象，但可以用来验证用户的请求。</p>
<p>GenericAdmissionWebhook 的开启方法</p>
<ul>
<li>kube-apiserver 配置 <code>--admission-control=...,GenericAdmissionWebhook</code></li>
<li>kube-apiserver 开启 <code>admissionregistration.k8s.io/v1alpha1</code> API，即配置 <code>--runtime-config=admissionregistration.k8s.io/v1alpha1</code></li>
<li>实现并部署 webhook 准入控制器，参考 <a href="https://github.com/caesarxuchao/example-webhook-admission-controller" target="_blank">这里</a> 的示例</li>
</ul>
<p>注意，webhook 准入控制器必须使用 TLS，并需要通过 <code>externaladmissionhookconfigurations.clientConfig.caBundle</code> 向 kube-apiserver 注册：</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> admissionregistration.k8s.io/v1alpha1
<span class="hljs-attr">kind:</span> ExternalAdmissionHookConfiguration
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> example-config
<span class="hljs-attr">externalAdmissionHooks:</span>
<span class="hljs-attr">- name:</span> pod-image.k8s.io
<span class="hljs-attr">  rules:</span>
<span class="hljs-attr">  - apiGroups:</span>
<span class="hljs-bullet">    -</span> <span class="hljs-string">""</span>
<span class="hljs-attr">    apiVersions:</span>
<span class="hljs-bullet">    -</span> v1
<span class="hljs-attr">    operations:</span>
<span class="hljs-bullet">    -</span> CREATE
<span class="hljs-attr">    resources:</span>
<span class="hljs-bullet">    -</span> pods
  <span class="hljs-comment"># fail upon a communication error with the webhook admission controller</span>
  <span class="hljs-comment"># Other options: Ignore</span>
<span class="hljs-attr">  failurePolicy:</span> Fail
<span class="hljs-attr">  clientConfig:</span>
<span class="hljs-attr">    caBundle:</span> <pem encoded ca cert that signs the server cert used by the webhook<span class="hljs-string">>
</span><span class="hljs-attr">    service:</span>
<span class="hljs-attr">      name:</span> <name of the front-end service<span class="hljs-string">>
</span><span class="hljs-attr">      namespace:</span> <namespace of the front-end service<span class="hljs-string">>
</span></code></pre>
<h2 id="推荐配置">推荐配置</h2>
<p>对于 Kubernetes >= 1.9.0，推荐配置以下插件</p>
<pre><code class="lang-sh">--admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota
</code></pre>
<p>对于 Kubernetes >= 1.6.0，推荐 kube-apiserver 开启以下插件</p>
<pre><code class="lang-sh">--admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeLabel,DefaultStorageClass,ResourceQuota,DefaultTolerationSeconds
</code></pre>
<p>对于 Kubernetes >= 1.4.0，推荐配置以下插件</p>
<pre><code class="lang-sh">--admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota
</code></pre>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://kubernetes.io/docs/admin/admission-controllers/" target="_blank">Using Admission Controllers</a></li>
<li><a href="https://medium.com/google-cloud/how-kubernetes-initializers-work-22f6586e1589" target="_blank">How Kubernetes Initializers work</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="scheduler-扩展" class="level2">Scheduler扩展</h1>
<p>如果默认的调度器不满足要求，还可以部署自定义的调度器。并且，在整个集群中还可以同时运行多个调度器实例，通过 <code>podSpec.schedulerName</code> 来选择使用哪一个调度器（默认使用内置的调度器）。</p>
<h2 id="开发自定义调度器">开发自定义调度器</h2>
<p>自定义调度器主要的功能是查询未调度的 Pod，按照自定义的调度策略选择新的 Node，并将其更新到 Pod 的 Node Binding 上。</p>
<p>比如，一个最简单的调度器可以用 shell 来编写（假设 Kubernetes 监听在 <code>localhost:8001</code>）：</p>
<pre><code class="lang-sh"><span class="hljs-meta">#!/bin/bash</span>
SERVER=<span class="hljs-string">'localhost:8001'</span>
<span class="hljs-keyword">while</span> <span class="hljs-literal">true</span>;
<span class="hljs-keyword">do</span>
    <span class="hljs-keyword">for</span> PODNAME <span class="hljs-keyword">in</span> $(kubectl --server <span class="hljs-variable">$SERVER</span> get pods -o json | jq <span class="hljs-string">'.items[] | select(.spec.schedulerName =="my-scheduler") | select(.spec.nodeName == null) | .metadata.name'</span> | tr <span class="hljs-_">-d</span> <span class="hljs-string">'"'</span>)
;
    <span class="hljs-keyword">do</span>
        NODES=($(kubectl --server <span class="hljs-variable">$SERVER</span> get nodes -o json | jq <span class="hljs-string">'.items[].metadata.name'</span> | tr <span class="hljs-_">-d</span> <span class="hljs-string">'"'</span>))
        NUMNODES=<span class="hljs-variable">${#NODES[@]}</span>
        CHOSEN=<span class="hljs-variable">${NODES[$[ $RANDOM % $NUMNODES]]}</span>
        curl --header <span class="hljs-string">"Content-Type:application/json"</span> --request POST --data <span class="hljs-string">'{"apiVersion":"v1","kind":"Binding","metadata": {"name":"'</span><span class="hljs-variable">$PODNAME</span><span class="hljs-string">'"},"target": {"apiVersion":"v1","kind"
: "Node", "name": "'</span><span class="hljs-variable">$CHOSEN</span><span class="hljs-string">'"}}'</span> http://<span class="hljs-variable">$SERVER</span>/api/v1/namespaces/default/pods/<span class="hljs-variable">$PODNAME</span>/binding/
        <span class="hljs-built_in">echo</span> <span class="hljs-string">"Assigned <span class="hljs-variable">$PODNAME</span> to <span class="hljs-variable">$CHOSEN</span>"</span>
    <span class="hljs-keyword">done</span>
    sleep 1
<span class="hljs-keyword">done</span>
</code></pre>
<h2 id="使用自定义调度器">使用自定义调度器</h2>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> nginx
<span class="hljs-attr">  labels:</span>
<span class="hljs-attr">    app:</span> nginx
<span class="hljs-attr">spec:</span>
  <span class="hljs-comment"># 选择使用自定义调度器 my-scheduler</span>
<span class="hljs-attr">  schedulerName:</span> my-scheduler
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">  - name:</span> nginx
<span class="hljs-attr">    image:</span> nginx:<span class="hljs-number">1.10</span>
</code></pre>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="kubernetes-网络模型" class="level2">网络插件</h1>
<h2 id="网络模型">网络模型</h2>
<ul>
<li>IP-per-Pod，每个 Pod 都拥有一个独立 IP 地址，Pod 内所有容器共享一个网络命名空间</li>
<li>集群内所有 Pod 都在一个直接连通的扁平网络中，可通过 IP 直接访问<ul>
<li>所有容器之间无需 NAT 就可以直接互相访问</li>
<li>所有 Node 和所有容器之间无需 NAT 就可以直接互相访问</li>
<li>容器自己看到的 IP 跟其他容器看到的一样</li>
</ul>
</li>
<li>Service cluster IP 尽可在集群内部访问，外部请求需要通过 NodePort、LoadBalance 或者 Ingress 来访问</li>
</ul>
<h2 id="官方插件">官方插件</h2>
<p>目前，Kubernetes 支持以下两种插件：</p>
<ul>
<li>kubenet：这是一个基于 CNI bridge 的网络插件（在 bridge 插件的基础上扩展了 port mapping 和 traffic shaping ），是目前推荐的默认插件</li>
<li>CNI：CNI 网络插件，需要用户将网络配置放到 <code>/etc/cni/net.d</code> 目录中，并将 CNI 插件的二进制文件放入 <code>/opt/cni/bin</code></li>
<li><del>exec：通过第三方的可执行文件来为容器配置网络，已在 v1.6 中移除，见 <a href="https://github.com/kubernetes/kubernetes/pull/39254" target="_blank">kubernetes#39254</a></del></li>
</ul>
<h2 id="kubenet">kubenet</h2>
<p>kubenet 是一个基于 CNI bridge 的网络插件，它为每个容器建立一对 veth pair 并连接到 cbr0 网桥上。kubenet 在 bridge 插件的基础上拓展了很多功能，包括</p>
<ul>
<li><p>使用 host-local IPAM 插件为容器分配 IP 地址， 并定期释放已分配但未使用的 IP 地址</p>
</li>
<li><p>设置 sysctl <code>net.bridge.bridge-nf-call-iptables = 1</code></p>
</li>
<li><p>为 Pod IP 创建 SNAT 规则</p>
<ul>
<li><code>-A POSTROUTING ! -d 10.0.0.0/8 -m comment --comment "kubenet: SNAT for outbound traffic from cluster" -m addrtype ! --dst-type LOCAL -j MASQUERADE</code></li>
</ul>
</li>
<li><p>开启网桥的 hairpin 和 promisc 模式，允许 Pod 访问它自己所在的 Service IP（即通过 NAT 后再访问 Pod 自己）</p>
<pre><code class="lang-sh">-A OUTPUT -j KUBE-DEDUP
-A KUBE-DEDUP -p IPv4 <span class="hljs-_">-s</span> a:58:a:f4:2:1 -o veth+ --ip-src 10.244.2.1 -j ACCEPT
-A KUBE-DEDUP -p IPv4 <span class="hljs-_">-s</span> a:58:a:f4:2:1 -o veth+ --ip-src 10.244.2.0/24 -j DROP
</code></pre>
</li>
<li><p>HostPort 管理以及设置端口映射</p>
</li>
<li><p>Traffic shaping，支持通过 <code>kubernetes.io/ingress-bandwidth</code> 和 <code>kubernetes.io/egress-bandwidth</code> 等 Annotation 设置 Pod 网络带宽限制</p>
</li>
</ul>
<p>未来 kubenet 插件会迁移到标准的 CNI 插件（如 ptp），具体计划见 <a href="https://docs.google.com/document/d/1glJLMHrE2eqwRrAN4fdsz4Vg3R1Iqt6bm5GJQ4GdjlQ/edit#" target="_blank">这里</a>。</p>
<h2 id="cni-plugin">CNI plugin</h2>
<p>安装 CNI：</p>
<pre><code class="lang-sh">cat <<EOF> /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=http://yum.kubernetes.io/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg
       https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF

yum install -y kubernetes-cni
</code></pre>
<p>配置 CNI brige 插件：</p>
<pre><code class="lang-sh">    mkdir -p /etc/cni/net.d
cat >/etc/cni/net.d/10-mynet.conf <<-EOF
{
    <span class="hljs-string">"cniVersion"</span>: <span class="hljs-string">"0.3.0"</span>,
    <span class="hljs-string">"name"</span>: <span class="hljs-string">"mynet"</span>,
    <span class="hljs-string">"type"</span>: <span class="hljs-string">"bridge"</span>,
    <span class="hljs-string">"bridge"</span>: <span class="hljs-string">"cni0"</span>,
    <span class="hljs-string">"isGateway"</span>: <span class="hljs-literal">true</span>,
    <span class="hljs-string">"ipMasq"</span>: <span class="hljs-literal">true</span>,
    <span class="hljs-string">"ipam"</span>: {
        <span class="hljs-string">"type"</span>: <span class="hljs-string">"host-local"</span>,
        <span class="hljs-string">"subnet"</span>: <span class="hljs-string">"10.244.0.0/16"</span>,
        <span class="hljs-string">"routes"</span>: [
            {<span class="hljs-string">"dst"</span>: <span class="hljs-string">"0.0.0.0/0"</span>}
        ]
    }
}
EOF
cat >/etc/cni/net.d/99-loopback.conf <<-EOF
{
    <span class="hljs-string">"cniVersion"</span>: <span class="hljs-string">"0.3.0"</span>,
    <span class="hljs-string">"type"</span>: <span class="hljs-string">"loopback"</span>
}
EOF
</code></pre>
<p>更多 CNI 网络插件的说明请参考 <a href="cni/">CNI 网络插件</a>。</p>
<h2 id="flannel"><a href="flannel/">Flannel</a></h2>
<p><a href="https://github.com/coreos/flannel/blob/master/Documentation/kube-flannel.yml" target="_blank">Flannel</a> 是一个为 Kubernetes 提供 overlay network 的网络插件，它基于 Linux TUN/TAP，使用 UDP 封装 IP 包来创建 overlay 网络，并借助 etcd 维护网络的分配情况。</p>
<pre><code class="lang-sh">kubectl create <span class="hljs-_">-f</span> https://github.com/coreos/flannel/raw/master/Documentation/kube-flannel-rbac.yml
kubectl create <span class="hljs-_">-f</span> https://github.com/coreos/flannel/raw/master/Documentation/kube-flannel.yml
</code></pre>
<h2 id="weave-net"><a href="weave/">Weave Net</a></h2>
<p>Weave Net 是一个多主机容器网络方案，支持去中心化的控制平面，各个 host 上的 wRouter 间通过建立 Full Mesh 的 TCP 链接，并通过 Gossip 来同步控制信息。这种方式省去了集中式的 K/V Store，能够在一定程度上减低部署的复杂性，Weave 将其称为 “data centric”，而非 RAFT 或者 Paxos 的 “algorithm centric”。</p>
<p>数据平面上，Weave 通过 UDP 封装实现 L2 Overlay，封装支持两种模式，一种是运行在 user space 的 sleeve mode，另一种是运行在 kernal space 的 fastpath mode。Sleeve mode 通过 pcap 设备在 Linux bridge 上截获数据包并由 wRouter 完成 UDP 封装，支持对 L2 traffic 进行加密，还支持 Partial Connection，但是性能损失明显。Fastpath mode 即通过 OVS 的 odp 封装 VxLAN 并完成转发，wRouter 不直接参与转发，而是通过下发 odp 流表的方式控制转发，这种方式可以明显地提升吞吐量，但是不支持加密等高级功能。</p>
<pre><code class="lang-sh">kubectl apply <span class="hljs-_">-f</span> https://git.io/weave-kube
</code></pre>
<h2 id="calico"><a href="calico/">Calico</a></h2>
<p><a href="https://www.projectcalico.org/" target="_blank">Calico</a> 是一个基于 BGP 的纯三层的数据中心网络方案（不需要 Overlay），并且与 OpenStack、Kubernetes、AWS、GCE 等 IaaS 和容器平台都有良好的集成。</p>
<p>Calico 在每一个计算节点利用 Linux Kernel 实现了一个高效的 vRouter 来负责数据转发，而每个 vRouter 通过 BGP 协议负责把自己上运行的 workload 的路由信息像整个 Calico 网络内传播——小规模部署可以直接互联，大规模下可通过指定的 BGP route reflector 来完成。 这样保证最终所有的 workload 之间的数据流量都是通过 IP 路由的方式完成互联的。Calico 节点组网可以直接利用数据中心的网络结构（无论是 L2 或者 L3），不需要额外的 NAT，隧道或者 Overlay Network。</p>
<p>此外，Calico 基于 iptables 还提供了丰富而灵活的网络 Policy，保证通过各个节点上的 ACLs 来提供 Workload 的多租户隔离、安全组以及其他可达性限制等功能。</p>
<pre><code class="lang-sh">kubectl apply <span class="hljs-_">-f</span> http://docs.projectcalico.org/v2.1/getting-started/kubernetes/installation/hosted/kubeadm/1.6/calico.yaml
</code></pre>
<h2 id="ovs">OVS</h2>
<p><a href="https://kubernetes.io/docs/admin/ovs-networking/" target="_blank">https://kubernetes.io/docs/admin/ovs-networking/</a> 提供了一种简单的基于 OVS 的网络配置方法：</p>
<ul>
<li>每台机器创建一个 Linux 网桥 kbr0，并配置 docker 使用该网桥（而不是默认的 docker0），其子网为 10.244.x.0/24</li>
<li>每台机器创建一个 OVS 网桥 obr0，通过 veth pair 连接 kbr0 并通过 GRE 将所有机器互联</li>
<li>开启 STP</li>
<li>路由 10.244.0.0/16 到 OVS 隧道</li>
</ul>
<p><img src="ovs-networking.png" alt=""/></p>
<h2 id="ovn"><a href="ovn-kubernetes.html">OVN</a></h2>
<p><a href="http://openvswitch.org/support/dist-docs/ovn-architecture.7.html" target="_blank">OVN (Open Virtual Network)</a> 是 OVS 提供的原生虚拟化网络方案，旨在解决传统 SDN 架构（比如 Neutron DVR）的性能问题。</p>
<p>OVN 为 Kubernetes 提供了两种网络方案：</p>
<ul>
<li>Overaly: 通过 ovs overlay 连接容器</li>
<li>Underlay: 将 VM 内的容器连到 VM 所在的相同网络（开发中）</li>
</ul>
<p>其中，容器网络的配置是通过 OVN 的 CNI 插件来实现。</p>
<h2 id="contiv"><a href="contiv/">Contiv</a></h2>
<p><a href="http://contiv.github.io" target="_blank">Contiv</a> 是思科开源的容器网络方案，主要提供基于 Policy 的网络管理，并与主流容器编排系统集成。Contiv 最主要的优势是直接提供了多租户网络，并支持 L2(VLAN), L3(BGP), Overlay (VXLAN) 以及思科自家的 ACI。</p>
<h2 id="romana"><a href="romana/">Romana</a></h2>
<p>Romana 是 Panic Networks 在 2016 年提出的开源项目，旨在借鉴 route aggregation 的思路来解决 Overlay 方案给网络带来的开销。</p>
<h2 id="opencontrail"><a href="opencontrail/">OpenContrail</a></h2>
<p>OpenContrail 是 Juniper 推出的开源网络虚拟化平台，其商业版本为 Contrail。其主要由控制器和 vRouter 组成：</p>
<ul>
<li>控制器提供虚拟网络的配置、控制和分析功能</li>
<li>vRouter 提供分布式路由，负责虚拟路由器、虚拟网络的建立以及数据转发</li>
</ul>
<p>其中，vRouter 支持三种模式</p>
<ul>
<li>Kernel vRouter：类似于 ovs 内核模块</li>
<li>DPDK vRouter：类似于 ovs-dpdk</li>
<li>Netronome Agilio Solution (商业产品)：支持 DPDK, SR-IOV and Express Virtio (XVIO)</li>
</ul>
<p><a href="https://github.com/Juniper/contrail-kubernetes" target="_blank">Juniper/contrail-kubernetes</a> 提供了 Kubernetes 的集成，包括两部分：</p>
<ul>
<li>kubelet network plugin 基于 kubernetes v1.6 已经删除的 <a href="https://github.com/kubernetes/kubernetes/pull/39254" target="_blank">exec network plugin</a></li>
<li>kube-network-manager 监听 kubernetes API，并根据 label 信息来配置网络策略</li>
</ul>
<h2 id="midonet"><a href="midonet/index.md">Midonet</a></h2>
<p><a href="https://www.midonet.org/" target="_blank">Midonet</a> 是 Midokura 公司开源的 OpenStack 网络虚拟化方案。</p>
<ul>
<li>从组件来看，Midonet 以 Zookeeper+Cassandra 构建分布式数据库存储 VPC 资源的状态——Network State DB Cluster，并将 controller 分布在转发设备（包括 vswitch 和 L3 Gateway）本地——Midolman（L3 Gateway 上还有 quagga bgpd），设备的转发则保留了 ovs kernel 作为 fast datapath。可以看到，Midonet 和 DragonFlow、OVN 一样，在架构的设计上都是沿着 OVS-Neutron-Agent 的思路，将 controller 分布到设备本地，并在 neutron plugin 和设备 agent 间嵌入自己的资源数据库作为 super controller。</li>
<li>从接口来看，NSDB 与 Neutron 间是 REST API，Midolman 与 NSDB 间是 RPC，这俩没什么好说的。Controller 的南向方面，Midolman 并没有用 OpenFlow 和 OVSDB，它干掉了 user space 中的 vswitchd 和 ovsdb-server，直接通过 linux netlink 机制操作 kernel space 中的 ovs datapath。</li>
</ul>
<h2 id="host-network">Host network</h2>
<p>最简单的网络模型就是让容器共享 Host 的 network namespace，使用宿主机的网络协议栈。这样，不需要额外的配置，容器就可以共享宿主的各种网络资源。</p>
<p>优点</p>
<ul>
<li>简单，不需要任何额外配置</li>
<li>高效，没有 NAT 等额外的开销</li>
</ul>
<p>缺点</p>
<ul>
<li>没有任何的网络隔离</li>
<li>容器和 Host 的端口号容易冲突</li>
<li>容器内任何网络配置都会影响整个宿主机</li>
</ul>
<blockquote>
<p>注意：HostNetwork 是在 Pod 配置文件中设置的，kubelet 在启动时还是需要配置使用 CNI 或者 kubenet 插件（默认 kubenet）。</p>
</blockquote>
<h2 id="其他">其他</h2>
<h3 id="ipvs"><a href="ipvs/index.md">ipvs</a></h3>
<p>Kubernetes v1.8 已经支持 ipvs 负载均衡模式（alpha 版）。</p>
<h3 id="canal"><a href="https://github.com/tigera/canal" target="_blank">Canal</a></h3>
<p><a href="https://github.com/tigera/canal" target="_blank">Canal</a> 是 Flannel 和 Calico 联合发布的一个统一网络插件，提供 CNI 网络插件，并支持 network policy。</p>
<h3 id="kuryr-kubernetes"><a href="https://github.com/openstack/kuryr-kubernetes" target="_blank">kuryr-kubernetes</a></h3>
<p><a href="https://github.com/openstack/kuryr-kubernetes" target="_blank">kuryr-kubernetes</a> 是 OpenStack 推出的集成 Neutron 网络插件，主要包括 Controller 和 CNI 插件两部分，并且也提供基于 Neutron LBaaS 的 Service 集成。</p>
<h3 id="cilium"><a href="https://github.com/cilium/cilium" target="_blank">Cilium</a></h3>
<p><a href="https://github.com/cilium/cilium" target="_blank">Cilium</a> 是一个基于 eBPF 和 XDP 的高性能容器网络方案，提供了 CNI 和 CNM 插件。</p>
<p>项目主页为 <a href="https://github.com/cilium/cilium" target="_blank">https://github.com/cilium/cilium</a>。</p>
<h3 id="kope"><a href="https://github.com/kopeio/kope-routing" target="_blank">kope</a></h3>
<p><a href="https://github.com/kopeio/kope-routing" target="_blank">kope</a> 是一个旨在简化 Kubernetes 网络配置的项目，支持三种模式：</p>
<ul>
<li>Layer2：自动为每个 Node 配置路由</li>
<li>Vxlan：为主机配置 vxlan 连接，并建立主机和 Pod 的连接（通过 vxlan interface 和 ARP entry）</li>
<li>ipsec：加密链接</li>
</ul>
<p>项目主页为 <a href="https://github.com/kopeio/kope-routing" target="_blank">https://github.com/kopeio/kope-routing</a>。</p>
<h3 id="kube-router"><a href="https://github.com/cloudnativelabs/kube-router" target="_blank">Kube-router</a></h3>
<p><a href="https://github.com/cloudnativelabs/kube-router" target="_blank">Kube-router</a> 是一个基于 BGP 的网络插件，并提供了可选的 ipvs 服务发现（替代 kube-proxy）以及网络策略功能。</p>
<p>部署 Kube-router：</p>
<pre><code class="lang-sh">kubectl apply <span class="hljs-_">-f</span> https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/kubeadm-kuberouter.yaml
</code></pre>
<p>部署 Kube-router 并替换 kube-proxy（这个功能其实不需要了，kube-proxy 已经内置了 ipvs 模式的支持）：</p>
<pre><code class="lang-sh">kubectl apply <span class="hljs-_">-f</span> https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/kubeadm-kuberouter-all-features.yaml
<span class="hljs-comment"># Remove kube-proxy</span>
kubectl -n kube-system delete ds kube-proxy
docker run --privileged --net=host gcr.io/google_containers/kube-proxy-amd64:v1.7.3 kube-proxy --cleanup-iptables
</code></pre>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="cni-container-network-interface" class="level3">CNI</h1>
<p>Container Network Interface (CNI) 最早是由CoreOS发起的容器网络规范，是Kubernetes网络插件的基础。其基本思想为：Container Runtime在创建容器时，先创建好network namespace，然后调用CNI插件为这个netns配置网络，其后再启动容器内的进程。现已加入CNCF，成为CNCF主推的网络模型。</p>
<p>CNI插件包括两部分：</p>
<ul>
<li>CNI Plugin负责给容器配置网络，它包括两个基本的接口<ul>
<li>配置网络: AddNetwork(net <em>NetworkConfig, rt </em>RuntimeConf) (types.Result, error)</li>
<li>清理网络: DelNetwork(net <em>NetworkConfig, rt </em>RuntimeConf) error</li>
</ul>
</li>
<li>IPAM Plugin负责给容器分配IP地址，主要实现包括host-local和dhcp。</li>
</ul>
<p>Kubernetes Pod 中的其他容器都是Pod所属pause容器的网络，创建过程为：</p>
<ol>
<li>kubelet 先创建pause容器生成network namespace</li>
<li>调用网络CNI driver</li>
<li>CNI driver 根据配置调用具体的cni 插件</li>
<li>cni 插件给pause 容器配置网络</li>
<li>pod 中其他的容器都使用 pause 容器的网络</li>
</ol>
<p><img src="Chart_Container-Network-Interface-Drivers.png" alt=""/></p>
<p>所有CNI插件均支持通过环境变量和标准输入传入参数：</p>
<pre><code class="lang-sh">$ <span class="hljs-built_in">echo</span> <span class="hljs-string">'{"cniVersion": "0.3.1","name": "mynet","type": "macvlan","bridge": "cni0","isGateway": true,"ipMasq": true,"ipam": {"type": "host-local","subnet": "10.244.1.0/24","routes": [{ "dst": "0.0.0.0/0" }]}}'</span> | sudo CNI_COMMAND=ADD CNI_NETNS=/var/run/netns/a CNI_PATH=./bin CNI_IFNAME=eth0 CNI_CONTAINERID=a CNI_VERSION=0.3.1 ./bin/bridge

$ <span class="hljs-built_in">echo</span> <span class="hljs-string">'{"cniVersion": "0.3.1","type":"IGNORED", "name": "a","ipam": {"type": "host-local", "subnet":"10.1.2.3/24"}}'</span> | sudo CNI_COMMAND=ADD CNI_NETNS=/var/run/netns/a CNI_PATH=./bin CNI_IFNAME=a CNI_CONTAINERID=a CNI_VERSION=0.3.1 ./bin/host-local
</code></pre>
<p>常见的CNI网络插件有</p>
<p><img src="cni-plugins.png" alt=""/></p>
<p><strong>CNI Plugin Chains</strong></p>
<p>CNI还支持Plugin Chains，即指定一个插件列表，由Runtime依次执行每个插件。这对支持端口映射（portmapping）、虚拟机等非常有帮助。配置方法可以参考后面的<a href="#端口映射示例">端口映射示例</a>。</p>
<h2 id="bridge">Bridge</h2>
<p>Bridge是最简单的CNI网络插件，它首先在Host创建一个网桥，然后再通过veth pair连接该网桥到container netns。</p>
<p><img src="cni-bridge.png" alt=""/></p>
<p>注意：<strong>Bridge模式下，多主机网络通信需要额外配置主机路由，或使用overlay网络</strong>。可以借助<a href="../flannel/index.html">Flannel</a>或者Quagga动态路由等来自动配置。比如overlay情况下的网络结构为</p>
<p><img src="cni-overlay.png" alt=""/></p>
<p>配置示例</p>
<pre><code class="lang-json">{
    <span class="hljs-string">"cniVersion"</span>: <span class="hljs-string">"0.3.0"</span>,
    <span class="hljs-string">"name"</span>: <span class="hljs-string">"mynet"</span>,
    <span class="hljs-string">"type"</span>: <span class="hljs-string">"bridge"</span>,
    <span class="hljs-string">"bridge"</span>: <span class="hljs-string">"mynet0"</span>,
    <span class="hljs-string">"isDefaultGateway"</span>: <span class="hljs-literal">true</span>,
    <span class="hljs-string">"forceAddress"</span>: <span class="hljs-literal">false</span>,
    <span class="hljs-string">"ipMasq"</span>: <span class="hljs-literal">true</span>,
    <span class="hljs-string">"hairpinMode"</span>: <span class="hljs-literal">true</span>,
    <span class="hljs-string">"ipam"</span>: {
        <span class="hljs-string">"type"</span>: <span class="hljs-string">"host-local"</span>,
        <span class="hljs-string">"subnet"</span>: <span class="hljs-string">"10.10.0.0/16"</span>
    }
}
</code></pre>
<pre><code># export CNI_PATH=/opt/cni/bin
# ip netns add ns
# /opt/cni/bin/cnitool add mynet /var/run/netns/ns
{
    "interfaces": [
        {
            "name": "mynet0",
            "mac": "0a:58:0a:0a:00:01"
        },
        {
            "name": "vethc763e31a",
            "mac": "66:ad:63:b4:c6:de"
        },
        {
            "name": "eth0",
            "mac": "0a:58:0a:0a:00:04",
            "sandbox": "/var/run/netns/ns"
        }
    ],
    "ips": [
        {
            "version": "4",
            "interface": 2,
            "address": "10.10.0.4/16",
            "gateway": "10.10.0.1"
        }
    ],
    "routes": [
        {
            "dst": "0.0.0.0/0",
            "gw": "10.10.0.1"
        }
    ],
    "dns": {}
}
# ip netns exec ns ip addr
1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN group default qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
9: eth0@if8: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default
    link/ether 0a:58:0a:0a:00:04 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 10.10.0.4/16 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::8c78:6dff:fe19:f6bf/64 scope link tentative dadfailed
       valid_lft forever preferred_lft forever
# ip netns exec ns ip route
default via 10.10.0.1 dev eth0
10.10.0.0/16 dev eth0  proto kernel  scope link  src 10.10.0.4
</code></pre><h2 id="ipam">IPAM</h2>
<h3 id="dhcp">DHCP</h3>
<p>DHCP插件是最主要的IPAM插件之一，用来通过DHCP方式给容器分配IP地址，在macvlan插件中也会用到DHCP插件。</p>
<p>在使用DHCP插件之前，需要先启动dhcp daemon:</p>
<pre><code class="lang-sh">/opt/cni/bin/dhcp daemon &
</code></pre>
<p>然后配置网络使用dhcp作为IPAM插件</p>
<pre><code class="lang-json">{
    ...
    <span class="hljs-string">"ipam"</span>: {
        <span class="hljs-string">"type"</span>: <span class="hljs-string">"dhcp"</span>,
    }
}
</code></pre>
<h3 id="host-local">host-local</h3>
<p>host-local是最常用的CNI IPAM插件，用来给container分配IP地址。</p>
<p>IPv4:</p>
<pre><code class="lang-json">{
    <span class="hljs-string">"ipam"</span>: {
        <span class="hljs-string">"type"</span>: <span class="hljs-string">"host-local"</span>,
        <span class="hljs-string">"subnet"</span>: <span class="hljs-string">"10.10.0.0/16"</span>,
        <span class="hljs-string">"rangeStart"</span>: <span class="hljs-string">"10.10.1.20"</span>,
        <span class="hljs-string">"rangeEnd"</span>: <span class="hljs-string">"10.10.3.50"</span>,
        <span class="hljs-string">"gateway"</span>: <span class="hljs-string">"10.10.0.254"</span>,
        <span class="hljs-string">"routes"</span>: [
            { <span class="hljs-string">"dst"</span>: <span class="hljs-string">"0.0.0.0/0"</span> },
            { <span class="hljs-string">"dst"</span>: <span class="hljs-string">"192.168.0.0/16"</span>, <span class="hljs-string">"gw"</span>: <span class="hljs-string">"10.10.5.1"</span> }
        ],
        <span class="hljs-string">"dataDir"</span>: <span class="hljs-string">"/var/my-orchestrator/container-ipam-state"</span>
    }
}
</code></pre>
<p>IPv6:</p>
<pre><code class="lang-json">{
  <span class="hljs-string">"ipam"</span>: {
        <span class="hljs-string">"type"</span>: <span class="hljs-string">"host-local"</span>,
        <span class="hljs-string">"subnet"</span>: <span class="hljs-string">"3ffe:ffff:0:01ff::/64"</span>,
        <span class="hljs-string">"rangeStart"</span>: <span class="hljs-string">"3ffe:ffff:0:01ff::0010"</span>,
        <span class="hljs-string">"rangeEnd"</span>: <span class="hljs-string">"3ffe:ffff:0:01ff::0020"</span>,
        <span class="hljs-string">"routes"</span>: [
            { <span class="hljs-string">"dst"</span>: <span class="hljs-string">"3ffe:ffff:0:01ff::1/64"</span> }
        ],
        <span class="hljs-string">"resolvConf"</span>: <span class="hljs-string">"/etc/resolv.conf"</span>
    }
}
</code></pre>
<h2 id="ptp">ptp</h2>
<p>ptp插件通过veth pair给容器和host创建点对点连接：veth pair一端在container netns内，另一端在host上。可以通过配置host端的IP和路由来让ptp连接的容器之前通信。</p>
<pre><code class="lang-json">{
    <span class="hljs-string">"name"</span>: <span class="hljs-string">"mynet"</span>,
    <span class="hljs-string">"type"</span>: <span class="hljs-string">"ptp"</span>,
    <span class="hljs-string">"ipam"</span>: {
        <span class="hljs-string">"type"</span>: <span class="hljs-string">"host-local"</span>,
        <span class="hljs-string">"subnet"</span>: <span class="hljs-string">"10.1.1.0/24"</span>
    },
    <span class="hljs-string">"dns"</span>: {
        <span class="hljs-string">"nameservers"</span>: [ <span class="hljs-string">"10.1.1.1"</span>, <span class="hljs-string">"8.8.8.8"</span> ]
    }
}
</code></pre>
<h2 id="ipvlan">IPVLAN</h2>
<p>IPVLAN 和 MACVLAN 类似，都是从一个主机接口虚拟出多个虚拟网络接口。一个重要的区别就是所有的虚拟接口都有相同的 mac 地址，而拥有不同的 ip 地址。因为所有的虚拟接口要共享 mac 地址，所以有些需要注意的地方：</p>
<ul>
<li>DHCP 协议分配 ip 的时候一般会用 mac 地址作为机器的标识。这个情况下，客户端动态获取 ip 的时候需要配置唯一的 ClientID 字段，并且 DHCP server 也要正确配置使用该字段作为机器标识，而不是使用 mac 地址</li>
</ul>
<p>IPVLAN支持两种模式：</p>
<ul>
<li>L2 模式：此时跟macvlan bridge 模式工作原理很相似，父接口作为交换机来转发子接口的数据。同一个网络的子接口可以通过父接口来转发数据，而如果想发送到其他网络，报文则会通过父接口的路由转发出去。</li>
<li>L3 模式：此时ipvlan 有点像路由器的功能，它在各个虚拟网络和主机网络之间进行不同网络报文的路由转发工作。只要父接口相同，即使虚拟机/容器不在同一个网络，也可以互相 ping 通对方，因为 ipvlan 会在中间做报文的转发工作。注意 L3 模式下的虚拟接口 不会接收到多播或者广播的报文（这个模式下，所有的网络都会发送给父接口，所有的 ARP 过程或者其他多播报文都是在底层的父接口完成的）。另外外部网络默认情况下是不知道 ipvlan 虚拟出来的网络的，如果不在外部路由器上配置好对应的路由规则，ipvlan 的网络是不能被外部直接访问的。</li>
</ul>
<p>创建ipvlan的简单方法为</p>
<pre><code>ip link add link <master-dev> <slave-dev> type ipvlan mode { l2 | L3 }
</code></pre><p>cni配置格式为</p>
<pre><code>{
    "name": "mynet",
    "type": "ipvlan",
    "master": "eth0",
    "ipam": {
        "type": "host-local",
        "subnet": "10.1.2.0/24"
    }
}
</code></pre><p>需要注意的是</p>
<ul>
<li>ipvlan插件下，容器不能跟Host网络通信</li>
<li>主机接口（也就是master interface）不能同时作为ipvlan和macvlan的master接口</li>
</ul>
<h2 id="macvlan">MACVLAN</h2>
<p>MACVLAN可以从一个主机接口虚拟出多个macvtap，且每个macvtap设备都拥有不同的mac地址（对应不同的linux字符设备）。MACVLAN支持四种模式</p>
<ul>
<li>bridge模式：数据可以在同一master设备的子设备之间转发</li>
<li>vepa模式：VEPA 模式是对 802.1Qbg 标准中的 VEPA 机制的软件实现，MACVTAP 设备简单的将数据转发到master设备中，完成数据汇聚功能，通常需要外部交换机支持 Hairpin 模式才能正常工作</li>
<li>private模式：Private 模式和 VEPA 模式类似，区别是子 MACVTAP 之间相互隔离</li>
<li>passthrough模式：内核的 MACVLAN 数据处理逻辑被跳过，硬件决定数据如何处理，从而释放了 Host CPU 资源</li>
</ul>
<p>创建macvlan的简单方法为</p>
<pre><code class="lang-sh">ip link add link <master-dev> name macvtap0 <span class="hljs-built_in">type</span> macvtap
</code></pre>
<p>cni配置格式为</p>
<pre><code>{
    "name": "mynet",
    "type": "macvlan",
    "master": "eth0",
    "ipam": {
        "type": "dhcp"
    }
}
</code></pre><p>需要注意的是</p>
<ul>
<li>macvlan需要大量 mac 地址，每个虚拟接口都有自己的 mac 地址</li>
<li>无法和 802.11(wireless) 网络一起工作</li>
<li>主机接口（也就是master interface）不能同时作为ipvlan和macvlan的master接口</li>
</ul>
<h2 id="flannel"><a href="../flannel/">Flannel</a></h2>
<p><a href="https://github.com/coreos/flannel" target="_blank">Flannel</a>通过给每台宿主机分配一个子网的方式为容器提供虚拟网络，它基于Linux TUN/TAP，使用UDP封装IP包来创建overlay网络，并借助etcd维护网络的分配情况。</p>
<h2 id="weave-net"><a href="../weave/">Weave Net</a></h2>
<p>Weave Net是一个多主机容器网络方案，支持去中心化的控制平面，各个host上的wRouter间通过建立Full Mesh的TCP链接，并通过Gossip来同步控制信息。这种方式省去了集中式的K/V Store，能够在一定程度上减低部署的复杂性，Weave将其称为“data centric”，而非RAFT或者Paxos的“algorithm centric”。</p>
<p>数据平面上，Weave通过UDP封装实现L2 Overlay，封装支持两种模式，一种是运行在user space的sleeve mode，另一种是运行在kernal space的 fastpath mode。Sleeve mode通过pcap设备在Linux bridge上截获数据包并由wRouter完成UDP封装，支持对L2 traffic进行加密，还支持Partial Connection，但是性能损失明显。Fastpath mode即通过OVS的odp封装VxLAN并完成转发，wRouter不直接参与转发，而是通过下发odp 流表的方式控制转发，这种方式可以明显地提升吞吐量，但是不支持加密等高级功能。</p>
<h2 id="contiv"><a href="../contiv/">Contiv</a></h2>
<p><a href="http://contiv.github.io" target="_blank">Contiv</a>是思科开源的容器网络方案，主要提供基于Policy的网络管理，并与主流容器编排系统集成。Contiv最主要的优势是直接提供了多租户网络，并支持L2(VLAN), L3(BGP), Overlay (VXLAN)以及思科自家的ACI。</p>
<h2 id="calico"><a href="../calico/">Calico</a></h2>
<p><a href="https://www.projectcalico.org/" target="_blank">Calico</a> 是一个基于BGP的纯三层的数据中心网络方案（不需要Overlay），并且与OpenStack、Kubernetes、AWS、GCE等IaaS和容器平台都有良好的集成。</p>
<p>Calico在每一个计算节点利用Linux Kernel实现了一个高效的vRouter来负责数据转发，而每个vRouter通过BGP协议负责把自己上运行的workload的路由信息像整个Calico网络内传播——小规模部署可以直接互联，大规模下可通过指定的BGP route reflector来完成。 这样保证最终所有的workload之间的数据流量都是通过IP路由的方式完成互联的。Calico节点组网可以直接利用数据中心的网络结构（无论是L2或者L3），不需要额外的NAT，隧道或者Overlay Network。</p>
<p>此外，Calico基于iptables还提供了丰富而灵活的网络Policy，保证通过各个节点上的ACLs来提供Workload的多租户隔离、安全组以及其他可达性限制等功能。</p>
<h2 id="ovn"><a href="../ovn-kubernetes.html">OVN</a></h2>
<p><a href="http://openvswitch.org/support/dist-docs/ovn-architecture.7.html" target="_blank">OVN (Open Virtual Network)</a> 是OVS提供的原生虚拟化网络方案，旨在解决传统SDN架构（比如Neutron DVR）的性能问题。</p>
<p>OVN为Kubernetes提供了两种网络方案：</p>
<ul>
<li>Overaly: 通过ovs overlay连接容器</li>
<li>Underlay: 将VM内的容器连到VM所在的相同网络（开发中）</li>
</ul>
<p>其中，容器网络的配置是通过OVN的CNI插件来实现。</p>
<h2 id="sr-iov">SR-IOV</h2>
<p>Intel维护了一个SR-IOV的<a href="https://github.com/Intel-Corp/sriov-cni" target="_blank">CNI插件</a>，fork自<a href="https://github.com/hustcat/sriov-cni" target="_blank">hustcat/sriov-cni</a>，并扩展了DPDK的支持。</p>
<p>项目主页见<a href="https://github.com/Intel-Corp/sriov-cni" target="_blank">https://github.com/Intel-Corp/sriov-cni</a>。</p>
<h2 id="romana"><a href="../romana/">Romana</a></h2>
<p>Romana是Panic Networks在2016年提出的开源项目，旨在借鉴 route aggregation的思路来解决Overlay方案给网络带来的开销。</p>
<h2 id="opencontrail"><a href="../opencontrail/">OpenContrail</a></h2>
<p>OpenContrail是Juniper推出的开源网络虚拟化平台，其商业版本为Contrail。其主要由控制器和vRouter组成：</p>
<ul>
<li>控制器提供虚拟网络的配置、控制和分析功能</li>
<li>vRouter提供分布式路由，负责虚拟路由器、虚拟网络的建立以及数据转发</li>
</ul>
<p>其中，vRouter支持三种模式</p>
<ul>
<li>Kernel vRouter：类似于ovs内核模块</li>
<li>DPDK vRouter：类似于ovs-dpdk</li>
<li>Netronome Agilio Solution (商业产品)：支持DPDK, SR-IOV and Express Virtio (XVIO)</li>
</ul>
<p><a href="https://github.com/michaelhenkel/opencontrail-cni-plugin" target="_blank">michaelhenkel/opencontrail-cni-plugin</a>提供了一个OpenContrail的CNI插件。</p>
<h3 id="network-configuration-lists">Network Configuration Lists</h3>
<p><a href="https://github.com/containernetworking/cni/blob/master/SPEC.md#network-configuration-lists" target="_blank">CNI SPEC</a> 支持指定网络配置列表，包含多个网络插件，由 Runtime 依次执行。注意</p>
<ul>
<li>ADD 操作，按顺序依次调用每个插件；而 DEL 操作调用顺序相反</li>
<li>ADD 操作，除最后一个插件，前面每个插件需要增加 <code>prevResult</code> 传递给其后的插件</li>
<li>第一个插件必须要包含 ipam 插件</li>
</ul>
<h3 id="端口映射示例">端口映射示例</h3>
<p>下面的例子展示了 bridge+<a href="https://github.com/containernetworking/plugins/tree/master/plugins/meta/portmap" target="_blank">portmap</a> 插件的用法。</p>
<p>首先，配置 CNI 网络使用 bridge+portmap 插件：</p>
<pre><code class="lang-sh"><span class="hljs-comment"># cat /root/mynet.conflist</span>
{
  <span class="hljs-string">"name"</span>: <span class="hljs-string">"mynet"</span>,
  <span class="hljs-string">"cniVersion"</span>: <span class="hljs-string">"0.3.0"</span>,
  <span class="hljs-string">"plugins"</span>: [
    {
      <span class="hljs-string">"type"</span>: <span class="hljs-string">"bridge"</span>,
      <span class="hljs-string">"bridge"</span>: <span class="hljs-string">"mynet"</span>,
      <span class="hljs-string">"ipMasq"</span>: <span class="hljs-literal">true</span>,
      <span class="hljs-string">"isGateway"</span>: <span class="hljs-literal">true</span>,
      <span class="hljs-string">"ipam"</span>: {
      <span class="hljs-string">"type"</span>: <span class="hljs-string">"host-local"</span>,
      <span class="hljs-string">"subnet"</span>: <span class="hljs-string">"10.244.10.0/24"</span>,
      <span class="hljs-string">"routes"</span>: [
          {<span class="hljs-string">"dst"</span>: <span class="hljs-string">"0.0.0.0/0"</span>}
      ]
      }
    },
    {
       <span class="hljs-string">"type"</span>: <span class="hljs-string">"portmap"</span>,
       <span class="hljs-string">"capabilities"</span>: {<span class="hljs-string">"portMappings"</span>: <span class="hljs-literal">true</span>}
    }
  ]
}
</code></pre>
<p>然后通过 <code>CAP_ARGS</code> 设置端口映射参数：</p>
<pre><code class="lang-sh"><span class="hljs-comment"># export CAP_ARGS='{</span>
    <span class="hljs-string">"portMappings"</span>: [
        {
            <span class="hljs-string">"hostPort"</span>:      9090,
            <span class="hljs-string">"containerPort"</span>: 80,
            <span class="hljs-string">"protocol"</span>:      <span class="hljs-string">"tcp"</span>,
            <span class="hljs-string">"hostIP"</span>:        <span class="hljs-string">"127.0.0.1"</span>
        }
    ]
}<span class="hljs-string">'
</span></code></pre>
<p>测试添加网络接口：</p>
<pre><code class="lang-sh"><span class="hljs-comment"># ip netns add test</span>
<span class="hljs-comment"># CNI_PATH=/opt/cni/bin NETCONFPATH=/root ./cnitool add mynet /var/run/netns/test</span>
{
    <span class="hljs-string">"interfaces"</span>: [
        {
            <span class="hljs-string">"name"</span>: <span class="hljs-string">"mynet"</span>,
            <span class="hljs-string">"mac"</span>: <span class="hljs-string">"0a:58:0a:f4:0a:01"</span>
        },
        {
            <span class="hljs-string">"name"</span>: <span class="hljs-string">"veth2cfb1d64"</span>,
            <span class="hljs-string">"mac"</span>: <span class="hljs-string">"4a:dc:1f:b7:56:b1"</span>
        },
        {
            <span class="hljs-string">"name"</span>: <span class="hljs-string">"eth0"</span>,
            <span class="hljs-string">"mac"</span>: <span class="hljs-string">"0a:58:0a:f4:0a:07"</span>,
            <span class="hljs-string">"sandbox"</span>: <span class="hljs-string">"/var/run/netns/test"</span>
        }
    ],
    <span class="hljs-string">"ips"</span>: [
        {
            <span class="hljs-string">"version"</span>: <span class="hljs-string">"4"</span>,
            <span class="hljs-string">"interface"</span>: 2,
            <span class="hljs-string">"address"</span>: <span class="hljs-string">"10.244.10.7/24"</span>,
            <span class="hljs-string">"gateway"</span>: <span class="hljs-string">"10.244.10.1"</span>
        }
    ],
    <span class="hljs-string">"routes"</span>: [
        {
            <span class="hljs-string">"dst"</span>: <span class="hljs-string">"0.0.0.0/0"</span>
        }
    ],
    <span class="hljs-string">"dns"</span>: {}
}
</code></pre>
<p>可以从 iptables 规则中看到添加的规则：</p>
<pre><code class="lang-sh"><span class="hljs-comment"># iptables-save | grep 10.244.10.7</span>
-A CNI-DN-be1eedf7a76853f303ebd <span class="hljs-_">-d</span> 127.0.0.1/32 -p tcp -m tcp --dport 9090 -j DNAT --to-destination 10.244.10.7:80
-A CNI-SN-be1eedf7a76853f303ebd <span class="hljs-_">-s</span> 127.0.0.1/32 <span class="hljs-_">-d</span> 10.244.10.7/32 -p tcp -m tcp --dport 80 -j MASQUERADE
</code></pre>
<p>最后，清理网络接口：</p>
<pre><code># CNI_PATH=/opt/cni/bin NETCONFPATH=/root ./cnitool del mynet /var/run/netns/test
</code></pre><h2 id="其他">其他</h2>
<h3 id="canal"><a href="https://github.com/tigera/canal" target="_blank">Canal</a></h3>
<p><a href="https://github.com/tigera/canal" target="_blank">Canal</a>是Flannel和Calico联合发布的一个统一网络插件，提供CNI网络插件，并支持network policy。</p>
<h3 id="kuryr-kubernetes"><a href="https://github.com/openstack/kuryr-kubernetes" target="_blank">kuryr-kubernetes</a></h3>
<p><a href="https://github.com/openstack/kuryr-kubernetes" target="_blank">kuryr-kubernetes</a>是OpenStack推出的集成Neutron网络插件，主要包括Controller和CNI插件两部分，并且也提供基于Neutron LBaaS的Service集成。</p>
<h3 id="cilium"><a href="https://github.com/cilium/cilium" target="_blank">Cilium</a></h3>
<p><a href="https://github.com/cilium/cilium" target="_blank">Cilium</a>是一个基于eBPF和XDP的高性能容器网络方案，提供了CNI和CNM插件。</p>
<p>项目主页为<a href="https://github.com/cilium/cilium" target="_blank">https://github.com/cilium/cilium</a>。</p>
<h2 id="cni-genie"><a href="https://github.com/Huawei-PaaS/CNI-Genie" target="_blank">CNI-Genie</a></h2>
<p><a href="https://github.com/Huawei-PaaS/CNI-Genie" target="_blank">CNI-Genie</a>是华为PaaS团队推出的同时支持多种网络插件（支持calico, canal, romana, weave等）的CNI插件。</p>
<p>项目主页为<a href="https://github.com/Huawei-PaaS/CNI-Genie" target="_blank">https://github.com/Huawei-PaaS/CNI-Genie</a>。</p>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="flannel" class="level3">Flannel</h1>
<p><a href="https://github.com/coreos/flannel" target="_blank">Flannel</a>通过给每台宿主机分配一个子网的方式为容器提供虚拟网络，它基于Linux TUN/TAP，使用UDP封装IP包来创建overlay网络，并借助etcd维护网络的分配情况。</p>
<h2 id="flannel原理">Flannel原理</h2>
<p>控制平面上host本地的flanneld负责从远端的ETCD集群同步本地和其它host上的subnet信息，并为POD分配IP地址。数据平面flannel通过Backend（比如UDP封装）来实现L3 Overlay，既可以选择一般的TUN设备又可以选择VxLAN设备。</p>
<pre><code class="lang-json">{
    <span class="hljs-string">"Network"</span>: <span class="hljs-string">"10.0.0.0/8"</span>,
    <span class="hljs-string">"SubnetLen"</span>: <span class="hljs-number">20</span>,
    <span class="hljs-string">"SubnetMin"</span>: <span class="hljs-string">"10.10.0.0"</span>,
    <span class="hljs-string">"SubnetMax"</span>: <span class="hljs-string">"10.99.0.0"</span>,
    <span class="hljs-string">"Backend"</span>: {
        <span class="hljs-string">"Type"</span>: <span class="hljs-string">"udp"</span>,
        <span class="hljs-string">"Port"</span>: <span class="hljs-number">7890</span>
    }
}
</code></pre>
<p><img src="flannel.png" alt=""/></p>
<p>除了UDP，Flannel还支持很多其他的Backend：</p>
<ul>
<li>udp：使用用户态udp封装，默认使用8285端口。由于是在用户态封装和解包，性能上有较大的损失</li>
<li>vxlan：vxlan封装，需要配置VNI，Port（默认8472）和<a href="https://github.com/torvalds/linux/commit/3511494ce2f3d3b77544c79b87511a4ddb61dc89" target="_blank">GBP</a></li>
<li>host-gw：直接路由的方式，将容器网络的路由信息直接更新到主机的路由表中，仅适用于二层直接可达的网络</li>
<li>aws-vpc：使用 Amazon VPC route table 创建路由，适用于AWS上运行的容器</li>
<li>gce：使用Google Compute Engine Network创建路由，所有instance需要开启IP forwarding，适用于GCE上运行的容器</li>
<li>ali-vpc：使用阿里云VPC route table 创建路由，适用于阿里云上运行的容器</li>
</ul>
<h2 id="docker集成">Docker集成</h2>
<pre><code class="lang-sh"><span class="hljs-built_in">source</span> /run/flannel/subnet.env
docker daemon --bip=<span class="hljs-variable">${FLANNEL_SUBNET}</span> --mtu=<span class="hljs-variable">${FLANNEL_MTU}</span> &
</code></pre>
<h2 id="cni集成">CNI集成</h2>
<p>CNI flannel插件会将flannel网络配置转换为bridge插件配置，并调用bridge插件给容器netns配置网络。比如下面的flannel配置</p>
<pre><code class="lang-json">{
    <span class="hljs-string">"name"</span>: <span class="hljs-string">"mynet"</span>,
    <span class="hljs-string">"type"</span>: <span class="hljs-string">"flannel"</span>,
    <span class="hljs-string">"delegate"</span>: {
        <span class="hljs-string">"bridge"</span>: <span class="hljs-string">"mynet0"</span>,
        <span class="hljs-string">"mtu"</span>: <span class="hljs-number">1400</span>
    }
}
</code></pre>
<p>会被cni flannel插件转换为</p>
<pre><code class="lang-json">{
    <span class="hljs-string">"name"</span>: <span class="hljs-string">"mynet"</span>,
    <span class="hljs-string">"type"</span>: <span class="hljs-string">"bridge"</span>,
    <span class="hljs-string">"mtu"</span>: <span class="hljs-number">1472</span>,
    <span class="hljs-string">"ipMasq"</span>: <span class="hljs-literal">false</span>,
    <span class="hljs-string">"isGateway"</span>: <span class="hljs-literal">true</span>,
    <span class="hljs-string">"ipam"</span>: {
        <span class="hljs-string">"type"</span>: <span class="hljs-string">"host-local"</span>,
        <span class="hljs-string">"subnet"</span>: <span class="hljs-string">"10.1.17.0/24"</span>
    }
}
</code></pre>
<h2 id="kubernetes集成">Kubernetes集成</h2>
<p>使用flannel前需要配置<code>kube-controller-manager --allocate-node-cidrs=true --cluster-cidr=10.244.0.0/16</code>。</p>
<pre><code class="lang-sh">kubectl apply <span class="hljs-_">-f</span> https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
</code></pre>
<p>这会启动flanneld容器，并配置CNI网络插件：</p>
<pre><code class="lang-sh">$ ps -ef | grep flannel | grep -v grep
root      3625  3610  0 13:57 ?        00:00:00 /opt/bin/flanneld --ip-masq --kube-subnet-mgr
root      9640  9619  0 13:51 ?        00:00:00 /bin/sh -c <span class="hljs-built_in">set</span> <span class="hljs-_">-e</span> -x; cp <span class="hljs-_">-f</span> /etc/kube-flannel/cni-conf.json /etc/cni/net.d/10-flannel.conf; <span class="hljs-keyword">while</span> <span class="hljs-literal">true</span>; <span class="hljs-keyword">do</span> sleep 3600; <span class="hljs-keyword">done</span>

$ cat /etc/cni/net.d/10-flannel.conf
{
  <span class="hljs-string">"name"</span>: <span class="hljs-string">"cbr0"</span>,
  <span class="hljs-string">"type"</span>: <span class="hljs-string">"flannel"</span>,
  <span class="hljs-string">"delegate"</span>: {
    <span class="hljs-string">"isDefaultGateway"</span>: <span class="hljs-literal">true</span>
  }
}
</code></pre>
<p><img src="flannel-components.png" alt=""/></p>
<p>flanneld自动连接kubernetes API，根据<code>node.Spec.PodCIDR</code>配置本地的flannel网络子网，并为容器创建vxlan和相关的子网路由。</p>
<pre><code class="lang-sh">$ cat /run/flannel/subnet.env
FLANNEL_NETWORK=10.244.0.0/16
FLANNEL_SUBNET=10.244.0.1/24
FLANNEL_MTU=1410
FLANNEL_IPMASQ=<span class="hljs-literal">true</span>

$ ip <span class="hljs-_">-d</span> link show flannel.1
12: flannel.1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1410 qdisc noqueue state UNKNOWN mode DEFAULT group default
    link/ether 8e:5a:0d:07:0f:0d brd ff:ff:ff:ff:ff:ff promiscuity 0
    vxlan id 1 <span class="hljs-built_in">local</span> 10.146.0.2 dev ens4 srcport 0 0 dstport 8472 nolearning ageing 300 udpcsum addrgenmode eui64
</code></pre>
<p><img src="flannel-network.png" alt=""/></p>
<h2 id="优点">优点</h2>
<ul>
<li>配置安装简单，使用方便</li>
<li>与云平台集成较好，VPC的方式没有额外的性能损失</li>
</ul>
<h2 id="缺点">缺点</h2>
<ul>
<li>VXLAN模式对zero-downtime restarts支持不好</li>
</ul>
<blockquote>
<p>When running with a backend other than udp, the kernel is providing the data path with flanneld acting as the control plane. As such, flanneld can be restarted (even to do an upgrade) without disturbing existing flows. However in the case of vxlan backend, this needs to be done within a few seconds as ARP entries can start to timeout requiring the flannel daemon to refresh them. Also, to avoid interruptions during restart, the configuration must not be changed (e.g. VNI, --iface values).</p>
</blockquote>
<p><strong>参考文档</strong></p>
<ul>
<li><a href="https://github.com/coreos/flannel" target="_blank">https://github.com/coreos/flannel</a></li>
<li><a href="https://coreos.com/flannel/docs/latest/" target="_blank">https://coreos.com/flannel/docs/latest/</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="calico" class="level3">Calico</h1>
<p><a href="https://www.projectcalico.org/" target="_blank">Calico</a> 是一个纯三层的数据中心网络方案（不需要 Overlay），并且与 OpenStack、Kubernetes、AWS、GCE 等 IaaS 和容器平台都有良好的集成。</p>
<p>Calico 在每一个计算节点利用 Linux Kernel 实现了一个高效的 vRouter 来负责数据转发，而每个 vRouter 通过 BGP 协议负责把自己上运行的 workload 的路由信息像整个 Calico 网络内传播——小规模部署可以直接互联，大规模下可通过指定的 BGP route reflector 来完成。 这样保证最终所有的 workload 之间的数据流量都是通过 IP 路由的方式完成互联的。Calico 节点组网可以直接利用数据中心的网络结构（无论是 L2 或者 L3），不需要额外的 NAT，隧道或者 Overlay Network。</p>
<p>此外，Calico 基于 iptables 还提供了丰富而灵活的网络 Policy，保证通过各个节点上的 ACLs 来提供 Workload 的多租户隔离、安全组以及其他可达性限制等功能。</p>
<h2 id="calico-架构">Calico 架构</h2>
<p><img src="calico.png" alt=""/></p>
<p>Calico 主要由 Felix、etcd、BGP client 以及 BGP Route Reflector 组成</p>
<ol>
<li>Felix，Calico Agent，跑在每台需要运行 Workload 的节点上，主要负责配置路由及 ACLs 等信息来确保 Endpoint 的连通状态；</li>
<li>etcd，分布式键值存储，主要负责网络元数据一致性，确保 Calico 网络状态的准确性；</li>
<li>BGP Client（BIRD）, 主要负责把 Felix 写入 Kernel 的路由信息分发到当前 Calico 网络，确保 Workload 间的通信的有效性；</li>
<li>BGP Route Reflector（BIRD），大规模部署时使用，摒弃所有节点互联的 mesh 模式，通过一个或者多个 BGP Route Reflector 来完成集中式的路由分发。</li>
<li>calico/calico-ipam，主要用作 Kubernetes 的 CNI 插件</li>
</ol>
<p><img src="calico2.png" alt=""/></p>
<h2 id="ip-in-ip">IP-in-IP</h2>
<p>Calico 控制平面的设计要求物理网络得是 L2 Fabric，这样 vRouter 间都是直接可达的，路由不需要把物理设备当做下一跳。为了支持 L3 Fabric，Calico 推出了 IPinIP 的选项。</p>
<h2 id="calico-cni">Calico CNI</h2>
<p>见 <a href="https://github.com/projectcalico/cni-plugin" target="_blank">https://github.com/projectcalico/cni-plugin</a>。</p>
<h2 id="calico-cnm">Calico CNM</h2>
<p>Calico 通过 Pool 和 Profile 的方式实现了 docker CNM 网络：</p>
<ol>
<li>Pool，定义可用于 Docker Network 的 IP 资源范围，比如：10.0.0.0/8 或者 192.168.0.0/16；</li>
<li>Profile，定义 Docker Network Policy 的集合，由 tags 和 rules 组成；每个 Profile 默认拥有一个和 Profile 名字相同的 Tag，每个 Profile 可以有多个 Tag，以 List 形式保存。</li>
</ol>
<p>具体实现见 <a href="https://github.com/projectcalico/libnetwork-plugin" target="_blank">https://github.com/projectcalico/libnetwork-plugin</a>，而使用方法可以参考 <a href="https://docs.projectcalico.org/master/getting-started/docker/" target="_blank">https://docs.projectcalico.org/master/getting-started/docker/</a>。</p>
<h2 id="calico-kubernetes">Calico Kubernetes</h2>
<p>对于使用 kubeadm 创建的 Kubernetes 集群，使用以下配置安装 calico 时需要配置</p>
<ul>
<li><code>--pod-network-cidr=192.168.0.0/16</code></li>
<li><code>--service-cidr=10.96.0.0/12</code> （不能与 Calico 网络重叠）</li>
</ul>
<p>然后运行</p>
<pre><code class="lang-sh">kubectl apply <span class="hljs-_">-f</span> https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/rbac-kdd.yaml
kubectl apply <span class="hljs-_">-f</span> https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calico-networking/1.7/calico.yaml
</code></pre>
<p>更详细的自定义配置方法见 <a href="https://docs.projectcalico.org/v3.0/getting-started/kubernetes" target="_blank">https://docs.projectcalico.org/v3.0/getting-started/kubernetes</a>。</p>
<p>这会在 Pod 中启动 Calico-etcd，在所有 Node 上启动 bird6、felix 以及 confd，并配置 CNI 网络为 calico 插件：</p>
<p><img src="calico-components.png" alt=""/></p>
<pre><code class="lang-sh"><span class="hljs-comment"># Calico 相关进程</span>
$ ps -ef | grep calico | grep -v grep
root      9012  8995  0 14:51 ?        00:00:00 /bin/sh -c /usr/<span class="hljs-built_in">local</span>/bin/etcd --name=calico --data-dir=/var/etcd/calico-data --advertise-client-urls=http://<span class="hljs-variable">$CALICO_ETCD_IP</span>:6666 --listen-client-urls=http://0.0.0.0:6666 --listen-peer-urls=http://0.0.0.0:6667
root      9038  9012  0 14:51 ?        00:00:01 /usr/<span class="hljs-built_in">local</span>/bin/etcd --name=calico --data-dir=/var/etcd/calico-data --advertise-client-urls=http://10.146.0.2:6666 --listen-client-urls=http://0.0.0.0:6666 --listen-peer-urls=http://0.0.0.0:6667
root      9326  9325  0 14:51 ?        00:00:00 bird6 -R <span class="hljs-_">-s</span> /var/run/calico/bird6.ctl <span class="hljs-_">-d</span> -c /etc/calico/confd/config/bird6.cfg
root      9327  9322  0 14:51 ?        00:00:00 confd -confdir=/etc/calico/confd -interval=5 -watch --log-level=debug -node=http://10.96.232.136:6666 -client-key= -client-cert= -client-ca-keys=
root      9328  9324  0 14:51 ?        00:00:00 bird -R <span class="hljs-_">-s</span> /var/run/calico/bird.ctl <span class="hljs-_">-d</span> -c /etc/calico/confd/config/bird.cfg
root      9329  9323  1 14:51 ?        00:00:04 calico-felix
</code></pre>
<pre><code class="lang-sh"><span class="hljs-comment"># CNI 网络插件配置</span>
$ cat /etc/cni/net.d/10-calico.conf
{
    <span class="hljs-string">"name"</span>: <span class="hljs-string">"k8s-pod-network"</span>,
    <span class="hljs-string">"cniVersion"</span>: <span class="hljs-string">"0.1.0"</span>,
    <span class="hljs-string">"type"</span>: <span class="hljs-string">"calico"</span>,
    <span class="hljs-string">"etcd_endpoints"</span>: <span class="hljs-string">"http://10.96.232.136:6666"</span>,
    <span class="hljs-string">"log_level"</span>: <span class="hljs-string">"info"</span>,
    <span class="hljs-string">"ipam"</span>: {
        <span class="hljs-string">"type"</span>: <span class="hljs-string">"calico-ipam"</span>
    },
    <span class="hljs-string">"policy"</span>: {
        <span class="hljs-string">"type"</span>: <span class="hljs-string">"k8s"</span>,
         <span class="hljs-string">"k8s_api_root"</span>: <span class="hljs-string">"https://10.96.0.1:443"</span>,
         <span class="hljs-string">"k8s_auth_token"</span>: <span class="hljs-string">"<token>"</span>
    },
    <span class="hljs-string">"kubernetes"</span>: {
        <span class="hljs-string">"kubeconfig"</span>: <span class="hljs-string">"/etc/cni/net.d/calico-kubeconfig"</span>
    }
}

$ cat /etc/cni/net.d/calico-kubeconfig
<span class="hljs-comment"># Kubeconfig file for Calico CNI plugin.</span>
apiVersion: v1
kind: Config
clusters:
- name: <span class="hljs-built_in">local</span>
  cluster:
    insecure-skip-tls-verify: <span class="hljs-literal">true</span>
users:
- name: calico
contexts:
- name: calico-context
  context:
    cluster: <span class="hljs-built_in">local</span>
    user: calico
current-context: calico-context
</code></pre>
<p><img src="calico-flow.png" alt=""/></p>
<h2 id="calico-的不足">Calico 的不足</h2>
<ul>
<li>既然是三层实现，当然不支持 VRF</li>
<li>不支持多租户网络的隔离功能，在多租户场景下会有网络安全问题</li>
<li>Calico 控制平面的设计要求物理网络得是 L2 Fabric，这样 vRouter 间都是直接可达的</li>
</ul>
<p><strong> 参考文档 </strong></p>
<ul>
<li><a href="https://xuxinkun.github.io/2016/07/22/cni-cnm/" target="_blank">https://xuxinkun.github.io/2016/07/22/cni-cnm/</a></li>
<li><a href="https://www.projectcalico.org/" target="_blank">https://www.projectcalico.org/</a></li>
<li><a href="http://blog.dataman-inc.com/shurenyun-docker-133/" target="_blank">http://blog.dataman-inc.com/shurenyun-docker-133/</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="weave-net" class="level3">Weave</h1>
<p>Weave Net是一个多主机容器网络方案，支持去中心化的控制平面，各个host上的wRouter间通过建立Full Mesh的TCP链接，并通过Gossip来同步控制信息。这种方式省去了集中式的K/V Store，能够在一定程度上减低部署的复杂性，Weave将其称为“data centric”，而非RAFT或者Paxos的“algorithm centric”。</p>
<p>数据平面上，Weave通过UDP封装实现L2 Overlay，封装支持两种模式：</p>
<ul>
<li>运行在user space的sleeve mode：通过pcap设备在Linux bridge上截获数据包并由wRouter完成UDP封装，支持对L2 traffic进行加密，还支持Partial Connection，但是性能损失明显。</li>
<li>运行在kernal space的 fastpath mode：即通过OVS的odp封装VxLAN并完成转发，wRouter不直接参与转发，而是通过下发odp 流表的方式控制转发，这种方式可以明显地提升吞吐量，但是不支持加密等高级功能。</li>
</ul>
<p>Sleeve Mode:</p>
<p><img src="1.png" alt=""/></p>
<p>Fastpath Mode:</p>
<p><img src="2.png" alt=""/></p>
<p>关于Service的发布，weave做的也比较完整。首先，wRouter集成了DNS功能，能够动态地进行服务发现和负载均衡，另外，与libnetwork 的overlay driver类似，weave要求每个POD有两个网卡，一个就连在lb/ovs上处理L2 流量，另一个则连在docker0上处理Service流量，docker0后面仍然是iptables作NAT。</p>
<p><img src="3.png" alt=""/></p>
<p>Weave已经集成了主流的容器系统</p>
<ul>
<li>Docker: <a href="https://www.weave.works/docs/net/latest/plugin/" target="_blank">https://www.weave.works/docs/net/latest/plugin/</a></li>
<li>Kubernetes: <a href="https://www.weave.works/docs/net/latest/kube-addon/" target="_blank">https://www.weave.works/docs/net/latest/kube-addon/</a><ul>
<li><code>kubectl apply -f https://git.io/weave-kube</code></li>
</ul>
</li>
<li>CNI: <a href="https://www.weave.works/docs/net/latest/cni-plugin/" target="_blank">https://www.weave.works/docs/net/latest/cni-plugin/</a></li>
<li>Prometheus: <a href="https://www.weave.works/docs/net/latest/metrics/" target="_blank">https://www.weave.works/docs/net/latest/metrics/</a></li>
</ul>
<h2 id="weave-kubernetes">Weave Kubernetes</h2>
<pre><code class="lang-sh">kubectl apply -n kube-system <span class="hljs-_">-f</span> <span class="hljs-string">"https://cloud.weave.works/k8s/net?k8s-version=<span class="hljs-variable">$(kubectl version | base64 | tr -d '\n')</span>"</span>
</code></pre>
<p>这会在所有Node上启动Weave插件以及Network policy controller：</p>
<pre><code class="lang-sh">$ ps -ef | grep weave | grep -v grep
root     25147 25131  0 16:22 ?        00:00:00 /bin/sh /home/weave/launch.sh
root     25204 25147  0 16:22 ?        00:00:00 /home/weave/weaver --port=6783 --datapath=datapath --host-root=/host --http-addr=127.0.0.1:6784 --status-addr=0.0.0.0:6782 --docker-api= --no-dns --db-prefix=/weavedb/weave-net --ipalloc-range=10.32.0.0/12 --nickname=ubuntu-0 --ipalloc-init consensus=2 --conn-limit=30 --expect-npc 10.146.0.2 10.146.0.3
root     25669 25654  0 16:22 ?        00:00:00 /usr/bin/weave-npc
</code></pre>
<p>这样，容器网络为</p>
<ul>
<li>所有容器都连接到weave网桥</li>
<li>weave网桥通过veth pair连到内核的openvswitch模块</li>
<li>跨主机容器通过openvswitch vxlan通信</li>
<li>policy controller通过配置iptables规则为容器设置网络策略</li>
</ul>
<p><img src="weave-flow.png" alt=""/></p>
<h2 id="weave-scope">Weave Scope</h2>
<p>Weave Scope是一个容器监控和故障排查工具，可以方便的生成整个集群的拓扑并智能分组（Automatic Topologies and Intelligent Grouping）。</p>
<p>Weave Scope主要由scope-probe和scope-app组成</p>
<pre><code>+--Docker host----------+
|  +--Container------+  |    .---------------.
|  |                 |  |    | Browser       |
|  |  +-----------+  |  |    |---------------|
|  |  | scope-app |<---------|               |
|  |  +-----------+  |  |    |               |
|  |        ^        |  |    |               |
|  |        |        |  |    '---------------'
|  | +-------------+ |  |
|  | | scope-probe | |  |
|  | +-------------+ |  |
|  |                 |  |
|  +-----------------+  |
+-----------------------+
</code></pre><h2 id="优点">优点</h2>
<ul>
<li>去中心化</li>
<li>故障自动恢复</li>
<li>加密通信</li>
<li>Multicast networking</li>
</ul>
<h2 id="缺点">缺点</h2>
<ul>
<li>UDP模式性能损失较大</li>
</ul>
<p><strong>参考文档</strong></p>
<ul>
<li><a href="https://github.com/weaveworks/weave" target="_blank">https://github.com/weaveworks/weave</a></li>
<li><a href="https://www.weave.works/products/weave-net/" target="_blank">https://www.weave.works/products/weave-net/</a></li>
<li><a href="https://github.com/weaveworks/scope" target="_blank">https://github.com/weaveworks/scope</a></li>
<li><a href="https://www.weave.works/guides/monitor-docker-containers/" target="_blank">https://www.weave.works/guides/monitor-docker-containers/</a></li>
<li><a href="http://www.sdnlab.com/17141.html" target="_blank">http://www.sdnlab.com/17141.html</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="cilium" class="level3">Cilium</h1>
<p><a href="https://github.com/cilium/cilium" target="_blank">Cilium </a>是一个基于 eBPF 和 XDP 的高性能容器网络方案，代码开源在 <a href="https://github.com/cilium/cilium" target="_blank">https://github.com/cilium/cilium</a>。其主要功能特性包括</p>
<ul>
<li>安全上，支持 L3/L4/L7 安全策略，这些策略按照使用方法又可以分为<ul>
<li>基于身份的安全策略（security identity）</li>
<li>基于 CIDR 的安全策略</li>
<li>基于标签的安全策略</li>
</ul>
</li>
<li>网络上，支持三层平面网络（flat layer 3 network），如<ul>
<li>覆盖网络（Overlay），包括 VXLAN 和 Geneve 等</li>
<li>Linux 路由网络，包括原生的 Linux 路由和云服务商的高级网络路由等</li>
</ul>
</li>
<li>提供基于 BPF 的负载均衡</li>
<li>提供便利的监控和排错能力</li>
</ul>
<p><img src="cilium.png" alt=""/></p>
<h2 id="ebpf-和-xdp">eBPF 和 XDP</h2>
<p>eBPF（extended Berkeley Packet Filter）起源于BPF，它提供了内核的数据包过滤机制。BPF的基本思想是对用户提供两种SOCKET选项：<code>SO_ATTACH_FILTER</code>和<code>SO_ATTACH_BPF</code>，允许用户在sokcet上添加自定义的filter，只有满足该filter指定条件的数据包才会上发到用户空间。<code>SO_ATTACH_FILTER</code>插入的是cBPF代码，<code>SO_ATTACH_BPF</code>插入的是eBPF代码。eBPF是对cBPF的增强，目前用户端的tcpdump等程序还是用的cBPF版本，其加载到内核中后会被内核自动的转变为eBPF。Linux 3.15 开始引入 eBPF。其扩充了 BPF 的功能，丰富了指令集。它在内核提供了一个虚拟机，用户态将过滤规则以虚拟机指令的形式传递到内核，由内核根据这些指令来过滤网络数据包。</p>
<p><img src="bpf.png" alt=""/></p>
<p>XDP（eXpress Data Path）为Linux内核提供了高性能、可编程的网络数据路径。由于网络包在还未进入网络协议栈之前就处理，它给Linux网络带来了巨大的性能提升。XDP 看起来跟 DPDK 比较像，但它比 DPDK 有更多的优点，如</p>
<ul>
<li>无需第三方代码库和许可</li>
<li>同时支持轮询式和中断式网络</li>
<li>无需分配大页</li>
<li>无需专用的CPU</li>
<li>无需定义新的安全网络模型</li>
</ul>
<p>当然，XDP的性能提升是有代价的，它牺牲了通用型和公平性：（1）不提供缓存队列（qdisc），TX设备太慢时直接丢包，因而不要在RX比TX快的设备上使用XDP；（2）XDP程序是专用的，不具备网络协议栈的通用性。</p>
<h2 id="部署">部署</h2>
<p>版本要求</p>
<ul>
<li>Linux Kernel >= 4.8 （推荐 4.9.17 LTS）</li>
<li>KV 存储（etcd >= 3.1.0 或 consul >= 0.6.4）</li>
</ul>
<h3 id="kubernetes-cluster">Kubernetes Cluster</h3>
<pre><code class="lang-sh"><span class="hljs-comment"># mount BPF filesystem on all nodes</span>
$ mount bpffs /sys/fs/bpf -t bpf

$ wget https://raw.githubusercontent.com/cilium/cilium/doc-1.0/examples/kubernetes/1.10/cilium.yaml
$ vim cilium.yaml
[adjust the etcd address]

$ kubectl create <span class="hljs-_">-f</span> ./cilium.yaml
</code></pre>
<h3 id="minikube">minikube</h3>
<pre><code class="lang-sh">minikube start --network-plugin=cni --bootstrapper=localkube --memory=4096 --extra-config=apiserver.Authorization.Mode=RBAC
kubectl create clusterrolebinding kube-system-default-binding-cluster-admin --clusterrole=cluster-admin --serviceaccount=kube-system:default
kubectl create <span class="hljs-_">-f</span> https://raw.githubusercontent.com/cilium/cilium/HEAD/examples/kubernetes/addons/etcd/standalone-etcd.yaml
kubectl create <span class="hljs-_">-f</span> https://raw.githubusercontent.com/cilium/cilium/HEAD/examples/kubernetes/1.10/cilium.yaml
</code></pre>
<h3 id="istio">Istio</h3>
<pre><code class="lang-sh"><span class="hljs-comment"># cluster clusterrolebindings</span>
kubectl create clusterrolebinding kube-system-default-binding-cluster-admin --clusterrole=cluster-admin --serviceaccount=kube-system:default
<span class="hljs-comment"># etcd</span>
kubectl create <span class="hljs-_">-f</span> https://raw.githubusercontent.com/cilium/cilium/HEAD/examples/kubernetes/addons/etcd/standalone-etcd.yaml

<span class="hljs-comment"># cilium</span>
curl <span class="hljs-_">-s</span> https://raw.githubusercontent.com/cilium/cilium/HEAD/examples/kubernetes/1.10/cilium.yaml | \
  sed <span class="hljs-_">-e</span> <span class="hljs-string">'s/sidecar-http-proxy: "false"/sidecar-http-proxy: "true"/'</span> | \
  kubectl create <span class="hljs-_">-f</span> -

<span class="hljs-comment"># Istio</span>
curl -L https://git.io/getLatestIstio | sh -
ISTIO_VERSION=$(curl -L <span class="hljs-_">-s</span> https://api.github.com/repos/istio/istio/releases/latest | jq -r .tag_name)
<span class="hljs-built_in">cd</span> istio-<span class="hljs-variable">${ISTIO_VERSION}</span>
cp bin/istioctl /usr/<span class="hljs-built_in">local</span>/bin

<span class="hljs-comment"># Patch with cilium pilot</span>
sed <span class="hljs-_">-e</span> <span class="hljs-string">'s,docker\.io/istio/pilot:,docker.io/cilium/istio_pilot:,'</span> \
      < install/kubernetes/istio.yaml | \
      kubectl create <span class="hljs-_">-f</span> -

<span class="hljs-comment"># Configure Istio’s sidecar injection to use Cilium’s Docker images for the sidecar proxies</span>
kubectl create <span class="hljs-_">-f</span> https://raw.githubusercontent.com/cilium/cilium/HEAD/examples/kubernetes-istio/istio-sidecar-injector-configmap-release.yaml
</code></pre>
<h2 id="安全策略">安全策略</h2>
<p>TCP 策略：</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">"cilium.io/v2"</span>
<span class="hljs-attr">kind:</span> CiliumNetworkPolicy
<span class="hljs-attr">description:</span> <span class="hljs-string">"L3-L4 policy to restrict deathstar access to empire ships only"</span>
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> <span class="hljs-string">"rule1"</span>
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  endpointSelector:</span>
<span class="hljs-attr">    matchLabels:</span>
<span class="hljs-attr">      org:</span> empire
<span class="hljs-attr">      class:</span> deathstar
<span class="hljs-attr">  ingress:</span>
<span class="hljs-attr">  - fromEndpoints:</span>
<span class="hljs-attr">    - matchLabels:</span>
<span class="hljs-attr">        org:</span> empire
<span class="hljs-attr">    toPorts:</span>
<span class="hljs-attr">    - ports:</span>
<span class="hljs-attr">      - port:</span> <span class="hljs-string">"80"</span>
<span class="hljs-attr">        protocol:</span> TCP
</code></pre>
<p>CIDR 策略</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">"cilium.io/v2"</span>
<span class="hljs-attr">kind:</span> CiliumNetworkPolicy
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> <span class="hljs-string">"cidr-rule"</span>
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  endpointSelector:</span>
<span class="hljs-attr">    matchLabels:</span>
<span class="hljs-attr">      app:</span> myService
<span class="hljs-attr">  egress:</span>
<span class="hljs-attr">  - toCIDR:</span>
<span class="hljs-bullet">    -</span> <span class="hljs-number">20.1</span><span class="hljs-number">.1</span><span class="hljs-number">.1</span>/<span class="hljs-number">32</span>
<span class="hljs-attr">  - toCIDRSet:</span>
<span class="hljs-attr">    - cidr:</span> <span class="hljs-number">10.0</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span>/<span class="hljs-number">8</span>
<span class="hljs-attr">      except:</span>
<span class="hljs-bullet">      -</span> <span class="hljs-number">10.96</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span>/<span class="hljs-number">12</span>
</code></pre>
<p>L7 HTTP 策略：</p>
<pre><code class="lang-sh">apiVersion: <span class="hljs-string">"cilium.io/v2"</span>
kind: CiliumNetworkPolicy
description: <span class="hljs-string">"L7 policy to restrict access to specific HTTP call"</span>
metadata:
  name: <span class="hljs-string">"rule1"</span>
spec:
  endpointSelector:
    matchLabels:
      org: empire
      class: deathstar
  ingress:
  - fromEndpoints:
    - matchLabels:
        org: empire
    toPorts:
    - ports:
      - port: <span class="hljs-string">"80"</span>
        protocol: TCP
      rules:
        http:
        - method: <span class="hljs-string">"POST"</span>
          path: <span class="hljs-string">"/v1/request-landing"</span>
</code></pre>
<h2 id="监控">监控</h2>
<p><a href="https://github.com/cilium/microscope" target="_blank">microscope</a> 汇集了所有 Nodes 的监控数据（从 <code>cilium monitor</code> 获取）。使用方法为：</p>
<pre><code class="lang-sh">$ kubectl apply <span class="hljs-_">-f</span>
https://github.com/cilium/microscope/blob/master/docs/microscope.yaml
$ kubectl <span class="hljs-built_in">exec</span> -n kube-system microscope -- microscope -h
</code></pre>
<h2 id="参考资料">参考资料</h2>
<ul>
<li><a href="http://cilium.readthedocs.io/" target="_blank">Cilium documentation</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="open-virtual-networking-ovn" class="level3">OVN</h1>
<p><a href="https://github.com/openvswitch/ovn-kubernetes" target="_blank">ovn-kubernetes</a> 提供了一个ovs OVN 网络插件，支持 underlay 和 overlay 两种模式。</p>
<ul>
<li>underlay：容器运行在虚拟机中，而ovs则运行在虚拟机所在的物理机上，OVN将容器网络和虚拟机网络连接在一起</li>
<li>overlay：OVN通过logical overlay network连接所有节点的容器，此时ovs可以直接运行在物理机或虚拟机上</li>
</ul>
<h2 id="overlay模式">Overlay模式</h2>
<p><img src="images/ovn-kubernetes.png" alt=""/></p>
<h3 id="配置master">配置master</h3>
<pre><code class="lang-sh"><span class="hljs-comment"># start ovn</span>
/usr/share/openvswitch/scripts/ovn-ctl start_northd
/usr/share/openvswitch/scripts/ovn-ctl start_controller

<span class="hljs-comment"># start ovnkube</span>
nohup sudo ovnkube -k8s-kubeconfig kubeconfig.yaml -net-controller \
 -loglevel=4 \
 -k8s-apiserver=<span class="hljs-string">"http://<span class="hljs-variable">$CENTRAL_IP</span>:8080"</span> \
 -logfile=<span class="hljs-string">"/var/log/openvswitch/ovnkube.log"</span> \
 -init-master=<span class="hljs-variable">$NODE_NAME</span> -cluster-subnet=<span class="hljs-string">"<span class="hljs-variable">$CLUSTER_IP_SUBNET</span>"</span> \
 -service-cluster-ip-range=<span class="hljs-variable">$SERVICE_IP_SUBNET</span> \
 -nodeport \
 -nb-address=<span class="hljs-string">"tcp://<span class="hljs-variable">$CENTRAL_IP</span>:6631"</span> \
 -sb-address=<span class="hljs-string">"tcp://<span class="hljs-variable">$CENTRAL_IP</span>:6632"</span> 2>&1 &
</code></pre>
<h3 id="配置node">配置Node</h3>
<pre><code class="lang-sh">nohup sudo ovnkube -k8s-kubeconfig kubeconfig.yaml -loglevel=4 \
    -logfile=<span class="hljs-string">"/var/log/openvswitch/ovnkube.log"</span> \
    -k8s-apiserver=<span class="hljs-string">"http://<span class="hljs-variable">$CENTRAL_IP</span>:8080"</span> \
    -init-node=<span class="hljs-string">"<span class="hljs-variable">$NODE_NAME</span>"</span>  \
    -nodeport \
    -nb-address=<span class="hljs-string">"tcp://<span class="hljs-variable">$CENTRAL_IP</span>:6631"</span> \
    -sb-address=<span class="hljs-string">"tcp://<span class="hljs-variable">$CENTRAL_IP</span>:6632"</span> -k8s-token=<span class="hljs-string">"<span class="hljs-variable">$TOKEN</span>"</span> \
    -init-gateways \
    -service-cluster-ip-range=<span class="hljs-variable">$SERVICE_IP_SUBNET</span> \
    -cluster-subnet=<span class="hljs-variable">$CLUSTER_IP_SUBNET</span> 2>&1 &
</code></pre>
<h3 id="cni插件原理">CNI插件原理</h3>
<h4 id="add操作">ADD操作</h4>
<ul>
<li>从<code>ovn</code> annotation获取ip/mac/gateway</li>
<li>在容器netns中配置接口和路由</li>
<li>添加ovs端口</li>
</ul>
<pre><code class="lang-sh">ovs-vsctl add-port br-int veth_outside \
  --set interface veth_outside \
    external_ids:attached_mac=mac_address \
    external_ids:iface-id=namespace_pod \
    external_ids:ip_address=ip_address
</code></pre>
<h4 id="del操作">DEL操作</h4>
<pre><code class="lang-sh">ovs-vsctl del-port br-int port
</code></pre>
<h2 id="underlay模式">Underlay模式</h2>
<p>暂未实现。</p>
<h2 id="ovn-安装方法">OVN 安装方法</h2>
<p>所有节点配置安装源并安装公共依赖</p>
<pre><code class="lang-sh">sudo apt-get install apt-transport-https
<span class="hljs-built_in">echo</span> <span class="hljs-string">"deb https://packages.wand.net.nz <span class="hljs-variable">$(lsb_release -sc)</span> main"</span> | sudo tee /etc/apt/sources.list.d/wand.list
sudo curl https://packages.wand.net.nz/keyring.gpg -o /etc/apt/trusted.gpg.d/wand.gpg
sudo apt-get update

sudo apt-get build-dep dkms
sudo apt-get install python-six openssl python-pip -y
sudo -H pip install --upgrade pip

sudo apt-get install openvswitch-datapath-dkms -y
sudo apt-get install openvswitch-switch openvswitch-common -y
sudo -H pip install ovs
</code></pre>
<p>Master 节点安装 ovn-central</p>
<pre><code class="lang-sh">sudo apt-get install ovn-central ovn-common ovn-host -y
</code></pre>
<p>Node 节点安装 ovn-host</p>
<pre><code class="lang-sh">sudo apt-get install ovn-host ovn-common -y
</code></pre>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://github.com/openvswitch/ovn-kubernetes" target="_blank">https://github.com/openvswitch/ovn-kubernetes</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="contiv" class="level3">Contiv</h1>
<p><a href="http://contiv.github.io" target="_blank">Contiv</a>是思科开源的容器网络方案，是一个用于跨虚拟机、裸机、公有云或私有云的异构容器部署的开源容器网络架构，并与主流容器编排系统集成。Contiv最主要的优势是直接提供了多租户网络，并支持L2(VLAN), L3(BGP), Overlay (VXLAN)以及思科自家的ACI。</p>
<p><img src="Contiv_Blog_image.jpg" alt=""/></p>
<p>主要特征</p>
<ul>
<li>原生的Tenant支持，一个Tenant就是一个virtual routing and forwarding (VRF)</li>
<li>两种网络模式<ul>
<li>L2 VLAN Bridged</li>
<li>Routed network, e.g. vxlan, BGP, ACI</li>
</ul>
</li>
<li>Network Policy，如Bandwidth, Isolation等</li>
</ul>
<p><img src="contiv.png" alt=""/></p>
<p><img src="contiv2.png" alt=""/></p>
<p><img src="contiv3.png" alt=""/></p>
<p><img src="https://raw.githubusercontent.com/contiv/ofnet/master/docs/Architecture.jpg" alt=""/></p>
<h2 id="kubernetes集成">Kubernetes集成</h2>
<p>Ansible部署见<a href="https://github.com/kubernetes/contrib/tree/master/ansible/roles/contiv" target="_blank">https://github.com/kubernetes/contrib/tree/master/ansible/roles/contiv</a>。</p>
<pre><code class="lang-sh"><span class="hljs-built_in">export</span> VERSION=1.0.0-beta.3
curl -L -O https://github.com/contiv/install/releases/download/<span class="hljs-variable">$VERSION</span>/contiv-<span class="hljs-variable">$VERSION</span>.tgz
tar xf contiv-<span class="hljs-variable">$VERSION</span>.tgz
<span class="hljs-built_in">cd</span> ~/contiv/contiv-<span class="hljs-variable">$VERSION</span>/install/k8s
netctl --netmaster http://<span class="hljs-variable">$netmaster</span>:9999 global <span class="hljs-built_in">set</span> --fwd-mode routing

<span class="hljs-built_in">cd</span> ~/contiv/contiv-<span class="hljs-variable">$VERSION</span>
install/k8s/install.sh -n 10.87.49.77 -v b -w routing

<span class="hljs-comment"># check contiv pods</span>
<span class="hljs-built_in">export</span> NETMASTER=http://10.87.49.77:9999
netctl global info

<span class="hljs-comment"># create a network</span>
<span class="hljs-comment"># netctl network create --encap=vlan --pkt-tag=3280 --subnet=10.100.100.215-10.100.100.220/27 --gateway=10.100.100.193 vlan3280</span>
netctl net create -t default --subnet=20.1.1.0/24 default-net

<span class="hljs-comment">#  create BGP connections to each of the nodes</span>
netctl bgp create devstack-77 --router-ip=<span class="hljs-string">"30.30.30.77/24"</span> --as=<span class="hljs-string">"65000"</span> --neighbor-as=<span class="hljs-string">"65000"</span> --neighbor=<span class="hljs-string">"30.30.30.2"</span>
netctl bgp create devstack-78 --router-ip=<span class="hljs-string">"30.30.30.78/24"</span> --as=<span class="hljs-string">"65000"</span> --neighbor-as=<span class="hljs-string">"65000"</span> --neighbor=<span class="hljs-string">"30.30.30.2"</span>
netctl bgp create devstack-71 --router-ip=<span class="hljs-string">"30.30.30.79/24"</span> --as=<span class="hljs-string">"65000"</span> --neighbor-as=<span class="hljs-string">"65000"</span> --neighbor=<span class="hljs-string">"30.30.30.2"</span>

<span class="hljs-comment"># then create pod with label "io.contiv.network"</span>
</code></pre>
<p><strong>参考文档</strong></p>
<ul>
<li><a href="http://contiv.github.io/" target="_blank">http://contiv.github.io/</a></li>
<li><a href="https://github.com/contiv/netplugin" target="_blank">https://github.com/contiv/netplugin</a></li>
<li><a href="http://blogs.cisco.com/cloud/introducing-contiv-1-0" target="_blank">http://blogs.cisco.com/cloud/introducing-contiv-1-0</a></li>
<li><a href="http://blog.michali.net/2017/03/20/kubernetes-and-contiv-on-bare-metal-with-l3bgp/" target="_blank">Kubernetes and Contiv on Bare-Metal with L3/BGP</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="sr-iov" class="level3">SR-IOV</h1>
<p>SR-IOV 技术是一种基于硬件的虚拟化解决方案，可提高性能和可伸缩性</p>
<blockquote>
<p>SR-IOV 标准允许在虚拟机之间高效共享 PCIe（Peripheral Component Interconnect Express，快速外设组件互连）设备，并且它是在硬件中实现的，可以获得能够与本机性能媲美的 I/O 性能。SR-IOV 规范定义了新的标准，根据该标准，创建的新设备可允许将虚拟机直接连接到 I/O 设备（SR-IOV 规范由 PCI-SIG 在 <a href="http://www.pcisig.com" target="_blank">http://www.pcisig.com</a> 上进行定义和维护）。单个 I/O 资源可由许多虚拟机共享。共享的设备将提供专用的资源，并且还使用共享的通用资源。这样，每个虚拟机都可访问唯一的资源。因此，启用了 SR-IOV 并且具有适当的硬件和 OS 支持的 PCIe 设备（例如以太网端口）可以显示为多个单独的物理设备，每个都具有自己的 PCIe 配置空间。</p>
</blockquote>
<p>SR-IOV主要用于虚拟化中，当然也可以用于容器。</p>
<p><img src="sriov.png" alt=""/></p>
<h2 id="sr-iov配置">SR-IOV配置</h2>
<pre><code class="lang-sh">modprobe ixgbevf
lspci -Dvmm|grep -B 1 -A 4 Ethernet
<span class="hljs-built_in">echo</span> 2 > /sys/bus/pci/devices/0000:82:00.0/sriov_numvfs
<span class="hljs-comment"># check ifconfig -a. You should see a number of new interfaces created, starting with “eth”, e.g. eth4</span>
</code></pre>
<h2 id="docker-sriov-plugin">docker sriov plugin</h2>
<p>Intel给docker写了一个SR-IOV network plugin，源码位于<a href="https://github.com/clearcontainers/sriov" target="_blank">https://github.com/clearcontainers/sriov</a>，同时支持runc和clearcontainer。</p>
<h2 id="cni插件">CNI插件</h2>
<p>Intel维护了一个SR-IOV的<a href="https://github.com/Intel-Corp/sriov-cni" target="_blank">CNI插件</a>，fork自<a href="https://github.com/hustcat/sriov-cni" target="_blank">hustcat/sriov-cni</a>，并扩展了DPDK的支持。</p>
<p>项目主页见<a href="https://github.com/Intel-Corp/sriov-cni" target="_blank">https://github.com/Intel-Corp/sriov-cni</a>。</p>
<h2 id="优点">优点</h2>
<ul>
<li>性能好</li>
<li>不占用计算资源</li>
</ul>
<h2 id="缺点">缺点</h2>
<ul>
<li>VF数量有限</li>
<li>硬件绑定，不支持容器迁移</li>
</ul>
<p><strong>参考文档</strong></p>
<ul>
<li><a href="http://blog.scottlowe.org/2009/12/02/what-is-sr-iov/" target="_blank">http://blog.scottlowe.org/2009/12/02/what-is-sr-iov/</a></li>
<li><a href="https://github.com/clearcontainers/sriov" target="_blank">https://github.com/clearcontainers/sriov</a></li>
<li><a href="https://software.intel.com/en-us/articles/single-root-inputoutput-virtualization-sr-iov-with-linux-containers" target="_blank">https://software.intel.com/en-us/articles/single-root-inputoutput-virtualization-sr-iov-with-linux-containers</a></li>
<li><a href="http://jason.digitalinertia.net/exposing-docker-containers-with-sr-iov/" target="_blank">http://jason.digitalinertia.net/exposing-docker-containers-with-sr-iov/</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="romana" class="level3">Romana</h1>
<p>Romana是Panic Networks在2016年提出的开源项目，旨在解决Overlay方案给网络带来的开销。</p>
<h2 id="kubernetes部署">Kubernetes部署</h2>
<p>对使用kubeadm部署的Kubernetes集群：</p>
<pre><code class="lang-sh">kubectl apply <span class="hljs-_">-f</span> https://raw.githubusercontent.com/romana/romana/master/docs/kubernetes/romana-kubeadm.yml
</code></pre>
<p>对使用kops部署的Kubernetes集群:</p>
<pre><code class="lang-sh">kubectl apply <span class="hljs-_">-f</span> https://raw.githubusercontent.com/romana/romana/master/docs/kubernetes/romana-kops.yml
</code></pre>
<p>使用kops时要注意</p>
<ul>
<li>设置网络插件使用CNI <code>--networking cni</code></li>
<li>对于aws还提供<code>romana-aws</code>和<code>romana-vpcrouter</code>自动配置Node和Zone之间的路由</li>
</ul>
<h2 id="工作原理">工作原理</h2>
<p><img src="romana.png" alt=""/></p>
<p><img src="routeagg.png" alt=""/></p>
<ul>
<li>layer 3 networking，消除overlay带来的开销</li>
<li>基于iptables ACL的网络隔离</li>
<li>基于hierarchy CIDR管理Host/Tenant/Segment ID</li>
</ul>
<p><img src="cidr.png" alt=""/></p>
<h2 id="优点">优点</h2>
<ul>
<li>纯三层网络，性能好</li>
</ul>
<h2 id="缺点">缺点</h2>
<ul>
<li>基于IP管理租户，有规模上的限制</li>
<li>物理设备变更或地址规划变更麻烦</li>
</ul>
<p><strong>参考文档</strong></p>
<ul>
<li><a href="http://romana.io/" target="_blank">http://romana.io/</a></li>
<li><a href="http://romana.io/how/romana_basics/" target="_blank">Romana basics</a></li>
<li><a href="https://github.com/romana/romana" target="_blank">Romana Github</a></li>
<li><a href="http://romana.readthedocs.io/en/latest/index.html" target="_blank">Romana 2.0</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="opencontrail" class="level3">OpenContrail</h1>
<p>OpenContrail是Juniper推出的开源网络虚拟化平台，其商业版本为Contrail。</p>
<h2 id="架构">架构</h2>
<p>OpenContrail主要由控制器和vRouter组成：</p>
<ul>
<li>控制器提供虚拟网络的配置、控制和分析功能</li>
<li>vRouter提供分布式路由，负责虚拟路由器、虚拟网络的建立以及数据转发</li>
</ul>
<p><img src="Figure01.png" alt=""/></p>
<p>vRouter支持三种模式</p>
<ul>
<li>Kernel vRouter：类似于ovs内核模块</li>
<li>DPDK vRouter：类似于ovs-dpdk</li>
<li>Netronome Agilio Solution (商业产品)：支持DPDK, SR-IOV and Express Virtio (XVIO) </li>
</ul>
<p><img src="image05.png" alt=""/></p>
<p><strong>参考文档</strong></p>
<ul>
<li><a href="http://www.opencontrail.org/opencontrail-architecture-documentation/" target="_blank">http://www.opencontrail.org/opencontrail-architecture-documentation/</a></li>
<li><a href="http://www.opencontrail.org/network-virtualization-architecture-deep-dive/" target="_blank">http://www.opencontrail.org/network-virtualization-architecture-deep-dive/</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="kuryr" class="level3">Kuryr</h1>
<p>Kuryr 是 OpenStack Neutron 的子项目，其主要目标是透过该项目来集成 OpenStack 与 Kubernetes 的网络。该项目在 Kubernetes 中实作了原生 Neutron-based 的网络，因此使用 Kuryr-Kubernetes 可以让 OpenStack VM 与 Kubernetes Pods 能够选择在同一个子网络上运作，并且能够使用 Neutron L3 与 Security Group 来对网络进行路由，以及阻挡特定来源 Port，并且也提供基于 Neutron LBaaS 的 Service 集成。</p>
<p><img src="https://i.imgur.com/2XfP3vb.png" alt=""/></p>
<p>Kuryr-Kubernetes 有以两个主要部分组成：</p>
<ol>
<li><strong>Kuryr Controller</strong>: Controller 主要目的是监控 Kubernetes API 的来获取 Kubernetes 资源的变化，然后依据 Kubernetes 资源的需求来运行子资源的分配和资源管理。</li>
<li><strong>Kuryr CNI</strong>：主要是依据 Kuryr Controller 分配的资源来绑定网络至 Pods 上。</li>
</ol>
<h2 id="devstack-部署">devstack 部署</h2>
<p>最简单的方式是使用 devstack 部署一个单机环境：</p>
<pre><code class="lang-sh">$ git <span class="hljs-built_in">clone</span> https://git.openstack.org/openstack-dev/devstack
$ ./devstack/tools/create-stack-user.sh
$ sudo su stack

$ git <span class="hljs-built_in">clone</span> https://git.openstack.org/openstack-dev/devstack
$ git <span class="hljs-built_in">clone</span> https://git.openstack.org/openstack/kuryr-kubernetes
$ cp kuryr-kubernetes/devstack/local.conf.sample devstack/local.conf

<span class="hljs-comment"># start install</span>
$ ./devstack/stack.sh
</code></pre>
<p>部署完成后，验证安装成功</p>
<pre><code class="lang-sh">$ <span class="hljs-built_in">source</span> /devstack/openrc admin admin
$ openstack service list
+----------------------------------+------------------+------------------+
| ID                               | Name             | Type             |
+----------------------------------+------------------+------------------+
| 091e3e2813cc4904b74b60c41e8a98b3 | kuryr-kubernetes | kuryr-kubernetes |
| 2b6076dd5<span class="hljs-built_in">fc</span>04bf180e935f78c12d431 | neutron          | network          |
| b598216086944714aed2c233123<span class="hljs-built_in">fc</span>22d | keystone         | identity         |
+----------------------------------+------------------+------------------+

$ kubectl get nodes
NAME        STATUS    AGE       VERSION
localhost   Ready     2m        v1.6.2
</code></pre>
<h2 id="多机部署">多机部署</h2>
<p>本篇我們將說明如何利用 <code>DevStack</code> 與 <code>Kubespray</code> 建立一個簡單的測試環境。</p>
<h3 id="环境资源与事前准备">环境资源与事前准备</h3>
<p>准备两台实体机器，这边测试的作业系统为 <code>CentOS 7.x</code>，该环境将在平面的网络下进行。</p>
<table>
<thead>
<tr>
<th>IP Address 1</th>
<th>Role</th>
</tr>
</thead>
<tbody>
<tr>
<td>172.24.0.34</td>
<td>controller, k8s-master</td>
</tr>
<tr>
<td>172.24.0.80</td>
<td>compute1, k8s-node1</td>
</tr>
<tr>
<td>172.24.0.81</td>
<td>compute2, k8s-node2</td>
</tr>
</tbody>
</table>
<p>更新每台节点的 CentOS 7.x 包:</p>
<pre><code class="lang-shell=">$ sudo yum --enablerepo=cr update -y
</code></pre>
<p>然后关闭 firewalld 以及 SELinux 来避免实现发生问题：</p>
<pre><code class="lang-shell=">$ sudo setenforce 0
$ sudo systemctl disable firewalld && sudo systemctl stop firewalld
</code></pre>
<h3 id="openstack-controller-安裝">OpenStack Controller 安裝</h3>
<p>首先进入 <code>172.24.0.34（controller）</code>，并且运行以下命令。</p>
<p>然后运行以下命令来建立 DevStack 专用用户：</p>
<pre><code class="lang-shell=">$ sudo useradd -s /bin/bash -d /opt/stack -m stack
$ echo "stack ALL=(ALL) NOPASSWD: ALL" | sudo tee /etc/sudoers.d/stack
</code></pre>
<p>接着切换至该用户环境来创建 OpenStack：</p>
<pre><code class="lang-shell=">$ sudo su - stack
</code></pre>
<p>下载 DevStack：</p>
<pre><code class="lang-shell=">$ git clone https://git.openstack.org/openstack-dev/devstack
$ cd devstack
</code></pre>
<p>新增 <code>local.conf</code> 文档，来描述部署资讯：</p>
<pre><code>[[local|localrc]]
HOST_IP=172.24.0.34
GIT_BASE=https://github.com

ADMIN_PASSWORD=passwd
DATABASE_PASSWORD=passwd
RABBIT_PASSWORD=passwd
SERVICE_PASSWORD=passwd
SERVICE_TOKEN=passwd
MULTI_HOST=1
</code></pre><blockquote>
<p>修改 HOST_IP 为自己的 IP 。</p>
</blockquote>
<p>完成后，运行以下命令开始部署：</p>
<pre><code class="lang-shell=">$ ./stack.sh
</code></pre>
<h3 id="openstack-compute-安装">Openstack Compute 安装</h3>
<p>进入到 <code>172.24.0.80（compute）</code> 與 <code>172.24.0.81（node2）</code>，并且运行以下命令。</p>
<p>然后运行以下命令来建立 DevStack 专用用户：</p>
<pre><code class="lang-shell=">$ sudo useradd -s /bin/bash -d /opt/stack -m stack
$ echo "stack ALL=(ALL) NOPASSWD: ALL" | sudo tee /etc/sudoers.d/stack
</code></pre>
<p>接着切换至该用户环境来创建 OpenStack：</p>
<pre><code class="lang-shell=">$ sudo su - stack
</code></pre>
<p>下载 DevStack：</p>
<pre><code class="lang-shell=">$ git clone https://git.openstack.org/openstack-dev/devstack
$ cd devstack
</code></pre>
<p>新增 <code>local.conf</code> 文档，来描述部署资讯：</p>
<pre><code>[[local|localrc]]
HOST_IP=172.24.0.80
GIT_BASE=https://github.com
MULTI_HOST=1
LOGFILE=/opt/stack/logs/stack.sh.log
ADMIN_PASSWORD=passwd
DATABASE_PASSWORD=passwd
RABBIT_PASSWORD=passwd
SERVICE_PASSWORD=passwd
DATABASE_TYPE=mysql

SERVICE_HOST=172.24.0.34
MYSQL_HOST=$SERVICE_HOST
RABBIT_HOST=$SERVICE_HOST
GLANCE_HOSTPORT=$SERVICE_HOST:9292
ENABLED_SERVICES=n-cpu,q-agt,n-api-meta,c-vol,placement-client
NOVA_VNC_ENABLED=True
NOVNCPROXY_URL="http://$SERVICE_HOST:6080/vnc_auto.html"
VNCSERVER_LISTEN=$HOST_IP
VNCSERVER_PROXYCLIENT_ADDRESS=$VNCSERVER_LISTEN
</code></pre><blockquote>
<p>修改 HOST_IP 为自己的主机位置。
修改 SERVICE_HOST 为 Master 的 IP。</p>
</blockquote>
<p>完成后，运行以下命令开始部署：</p>
<pre><code class="lang-shell=">$ ./stack.sh
</code></pre>
<h3 id="创建-kubernetes-集群环境">创建 Kubernetes 集群环境</h3>
<p>首先确认所有节点之间不需要 SSH 密码即可登入，接着进入到 <code>172.24.0.34（k8s-master）</code> 并且运行以下命令。</p>
<p>接着安装所需要的软件包：</p>
<pre><code class="lang-shell=">$ sudo yum -y install software-properties-common ansible git gcc python-pip python-devel libffi-devel openssl-devel
$ sudo pip install -U kubespray
</code></pre>
<p>完成后，创建 kubespray 配置档：</p>
<pre><code class="lang-shell=">$ cat <<EOF>  ~/.kubespray.yml
kubespray_git_repo: "https://github.com/kubernetes-incubator/kubespray.git"
# Logging options
loglevel: "info"
EOF
</code></pre>
<p>利用 kubespray-cli 快速产生环境的 <code>inventory</code> 文档，并修改部分内容：</p>
<pre><code class="lang-shell=">$ sudo -i
$ kubespray prepare --masters master --etcds master --nodes node1
</code></pre>
<p>编辑 <code>/root/.kubespray/inventory/inventory.cfg</code> 文档，修改以下内容：</p>
<pre><code>[all]
master  ansible_host=172.24.0.34 ansible_user=root ip=172.24.0.34
node1    ansible_host=172.24.0.80 ansible_user=root ip=172.24.0.80
node2    ansible_host=172.24.0.81 ansible_user=root ip=172.24.0.81

[kube-master]
master

[kube-node]
master
node1
node2

[etcd]
master

[k8s-cluster:children]
kube-node
kube-master
</code></pre><p>完成后，即可利用 kubespray-cli 来进行部署：</p>
<pre><code class="lang-shell=">$ kubespray deploy --verbose -u root -k .ssh/id_rsa -n calico
</code></pre>
<p>经过一段时间后就会部署完成，这时候检查节点是否正常：</p>
<pre><code class="lang-shell=">$ kubectl get no
NAME      STATUS         AGE       VERSION
master    Ready,master   2m        v1.7.4
node1     Ready          2m        v1.7.4
node2     Ready          2m        v1.7.4
</code></pre>
<p>接着为了方便让 Kuryr Controller 简单取得 K8s API Server，这边修改 <code>/etc/kubernetes/manifests/kube-apiserver.yml</code> 文档，加入以下内容：</p>
<pre><code>- "--insecure-bind-address=0.0.0.0"
- "--insecure-port=8080"
</code></pre><blockquote>
<p>将 insecure 绑定到 0.0.0.0 之上，以及开启 8080 Port。</p>
</blockquote>
<h3 id="安装-openstack-kuryr-controller">安装 Openstack Kuryr Controller</h3>
<p>进入到 <code>172.24.0.34（controller）</code>，并且运行以下命令。</p>
<p>首先在节点安装所需要的软件包：</p>
<pre><code class="lang-shell=">$ sudo yum -y install  gcc libffi-devel python-devel openssl-devel install python-pip
</code></pre>
<p>下载 kuryr-kubernetes 并进行安装：</p>
<pre><code class="lang-shell=">$ git clone http://git.openstack.org/openstack/kuryr-kubernetes
$ pip install -e kuryr-kubernetes
</code></pre>
<p>创建 <code>kuryr.conf</code> 至 <code>/etc/kuryr</code> 目录</p>
<pre><code class="lang-shell=">$ cd kuryr-kubernetes
$ ./tools/generate_config_file_samples.sh
$ sudo mkdir -p /etc/kuryr/
$ sudo cp etc/kuryr.conf.sample /etc/kuryr/kuryr.conf
</code></pre>
<p>使用 OpenStack Dashboard 建立项目，在浏览器输入 <code>http://172.24.0.34</code>，并运行以下步骤。</p>
<ol>
<li>创建 k8s project。</li>
<li>创建 kuryr-kubernetes service，并修改 k8s project member 加入到 service project。</li>
<li>在该 Project 中新增 Security Groups，参考 <a href="https://docs.openstack.org/kuryr-kubernetes/latest/installation/manual.html" target="_blank">kuryr-kubernetes manually</a>。</li>
<li>在该 Project 中新增 pod_subnet 子网络。</li>
<li>在该 Project 中新增 service_subnet 子网络。</li>
</ol>
<p>完成后，修改 <code>/etc/kuryr/kuryr.conf</code> 文档，加入以下内容：</p>
<pre><code>[DEFAULT]
use_stderr = true
bindir = /usr/local/libexec/kuryr

[kubernetes]
api_root = http://172.24.0.34:8080

[neutron]
auth_url = http://172.24.0.34/identity
username = admin
user_domain_name = Default
password = admin
project_name = service
project_domain_name = Default
auth_type = password

[neutron_defaults]
ovs_bridge = br-int
pod_security_groups = {id_of_secuirity_group_for_pods}
pod_subnet = {id_of_subnet_for_pods}
project = {id_of_project}
service_subnet = {id_of_subnet_for_k8s_services}
</code></pre><p>完成后运行 kuryr-k8s-controller：</p>
<pre><code class="lang-shell=">$ kuryr-k8s-controller --config-file /etc/kuryr/kuryr.conf&
</code></pre>
<h3 id="安装-kuryr-cni">安装 Kuryr-CNI</h3>
<p>进入到 <code>172.24.0.80（node1）</code> 與 <code>172.24.0.81（node2）</code> 并运行以下命令。</p>
<p>首先在节点安装所需要的软件包</p>
<pre><code class="lang-shell=">$ sudo yum -y install  gcc libffi-devel python-devel openssl-devel python-pip
</code></pre>
<p>安装 Kuryr-CNI 来提供给 kubelet 使用：</p>
<pre><code class="lang-shell=">$ git clone http://git.openstack.org/openstack/kuryr-kubernetes
$ sudo pip install -e kuryr-kubernetes
</code></pre>
<p>创建 <code>kuryr.conf</code> 至 <code>/etc/kuryr</code> 目录：</p>
<pre><code class="lang-shell=">$ cd kuryr-kubernetes
$ ./tools/generate_config_file_samples.sh
$ sudo mkdir -p /etc/kuryr/
$ sudo cp etc/kuryr.conf.sample /etc/kuryr/kuryr.conf
</code></pre>
<p>修改 <code>/etc/kuryr/kuryr.conf</code> 文档，加入以下内容：</p>
<pre><code>[DEFAULT]
use_stderr = true
bindir = /usr/local/libexec/kuryr
[kubernetes]
api_root = http://172.24.0.34:8080
</code></pre><p>创建 CNI bin 与 Conf 目录：</p>
<pre><code class="lang-shell=">$ sudo mkdir -p /opt/cni/bin
$ sudo ln -s $(which kuryr-cni) /opt/cni/bin/
$ sudo mkdir -p /etc/cni/net.d/
</code></pre>
<p>新增 <code>/etc/cni/net.d/10-kuryr.conf</code> CNI 配置档：</p>
<pre><code>{
    "cniVersion": "0.3.0",
    "name": "kuryr",
    "type": "kuryr-cni",
    "kuryr_conf": "/etc/kuryr/kuryr.conf",
    "debug": true
}
</code></pre><p>完成后，更新 oslo 与 vif python 库：</p>
<pre><code class="lang-shell=">$ sudo pip install 'oslo.privsep>=1.20.0' 'os-vif>=1.5.0'
</code></pre>
<p>最后重新启动服务：</p>
<pre><code>$ sudo systemctl daemon-reload && systemctl restart kubelet.service
</code></pre><h2 id="测试结果">测试结果</h2>
<p>创建一个 Pod 与 OpenStack VM 来进行通信：
<img src="https://i.imgur.com/UYXdKud.png" alt=""/></p>
<p><img src="https://i.imgur.com/dwoEytW.png" alt=""/></p>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://docs.openstack.org/kuryr-kubernetes/latest/index.html" target="_blank">Kuryr kubernetes documentation</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="container-runtime-interface-cri" class="level2">运行时插件CRI</h1>
<p>容器运行时插件（Container Runtime Interface，简称 CRI）是 Kubernetes v1.5 引入的容器运行时接口，它将 Kubelet 与容器运行时解耦，将原来完全面向 Pod 级别的内部接口拆分成面向 Sandbox 和 Container 的 gRPC 接口，并将镜像管理和容器管理分离到不同的服务。</p>
<p><img src="images/cri.png" alt=""/></p>
<p>CRI 最早从从 1.4 版就开始设计讨论和开发，在 v1.5 中发布第一个测试版。在 v1.6 时已经有了很多外部容器运行时，如 frakti 和 cri-o 等。v1.7 中又新增了 cri-containerd 支持用 Containerd 来管理容器。</p>
<h2 id="cri-接口">CRI 接口</h2>
<p>CRI 基于 gRPC 定义了 RuntimeService 和 ImageService 等两个 gRPC 服务，分别用于容器运行时和镜像的管理。其定义在</p>
<ul>
<li>v1.10-v1.12: <a href="https://github.com/kubernetes/kubernetes/tree/master/pkg/kubelet/apis/cri/runtime/v1alpha2" target="_blank">pkg/kubelet/apis/cri/runtime/v1alpha2</a></li>
<li>v1.7-v1.9: <a href="https://github.com/kubernetes/kubernetes/tree/release-1.9/pkg/kubelet/apis/cri/v1alpha1/runtime" target="_blank">pkg/kubelet/apis/cri/v1alpha1/runtime</a></li>
<li>v1.6: <a href="https://github.com/kubernetes/kubernetes/tree/release-1.6/pkg/kubelet/api/v1alpha1/runtime" target="_blank">pkg/kubelet/api/v1alpha1/runtime</a></li>
</ul>
<p>Kubelet 作为 CRI 的客户端，而容器运行时则需要实现 CRI 的服务端（即 gRPC server，通常称为 CRI shim）。容器运行时在启动 gRPC server 时需要监听在本地的 Unix Socket （Windows 使用 tcp 格式）。</p>
<h3 id="开发-cri-容器运行时">开发 CRI 容器运行时</h3>
<p>开发新的容器运行时只需要实现 CRI 的 gRPC Server，包括 RuntimeService 和 ImageService。该 gRPC Server 需要监听在本地的 unix socket（Linux 支持 unix socket 格式，Windows 支持 tcp 格式）。</p>
<p>一个简单的示例为</p>
<pre><code class="lang-go"><span class="hljs-keyword">import</span> (
    <span class="hljs-comment">// Import essential packages</span>
    <span class="hljs-string">"google.golang.org/grpc"</span>
    runtime <span class="hljs-string">"k8s.io/kubernetes/pkg/kubelet/apis/cri/runtime/v1alpha2"</span>
)

<span class="hljs-comment">// Serivice implements runtime.ImageService and runtime.RuntimeService.</span>
<span class="hljs-keyword">type</span> Service <span class="hljs-keyword">struct</span> {
    ...
}

<span class="hljs-keyword">func</span> main() {
    service := &Service{}
    s := grpc.NewServer(grpc.MaxRecvMsgSize(maxMsgSize),
        grpc.MaxSendMsgSize(maxMsgSize))
    runtime.RegisterRuntimeServiceServer(s, service)
    runtime.RegisterImageServiceServer(s, service)
    lis, err := net.Listen(<span class="hljs-string">"unix"</span>, <span class="hljs-string">"/var/run/runtime.sock"</span>)
    <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> {
        logrus.Fatalf(<span class="hljs-string">"Failed to create listener: %v"</span>, err)
    }
    <span class="hljs-keyword">go</span> s.Serve(lis)

    <span class="hljs-comment">// Other codes</span>
}
</code></pre>
<p>对于 Streaming API（Exec、PortForward 和 Attach），CRI 要求容器运行时返回一个 streaming server 的 URL 以便 Kubelet 重定向 API Server 发送过来的请求。在 v1.10 及更早版本中，容器运行时必需返回一个 API Server 可直接访问的 URL（通常跟 Kubelet 使用相同的监听地址）；而从 v1.11 开始，Kubelet 新增了 <code>--redirect-container-streaming</code>（默认为 false），默认不再转发而是代理 Streaming 请求，这样运行时可以返回一个 localhost 的 URL（当然也不再需要配置 TLS）。</p>
<p>详细的实现方法可以参考 <a href="https://github.com/kubernetes/kubernetes/tree/master/pkg/kubelet/dockershim" target="_blank">dockershim</a> 或者 <a href="https://github.com/kubernetes-incubator/cri-o" target="_blank">cri-o</a>。</p>
<h3 id="kubelet-配置">Kubelet 配置</h3>
<p>在启动 kubelet 时传入容器运行时监听的 Unix Socket 文件路径，比如</p>
<pre><code class="lang-sh">kubelet --container-runtime=remote --container-runtime-endpoint=unix:///var/run/runtime.sock --image-service-endpoint=unix:///var/run/runtime.sock
</code></pre>
<h2 id="容器运行时">容器运行时</h2>
<p>目前基于 CRI 容器引擎已经比较丰富了，包括</p>
<ul>
<li>Docker: 核心代码依然保留在 kubelet 内部（<a href="https://github.com/kubernetes/kubernetes/tree/master/pkg/kubelet/dockershim" target="_blank">pkg/kubelet/dockershim</a>），是最稳定和特性支持最好的运行时</li>
<li>OCI 容器运行时：<ul>
<li>社区有两个实现<ul>
<li><a href="https://github.com/containerd/cri" target="_blank">Containerd</a>，支持 kubernetes v1.7+</li>
<li><a href="https://github.com/kubernetes-incubator/cri-o" target="_blank">CRI-O</a>，支持 Kubernetes v1.6+</li>
</ul>
</li>
<li>支持的 OCI 容器引擎包括<ul>
<li><a href="https://github.com/opencontainers/runc" target="_blank">runc</a>：OCI 标准容器引擎</li>
<li><a href="https://github.com/google/gvisor" target="_blank">gVisor</a>：谷歌开源的基于用户空间内核的沙箱容器引擎</li>
<li><a href="https://github.com/clearcontainers/runtime" target="_blank">Clear Containers</a>：Intel 开源的基于虚拟化的容器引擎</li>
<li><a href="https://github.com/kata-containers/runtime" target="_blank">Kata Containers</a>：基于虚拟化的容器引擎，由 Clear Containers 和 runV 合并而来</li>
</ul>
</li>
</ul>
</li>
<li><a href="https://github.com/alibaba/pouch" target="_blank">PouchContainer</a>：阿里巴巴开源的胖容器引擎</li>
<li><a href="https://github.com/kubernetes/frakti" target="_blank">Frakti</a>：支持 Kubernetes v1.6+，提供基于 hypervisor 和 docker 的混合运行时，适用于运行非可信应用，如多租户和 NFV 等场景</li>
<li><a href="https://github.com/kubernetes-incubator/rktlet" target="_blank">Rktlet</a>：支持 <a href="https://github.com/rkt/rkt" target="_blank">rkt</a> 容器引擎（rknetes 代码已在 v1.10 中弃用）</li>
<li><a href="https://github.com/Mirantis/virtlet" target="_blank">Virtlet</a>：Mirantis 开源的虚拟机容器引擎，直接管理 libvirt 虚拟机，镜像须是 qcow2 格式</li>
<li><a href="https://github.com/apporbit/infranetes" target="_blank">Infranetes</a>：直接管理 IaaS 平台虚拟机，如 GCE、AWS 等</li>
</ul>
<h3 id="containerd">Containerd</h3>
<p>以 Containerd 为例，在 1.0 及以前版本将 dockershim 和 docker daemon 替换为 cri-containerd + containerd，而在 1.1 版本直接将 cri-containerd 内置在 Containerd 中，简化为一个 CRI 插件。</p>
<p><img src="images/cri-containerd.png" alt=""/></p>
<p>Containerd 内置的 CRI 插件实现了 Kubelet CRI 接口中的 Image Service 和 Runtime Service，通过内部接口管理容器和镜像，并通过 CNI 插件给 Pod 配置网络。</p>
<p><img src="images/containerd.png" alt=""/></p>
<h2 id="runtimeclass">RuntimeClass</h2>
<p>RuntimeClass 是 v1.12 引入的新 API 对象，用来支持多容器运行时，比如</p>
<ul>
<li>Kata Containers/gVisor + runc</li>
<li>Windows Process isolation + Hyper-V isolation containers</li>
</ul>
<p>RuntimeClass 表示一个运行时对象，在使用前需要开启特性开关 <code>RuntimeClass</code>，并创建 RuntimeClass CRD：</p>
<pre><code class="lang-sh">kubectl apply <span class="hljs-_">-f</span> https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/runtimeclass/runtimeclass_crd.yaml
</code></pre>
<p>然后就可以定义 RuntimeClass 对象</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> node.k8s.io/v1alpha1  <span class="hljs-comment"># RuntimeClass is defined in the node.k8s.io API group</span>
<span class="hljs-attr">kind:</span> RuntimeClass
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> myclass  <span class="hljs-comment"># The name the RuntimeClass will be referenced by</span>
  <span class="hljs-comment"># RuntimeClass is a non-namespaced resource</span>
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  runtimeHandler:</span> myconfiguration  <span class="hljs-comment"># The name of the corresponding CRI configuration</span>
</code></pre>
<p>而在 Pod 中定义使用哪个 RuntimeClass：</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> mypod
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  runtimeClassName:</span> myclass
  <span class="hljs-comment"># ...</span>
</code></pre>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://kubernetes.io/docs/concepts/containers/runtime-class/#runtime-class" target="_blank">Runtime Class Documentation</a></li>
<li><a href="https://docs.google.com/document/d/1fe7lQUjYKR0cijRmSbH_y0_l3CYPkwtQa5ViywuNo8Q/preview" target="_blank">Sandbox Isolation Level Decision</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="cri-tools" class="level3">CRI-tools</h1>
<p>通常，容器引擎会提供一个命令行工具来帮助用户调试容器应用并简化故障排错。比如使用 Docker 作为容器运行时的时候，可以使用 <code>docker</code> 命令来查看容器和镜像的状态，并验证容器的配置是否正确。但在使用其他容器引擎时，推荐使用 <code>crictl</code> 来替代 <code>docker</code> 工具。</p>
<p><code>crictl</code> 是 <a href="https://github.com/kubernetes-incubator/cri-tools" target="_blank">cri-tools</a> 的一部分，它提供了类似于 docker 的命令行工具，不需要通过 Kubelet 就可以通过 CRI 跟容器运行时通信。它是专门为 Kubernetes 设计的，提供了Pod、容器和镜像等资源的管理命令，可以帮助用户和开发者调试容器应用或者排查异常问题。<code>crictl</code> 可以用于所有实现了 CRI 接口的容器运行时。</p>
<p>注意，<code>crictl</code> 并非 <code>kubectl</code> 的替代品，它只通过 CRI 接口与容器运行时通信，可以用来调试和排错，但并不用于运行容器。虽然 crictl 也提供运行 Pod 和容器的子命令，但这些命令仅推荐用于调试。需要注意的是，如果是在 Kubernetes Node 上面创建了新的 Pod，那么它们会被 Kubelet 停止并删除。</p>
<p>除了 <code>crictl</code>，cri-tools 还提供了用于验证容器运行时是否实现 CRI 需要功能的验证测试工具 <code>critest</code>。<code>critest</code> 通过运行一系列的测试验证容器运行时在实现 CRI 时是否与 Kubelet 的需求一致，推荐所有的容器运行时在发布前都要通过其测试。一般情况下，<code>critest</code> 可以作为容器运行时集成测试的一部分，用以保证代码更新不会破坏 CRI 功能。</p>
<p>cri-tools 已在 v1.11 版 GA，详细使用方法请参考 <a href="https://github.com/kubernetes-sigs/cri-tools" target="_blank">kubernetes-sigs/cri-tools</a> 和 <a href="https://kubernetes.io/docs/tasks/debug-application-cluster/crictl/" target="_blank">Debugging Kubernetes nodes with crictl</a>。</p>
<h2 id="crictl-示例">crictl 示例</h2>
<h3 id="查询-pod">查询 Pod</h3>
<pre><code class="lang-sh">$ crictl pods --name nginx-65899c769f-wv2gp
POD ID              CREATED             STATE               NAME                     NAMESPACE           ATTEMPT
4dccb216c4adb       2 minutes ago       Ready               nginx-65899c769f-wv2gp   default             0
</code></pre>
<h3 id="pod-列表">Pod 列表</h3>
<pre><code class="lang-sh">$ crictl pods
POD ID              CREATED              STATE               NAME                         NAMESPACE           ATTEMPT
926f1b5a1d33a       About a minute ago   Ready               sh-84d7dcf559-4r2gq          default             0
4dccb216c4adb       About a minute ago   Ready               nginx-65899c769f-wv2gp       default             0
a86316e96fa89       17 hours ago         Ready               kube-proxy-gblk4             kube-system         0
919630b8f81f1       17 hours ago         Ready               nvidia-device-plugin-zgbbv   kube-system         0
</code></pre>
<h3 id="镜像列表">镜像列表</h3>
<pre><code class="lang-sh">$ crictl images
IMAGE                                     TAG                 IMAGE ID            SIZE
busybox                                   latest              8c811b4aec35f       1.15MB
k8s-gcrio.azureedge.net/hyperkube-amd64   v1.10.3             e179bbfe5d238       665MB
k8s-gcrio.azureedge.net/pause-amd64       3.1                 da86e6ba6ca19       742kB
nginx                                     latest              <span class="hljs-built_in">cd</span>5239a0906a6       109MB
</code></pre>
<h3 id="容器列表">容器列表</h3>
<pre><code class="lang-sh">$ crictl ps <span class="hljs-_">-a</span>
CONTAINER ID        IMAGE                                                                                                             CREATED             STATE               NAME                       ATTEMPT
1f73f2d81bf98       busybox@sha256:141c253bc4c3fd0a201d32dc1f493bcf3fff003b6df416dea4f41046e0f37d47                                   7 minutes ago       Running             sh                         1
9c5951df22c78       busybox@sha256:141c253bc4c3fd0a201d32dc1f493bcf3fff003b6df416dea4f41046e0f37d47                                   8 minutes ago       Exited              sh                         0
87d3992f84f74       nginx@sha256:d0a8828cccb73397acb0073bf34f4d7d8aa315263f1e7806bf8c55d8ac139d5f                                     8 minutes ago       Running             nginx                      0
1941fb4da154f       k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:00d814b1f7763f4ab5be80c58e98140dfc69df107f253d7fdd714b30a714260a   18 hours ago        Running             kube-proxy                 0
</code></pre>
<h3 id="容器内执行命令">容器内执行命令</h3>
<pre><code class="lang-sh">$ crictl <span class="hljs-built_in">exec</span> -i -t 1f73f2d81bf98 ls
bin   dev   etc   home  proc  root  sys   tmp   usr   var
</code></pre>
<h3 id="容器日志">容器日志</h3>
<pre><code class="lang-sh">crictl logs 87d3992f84f74
10.240.0.96 - - [06/Jun/2018:02:45:49 +0000] <span class="hljs-string">"GET / HTTP/1.1"</span> 200 612 <span class="hljs-string">"-"</span> <span class="hljs-string">"curl/7.47.0"</span> <span class="hljs-string">"-"</span>
10.240.0.96 - - [06/Jun/2018:02:45:50 +0000] <span class="hljs-string">"GET / HTTP/1.1"</span> 200 612 <span class="hljs-string">"-"</span> <span class="hljs-string">"curl/7.47.0"</span> <span class="hljs-string">"-"</span>
10.240.0.96 - - [06/Jun/2018:02:45:51 +0000] <span class="hljs-string">"GET / HTTP/1.1"</span> 200 612 <span class="hljs-string">"-"</span> <span class="hljs-string">"curl/7.47.0"</span> <span class="hljs-string">"-"</span>
</code></pre>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/crictl/" target="_blank">Debugging Kubernetes nodes with crictl</a></li>
<li><a href="https://github.com/kubernetes-sigs/cri-tools" target="_blank">https://github.com/kubernetes-sigs/cri-tools</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="frakti" class="level3">Frakti</h1>
<h2 id="简介">简介</h2>
<p>Frakti是一个基于Kubelet CRI的运行时，它提供了hypervisor级别的隔离性，特别适用于运行不可信应用以及多租户场景下。Frakti实现了一个混合运行时：</p>
<ul>
<li>特权容器以Docker container的方式运行</li>
<li>而普通容器则以hyper container的方法运行在VM内</li>
</ul>
<h2 id="allinone安装方法">Allinone安装方法</h2>
<p>Frakti提供了一个简便的安装脚本，可以一键在Ubuntu或CentOS上启动一个本机的Kubernetes+frakti集群。</p>
<pre><code class="lang-sh">curl <span class="hljs-_">-s</span>SL https://github.com/kubernetes/frakti/raw/master/cluster/allinone.sh | bash
</code></pre>
<h2 id="集群部署">集群部署</h2>
<p>首先需要在所有机器上安装hyperd, docker, frakti, CNI 和 kubelet。</p>
<h3 id="安装hyperd">安装hyperd</h3>
<p>Ubuntu 16.04+:</p>
<pre><code class="lang-sh">apt-get update && apt-get install -y qemu libvirt-bin
curl <span class="hljs-_">-s</span>SL https://hypercontainer.io/install | bash
</code></pre>
<p>CentOS 7:</p>
<pre><code class="lang-sh">curl <span class="hljs-_">-s</span>SL https://hypercontainer.io/install | bash
</code></pre>
<p>配置hyperd:</p>
<pre><code class="lang-sh"><span class="hljs-built_in">echo</span> <span class="hljs-_">-e</span> <span class="hljs-string">"Kernel=/var/lib/hyper/kernel\n\
Initrd=/var/lib/hyper/hyper-initrd.img\n\
Hypervisor=qemu\n\
StorageDriver=overlay\n\
gRPCHost=127.0.0.1:22318"</span> > /etc/hyper/config
systemctl <span class="hljs-built_in">enable</span> hyperd
systemctl restart hyperd
</code></pre>
<h3 id="安装docker">安装docker</h3>
<p>Ubuntu 16.04+:</p>
<pre><code class="lang-sh">apt-get update
apt-get install -y docker.io
</code></pre>
<p>CentOS 7:</p>
<pre><code class="lang-sh">yum install -y docker
</code></pre>
<p>启动docker:</p>
<pre><code class="lang-sh">systemctl <span class="hljs-built_in">enable</span> docker
systemctl start docker
</code></pre>
<h3 id="安装frakti">安装frakti</h3>
<pre><code class="lang-sh">curl <span class="hljs-_">-s</span>SL https://github.com/kubernetes/frakti/releases/download/v0.2/frakti -o /usr/bin/frakti
chmod +x /usr/bin/frakti
cgroup_driver=$(docker info | awk <span class="hljs-string">'/Cgroup Driver/{print $3}'</span>)
cat <<EOF > /lib/systemd/system/frakti.service
[Unit]
Description=Hypervisor-based container runtime <span class="hljs-keyword">for</span> Kubernetes
Documentation=https://github.com/kubernetes/frakti
After=network.target

[Service]
ExecStart=/usr/bin/frakti --v=3 \
          --log-dir=/var/<span class="hljs-built_in">log</span>/frakti \
          --logtostderr=<span class="hljs-literal">false</span> \
          --cgroup-driver=<span class="hljs-variable">${cgroup_driver}</span> \
          --listen=/var/run/frakti.sock \
          --streaming-server-addr=%H \
          --hyper-endpoint=127.0.0.1:22318
MountFlags=shared
TasksMax=8192
LimitNOFILE=1048576
LimitNPROC=1048576
LimitCORE=infinity
TimeoutStartSec=0
Restart=on-abnormal

[Install]
WantedBy=multi-user.target
EOF
</code></pre>
<h3 id="安装cni">安装CNI</h3>
<p>Ubuntu 16.04+:</p>
<pre><code class="lang-sh">apt-get update && apt-get install -y apt-transport-https
curl <span class="hljs-_">-s</span> https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
cat <<EOF > /etc/apt/sources.list.d/kubernetes.list
deb http://apt.kubernetes.io/ kubernetes-xenial main
EOF
apt-get update
apt-get install -y kubernetes-cni
</code></pre>
<p>CentOS 7:</p>
<pre><code class="lang-sh">cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=http://yum.kubernetes.io/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg
       https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF
setenforce 0
yum install -y kubernetes-cni
</code></pre>
<p>配置CNI网络，注意</p>
<ul>
<li>frakti目前仅支持bridge插件</li>
<li>所有机器上Pod的子网不能相同，比如master上可以用<code>10.244.1.0/24</code>，而第一个Node上可以用<code>10.244.2.0/24</code></li>
</ul>
<pre><code class="lang-sh">mkdir -p /etc/cni/net.d
cat >/etc/cni/net.d/10-mynet.conf <<-EOF
{
    <span class="hljs-string">"cniVersion"</span>: <span class="hljs-string">"0.3.0"</span>,
    <span class="hljs-string">"name"</span>: <span class="hljs-string">"mynet"</span>,
    <span class="hljs-string">"type"</span>: <span class="hljs-string">"bridge"</span>,
    <span class="hljs-string">"bridge"</span>: <span class="hljs-string">"cni0"</span>,
    <span class="hljs-string">"isGateway"</span>: <span class="hljs-literal">true</span>,
    <span class="hljs-string">"ipMasq"</span>: <span class="hljs-literal">true</span>,
    <span class="hljs-string">"ipam"</span>: {
        <span class="hljs-string">"type"</span>: <span class="hljs-string">"host-local"</span>,
        <span class="hljs-string">"subnet"</span>: <span class="hljs-string">"10.244.1.0/24"</span>,
        <span class="hljs-string">"routes"</span>: [
            { <span class="hljs-string">"dst"</span>: <span class="hljs-string">"0.0.0.0/0"</span>  }
        ]
    }
}
EOF
cat >/etc/cni/net.d/99-loopback.conf <<-EOF
{
    <span class="hljs-string">"cniVersion"</span>: <span class="hljs-string">"0.3.0"</span>,
    <span class="hljs-string">"type"</span>: <span class="hljs-string">"loopback"</span>
}
EOF
</code></pre>
<h3 id="安装kubelet">安装Kubelet</h3>
<p>Ubuntu 16.04+:</p>
<pre><code class="lang-sh">apt-get install -y kubelet kubeadm kubectl
</code></pre>
<p>CentOS 7:</p>
<pre><code class="lang-sh">yum install -y kubelet kubeadm kubectl
</code></pre>
<p>配置Kubelet使用frakti runtime:</p>
<pre><code class="lang-sh">sed -i <span class="hljs-string">'2 i\Environment="KUBELET_EXTRA_ARGS=--container-runtime=remote --container-runtime-endpoint=/var/run/frakti.sock --feature-gates=AllAlpha=true"'</span> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
systemctl daemon-reload
</code></pre>
<h3 id="配置master">配置Master</h3>
<pre><code class="lang-sh">kubeadm init kubeadm init --pod-network-cidr 10.244.0.0/16 --kubernetes-version latest

<span class="hljs-comment"># Optional: enable schedule pods on the master</span>
<span class="hljs-built_in">export</span> KUBECONFIG=/etc/kubernetes/admin.conf
kubectl taint nodes --all node-role.kubernetes.io/master:NoSchedule-
</code></pre>
<h3 id="配置node">配置Node</h3>
<pre><code class="lang-sh"><span class="hljs-comment"># get token on master node</span>
token=$(kubeadm token list | grep authentication,signing | awk <span class="hljs-string">'{print $1}'</span>)

<span class="hljs-comment"># join master on worker nodes</span>
kubeadm join --token <span class="hljs-variable">$token</span> <span class="hljs-variable">${master_ip}</span>
</code></pre>
<h3 id="配置cni网络路由">配置CNI网络路由</h3>
<p>在集群模式下，需要为容器网络配置直接路由，假设有一台master和两台Node：</p>
<pre><code>NODE   IP_ADDRESS   CONTAINER_CIDR
master 10.140.0.1  10.244.1.0/24
node-1 10.140.0.2  10.244.2.0/24
node-2 10.140.0.3  10.244.3.0/24
</code></pre><p>CNI的网络路由可以这么配置：</p>
<pre><code class="lang-sh"><span class="hljs-comment"># on master</span>
ip route add 10.244.2.0/24 via 10.140.0.2
ip route add 10.244.3.0/24 via 10.140.0.3

<span class="hljs-comment"># on node-1</span>
ip route add 10.244.1.0/24 via 10.140.0.1
ip route add 10.244.3.0/24 via 10.140.0.3

<span class="hljs-comment"># on node-2</span>
ip route add 10.244.1.0/24 via 10.140.0.1
ip route add 10.244.2.0/24 via 10.140.0.2
</code></pre>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://github.com/kubernetes/frakti/blob/master/docs/deploy.md" target="_blank">Frakti部署指南</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="存储插件" class="level2">存储插件</h1>
<p>Kubernetes 已经提供丰富的 <a href="../concepts/volume.html">Volume</a> 和 <a href="../concepts/persistent-volume.html">Persistent Volume</a> 插件，可以根据需要使用这些插件给容器提供持久化存储。</p>
<p>如果内置的这些 Volume 还不满足要求，则可以使用 <a href="flex-volume.html">FlexVolume</a> 或者 <a href="csi.html">容器存储接口 CSI</a> 实现自己的 Volume 插件。</p>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="container-storage-interface" class="level3">容器存储接口CSI</h1>
<p>Container Storage Interface (CSI) 是从 v1.9 引入的容器存储接口，用于扩展 Kubernetes 的存储生态。实际上，CSI 是整个容器生态的标准存储接口，同样适用于 Mesos、Cloud Foundry 等其他的容器集群调度系统。</p>
<p><strong>版本信息</strong></p>
<table>
<thead>
<tr>
<th>Kubernetes</th>
<th>CSI Spec</th>
<th>Status</th>
</tr>
</thead>
<tbody>
<tr>
<td>v1.9</td>
<td>v0.1</td>
<td>Alpha</td>
</tr>
<tr>
<td>v1.10</td>
<td>v0.2</td>
<td>Beta</td>
</tr>
<tr>
<td>v1.11-v1.12</td>
<td>v0.3</td>
<td>Beta</td>
</tr>
</tbody>
</table>
<p>Sidecar 容器版本</p>
<table>
<thead>
<tr>
<th>Container Name</th>
<th>CSI spec</th>
<th>Latest Release Tag</th>
</tr>
</thead>
<tbody>
<tr>
<td>csi-provisioner</td>
<td>v0.3</td>
<td>v0.3.1</td>
</tr>
<tr>
<td>csi-attacher</td>
<td>v0.3</td>
<td>v0.3.0</td>
</tr>
<tr>
<td>driver-registrar</td>
<td>v0.3</td>
<td>v0.3.0</td>
</tr>
</tbody>
</table>
<h2 id="原理">原理</h2>
<p>类似于 CRI，CSI 也是基于 gRPC 实现。详细的 CSI SPEC 可以参考 <a href="https://github.com/container-storage-interface/spec/blob/master/spec.md" target="_blank">这里</a>，它要求插件开发者要实现三个 gRPC 服务：</p>
<ul>
<li><strong>Identity Service</strong>：用于 Kubernetes 与 CSI 插件协调版本信息</li>
<li><strong>Controller Service</strong>：用于创建、删除以及管理 Volume 存储卷</li>
<li><strong>Node Service</strong>：用于将 Volume 存储卷挂载到指定的目录中以便 Kubelet 创建容器时使用（需要监听在 <code>/var/lib/kubelet/plugins/[SanitizedCSIDriverName]/csi.sock</code>）</li>
</ul>
<p>由于 CSI 监听在 unix socket 文件上， kube-controller-manager 并不能直接调用 CSI 插件。为了协调 Volume 生命周期的管理，并方便开发者实现 CSI 插件，Kubernetes 提供了几个 sidecar 容器并推荐使用下述方法来部署 CSI 插件：</p>
<p><img src="images/container-storage-interface_diagram1.png" alt=""/></p>
<p>该部署方法包括：</p>
<ul>
<li>StatefuelSet：副本数为 1 保证只有一个实例运行，它包含三个容器<ul>
<li>用户实现的 CSI 插件</li>
<li><a href="https://github.com/kubernetes-csi/external-attacher" target="_blank">External Attacher</a>：Kubernetes 提供的 sidecar 容器，它监听 <em>VolumeAttachment</em> 和 <em>PersistentVolume</em> 对象的变化情况，并调用 CSI 插件的 ControllerPublishVolume 和 ControllerUnpublishVolume 等 API 将 Volume 挂载或卸载到指定的 Node 上</li>
<li><a href="https://github.com/kubernetes-csi/external-provisioner" target="_blank">External Provisioner</a>：Kubernetes 提供的 sidecar 容器，它监听  <em>PersistentVolumeClaim</em> 对象的变化情况，并调用 CSI 插件的 <em>ControllerPublish</em> 和 <em>ControllerUnpublish</em> 等 API 管理 Volume</li>
</ul>
</li>
<li>Daemonset：将 CSI 插件运行在每个 Node 上，以便 Kubelet 可以调用。它包含 2 个容器<ul>
<li>用户实现的 CSI 插件</li>
<li><a href="https://github.com/kubernetes-csi/driver-registrar" target="_blank">Driver Registrar</a>：注册 CSI 插件到 kubelet 中，并初始化 <em>NodeId</em>（即给 Node 对象增加一个 Annotation <code>csi.volume.kubernetes.io/nodeid</code>）</li>
</ul>
</li>
</ul>
<h2 id="配置">配置</h2>
<ul>
<li>API Server 配置：</li>
</ul>
<pre><code class="lang-sh">--allow-privileged=<span class="hljs-literal">true</span>
--feature-gates=CSIPersistentVolume=<span class="hljs-literal">true</span>,MountPropagation=<span class="hljs-literal">true</span>
--runtime-config=storage.k8s.io/v1alpha1=<span class="hljs-literal">true</span>
</code></pre>
<ul>
<li>Controller-manager 配置：</li>
</ul>
<pre><code class="lang-sh">--feature-gates=CSIPersistentVolume=<span class="hljs-literal">true</span>
</code></pre>
<ul>
<li>Kubelet 配置：</li>
</ul>
<pre><code class="lang-sh">--allow-privileged=<span class="hljs-literal">true</span>
--feature-gates=CSIPersistentVolume=<span class="hljs-literal">true</span>,MountPropagation=<span class="hljs-literal">true</span>
</code></pre>
<h3 id="示例">示例</h3>
<p>Kubernetes 提供了几个 <a href="https://github.com/kubernetes-csi/drivers" target="_blank">CSI 示例</a>，包括 NFS、ISCSI、HostPath、Cinder 以及 FlexAdapter 等。在实现 CSI 插件时，这些示例可以用作参考。</p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Status</th>
<th>More Information</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://github.com/kubernetes/cloud-provider-openstack/tree/master/pkg/csi/cinder" target="_blank">Cinder</a></td>
<td>v0.2.0</td>
<td>A Container Storage Interface (CSI) Storage Plug-in for Cinder</td>
</tr>
<tr>
<td><a href="https://github.com/digitalocean/csi-digitalocean" target="_blank">DigitalOcean Block Storage</a></td>
<td>v0.0.1 (alpha)</td>
<td>A Container Storage Interface (CSI) Driver for DigitalOcean Block Storage</td>
</tr>
<tr>
<td><a href="https://github.com/kubernetes-sigs/aws-ebs-csi-driver" target="_blank">AWS Elastic Block Storage</a></td>
<td>v0.0.1(alpha)</td>
<td>A Container Storage Interface (CSI) Driver for AWS Elastic Block Storage (EBS)</td>
</tr>
<tr>
<td><a href="https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver" target="_blank">GCE Persistent Disk</a></td>
<td>Alpha</td>
<td>A Container Storage Interface (CSI) Storage Plugin for Google Compute Engine Persistent Disk</td>
</tr>
<tr>
<td><a href="https://www.opensds.io/" target="_blank">OpenSDS</a></td>
<td>Beta</td>
<td>For more information, please visit <a href="https://github.com/opensds/nbp/releases" target="_blank">releases</a> and <a href="https://github.com/opensds/nbp/tree/master/csi" target="_blank">https://github.com/opensds/nbp/tree/master/csi</a></td>
</tr>
<tr>
<td><a href="https://portworx.com/" target="_blank">Portworx</a></td>
<td>0.2.0</td>
<td>CSI implementation is available <a href="https://github.com/libopenstorage/openstorage/tree/master/csi" target="_blank">here</a> which can be used as an example also.</td>
</tr>
<tr>
<td><a href="https://github.com/ceph/ceph-csi" target="_blank">RBD</a></td>
<td>v0.2.0</td>
<td>A Container Storage Interface (CSI) Storage RBD Plug-in for Ceph</td>
</tr>
<tr>
<td><a href="https://github.com/ceph/ceph-csi" target="_blank">CephFS</a></td>
<td>v0.2.0</td>
<td>A Container Storage Interface (CSI) Storage Plug-in for CephFS</td>
</tr>
<tr>
<td><a href="https://github.com/thecodeteam/csi-scaleio" target="_blank">ScaleIO</a></td>
<td>v0.1.0</td>
<td>A Container Storage Interface (CSI) Storage Plugin for DellEMC ScaleIO</td>
</tr>
<tr>
<td><a href="https://github.com/thecodeteam/csi-vsphere" target="_blank">vSphere</a></td>
<td>v0.1.0</td>
<td>A Container Storage Interface (CSI) Storage Plug-in for VMware vSphere</td>
</tr>
<tr>
<td><a href="https://github.com/NetApp/trident" target="_blank">NetApp</a></td>
<td>v0.2.0 (alpha)</td>
<td>A Container Storage Interface (CSI) Storage Plug-in for NetApp's <a href="https://netapp-trident.readthedocs.io/" target="_blank">Trident</a> container storage orchestrator</td>
</tr>
<tr>
<td><a href="https://ember-csi.io/" target="_blank">Ember CSI</a></td>
<td>v0.2.0 (alpha)</td>
<td>Multi-vendor CSI plugin supporting over 80 storage drivers to provide block and mount storage to Container Orchestration systems.</td>
</tr>
<tr>
<td><a href="https://portal.nutanix.com/#/page/docs/details?targetId=CSI-Volume-Driver:CSI-Volume-Driver" target="_blank">Nutanix</a></td>
<td>beta</td>
<td>A Container Storage Interface (CSI) Storage Driver for Nutanix</td>
</tr>
<tr>
<td><a href="https://github.com/quobyte/quobyte-csi" target="_blank">Quobyte</a></td>
<td>v0.2.0</td>
<td>A Container Storage Interface (CSI) Plugin for Quobyte</td>
</tr>
</tbody>
</table>
<p>下面以 NFS 为例来看一下 CSI 插件的使用方法。</p>
<p>首先需要部署 NFS 插件：</p>
<pre><code class="lang-sh">git <span class="hljs-built_in">clone</span> https://github.com/kubernetes-csi/drivers
<span class="hljs-built_in">cd</span> drivers/pkg/nfs
kubectl create <span class="hljs-_">-f</span> deploy/kubernetes
</code></pre>
<p>然后创建一个使用 NFS 存储卷的容器</p>
<pre><code class="lang-sh">kubectl create <span class="hljs-_">-f</span> examples/kubernetes/nginx.yaml
</code></pre>
<p>该例中已直接创建 PV 的方式使用 NFS</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> PersistentVolume
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> data-nfsplugin
<span class="hljs-attr">  labels:</span>
<span class="hljs-attr">    name:</span> data-nfsplugin
<span class="hljs-attr">  annotations:</span>
    csi.volume.kubernetes.io/volume-attributes: <span class="hljs-string">'{"server":"10.10.10.10","share":"share"}'</span>
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  accessModes:</span>
<span class="hljs-bullet">  -</span> ReadWriteOnce
<span class="hljs-attr">  capacity:</span>
<span class="hljs-attr">    storage:</span> <span class="hljs-number">100</span>Gi
<span class="hljs-attr">  csi:</span>
<span class="hljs-attr">    driver:</span> csi-nfsplugin
<span class="hljs-attr">    volumeHandle:</span> data-id
<span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> PersistentVolumeClaim
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> data-nfsplugin
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  accessModes:</span>
<span class="hljs-bullet">  -</span> ReadWriteOnce
<span class="hljs-attr">  resources:</span>
<span class="hljs-attr">    requests:</span>
<span class="hljs-attr">      storage:</span> <span class="hljs-number">100</span>Gi
<span class="hljs-attr">  selector:</span>
<span class="hljs-attr">    matchExpressions:</span>
<span class="hljs-attr">    - key:</span> name
<span class="hljs-attr">      operator:</span> In
<span class="hljs-attr">      values:</span> [<span class="hljs-string">"data-nfsplugin"</span>]
</code></pre>
<p>也可以用在 StorageClass 中</p>
<pre><code class="lang-yaml"><span class="hljs-attr">kind:</span> StorageClass
<span class="hljs-attr">apiVersion:</span> storage.k8s.io/v1
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> csi-sc-nfsplugin
<span class="hljs-attr">provisioner:</span> csi-nfsplugin
<span class="hljs-attr">parameters:</span>

<span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> PersistentVolumeClaim
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> request-for-storage
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  accessModes:</span>
<span class="hljs-bullet">  -</span> ReadWriteOnce
<span class="hljs-attr">  resources:</span>
<span class="hljs-attr">    requests:</span>
<span class="hljs-attr">      storage:</span> <span class="hljs-number">5</span>Gi
<span class="hljs-attr">  storageClassName:</span> csi-sc-nfsplugin
</code></pre>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://kubernetes-csi.github.io/docs/Home.html" target="_blank">Kubernetes CSI Documentation</a></li>
<li><a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/container-storage-interface.md#recommended-mechanism-for-deploying-csi-drivers-on-kubernetes" target="_blank">CSI Volume Plugins in Kubernetes Design Doc</a> </li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="flexvolume" class="level3">FlexVolume</h1>
<p>FlexVolume 是 Kubernetes v1.8+ 支持的一种存储插件扩展方式。类似于 CNI 插件，它需要外部插件将二进制文件放到预先配置的路径中（如 <code>/usr/libexec/kubernetes/kubelet-plugins/volume/exec/</code>），并需要在系统中安装好所有需要的依赖。</p>
<h2 id="flexvolume-接口">FlexVolume 接口</h2>
<p>实现一个 FlexVolume 包括两个步骤</p>
<ul>
<li>实现 <a href="https://github.com/kubernetes/community/blob/master/contributors/devel/flexvolume.md" target="_blank">FlexVolume 插件接口</a>，包括 <code>init/attach/detach/waitforattach/isattached/mountdevice/unmountdevice/mount/umount</code> 等命令（可参考 <a href="https://github.com/kubernetes/examples/blob/master/staging/volumes/flexvolume/lvm" target="_blank">lvm 示例</a> 和 <a href="https://github.com/kubernetes/examples/blob/master/staging/volumes/flexvolume/nfs" target="_blank">NFS 示例</a>）</li>
<li>将插件放到 <code>/usr/libexec/kubernetes/kubelet-plugins/volume/exec/<vendor~driver>/<driver></code> 目录中</li>
</ul>
<p>FlexVolume 的接口包括</p>
<ul>
<li>init：kubelet/kube-controller-manager 初始化存储插件时调用，插件需要返回是否需要要 <code>attach</code> 和 <code>detach</code> 操作</li>
<li>attach：将存储卷挂载到 Node 上</li>
<li>detach：将存储卷从 Node 上卸载</li>
<li>waitforattach： 等待 attach 操作成功（超时时间为 10 分钟）</li>
<li>isattached：检查存储卷是否已经挂载</li>
<li>mountdevice：将设备挂载到指定目录中以便后续 bind mount 使用</li>
<li>unmountdevice：将设备取消挂载</li>
<li>mount：将存储卷挂载到指定目录中</li>
<li>umount：将存储卷取消挂载</li>
</ul>
<p>而存储驱动在实现这些接口时需要以 JSON 格式返回数据，数据格式为</p>
<pre><code class="lang-json">{
  <span class="hljs-string">"status"</span>: <span class="hljs-string">"<Success/Failure/Not supported>"</span>,
  <span class="hljs-string">"message"</span>: <span class="hljs-string">"<Reason for success/failure>"</span>,
  <span class="hljs-string">"device"</span>: <span class="hljs-string">"<Path to the device attached. This field is valid only for attach & waitforattach call-outs>"</span>,
  <span class="hljs-string">"volumeName"</span>: <span class="hljs-string">"<Cluster wide unique name of the volume. Valid only for getvolumename call-out>"</span>,
  <span class="hljs-string">"attached"</span>: <span class="hljs-string">"<True/False (Return true if volume is attached on the node. Valid only for isattached call-out)>"</span>,
    <span class="hljs-string">"capabilities"</span>:
    {
        <span class="hljs-string">"attach"</span>: <span class="hljs-string">"<True/False (Return true if the driver implements attach and detach)>"</span>
    }
}
</code></pre>
<h2 id="使用-flexvolume">使用 FlexVolume</h2>
<p>在使用 flexVolume 时，需要指定卷的 driver，格式为 <code><vendor~driver>/<driver></code>，如下面的例子使用了 <code>kubernetes.io/lvm</code></p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> nginx
<span class="hljs-attr">  namespace:</span> default
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">  - name:</span> nginx
<span class="hljs-attr">    image:</span> nginx
<span class="hljs-attr">    volumeMounts:</span>
<span class="hljs-attr">    - name:</span> test
<span class="hljs-attr">      mountPath:</span> /data
<span class="hljs-attr">    ports:</span>
<span class="hljs-attr">    - containerPort:</span> <span class="hljs-number">80</span>
<span class="hljs-attr">  volumes:</span>
<span class="hljs-attr">  - name:</span> test
<span class="hljs-attr">    flexVolume:</span>
<span class="hljs-attr">      driver:</span> <span class="hljs-string">"kubernetes.io/lvm"</span>
<span class="hljs-attr">      fsType:</span> <span class="hljs-string">"ext4"</span>
<span class="hljs-attr">      options:</span>
<span class="hljs-attr">        volumeID:</span> <span class="hljs-string">"vol1"</span>
<span class="hljs-attr">        size:</span> <span class="hljs-string">"1000m"</span>
<span class="hljs-attr">        volumegroup:</span> <span class="hljs-string">"kube_vg"</span>
</code></pre>
<p>注意：</p>
<ul>
<li>在 v1.7 版本，部署新的 FlevVolume 插件后需要重启 kubelet 和 kube-controller-manager；而从 v1.8 开始不需要重启它们了。</li>
<li>FlexVolume 不支持 <a href="https://kubernetes.io/docs/concepts/storage/dynamic-provisioning/" target="_blank">Dynamic Volume Provisioning</a>，以 PVC 方式使用前需要管理员预先创建好对应的 PV。</li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="glusterfs-持久化存储" class="level3">glusterfs</h1>
<p>我们复用 kubernetes 的三台主机做 GlusterFS 存储。</p>
<h2 id="安装-glusterfs">安装 GlusterFS</h2>
<p>我们直接在物理机上使用 yum 安装，如果你选择在 kubernetes 上安装，请参考 <a href="https://github.com/gluster/gluster-kubernetes/blob/master/docs/setup-guide.md" target="_blank">https://github.com/gluster/gluster-kubernetes/blob/master/docs/setup-guide.md</a>。</p>
<pre><code class="lang-bash"><span class="hljs-comment"># 先安装 gluster 源</span>
$ yum install centos-release-gluster -y

<span class="hljs-comment"># 安装 glusterfs 组件</span>
$ yum install -y glusterfs glusterfs-server glusterfs-fuse glusterfs-rdma glusterfs-geo-replication glusterfs-devel

<span class="hljs-comment">## 创建 glusterfs 目录</span>
$ mkdir /opt/glusterd

<span class="hljs-comment">## 修改 glusterd 目录</span>
$ sed -i <span class="hljs-string">'s/var\/lib/opt/g'</span> /etc/glusterfs/glusterd.vol

<span class="hljs-comment"># 启动 glusterfs</span>
$ systemctl start glusterd.service

<span class="hljs-comment"># 设置开机启动</span>
$ systemctl <span class="hljs-built_in">enable</span> glusterd.service

<span class="hljs-comment">#查看状态</span>
$ systemctl status glusterd.service
</code></pre>
<h2 id="配置-glusterfs">配置 GlusterFS</h2>
<pre><code class="lang-Bash"><span class="hljs-comment"># 配置 hosts</span>

$ vi /etc/hosts
172.20.0.113   sz-pg-oam-docker-test-001.tendcloud.com
172.20.0.114   sz-pg-oam-docker-test-002.tendcloud.com
172.20.0.115   sz-pg-oam-docker-test-003.tendcloud.com
</code></pre>
<pre><code class="lang-bash"><span class="hljs-comment"># 开放端口</span>
$ iptables -I INPUT -p tcp --dport 24007 -j ACCEPT

<span class="hljs-comment"># 创建存储目录</span>
$ mkdir /opt/gfs_data
</code></pre>
<pre><code class="lang-bash"><span class="hljs-comment"># 添加节点到 集群</span>
<span class="hljs-comment"># 执行操作的本机不需要 probe 本机</span>
[root@sz-pg-oam-docker-test-001 ~]<span class="hljs-comment">#</span>
gluster peer probe sz-pg-oam-docker-test-002.tendcloud.com
gluster peer probe sz-pg-oam-docker-test-003.tendcloud.com

<span class="hljs-comment"># 查看集群状态</span>
$ gluster peer status
Number of Peers: 2

Hostname: sz-pg-oam-docker-test-002.tendcloud.com
Uuid: f25546cc-2011-457d-ba24-342554b51317
State: Peer <span class="hljs-keyword">in</span> Cluster (Connected)

Hostname: sz-pg-oam-docker-test-003.tendcloud.com
Uuid: 42b6cad1-aa01-46d0-bbba<span class="hljs-_">-f</span>7ec6821d66d
State: Peer <span class="hljs-keyword">in</span> Cluster (Connected)
</code></pre>
<h2 id="配置-volume">配置 volume</h2>
<p>GlusterFS 中的 volume 的模式有很多种，包括以下几种：</p>
<ul>
<li><strong>分布卷(默认模式)</strong>：即 DHT, 也叫 分布卷: 将文件以 hash 算法随机分布到 一台服务器节点中存储。</li>
<li><strong>复制模式</strong>：即 AFR, 创建 volume 时带 replica x 数量: 将文件复制到 replica x 个节点中。</li>
<li><strong>条带模式</strong>：即 Striped, 创建 volume 时带 stripe x 数量： 将文件切割成数据块，分别存储到 stripe x 个节点中 (类似 raid 0)。</li>
<li><strong>分布式条带模式</strong>：最少需要 4 台服务器才能创建。 创建 volume 时 stripe 2 server = 4 个节点： 是 DHT 与 Striped 的组合型。</li>
<li><strong>分布式复制模式</strong>：最少需要 4 台服务器才能创建。 创建 volume 时 replica 2 server = 4 个节点：是 DHT 与 AFR 的组合型。</li>
<li><strong>条带复制卷模式</strong>：最少需要 4 台服务器才能创建。 创建 volume 时 stripe 2 replica 2 server = 4 个节点： 是 Striped 与 AFR 的组合型。</li>
<li><strong>三种模式混合</strong>： 至少需要 8 台 服务器才能创建。 stripe 2 replica 2 , 每 4 个节点 组成一个 组。</li>
</ul>
<p>这几种模式的示例图参考 <a href="https://docs.gluster.org/en/latest/Quick-Start-Guide/Architecture/#types-of-volumes" target="_blank">GlusterFS Documentation</a>。</p>
<p>因为我们只有三台主机，在此我们使用默认的<strong>分布卷模式</strong>。<strong>请勿在生产环境上使用该模式，容易导致数据丢失。</strong></p>
<pre><code class="lang-bash"><span class="hljs-comment"># 创建分布卷</span>
$ gluster volume create k8s-volume transport tcp sz-pg-oam-docker-test-001.tendcloud.com:/opt/gfs_data sz-pg-oam-docker-test-002.tendcloud.com:/opt/gfs_data sz-pg-oam-docker-test-003.tendcloud.com:/opt/gfs_data force

<span class="hljs-comment"># 查看 volume 状态</span>
$ gluster volume info
Volume Name: k8s-volume
Type: Distribute
Volume ID: 9a3b0710-4565-4eb7-abae-1d5c8ed625ac
Status: Created
Snapshot Count: 0
Number of Bricks: 3
Transport-type: tcp
Bricks:
Brick1: sz-pg-oam-docker-test-001.tendcloud.com:/opt/gfs_data
Brick2: sz-pg-oam-docker-test-002.tendcloud.com:/opt/gfs_data
Brick3: sz-pg-oam-docker-test-003.tendcloud.com:/opt/gfs_data
Options Reconfigured:
transport.address-family: inet
nfs.disable: on

<span class="hljs-comment"># 启动 分布卷</span>
$ gluster volume start k8s-volume
</code></pre>
<h2 id="glusterfs-调优">Glusterfs 调优</h2>
<pre><code class="lang-bash"><span class="hljs-comment"># 开启 指定 volume 的配额</span>
$ gluster volume quota k8s-volume <span class="hljs-built_in">enable</span>

<span class="hljs-comment"># 限制 指定 volume 的配额</span>
$ gluster volume quota k8s-volume <span class="hljs-built_in">limit</span>-usage / 1TB

<span class="hljs-comment"># 设置 cache 大小, 默认 32MB</span>
$ gluster volume <span class="hljs-built_in">set</span> k8s-volume performance.cache-size 4GB

<span class="hljs-comment"># 设置 io 线程, 太大会导致进程崩溃</span>
$ gluster volume <span class="hljs-built_in">set</span> k8s-volume performance.io-thread-count 16

<span class="hljs-comment"># 设置 网络检测时间, 默认 42s</span>
$ gluster volume <span class="hljs-built_in">set</span> k8s-volume network.ping-timeout 10

<span class="hljs-comment"># 设置 写缓冲区的大小, 默认 1M</span>
$ gluster volume <span class="hljs-built_in">set</span> k8s-volume performance.write-behind-window-size 1024MB
</code></pre>
<h2 id="kubernetes-中使用-glusterfs">Kubernetes 中使用 GlusterFS</h2>
<p>官方的文档见<a href="https://github.com/kubernetes/examples/tree/master/staging/volumes/glusterfs" target="_blank">https://github.com/kubernetes/examples/tree/master/staging/volumes/glusterfs</a>.</p>
<p>以下用到的所有 yaml 和 json 配置文件可以在 <a href="https://github.com/feiskyer/kubernetes-handbook/tree/master/manifests/glusterfs" target="_blank">glusterfs</a> 中找到。注意替换其中私有镜像地址为你自己的镜像地址。</p>
<h2 id="kubernetes-安装客户端">kubernetes 安装客户端</h2>
<pre><code class="lang-bash"><span class="hljs-comment"># 在所有 k8s node 中安装 glusterfs 客户端</span>
$ yum install -y glusterfs glusterfs-fuse

<span class="hljs-comment"># 配置 hosts</span>
$ vi /etc/hosts
172.20.0.113   sz-pg-oam-docker-test-001.tendcloud.com
172.20.0.114   sz-pg-oam-docker-test-002.tendcloud.com
172.20.0.115   sz-pg-oam-docker-test-003.tendcloud.com
</code></pre>
<p>因为我们 glusterfs 跟 kubernetes 集群复用主机，因为此这一步可以省去。</p>
<h2 id="配置-endpoints">配置 endpoints</h2>
<pre><code class="lang-bash">$ curl -O https://raw.githubusercontent.com/kubernetes/kubernetes/master/examples/volumes/glusterfs/glusterfs-endpoints.json

<span class="hljs-comment"># 修改 endpoints.json ，配置 glusters 集群节点 ip</span>
<span class="hljs-comment"># 每一个 addresses 为一个 ip 组</span>

    {
      <span class="hljs-string">"addresses"</span>: [
        {
          <span class="hljs-string">"ip"</span>: <span class="hljs-string">"172.22.0.113"</span>
        }
      ],
      <span class="hljs-string">"ports"</span>: [
        {
          <span class="hljs-string">"port"</span>: 1990
        }
      ]
    },

<span class="hljs-comment"># 导入 glusterfs-endpoints.json</span>

$ kubectl apply <span class="hljs-_">-f</span> glusterfs-endpoints.json

<span class="hljs-comment"># 查看 endpoints 信息</span>
$ kubectl get ep
</code></pre>
<h2 id="配置-service">配置 service</h2>
<pre><code class="lang-bash">$ curl -O https://raw.githubusercontent.com/kubernetes/kubernetes/master/examples/volumes/glusterfs/glusterfs-service.json

<span class="hljs-comment"># service.json 里面查找的是 enpointes 的名称与端口，端口默认配置为 1，我改成了 1990</span>

<span class="hljs-comment"># 导入 glusterfs-service.json</span>
$ kubectl apply <span class="hljs-_">-f</span> glusterfs-service.json

<span class="hljs-comment"># 查看 service 信息</span>
$ kubectl get svc
</code></pre>
<h2 id="创建测试-pod">创建测试 pod</h2>
<pre><code class="lang-bash">$ curl -O https://github.com/kubernetes/examples/raw/master/staging/volumes/glusterfs/glusterfs-pod.json

<span class="hljs-comment"># 编辑 glusterfs-pod.json</span>
<span class="hljs-comment"># 修改 volumes  下的 path 为上面创建的 volume 名称</span>

<span class="hljs-string">"path"</span>: <span class="hljs-string">"k8s-volume"</span>

<span class="hljs-comment"># 导入 glusterfs-pod.json</span>
$ kubectl apply <span class="hljs-_">-f</span> glusterfs-pod.json

<span class="hljs-comment"># 查看 pods 状态</span>
$ kubectl get pods
NAME                             READY     STATUS    RESTARTS   AGE
glusterfs                        1/1       Running   0          1m

<span class="hljs-comment"># 查看 pods 所在 node</span>
$ kubectl describe pods/glusterfs

<span class="hljs-comment"># 登陆 node 物理机，使用 df 可查看挂载目录</span>
$ df -h
172.20.0.113:k8s-volume  1.0T     0  1.0T   0% /var/lib/kubelet/pods/3de9<span class="hljs-built_in">fc</span>69-30b7-11e7-bfbd-8af1e3a7c5bd/volumes/kubernetes.io~glusterfs/glusterfsvol
</code></pre>
<h2 id="配置-persistentvolume">配置 PersistentVolume</h2>
<p>PersistentVolume（PV）和 PersistentVolumeClaim（PVC）是 kubernetes 提供的两种 API 资源，用于抽象存储细节。管理员关注于如何通过 pv 提供存储功能而无需关注用户如何使用，同样的用户只需要挂载 PVC 到容器中而不需要关注存储卷采用何种技术实现。PVC 和 PV 的关系跟 pod 和 node 关系类似，前者消耗后者的资源。PVC 可以向 PV 申请指定大小的存储资源并设置访问模式。</p>
<p><strong>PV 属性 </strong></p>
<ul>
<li>storage 容量</li>
<li>读写属性：分别为<ul>
<li>ReadWriteOnce：单个节点读写；</li>
<li>ReadOnlyMany：多节点只读 ；</li>
<li>ReadWriteMany：多节点读写</li>
</ul>
</li>
</ul>
<pre><code class="lang-bash">$ cat glusterfs-pv.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: gluster-dev-volume
spec:
  capacity:
    storage: 8Gi
  accessModes:
    - ReadWriteMany
  glusterfs:
    endpoints: <span class="hljs-string">"glusterfs-cluster"</span>
    path: <span class="hljs-string">"k8s-volume"</span>
    <span class="hljs-built_in">read</span>Only: <span class="hljs-literal">false</span>

<span class="hljs-comment"># 导入 PV</span>
$ kubectl apply <span class="hljs-_">-f</span> glusterfs-pv.yaml

<span class="hljs-comment"># 查看 pv</span>
$ kubectl get pv
NAME                 CAPACITY   ACCESSMODES   RECLAIMPOLICY   STATUS      CLAIM     STORAGECLASS   REASON    AGE
gluster-dev-volume   8Gi        RWX           Retain          Available                                      3s
</code></pre>
<p>PVC 属性</p>
<ul>
<li>访问属性与 PV 相同</li>
<li>容量：向 PV 申请的容量 <= PV 总容量</li>
</ul>
<h2 id="配置-pvc">配置 PVC</h2>
<pre><code class="lang-Bash">$ cat glusterfs-pvc.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: glusterfs-nginx
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 8Gi

<span class="hljs-comment"># 导入 pvc</span>
$ kubectl apply <span class="hljs-_">-f</span> glusterfs-pvc.yaml

<span class="hljs-comment"># 查看 pvc</span>

$ kubectl get pv
NAME              STATUS    VOLUME               CAPACITY   ACCESSMODES   STORAGECLASS   AGE
glusterfs-nginx   Bound     gluster-dev-volume   8Gi        RWX                          4s
</code></pre>
<h2 id="创建-nginx-deployment-挂载-volume">创建 nginx deployment 挂载 volume</h2>
<pre><code class="lang-Bash">$ vi nginx-deployment.yaml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: nginx-dm
spec:
  replicas: 2
  template:
    metadata:
      labels:
        name: nginx
    spec:
      containers:
        - name: nginx
          image: nginx:alpine
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 80
          volumeMounts:
            - name: gluster-dev-volume
              mountPath: <span class="hljs-string">"/usr/share/nginx/html"</span>
      volumes:
      - name: gluster-dev-volume
        persistentVolumeClaim:
          claimName: glusterfs-nginx

<span class="hljs-comment"># 导入 deployment</span>
$ kubectl apply <span class="hljs-_">-f</span> nginx-deployment.yaml

<span class="hljs-comment"># 查看 deployment</span>
$ kubectl get pods |grep nginx-dm
nginx-dm-3698525684-g0mvt       1/1       Running   0          6s
nginx-dm-3698525684-hbzq1       1/1       Running   0          6s

<span class="hljs-comment"># 查看 挂载</span>
$ kubectl <span class="hljs-built_in">exec</span> -it nginx-dm-3698525684-g0mvt -- df -h|grep k8s-volume
172.20.0.113:k8s-volume         1.0T     0  1.0T   0% /usr/share/nginx/html

<span class="hljs-comment"># 创建文件 测试</span>
$ kubectl <span class="hljs-built_in">exec</span> -it nginx-dm-3698525684-g0mvt -- touch /usr/share/nginx/html/index.html

$ kubectl <span class="hljs-built_in">exec</span> -it nginx-dm-3698525684-g0mvt -- ls <span class="hljs-_">-lt</span> /usr/share/nginx/html/index.html
-rw-r--r-- 1 root root 0 May  4 11:36 /usr/share/nginx/html/index.html

<span class="hljs-comment"># 验证 glusterfs</span>
<span class="hljs-comment"># 因为我们使用分布卷，所以可以看到某个节点中有文件</span>
[root@sz-pg-oam-docker-test-001 ~] ls /opt/gfs_data/
[root@sz-pg-oam-docker-test-002 ~] ls /opt/gfs_data/
index.html
[root@sz-pg-oam-docker-test-003 ~] ls /opt/gfs_data/
</code></pre>
<h2 id="参考">参考</h2>
<ul>
<li><a href="http://www.cnblogs.com/jicki/p/5801712.html" target="_blank">CentOS 7 安装 GlusterFS</a></li>
<li><a href="https://github.com/gluster/gluster-kubernetes" target="_blank">https://github.com/gluster/gluster-kubernetes</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="network-policy-扩展" class="level2">网络策略</h1>
<p><a href="../concepts/network-policy.html">Network Policy</a> 提供了基于策略的网络控制，用于隔离应用并减少攻击面。它使用标签选择器模拟传统的分段网络，并通过策略控制它们之间的流量以及来自外部的流量。Network Policy 需要网络插件来监测这些策略和 Pod 的变更，并为 Pod 配置流量控制。</p>
<h2 id="如何开发-network-policy-扩展">如何开发 Network Policy 扩展</h2>
<p>实现一个支持 Network Policy 的网络扩展需要至少包含两个组件</p>
<ul>
<li>CNI 网络插件：负责给 Pod 配置网络接口</li>
<li>Policy controller：监听 Network Policy 的变化，并将 Policy 应用到相应的网络接口</li>
</ul>
<p><img src="images/policy-controller.jpg" alt=""/></p>
<h2 id="支持-network-policy-的网络插件">支持 Network Policy 的网络插件</h2>
<ul>
<li><a href="https://www.projectcalico.org/" target="_blank">Calico</a></li>
<li><a href="https://github.com/romana/romana" target="_blank">Romana</a></li>
<li><a href="https://www.weave.works/" target="_blank">Weave Net</a></li>
<li><a href="https://github.com/aporeto-inc/trireme-kubernetes" target="_blank">Trireme</a></li>
<li><a href="http://www.opencontrail.org/" target="_blank">OpenContrail</a></li>
</ul>
<h2 id="network-policy-使用方法">Network Policy 使用方法</h2>
<p>具体 Network Policy 的使用方法可以参考 <a href="../concepts/network-policy.html">这里</a>。</p>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="ingress-controller-扩展" class="level2">IngressController</h1>
<p><a href="../concepts/ingress.html">Ingress</a> 为 Kubernetes 集群中的服务提供了外部入口以及路由，而 Ingress Controller 监测 Ingress 和 Service 资源的变更并根据规则配置负载均衡、路由规则和 DNS 等并提供访问入口。</p>
<h2 id="如何开发-ingress-controller-扩展">如何开发 Ingress Controller 扩展</h2>
<p><a href="https://github.com/kubernetes/ingress-nginx" target="_blank">NGINX Ingress Controller</a> 和 <a href="https://github.com/kubernetes/ingress-gce" target="_blank">GLBC</a> 提供了两个 Ingress Controller 的完整示例，可以在此基础上方便的开发新的 Ingress Controller。</p>
<h2 id="常见-ingress-controller">常见 Ingress Controller</h2>
<ul>
<li><a href="https://github.com/kubernetes/ingress-nginx" target="_blank">Nginx Ingress</a></li>
</ul>
<pre><code class="lang-sh">helm install stable/nginx-ingress --name nginx-ingress --set rbac.create=<span class="hljs-literal">true</span>
</code></pre>
<ul>
<li><p><a href="https://github.com/jcmoraisjr/haproxy-ingress" target="_blank">HAProxy Ingress controller</a></p>
</li>
<li><p><a href="https://linkerd.io/config/0.9.1/linkerd/index.html#ingress-identifier" target="_blank">Linkerd</a></p>
</li>
<li><p><a href="https://docs.traefik.io/configuration/backends/kubernetes/" target="_blank">traefik</a></p>
</li>
<li><p><a href="https://github.com/coreos/alb-ingress-controller" target="_blank">AWS Application Load Balancer Ingress Controller</a></p>
</li>
<li><p><a href="https://github.com/zalando-incubator/kube-ingress-aws-controller" target="_blank">kube-ingress-aws-controller</a></p>
</li>
<li><p><a href="https://github.com/appscode/voyager" target="_blank">Voyager: HAProxy Ingress Controller</a></p>
</li>
</ul>
<h2 id="ingress-使用方法">Ingress 使用方法</h2>
<p>具体 Ingress 的使用方法可以参考 <a href="../concepts/ingress.html">这里</a>。</p>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="kubernetes-ingress-lets-encrypt" class="level3">Ingress+Letsencrypt</h1>
<h2 id="申请域名">申请域名</h2>
<p>在使用 Let's Encrypt 之前需要申请一个域名，比如可以到 GoDaddy、Name 等网站购买。具体步骤这里不再细说，可以参考网络教程操作。</p>
<h2 id="部署-nginx-ingress-controller">部署 Nginx Ingress Controller</h2>
<p>直接使用 Helm 部署即可：</p>
<pre><code class="lang-sh">helm install stable/nginx-ingress --name nginx-ingress --set rbac.create=<span class="hljs-literal">true</span> --namespace=kube-system
</code></pre>
<p>部署成功后，查询 Ingress 服务的公网 IP 地址（下文中假设该 IP 是 <code>6.6.6.6</code>）：</p>
<pre><code class="lang-sh">$ kubectl -n kube-system get service nginx-ingress-controller
NAME                       TYPE           CLUSTER-IP     EXTERNAL-IP     PORT(S)                      AGE
nginx-ingress-controller   LoadBalancer   10.0.216.124   6.6.6.6         80:31935/TCP,443:31797/TCP   4d
</code></pre>
<p>然后到域名注册服务商网站中，创建 A 记录，将需要的域名解析到 <code>6.6.6.6</code>。</p>
<h2 id="开启--lets-encrypt">开启  Let's Encrypt</h2>
<pre><code class="lang-sh"><span class="hljs-comment"># Install cert-manager</span>
helm install --namespace=kube-system --name cert-manager stable/cert-manager --set ingressShim.defaultIssuerName=letsencrypt --set ingressShim.defaultIssuerKind=ClusterIssuer

<span class="hljs-comment"># create cluster issuer</span>
kubectl apply <span class="hljs-_">-f</span> https://raw.githubusercontent.com/feiskyer/kubernetes-handbook/master/manifests/ingress-nginx/cert-manager/cluster-issuer.yaml
</code></pre>
<h2 id="创建-ingress">创建 Ingress</h2>
<p>首先，创建一个 Secret，用于登录认证：</p>
<pre><code class="lang-sh">$ htpasswd -c auth foo
$ kubectl -n kube-system create secret generic basic-auth --from-file=auth
</code></pre>
<h3 id="http-ingress-示例">HTTP Ingress 示例</h3>
<p>为 nginx 服务（端口 80）创建 TLS Ingress，并且自动将 <code>http://echo-tls.example.com</code> 重定向到 <code>https://echo-tls.example.com</code>：</p>
<pre><code class="lang-sh">cat <<EOF | kubectl create <span class="hljs-_">-f</span>-
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: web
  namespace: default
  annotations:
    kubernetes.io/tls-acme: <span class="hljs-string">"true"</span>
    kubernetes.io/ingress.class: <span class="hljs-string">"nginx"</span>
    ingress.kubernetes.io/ssl-redirect: <span class="hljs-string">"true"</span>
    certmanager.k8s.io/cluster-issuer: letsencrypt
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  tls:
  - hosts:
    - <span class="hljs-built_in">echo</span>-tls.example.com
    secretName: web-tls
  rules:
  - host: <span class="hljs-built_in">echo</span>-tls.example.com
    http:
      paths:
      - path: /
        backend:
          serviceName: nginx
          servicePort: 80
EOF
</code></pre>
<h3 id="tls-ingress">TLS Ingress</h3>
<p>为 Kubernetes Dashboard 服务（端口443）创建 TLS Ingress，并且禁止该域名的 HTTP 访问：</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> extensions/v1beta1
<span class="hljs-attr">kind:</span> Ingress
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  annotations:</span>
    kubernetes.io/ingress.class: nginx
    kubernetes.io/tls-acme: <span class="hljs-string">"true"</span>
    kubernetes.io/ingress.allow-http: <span class="hljs-string">"false"</span>
    nginx.ingress.kubernetes.io/auth-realm: Authentication Required
    nginx.ingress.kubernetes.io/auth-secret: basic-auth
    nginx.ingress.kubernetes.io/auth-type: basic
    nginx.ingress.kubernetes.io/secure-backends: <span class="hljs-string">"true"</span>
    certmanager.k8s.io/cluster-issuer: letsencrypt
<span class="hljs-attr">  name:</span> dashboard
<span class="hljs-attr">  namespace:</span> kube-system
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  tls:</span>
<span class="hljs-attr">  - hosts:</span>
<span class="hljs-bullet">    -</span> dashboard.example.com
<span class="hljs-attr">    secretName:</span> dashboard-ingress-tls
<span class="hljs-attr">  rules:</span>
<span class="hljs-attr">  - host:</span> dashboard.example.com
<span class="hljs-attr">    http:</span>
<span class="hljs-attr">      paths:</span>
<span class="hljs-attr">      - path:</span> /
<span class="hljs-attr">        backend:</span>
<span class="hljs-attr">          serviceName:</span> kubernetes-dashboard
<span class="hljs-attr">          servicePort:</span> <span class="hljs-number">443</span>
</code></pre>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://kubernetes.github.io/ingress-nginx/" target="_blank">Nginx Ingress Controller Documentation</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="minikube-ingress" class="level3">minikubeIngress</h1>
<p>虽然 minikube 支持 LoadBalancer 类型的服务，但它并不会创建外部的负载均衡器，而是为这些服务开放一个 NodePort。这在使用 Ingress 时需要注意。</p>
<p>本节展示如何在 minikube 上开启 Ingress Controller 并创建和管理 Ingress 资源。</p>
<h2 id="启动-ingress-controller">启动 Ingress Controller</h2>
<p>minikube 已经内置了 ingress addon，只需要开启一下即可</p>
<pre><code class="lang-sh">$ minikube addons <span class="hljs-built_in">enable</span> ingress
</code></pre>
<p>稍等一会，nginx-ingress-controller 和 default-http-backend 就会起来</p>
<pre><code class="lang-sh">$ kubectl get pods -n kube-system
NAME                             READY     STATUS    RESTARTS   AGE
default-http-backend-5374j       1/1       Running   0          1m
kube-addon-manager-minikube      1/1       Running   0          2m
kube-dns-268032401-rhrx6         3/3       Running   0          1m
kubernetes-dashboard-xh74p       1/1       Running   0          2m
nginx-ingress-controller-78mk6   1/1       Running   0          1m
</code></pre>
<h2 id="创建-ingress">创建 Ingress</h2>
<p>首先启用一个 echo server 服务</p>
<pre><code class="lang-sh">$ kubectl run echoserver --image=gcr.io/google_containers/echoserver:1.4 --port=8080
$ kubectl expose deployment echoserver --type=NodePort
$ minikube service echoserver --url
http://192.168.64.36:31957
</code></pre>
<p>然后创建一个 Ingress，将 <code>http://mini-echo.io</code> 和 <code>http://mini-web.io/echo</code> 转发到刚才创建的 echoserver 服务上</p>
<pre><code class="lang-sh">$ cat <<EOF | kubectl create <span class="hljs-_">-f</span> -
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: <span class="hljs-built_in">echo</span>
  annotations:
    ingress.kubernetes.io/rewrite-target: /
spec:
  backend:
    serviceName: default-http-backend
    servicePort: 80
  rules:
  - host: mini-echo.io
    http:
      paths:
      - path: /
        backend:
          serviceName: echoserver
          servicePort: 8080
  - host: mini-web.io
    http:
      paths:
      - path: /<span class="hljs-built_in">echo</span>
        backend:
          serviceName: echoserver
          servicePort: 8080
EOF
</code></pre>
<p>为了访问 <code>mini-echo.io</code> 和 <code>mini-web.io</code> 这两个域名，手动在 hosts 中增加一个映射</p>
<pre><code class="lang-sh">$ <span class="hljs-built_in">echo</span> <span class="hljs-string">"<span class="hljs-variable">$(minikube ip)</span> mini-echo.io mini-web.io"</span> | sudo tee <span class="hljs-_">-a</span> /etc/hosts
</code></pre>
<p>然后，就可以通过 <code>http://mini-echo.io</code> 和 <code>http://mini-web.io/echo</code> 来访问服务了。</p>
<h2 id="使用-xipio">使用 xip.io</h2>
<p>前面的方法需要每次在使用不同域名时手动配置 hosts，借助 <code>xip.io</code> 可以省掉这个步骤。</p>
<p>跟前面类似，先启动一个 nginx 服务</p>
<pre><code class="lang-sh">$ kubectl run nginx --image=nginx --port=80
$ kubectl expose deployment nginx --type=NodePort
</code></pre>
<p>然后创建 Ingress，与前面不同的是 host 使用 <code>nginx.$(minikube ip).xip.io</code>：</p>
<pre><code class="lang-sh">$ cat <<EOF | kubectl create <span class="hljs-_">-f</span> -
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
 name: my-nginx-ingress
spec:
 rules:
  - host: nginx.$(minikube ip).xip.io
    http:
     paths:
      - path: /
        backend:
         serviceName: nginx
         servicePort: 80
EOF
</code></pre>
<p>然后就可以直接访问该域名了</p>
<pre><code class="lang-sh">$ curl nginx.$(minikube ip).xip.io
</code></pre>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="traefik-ingress" class="level3">TraefikIngress</h1>
<p><a href="https://traefik.io/" target="_blank">Traefik</a> 是一款开源的反向代理与负载均衡工具，它监听后端的变化并自动更新服务配置。Traefik 最大的优点是能够与常见的微服务系统直接整合，可以实现自动化动态配置。目前支持 Docker、Swarm,Marathon、Mesos、Kubernetes、Consul、Etcd、Zookeeper、BoltDB 和 Rest API 等后端模型。</p>
<p><img src="https://docs.traefik.io/img/architecture.png" alt=""/></p>
<p>主要功能包括</p>
<ul>
<li>Golang编写，部署容易</li>
<li>快（nginx的85%)</li>
<li>支持众多的后端（Docker, Swarm, Kubernetes, Marathon, Mesos, Consul, Etcd等）</li>
<li>内置Web UI、Metrics和Let’s Encrypt支持，管理方便</li>
<li>自动动态配置</li>
<li>集群模式高可用</li>
<li>支持 <a href="https://www.haproxy.org/download/1.8/doc/proxy-protocol.txt" target="_blank">Proxy Protocol</a></li>
</ul>
<h2 id="ingress简介">Ingress简介</h2>
<p>简单的说，ingress就是从kubernetes集群外访问集群的入口，将用户的URL请求转发到不同的service上。Ingress相当于nginx、apache等负载均衡反向代理服务器，其中还包括规则定义，即URL的路由信息，路由信息的刷新由 <a href="https://kubernetes.io/docs/concepts/services-networking/ingress/#ingress-controllers" target="_blank">Ingress controller</a> 来提供。</p>
<p>Ingress Controller 实质上可以理解为是个监视器，Ingress Controller 通过不断地跟 kubernetes API 打交道，实时的感知后端 service、pod 等变化，比如新增和减少 pod，service 增加与减少等；当得到这些变化信息后，Ingress Controller 再结合下文的 Ingress 生成配置，然后更新反向代理负载均衡器，并刷新其配置，达到服务发现的作用。</p>
<h2 id="helm-部署-traefik">Helm 部署 Traefik</h2>
<pre><code class="lang-sh"><span class="hljs-comment"># Setup domain, user and password first.</span>
$ <span class="hljs-built_in">export</span> USER=user
$ <span class="hljs-built_in">export</span> DOMAIN=ingress.feisky.xyz
$ htpasswd -c auth <span class="hljs-variable">$USER</span>
New password:
Re-type new password:
Adding password <span class="hljs-keyword">for</span> user user
$ PASSWORD=$(cat auth| awk -F: <span class="hljs-string">'{print $2}'</span>)

<span class="hljs-comment"># Deploy with helm.</span>
helm install stable/traefik --name --namespace kube-system --set rbac.enabled=<span class="hljs-literal">true</span>,acme.enabled=<span class="hljs-literal">true</span>,dashboard.enabled=<span class="hljs-literal">true</span>,acme.staging=<span class="hljs-literal">false</span>,acme.email=admin@<span class="hljs-variable">$DOMAIN</span>,dashboard.domain=ui.<span class="hljs-variable">$DOMAIN</span>,ssl.enabled=<span class="hljs-literal">true</span>,acme.challengeType=http-01,dashboard.auth.basic.<span class="hljs-variable">$USER</span>=<span class="hljs-variable">$PASSWORD</span>
</code></pre>
<p>稍等一会，traefik Pod 就会运行起来：</p>
<pre><code class="lang-sh">$ kubectl -n kube-system get pod <span class="hljs-_">-l</span> app=traefik
NAME                       READY     STATUS    RESTARTS   AGE
traefik-65d8dc4489-k97cg   1/1       Running   0          5m

$ kubectl -n kube-system get ingress
NAME                HOSTS                   ADDRESS   PORTS     AGE
traefik-dashboard   ui.ingress.feisky.xyz             80        25m

$ kubectl -n kube-system get svc traefik
NAME      TYPE           CLUSTER-IP    EXTERNAL-IP     PORT(S)                      AGE
traefik   LoadBalancer   10.0.206.26   172.20.0.115    80:31662/TCP,443:32618/TCP   24m
</code></pre>
<p>通过配置 DNS 解析（CNAME 记录域名到 Ingress Controller 服务的外网IP）、修改 <code>/etc/hosts</code> 添加域名映射（见下述测试部分）或者使用 <code>xip.io</code> （参考 <a href="../minikube-ingress.html">minikube ingress 使用方法</a>）等方法，就可以通过配置的域名直接访问所需服务了。比如上述的 Dashboard 服务可以通过域名 <code>ui.ingress.feisky.xyz</code> 来访问：</p>
<p><img src="images/traefik-dashboard.jpg" alt="kubernetes-dashboard"/></p>
<p>上图中，左侧黄色部分部分列出的是所有的rule，右侧绿色部分是所有的backend。</p>
<h2 id="ingress-示例">Ingress 示例</h2>
<p>下面来看一个更复杂的示例。<strong>创建名为<code>traefik-ingress</code>的 ingress</strong>，文件名traefik.yaml</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> extensions/v1beta1
<span class="hljs-attr">kind:</span> Ingress
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> traefik-ingress
<span class="hljs-attr">  annotations:</span>
    kubernetes.io/ingress.class: traefik
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  rules:</span>
<span class="hljs-attr">  - host:</span> traefik.nginx.io
<span class="hljs-attr">    http:</span>
<span class="hljs-attr">      paths:</span>
<span class="hljs-attr">      - path:</span> /
<span class="hljs-attr">        backend:</span>
<span class="hljs-attr">          serviceName:</span> nginx
<span class="hljs-attr">          servicePort:</span> <span class="hljs-number">80</span>
<span class="hljs-attr">  - host:</span> traefik.frontend.io
<span class="hljs-attr">    http:</span>
<span class="hljs-attr">      paths:</span>
<span class="hljs-attr">      - path:</span> /
<span class="hljs-attr">        backend:</span>
<span class="hljs-attr">          serviceName:</span> frontend
<span class="hljs-attr">          servicePort:</span> <span class="hljs-number">80</span>
</code></pre>
<p>其中，</p>
<ul>
<li><code>backend</code>中要配置 default namespace 中启动的 service 名字</li>
<li><code>path</code>就是URL地址后的路径，如<code>traefik.frontend.io/path</code></li>
<li>host 最好使用 <code>service-name.filed1.filed2.domain-name</code> 这种类似主机名称的命名方式，方便区分服务</li>
</ul>
<p>根据你自己环境中部署的 service 名称和端口自行修改，有新 service 增加时，修改该文件后可以使用<code>kubectl replace -f traefik.yaml</code>来更新。</p>
<h2 id="测试">测试</h2>
<p>在集群的任意一个节点上执行。假如现在我要访问nginx的"/"路径。</p>
<pre><code class="lang-bash">$ curl -H Host:traefik.nginx.io http://172.20.0.115/
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href=<span class="hljs-string">"http://nginx.org/"</span>>nginx.org</a>.<br/>
Commercial support is available at
<a href=<span class="hljs-string">"http://nginx.com/"</span>>nginx.com</a>.</p>

<p><em>Thank you <span class="hljs-keyword">for</span> using nginx.</em></p>
</body>
</html>
</code></pre>
<p>如果你需要在 kubernetes 集群以外访问就需要设置 DNS，或者修改本机的hosts文件在其中加入：</p>
<pre><code class="lang-sh">172.20.0.115 traefik.nginx.io
172.20.0.115 traefik.frontend.io
</code></pre>
<p>所有访问这些地址的流量都会发送给 <code>172.20.0.115</code> 这台主机，就是我们启动traefik的主机。Traefik会解析http请求header里的Host参数将流量转发给Ingress配置里的相应service。</p>
<p><img src="images/traefik-nginx.jpg" alt="traefik-nginx"/></p>
<p><img src="images/traefik-guestbook.jpg" alt="traefik-guestbook"/></p>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="http://www.colabug.com/thread-1703745-1-1.html" target="_blank">Traefik-kubernetes 初试</a></li>
<li><a href="http://www.tuicool.com/articles/ZnuEfay" target="_blank">Traefik简介</a></li>
<li><a href="https://github.com/kubernetes/examples/tree/master/guestbook" target="_blank">Guestbook example</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="keepalived-vip" class="level3">Keepalived-VIP</h1>
<p>Kubernetes 使用 <a href="http://www.keepalived.org" target="_blank">keepalived</a> 来产生虚拟 IP address</p>
<p>我们将探讨如何利用 <a href="http://www.linuxvirtualserver.org/software/ipvs.html" target="_blank">IPVS - The Linux Virtual Server Project</a>" 来 kubernetes 配置 VIP</p>
<h2 id="前言">前言</h2>
<p>kubernetes v1.6 版提供了三种方式去暴露 Service：</p>
<ol>
<li><strong>L4 的 LoadBalacncer</strong> : 只能在 <a href="https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/" target="_blank">cloud providers</a> 上被使用 像是 GCE 或 AWS</li>
<li><strong>NodePort</strong> : <a href="https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport" target="_blank">NodePort</a> 允许在每个节点上开启一个 port 口, 借由这个 port 口会再将请求导向到随机的 pod 上</li>
<li><strong>L7 Ingress</strong> :<a href="https://kubernetes.io/docs/concepts/services-networking/ingress/" target="_blank">Ingress</a> 为一个 LoadBalancer(例: nginx, HAProxy, traefik, vulcand) 会将 HTTP/HTTPS 的各个请求导向到相对应的 service endpoint</li>
</ol>
<p>有了这些方式, 为何我们还需要 <em>keepalived</em> ?</p>
<pre><code>                                                  ___________________
                                                 |                   |
                                           |-----| Host IP: 10.4.0.3 |
                                           |     |___________________|
                                           |
                                           |      ___________________
                                           |     |                   |
Public ----(example.com = 10.4.0.3/4/5)----|-----| Host IP: 10.4.0.4 |
                                           |     |___________________|
                                           |
                                           |      ___________________
                                           |     |                   |
                                           |-----| Host IP: 10.4.0.5 |
                                                 |___________________|
</code></pre><p>我们假设 Ingress 运行在 3 个 kubernetes 节点上, 并对外暴露 <code>10.4.0.x</code> 的 IP 去做 loadbalance</p>
<p>DNS Round Robin (RR) 将对应到 <code>example.com</code> 的请求轮循给这 3 个节点, 如果 <code>10.4.0.3</code> 掛了, 仍有三分之一的流量会导向 <code>10.4.0.3</code>, 这样就会有一段 downtime, 直到 DNS 发现 <code>10.4.0.3</code> 掛了并修正导向</p>
<p>严格来说, 这并没有真正的做到 High Availability (HA)</p>
<p>这边 IPVS 可以帮助我们解决这件事, 这个想法是虚拟 IP(VIP) 对应到每个 service 上, 并将 VIP 暴露到 kubernetes 群集之外</p>
<h3 id="与-service-loadbalancer-或-ingress-nginx-的区别">与 <a href="https://github.com/kubernetes/contrib/tree/master/service-loadbalancer" target="_blank">service-loadbalancer</a> 或 <a href="https://github.com/kubernetes/ingress-nginx" target="_blank">ingress-nginx</a> 的区别</h3>
<p>我们看到以下的图</p>
<pre><code class="lang-sh">                                               ___________________
                                              |                   |
                                              | VIP: 10.4.0.50    |
                                        |-----| Host IP: 10.4.0.3 |
                                        |     | Role: Master      |
                                        |     |___________________|
                                        |
                                        |      ___________________
                                        |     |                   |
                                        |     | VIP: Unassigned   |
Public ----(example.com = 10.4.0.50)----|-----| Host IP: 10.4.0.3 |
                                        |     | Role: Slave       |
                                        |     |___________________|
                                        |
                                        |      ___________________
                                        |     |                   |
                                        |     | VIP: Unassigned   |
                                        |-----| Host IP: 10.4.0.3 |
                                              | Role: Slave       |
                                              |___________________|
</code></pre>
<p>我们可以看到只有一个 node 被选为 Master(透过 VRRP 选择的), 而我们的 VIP 是 <code>10.4.0.50</code>, 如果 <code>10.4.0.3</code> 掛掉了, 那会从剩余的节点中选一个成为 Master 并接手 VIP, 这样我们就可以确保落实真正的 HA</p>
<h2 id="环境需求">环境需求</h2>
<p>只需要确认要运行 keepalived-vip 的 kubernetes 群集 <a href="../concepts/daemonset.html">DaemonSets</a> 功能是正常的就行了</p>
<h3 id="rbac">RBAC</h3>
<p>由于 kubernetes 在 1.6 后引进了 RBAC 的概念, 所以我们要先去设定 rule, 至於有关 RBAC 的详情请至 <a href="rbac.html">说明</a>。</p>
<p>vip-rbac.yaml</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> rbac.authorization.k8s.io/v1beta1
<span class="hljs-attr">kind:</span> ClusterRole
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> kube-keepalived-vip
<span class="hljs-attr">rules:</span>
<span class="hljs-attr">- apiGroups:</span> [<span class="hljs-string">""</span>]
<span class="hljs-attr">  resources:</span>
<span class="hljs-bullet">  -</span> pods
<span class="hljs-bullet">  -</span> nodes
<span class="hljs-bullet">  -</span> endpoints
<span class="hljs-bullet">  -</span> services
<span class="hljs-bullet">  -</span> configmaps
<span class="hljs-attr">  verbs:</span> [<span class="hljs-string">"get"</span>, <span class="hljs-string">"list"</span>, <span class="hljs-string">"watch"</span>]
<span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> ServiceAccount
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> kube-keepalived-vip
<span class="hljs-attr">  namespace:</span> default
<span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> rbac.authorization.k8s.io/v1beta1
<span class="hljs-attr">kind:</span> ClusterRoleBinding
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> kube-keepalived-vip
<span class="hljs-attr">roleRef:</span>
<span class="hljs-attr">  apiGroup:</span> rbac.authorization.k8s.io
<span class="hljs-attr">  kind:</span> ClusterRole
<span class="hljs-attr">  name:</span> kube-keepalived-vip
<span class="hljs-attr">subjects:</span>
<span class="hljs-attr">- kind:</span> ServiceAccount
<span class="hljs-attr">  name:</span> kube-keepalived-vip
<span class="hljs-attr">  namespace:</span> default
</code></pre>
<p>clusterrolebinding.yaml</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> rbac.authorization.k8s.io/v1alpha1
<span class="hljs-attr">kind:</span> ClusterRoleBinding
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> kube-keepalived-vip
<span class="hljs-attr">roleRef:</span>
<span class="hljs-attr">  apiGroup:</span> rbac.authorization.k8s.io
<span class="hljs-attr">  kind:</span> ClusterRole
<span class="hljs-attr">  name:</span> kube-keepalived-vip
<span class="hljs-attr">subjects:</span>
<span class="hljs-attr">  - kind:</span> ServiceAccount
<span class="hljs-attr">    name:</span> kube-keepalived-vip
<span class="hljs-attr">    namespace:</span> default
</code></pre>
<pre><code class="lang-sh">$ kubectl create <span class="hljs-_">-f</span> vip-rbac.yaml
$ kubectl create <span class="hljs-_">-f</span> clusterrolebinding.yaml
</code></pre>
<h2 id="示例">示例</h2>
<p>先建立一个简单的 service</p>
<p>nginx-deployment.yaml</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> apps/v1beta1
<span class="hljs-attr">kind:</span> Deployment
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> nginx-deployment
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  replicas:</span> <span class="hljs-number">3</span>
<span class="hljs-attr">  template:</span>
<span class="hljs-attr">    metadata:</span>
<span class="hljs-attr">      labels:</span>
<span class="hljs-attr">        app:</span> nginx
<span class="hljs-attr">    spec:</span>
<span class="hljs-attr">      containers:</span>
<span class="hljs-attr">      - name:</span> nginx
<span class="hljs-attr">        image:</span> nginx:<span class="hljs-number">1.7</span><span class="hljs-number">.9</span>
<span class="hljs-attr">        ports:</span>
<span class="hljs-attr">        - containerPort:</span> <span class="hljs-number">80</span>
<span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Service
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> nginx
<span class="hljs-attr">  labels:</span>
<span class="hljs-attr">    app:</span> nginx
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  type:</span> NodePort
<span class="hljs-attr">  ports:</span>
<span class="hljs-attr">  - port:</span> <span class="hljs-number">80</span>
<span class="hljs-attr">    nodePort:</span> <span class="hljs-number">30302</span>
<span class="hljs-attr">    targetPort:</span> <span class="hljs-number">80</span>
<span class="hljs-attr">    protocol:</span> TCP
<span class="hljs-attr">    name:</span> http
<span class="hljs-attr">  selector:</span>
<span class="hljs-attr">    app:</span> nginx
</code></pre>
<p>主要功能就是 pod 去监听听 80 port, 再开启 service NodePort 监听 30320</p>
<pre><code class="lang-sh">$ kubecrl create <span class="hljs-_">-f</span> nginx-deployment.yaml
</code></pre>
<p>接下来我们要做的是 config map</p>
<pre><code class="lang-sh">$ <span class="hljs-built_in">echo</span> <span class="hljs-string">"apiVersion: v1
kind: ConfigMap
metadata:
  name: vip-configmap
data:
  10.87.2.50: default/nginx"</span> | kubectl create <span class="hljs-_">-f</span> -
</code></pre>
<p>注意, 这边的 <code>10.87.2.50</code> 必须换成你自己同网段下无使用的 IP e.g. 10.87.2.X
后面 <code>nginx</code> 为 service 的 name, 这边可以自行更换</p>
<p>接着确认一下</p>
<pre><code class="lang-sh"><span class="hljs-variable">$kubectl</span> get configmap
NAME            DATA      AGE
vip-configmap   1         23h
</code></pre>
<p>再来就是设置 keepalived-vip</p>
<pre><code class="lang-yaml">
<span class="hljs-attr">apiVersion:</span> extensions/v1beta1
<span class="hljs-attr">kind:</span> DaemonSet
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> kube-keepalived-vip
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  template:</span>
<span class="hljs-attr">    metadata:</span>
<span class="hljs-attr">      labels:</span>
<span class="hljs-attr">        name:</span> kube-keepalived-vip
<span class="hljs-attr">    spec:</span>
<span class="hljs-attr">      hostNetwork:</span> <span class="hljs-literal">true</span>
<span class="hljs-attr">      containers:</span>
<span class="hljs-attr">        - image:</span> gcr.io/google_containers/kube-keepalived-vip:<span class="hljs-number">0.9</span>
<span class="hljs-attr">          name:</span> kube-keepalived-vip
<span class="hljs-attr">          imagePullPolicy:</span> Always
<span class="hljs-attr">          securityContext:</span>
<span class="hljs-attr">            privileged:</span> <span class="hljs-literal">true</span>
<span class="hljs-attr">          volumeMounts:</span>
<span class="hljs-attr">            - mountPath:</span> /lib/modules
<span class="hljs-attr">              name:</span> modules
<span class="hljs-attr">              readOnly:</span> <span class="hljs-literal">true</span>
<span class="hljs-attr">            - mountPath:</span> /dev
<span class="hljs-attr">              name:</span> dev
          <span class="hljs-comment"># use downward API</span>
<span class="hljs-attr">          env:</span>
<span class="hljs-attr">            - name:</span> POD_NAME
<span class="hljs-attr">              valueFrom:</span>
<span class="hljs-attr">                fieldRef:</span>
<span class="hljs-attr">                  fieldPath:</span> metadata.name
<span class="hljs-attr">            - name:</span> POD_NAMESPACE
<span class="hljs-attr">              valueFrom:</span>
<span class="hljs-attr">                fieldRef:</span>
<span class="hljs-attr">                  fieldPath:</span> metadata.namespace
          <span class="hljs-comment"># to use unicast</span>
<span class="hljs-attr">          args:</span>
<span class="hljs-bullet">          -</span> --services-configmap=default/vip-configmap
          <span class="hljs-comment"># unicast uses the ip of the nodes instead of multicast</span>
          <span class="hljs-comment"># this is useful if running in cloud providers (like AWS)</span>
          <span class="hljs-comment">#- --use-unicast=true</span>
<span class="hljs-attr">      volumes:</span>
<span class="hljs-attr">        - name:</span> modules
<span class="hljs-attr">          hostPath:</span>
<span class="hljs-attr">            path:</span> /lib/modules
<span class="hljs-attr">        - name:</span> dev
<span class="hljs-attr">          hostPath:</span>
<span class="hljs-attr">            path:</span> /dev
</code></pre>
<p>建立 daemonset</p>
<pre><code class="lang-sh">$ kubectl get daemonset kube-keepalived-vip
NAME                  DESIRED   CURRENT   READY     UP-TO-DATE   AVAILABLE   NODE-SELECTOR   AGE
kube-keepalived-vip   5         5         5         5            5
</code></pre>
<p>检查一下配置状态</p>
<pre><code class="lang-sh">kubectl get pod -o wide |grep keepalive
kube-keepalived-vip-c4sxw         1/1       Running            0          23h       10.87.2.6    10.87.2.6
kube-keepalived-vip-c9p7n         1/1       Running            0          23h       10.87.2.8    10.87.2.8
kube-keepalived-vip-psdp9         1/1       Running            0          23h       10.87.2.10   10.87.2.10
kube-keepalived-vip-xfmxg         1/1       Running            0          23h       10.87.2.12   10.87.2.12
kube-keepalived-vip-zjts7         1/1       Running            3          23h       10.87.2.4    10.87.2.4
</code></pre>
<p>可以随机挑一个 pod, 去看里面的配置</p>
<pre><code class="lang-sh"> $ kubectl <span class="hljs-built_in">exec</span> kube-keepalived-vip-c4sxw cat /etc/keepalived/keepalived.conf


global_defs {
  vrrp_version 3
  vrrp_iptables KUBE-KEEPALIVED-VIP
}

vrrp_instance vips {
  state BACKUP
  interface eno1
  virtual_router_id 50
  priority 103
  nopreempt
  advert_int 1

  track_interface {
    eno1
  }



  virtual_ipaddress {
    10.87.2.50
  }
}


<span class="hljs-comment"># Service: default/nginx</span>
virtual_server 10.87.2.50 80 { // 此为 service 开的口
  delay_loop 5
  lvs_<span class="hljs-built_in">sched</span> wlc
  lvs_method NAT
  persistence_timeout 1800
  protocol TCP


  real_server 10.2.49.30 8080 { // 这里说明 pod 的真实状况
    weight 1
    TCP_CHECK {
      connect_port 80
      connect_timeout 3
    }
  }

}
</code></pre>
<p>最后我们去测试这功能</p>
<pre><code class="lang-sh">$ curl  10.87.2.50
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href=<span class="hljs-string">"http://nginx.org/"</span>>nginx.org</a>.<br/>
Commercial support is available at
<a href=<span class="hljs-string">"http://nginx.com/"</span>>nginx.com</a>.</p>

<p><em>Thank you <span class="hljs-keyword">for</span> using nginx.</em></p>
</body>
</html>
</code></pre>
<p>10.87.2.50:80(我们假设的 VIP, 实际上其实没有 node 是用这 IP) 即可帮我们导向这个 service</p>
<p>以上的程式代码都在 <a href="https://github.com/kubernetes/contrib/tree/master/keepalived-vip" target="_blank">github</a> 上可以找到。</p>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://github.com/kweisamx/kubernetes-keepalived-vip" target="_blank">kweisamx/kubernetes-keepalived-vip</a></li>
<li><a href="https://github.com/kubernetes/contrib/tree/master/keepalived-vip" target="_blank">kubernetes/keepalived-vip</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="cloud-provider-扩展" class="level2">CloudProvider扩展</h1>
<p>当 Kubernetes 集群运行在云平台内部时，Cloud Provider 使得 Kubernetes 可以直接利用云平台实现持久化卷、负载均衡、网络路由、DNS 解析以及横向扩展等功能。</p>
<h2 id="常见-cloud-provider">常见 Cloud Provider</h2>
<p>Kubenretes 内置的 Cloud Provider 包括</p>
<ul>
<li>GCE</li>
<li>AWS</li>
<li>Azure</li>
<li>Mesos</li>
<li>OpenStack</li>
<li>CloudStack</li>
<li>Ovirt</li>
<li>Photon</li>
<li>Rackspace</li>
<li>Vsphere</li>
</ul>
<h2 id="当前-cloud-provider-工作原理">当前 Cloud Provider 工作原理</h2>
<ul>
<li>apiserver，kubelet，controller-manager 都配置 cloud provider 选项</li>
<li>Kubelet<ul>
<li>通过 Cloud Provider 接口查询 nodename</li>
<li>向 API Server 注册 Node 时查询 InstanceID、ProviderID、ExternalID 和 Zone 等信息</li>
<li>定期查询 Node 是否新增了 IP 地址</li>
<li>设置无法调度的条件（condition），直到云服务商的路由配置完成</li>
</ul>
</li>
<li>kube-apiserver<ul>
<li>向所有 Node 分发 SSH 密钥以便建立 SSH 隧道</li>
<li>PersistentVolumeLabel 负责 PV 标签</li>
<li>PersistentVolumeClainResize 动态扩展 PV 的大小</li>
</ul>
</li>
<li>kube-controller-manager<ul>
<li>Node 控制器检查 Node 所在 VM 的状态。当 VM 删除后自动从 API Server 中删除该 Node。</li>
<li>Volume 控制器向云提供商创建和删除持久化存储卷，并按需要挂载或卸载到指定的 VM 上。</li>
<li>Route 控制器给所有已注册的 Nodes 配置云路由。</li>
<li>Service 控制器给 LoadBalancer 类型的服务创建负载均衡器并更新服务的外网 IP。</li>
</ul>
</li>
</ul>
<h2 id="独立-cloud-provider-工作-原理-以及跟踪进度">独立 Cloud Provider 工作 <a href="https://kubernetes.io/docs/tasks/administer-cluster/running-cloud-controller/" target="_blank">原理</a> 以及<a href="https://github.com/kubernetes/features/issues/88" target="_blank">跟踪进度</a></h2>
<ul>
<li>Kubelet 必须配置 <code>--cloud-provider=external`，并且 `kube-apiserver` 和 `kube-controller-manager` 必须不配置 cloud provider。</code></li>
<li><code>kube-apiserver</code> 的准入控制选项不能包含 PersistentVolumeLabel。</li>
<li><code>cloud-controller-manager</code> 独立运行，并开启 <code>InitializerConifguration</code>。</li>
<li>Kubelet 可以通过 <code>provider-id</code> 选项配置 <code>ExternalID</code>，启动后会自动给 Node 添加 taint <code>node.cloudprovider.kubernetes.io/uninitialized=NoSchedule</code>。</li>
<li><code>cloud-controller-manager</code> 在收到 Node 注册的事件后再次初始化 Node 配置，添加 zone、类型等信息，并删除上一步 Kubelet 自动创建的 taint。</li>
<li>主要逻辑（也就是合并了 kube-apiserver 和 kube-controller-manager 跟云相关的逻辑）<ul>
<li>Node 控制器检查 Node 所在 VM 的状态。当 VM 删除后自动从 API Server 中删除该 Node。</li>
<li>Volume 控制器向云提供商创建和删除持久化存储卷，并按需要挂载或卸载到指定的 VM 上。</li>
<li>Route 控制器给所有已注册的 Nodes 配置云路由。</li>
<li>Service 控制器给 LoadBalancer 类型的服务创建负载均衡器并更新服务的外网 IP。</li>
<li>PersistentVolumeLable 准入控制负责 PV 标签</li>
<li>PersistentVolumeClainResize 准入控制动态扩展 PV 大小</li>
</ul>
</li>
</ul>
<h2 id="如何开发-cloud-provider-扩展">如何开发 Cloud Provider 扩展</h2>
<p>Kubernetes 的 Cloud Provider 目前正在重构中</p>
<ul>
<li>v1.6 添加了独立的 <code>cloud-controller-manager</code> 服务，云提供商可以构建自己的 <code>cloud-controller-manager</code> 而无须修改 Kubernetes 核心代码</li>
<li>v1.7-v1.10 进一步重构 <code>cloud-controller-manager</code>，解耦了 Controller Manager 与 Cloud Controller 的代码逻辑</li>
<li>v1.11 External Cloud Provider 升级为 Beta 版</li>
</ul>
<p>构建一个新的云提供商的 Cloud Provider 步骤为</p>
<ul>
<li>编写实现 <a href="https://github.com/kubernetes/kubernetes/blob/master/pkg/cloudprovider/cloud.go" target="_blank">cloudprovider.Interface</a> 的 cloudprovider 代码</li>
<li>将该 cloudprovider 链接到 <code>cloud-controller-manager</code><ul>
<li>在 <code>cloud-controller-manager</code> 中导入新的 cloudprovider：<code>import "pkg/new-cloud-provider"</code></li>
<li>初始化时传入新 cloudprovider 的名字，如 <code>cloudprovider.InitCloudProvider("rancher", s.CloudConfigFile)</code></li>
</ul>
</li>
<li>配置 kube-controller-manager <code>--cloud-provider=external</code></li>
<li>启动 <code>cloud-controller-manager</code></li>
</ul>
<p>具体实现方法可以参考 <a href="https://github.com/rancher/rancher-cloud-controller-manager" target="_blank">rancher-cloud-controller-manager</a> 和 <a href="https://github.com/kubernetes/kubernetes/blob/master/cmd/cloud-controller-manager/controller-manager.go" target="_blank">cloud-controller-manager</a>。</p>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="device-插件" class="level2">Device插件</h1>
<p>Kubernetes v1.8 开始增加了 Alpha 版的 Device 插件，用来支持 GPU、FPGA、高性能 NIC、InfiniBand 等各种设备。这样，设备厂商只需要根据 Device Plugin 的接口实现一个特定设备的插件，而不需要修改 Kubernetes 核心代码。</p>
<blockquote>
<p>在 v1.10 中该特性升级为 Beta 版本。</p>
</blockquote>
<h2 id="device-插件原理">Device 插件原理</h2>
<p>使用 Device 插件之前，首先要开启 DevicePlugins 功能，即配置 <code>--feature-gates=DevicePlugins=true</code>（默认是关闭的）。</p>
<p>Device 插件实际上是一个 <a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/resource-management/device-plugin.md" target="_blank">gPRC 接口</a>，需要实现 <code>ListAndWatch()</code> 和 <code>Allocate()</code> 等方法，并监听 gRPC Server 的 Unix Socket 在 <code>/var/lib/kubelet/device-plugins/</code> 目录中，如 <code>/var/lib/kubelet/device-plugins/nvidiaGPU.sock</code>。在实现 Device 插件时需要注意</p>
<ul>
<li>插件启动时，需要通过 <code>/var/lib/kubelet/device-plugins/kubelet.sock</code> 向 Kubelet 注册，同时提供插件的 Unix Socket 名称、API 的版本号和插件名称（格式为 <code>vendor-domain/resource</code>，如 <code>nvidia.com/gpu</code>）。Kubelet 会将这些设备暴露到 Node 状态中，方便后续调度器使用</li>
<li>插件启动后向 Kubelet 发送插件列表、按需分配设备并持续监控设备的实时状态</li>
<li>插件启动后要持续监控 Kubelet 的状态，并在 Kubelet 重启后重新注册自己。比如，Kubelet 刚启动后会清空 <code>/var/lib/kubelet/device-plugins/</code> 目录，所以插件作者可以监控自己监听的 unix socket 是否被删除了，并根据此事件重新注册自己</li>
</ul>
<p><img src="images/device-plugin-overview.png" alt=""/></p>
<p>Device 插件一般推荐使用 DaemonSet 的方式部署，并将 <code>/var/lib/kubelet/device-plugins</code> 以 Volume 的形式挂载到容器中。当然，也可以手动运行的方式来部署，但这样就没有失败自动恢复的功能了。</p>
<h2 id="nvidia-gpu-插件">NVIDIA GPU 插件</h2>
<p>NVIDIA 提供了一个基于 Device Plugins 接口的 GPU 设备插件 <a href="https://github.com/NVIDIA/k8s-device-plugin" target="_blank">NVIDIA/k8s-device-plugin</a>。</p>
<p>编译</p>
<pre><code class="lang-sh">git <span class="hljs-built_in">clone</span> https://github.com/NVIDIA/k8s-device-plugin
<span class="hljs-built_in">cd</span> k8s-device-plugin
docker build -t nvidia-device-plugin:1.0.0 .
</code></pre>
<p>部署</p>
<pre><code class="lang-sh">kubectl apply <span class="hljs-_">-f</span> https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/master/nvidia-device-plugin.yml
</code></pre>
<p>创建 Pod 时请求 GPU 资源</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> pod1
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  restartPolicy:</span> OnFailure
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">  - image:</span> nvidia/cuda
<span class="hljs-attr">    name:</span> pod1-ctr
<span class="hljs-attr">    command:</span> [<span class="hljs-string">"sleep"</span>]
<span class="hljs-attr">    args:</span> [<span class="hljs-string">"100000"</span>]

<span class="hljs-attr">    resources:</span>
<span class="hljs-attr">      limits:</span>
        nvidia.com/gpu: <span class="hljs-number">1</span>
</code></pre>
<p>注意：<strong>使用该插件时需要配置 <a href="https://github.com/NVIDIA/nvidia-docker/" target="_blank">nvidia-docker 2.0</a>，并配置 <code>nvidia</code> 为默认运行时 （即配置 docker daemon 的选项 <code>--default-runtime=nvidia</code>）</strong>。nvidia-docker 2.0 的安装方法为（以 Ubuntu Xenial 为例，其他系统的安装方法可以参考 <a href="http://nvidia.github.io/nvidia-docker/" target="_blank">这里</a>）：</p>
<pre><code class="lang-sh"><span class="hljs-comment"># Configure repository</span>
curl -L https://nvidia.github.io/nvidia-docker/gpgkey | \
sudo apt-key add -
sudo tee /etc/apt/sources.list.d/nvidia-docker.list <<< \
<span class="hljs-string">"deb https://nvidia.github.io/libnvidia-container/ubuntu16.04/amd64 /
deb https://nvidia.github.io/nvidia-container-runtime/ubuntu16.04/amd64 /
deb https://nvidia.github.io/nvidia-docker/ubuntu16.04/amd64 /"</span>
sudo apt-get update

<span class="hljs-comment"># Install nvidia-docker 2.0</span>
sudo apt-get install nvidia-docker2
sudo pkill -SIGHUP dockerd

<span class="hljs-comment"># Check installation</span>
docker run --runtime=nvidia --rm nvidia/cuda nvidia-smi
</code></pre>
<h2 id="gcp-gpu-插件">GCP GPU 插件</h2>
<p>GCP 也提供了一个 GPU 设备的插件，仅适用于 Google Container Engine，可以访问 <a href="https://github.com/GoogleCloudPlatform/container-engine-accelerators" target="_blank">GoogleCloudPlatform/container-engine-accelerators</a> 查看。</p>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/resource-management/device-plugin.md" target="_blank">Device Manager Proposal</a></li>
<li><a href="https://kubernetes.io/docs/concepts/cluster-administration/device-plugins/" target="_blank">Device Plugins</a></li>
<li><a href="https://github.com/NVIDIA/k8s-device-plugin" target="_blank">NVIDIA device plugin for Kubernetes</a></li>
<li><a href="https://github.com/NVIDIA/nvidia-docker/tree/2.0" target="_blank">NVIDIA/nvidia-docker 2.0</a></li>
</ul>
</section>
                            
    <h1 class='level1'>服务治理</h1><section class="normal markdown-section">
                                
                                <h1 id="kubernetes-服务治理" class="level2">服务治理</h1>
<p>本章介绍 Kubernetes 服务治理，包括容器应用管理、Service Mesh 以及 Operator 等。</p>
<p>目前最常用的是手动管理 Manifests，比如 kubernetes github 代码库就提供了很多的 manifest 示例</p>
<ul>
<li><a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons" target="_blank">https://github.com/kubernetes/kubernetes/tree/master/cluster/addons</a></li>
<li><a href="https://github.com/kubernetes/examples" target="_blank">https://github.com/kubernetes/examples</a></li>
<li><a href="https://github.com/kubernetes/contrib" target="_blank">https://github.com/kubernetes/contrib</a></li>
<li><a href="https://github.com/kubernetes/ingress-nginx" target="_blank">https://github.com/kubernetes/ingress-nginx</a></li>
</ul>
<p>手动管理的一个问题就是繁琐，特别是应用复杂并且 Manifest 比较多的时候，还需要考虑他们之间部署关系。Kubernetes 开源社区正在推动更易用的管理方法，如</p>
<ul>
<li><a href="patterns.html">一般准则</a></li>
<li><a href="service-rolling-update.html">滚动升级</a></li>
<li><a href="helm.html">Helm</a></li>
<li><a href="operator.html">Operator</a></li>
<li><a href="service-mesh.html">Service Mesh</a></li>
<li><a href="linkerd.html">Linkerd</a></li>
<li><a href="conduit.md">Conduit</a></li>
<li><a href="istio.html">Istio</a><ul>
<li><a href="istio-deploy.html">安装</a></li>
<li><a href="istio-traffic-management.html">流量管理</a></li>
<li><a href="istio-security.html">安全管理</a></li>
<li><a href="istio-policy.html">策略管理</a></li>
<li><a href="istio-metrics.html">Metrics</a></li>
<li><a href="istio-troubleshoot.html">排错</a></li>
<li><a href="istio-community.html">社区</a></li>
</ul>
</li>
<li><a href="devops.html">Devops</a><ul>
<li><a href="draft.html">Draft</a></li>
<li><a href="jenkinsx.html">Jenkins X</a></li>
<li><a href="spinnaker.html">Spinnaker</a></li>
<li><a href="kompose.html">Kompose</a></li>
<li><a href="skaffold.html">Skaffold</a></li>
<li><a href="argo.html">Argo</a></li>
<li><a href="flux.html">Flux GitOps</a></li>
</ul>
</li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="一般准则" class="level3">一般准则</h1>
<ul>
<li>分离构建和运行环境</li>
<li>使用<code>dumb-int</code>等避免僵尸进程</li>
<li>不推荐直接使用Pod，而是推荐使用Deployment/DaemonSet等</li>
<li>不推荐在容器中使用后台进程，而是推荐将进程前台运行，并使用探针保证服务确实在运行中</li>
<li>推荐容器中应用日志打到stdout和stderr，方便日志插件的处理</li>
<li>由于容器采用了COW，大量数据写入有可能会有性能问题，推荐将数据写入到Volume中</li>
<li>不推荐生产环境镜像使用<code>latest</code>标签，但开发环境推荐使用并设置<code>imagePullPolicy</code>为<code>Always</code></li>
<li>推荐使用Readiness探针检测服务是否真正运行起来了</li>
<li>使用<code>activeDeadlineSeconds</code>避免快速失败的Job无限重启</li>
<li>引入Sidecar处理代理、请求速率控制和连接控制等问题</li>
</ul>
<h2 id="分离构建和运行环境">分离构建和运行环境</h2>
<p>注意分离构建和运行环境，直接通过Dockerfile构建的镜像不仅体积大，包含了很多运行时不必要的包，并且还容易引入安全隐患，如包含了应用的源代码。</p>
<p>可以使用<a href="https://docs.docker.com/engine/userguide/eng-image/multistage-build/" target="_blank">Docker多阶段构建</a>来简化这个步骤。</p>
<pre><code class="lang-Dockerfile"><span class="hljs-keyword">FROM</span> golang:<span class="hljs-number">1.7</span>.<span class="hljs-number">3</span> as builder
<span class="hljs-keyword">WORKDIR</span> <span class="bash">/go/src/github.com/alexellis/href-counter/
</span><span class="hljs-keyword">RUN</span> <span class="bash">go get <span class="hljs-_">-d</span> -v golang.org/x/net/html
</span><span class="hljs-keyword">COPY</span> <span class="bash">app.go    .
</span><span class="hljs-keyword">RUN</span> <span class="bash">CGO_ENABLED=0 GOOS=linux go build <span class="hljs-_">-a</span> -installsuffix cgo -o app .
</span>
<span class="hljs-keyword">FROM</span> alpine:latest
<span class="hljs-keyword">RUN</span> <span class="bash">apk --no-cache add ca-certificates
</span><span class="hljs-keyword">WORKDIR</span> <span class="bash">/root/
</span><span class="hljs-keyword">COPY</span> <span class="bash">--from=builder /go/src/github.com/alexellis/href-counter/app .
</span><span class="hljs-keyword">CMD</span> <span class="bash">[<span class="hljs-string">"./app"</span>]
</span></code></pre>
<h2 id="僵尸进程和孤儿进程">僵尸进程和孤儿进程</h2>
<ul>
<li>孤儿进程：一个父进程退出，而它的一个或多个子进程还在运行，那么那些子进程将成为孤儿进程。孤儿进程将被init进程(进程号为1)所收养，并由init进程对它们完成状态收集工作。</li>
<li>僵尸进程：一个进程使用fork创建子进程，如果子进程退出，而父进程并没有调用wait或waitpid获取子进程的状态信息，那么子进程的进程描述符仍然保存在系统中。</li>
</ul>
<p>在容器中，很容易掉进的一个陷阱就是init进程没有正确处理SIGTERM等退出信号。这种情景很容易构造出来，比如</p>
<pre><code class="lang-sh"><span class="hljs-comment"># 首先运行一个容器</span>
$ docker run busybox sleep 10000

<span class="hljs-comment"># 打开另外一个terminal</span>
$ ps uax | grep sleep
sasha    14171  0.0  0.0 139736 17744 pts/18   Sl+  13:25   0:00 docker run busybox sleep 10000
root     14221  0.1  0.0   1188     4 ?        Ss   13:25   0:00 sleep 10000

<span class="hljs-comment"># 接着kill掉第一个进程</span>
$ <span class="hljs-built_in">kill</span> 14171
<span class="hljs-comment"># 现在会发现sleep进程并没有退出</span>
$ ps uax | grep sleep
root     14221  0.0  0.0   1188     4 ?        Ss   13:25   0:00 sleep 10000
</code></pre>
<p>解决方法就是保证容器的init进程可以正确处理SIGTERM等退出信号，比如使用dumb-init</p>
<pre><code class="lang-sh">$ docker run quay.io/gravitational/debian-tall /usr/bin/dumb-init /bin/sh -c <span class="hljs-string">"sleep 10000"</span>
</code></pre>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://github.com/gravitational/workshop/blob/master/k8sprod.md" target="_blank">Kubernetes Production Patterns</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="服务滚动升级" class="level3">滚动升级</h1>
<p>当有镜像发布新版本，新版本服务上线时如何实现服务的滚动和平滑升级？</p>
<p>如果你使用 <strong>ReplicationController</strong> 创建的 pod 可以使用 <code>kubectl rollingupdate</code> 命令滚动升级，如果使用的是 <strong>Deployment</strong> 创建的 Pod 可以直接修改 yaml 文件后执行 <code>kubectl apply</code> 即可。</p>
<p>Deployment 已经内置了 RollingUpdate strategy，因此不用再调用 <code>kubectl rollingupdate</code> 命令，升级的过程是先创建新版的 pod 将流量导入到新 pod 上后销毁原来的旧的 pod。</p>
<p>Rolling Update 适用于 <code>Deployment</code>、<code>Replication Controller</code>，官方推荐使用 Deployment 而不再使用 Replication Controller。</p>
<p>使用 ReplicationController 时的滚动升级请参考官网说明：<a href="https://kubernetes.io/docs/tasks/run-application/rolling-update-replication-controller/" target="_blank">https://kubernetes.io/docs/tasks/run-application/rolling-update-replication-controller/</a></p>
<h2 id="replicationcontroller-与-deployment-的关系">ReplicationController 与 Deployment 的关系</h2>
<p>ReplicationController 和 Deployment 的 RollingUpdate 命令有些不同，但是实现的机制是一样的，关于这两个 kind 的关系我引用了 <a href="https://segmentfault.com/a/1190000008232770" target="_blank">ReplicationController 与 Deployment 的区别</a> 中的部分内容如下，详细区别请查看原文。</p>
<h3 id="replicationcontroller">ReplicationController</h3>
<p>Replication Controller 为 Kubernetes 的一个核心内容，应用托管到 Kubernetes 之后，需要保证应用能够持续的运行，Replication Controller 就是这个保证的 key，主要的功能如下：</p>
<ul>
<li>确保 pod 数量：它会确保 Kubernetes 中有指定数量的 Pod 在运行。如果少于指定数量的 pod，Replication Controller 会创建新的，反之则会删除掉多余的以保证 Pod 数量不变。</li>
<li>确保 pod 健康：当 pod 不健康，运行出错或者无法提供服务时，Replication Controller 也会杀死不健康的 pod，重新创建新的。</li>
<li>弹性伸缩 ：在业务高峰或者低峰期的时候，可以通过 Replication Controller 动态的调整 pod 的数量来提高资源的利用率。同时，配置相应的监控功能（Hroizontal Pod Autoscaler），会定时自动从监控平台获取 Replication Controller 关联 pod 的整体资源使用情况，做到自动伸缩。</li>
<li>滚动升级：滚动升级为一种平滑的升级方式，通过逐步替换的策略，保证整体系统的稳定，在初始化升级的时候就可以及时发现和解决问题，避免问题不断扩大。</li>
</ul>
<h3 id="deployment">Deployment</h3>
<p>Deployment 同样为 Kubernetes 的一个核心内容，主要职责同样是为了保证 pod 的数量和健康，90% 的功能与 Replication Controller 完全一样，可以看做新一代的 Replication Controller。但是，它又具备了 Replication Controller 之外的新特性：</p>
<ul>
<li>Replication Controller 全部功能：Deployment 继承了上面描述的 Replication Controller 全部功能。</li>
<li>事件和状态查看：可以查看 Deployment 的升级详细进度和状态。</li>
<li>回滚：当升级 pod 镜像或者相关参数的时候发现问题，可以使用回滚操作回滚到上一个稳定的版本或者指定的版本。</li>
<li>版本记录: 每一次对 Deployment 的操作，都能保存下来，给予后续可能的回滚使用。</li>
<li>暂停和启动：对于每一次升级，都能够随时暂停和启动。</li>
<li>多种升级方案：Recreate：删除所有已存在的 pod, 重新创建新的; RollingUpdate：滚动升级，逐步替换的策略，同时滚动升级时，支持更多的附加参数，例如设置最大不可用 pod 数量，最小升级间隔时间等等。</li>
</ul>
<h2 id="创建测试镜像">创建测试镜像</h2>
<p>我们来创建一个特别简单的 web 服务，当你访问网页时，将输出一句版本信息。通过区分这句版本信息输出我们就可以断定升级是否完成。</p>
<p>所有配置和代码见 <a href="https://github.com/feiskyer/kubernetes-handbook/tree/master/manifests/test/rolling-update-test" target="_blank">manifests/test/rolling-update-test</a> 目录。</p>
<p><strong>Web 服务的代码 main.go</strong></p>
<pre><code class="lang-go"><span class="hljs-keyword">package</span> main

<span class="hljs-keyword">import</span> (
  <span class="hljs-string">"fmt"</span>
  <span class="hljs-string">"log"</span>
  <span class="hljs-string">"net/http"</span>
)

<span class="hljs-keyword">func</span> sayhello(w http.ResponseWriter, r *http.Request) {
  fmt.Fprintf(w, <span class="hljs-string">"This is version 1."</span>) <span class="hljs-comment">// 这个写入到 w 的是输出到客户端的</span>
}

<span class="hljs-keyword">func</span> main() {
  http.HandleFunc(<span class="hljs-string">"/"</span>, sayhello) <span class="hljs-comment">// 设置访问的路由</span>
  log.Println(<span class="hljs-string">"This is version 1."</span>)
  err := http.ListenAndServe(<span class="hljs-string">":9090"</span>, <span class="hljs-literal">nil</span>) <span class="hljs-comment">// 设置监听的端口</span>
  <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> {
    log.Fatal(<span class="hljs-string">"ListenAndServe:"</span>, err)
  }
}
</code></pre>
<p><strong>创建 Dockerfile</strong></p>
<pre><code class="lang-Dockerfile"><span class="hljs-keyword">FROM</span> alpine:<span class="hljs-number">3.5</span>
ADD hellov2 /
<span class="hljs-keyword">ENTRYPOINT</span> <span class="bash">[<span class="hljs-string">"/hellov2"</span>]
</span></code></pre>
<p>注意修改添加的文件的名称。</p>
<p><strong> 创建 Makefile</strong></p>
<p>修改镜像仓库的地址为你自己的私有镜像仓库地址。</p>
<p>修改 <code>Makefile</code> 中的 <code>TAG</code> 为新的版本号。</p>
<pre><code class="lang-cmake">all: build push clean
.PHONY: build push clean

TAG = v1

<span class="hljs-comment"># Build for linux amd64</span>
build:
  GOOS=linux GOARCH=amd64 go build -o hello<span class="hljs-variable">${TAG}</span> main.go
  docker build -t sz-pg-oam-docker-hub-<span class="hljs-number">001</span>.tendcloud.com/library/hello:<span class="hljs-variable">${TAG}</span> .

<span class="hljs-comment"># Push to tenxcloud</span>
push:
  docker push sz-pg-oam-docker-hub-<span class="hljs-number">001</span>.tendcloud.com/library/hello:<span class="hljs-variable">${TAG}</span>

<span class="hljs-comment"># Clean</span>
clean:
  rm -f hello<span class="hljs-variable">${TAG}</span>
</code></pre>
<p><strong> 编译 </strong></p>
<pre><code class="lang-Shell">make all
</code></pre>
<p>分别修改 main.go 中的输出语句、Dockerfile 中的文件名称和 Makefile 中的 TAG，创建两个版本的镜像。</p>
<h2 id="测试">测试</h2>
<p>我们使用 Deployment 部署服务来测试。</p>
<p>配置文件 <code>rolling-update-test.yaml</code>：</p>
<pre><code class="lang-Yaml"><span class="hljs-attr">apiVersion:</span> extensions/v1beta1
<span class="hljs-attr">kind:</span> Deployment
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">    name:</span> rolling-update-test
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  replicas:</span> <span class="hljs-number">3</span>
<span class="hljs-attr">  template:</span>
<span class="hljs-attr">    metadata:</span>
<span class="hljs-attr">      labels:</span>
<span class="hljs-attr">        app:</span> rolling-update-test
<span class="hljs-attr">    spec:</span>
<span class="hljs-attr">      containers:</span>
<span class="hljs-attr">      - name:</span> rolling-update-test
<span class="hljs-attr">        image:</span> sz-pg-oam-docker-hub<span class="hljs-bullet">-001.</span>tendcloud.com/library/hello:v1
<span class="hljs-attr">        ports:</span>
<span class="hljs-attr">        - containerPort:</span> <span class="hljs-number">9090</span>
<span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Service
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> rolling-update-test
<span class="hljs-attr">  labels:</span>
<span class="hljs-attr">    app:</span> rolling-update-test
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  ports:</span>
<span class="hljs-attr">  - port:</span> <span class="hljs-number">9090</span>
<span class="hljs-attr">    protocol:</span> TCP
<span class="hljs-attr">    name:</span> http
<span class="hljs-attr">  selector:</span>
<span class="hljs-attr">    app:</span> rolling-update-test
</code></pre>
<p><strong> 部署 service</strong></p>
<pre><code class="lang-shell">kubectl create -f rolling-update-test.yaml
</code></pre>
<p><strong> 修改 traefik ingress 配置 </strong></p>
<p>在 <code>ingress.yaml</code> 文件中增加新 service 的配置。</p>
<pre><code class="lang-Yaml"><span class="hljs-attr">  - host:</span> rolling-update-test.traefik.io
<span class="hljs-attr">    http:</span>
<span class="hljs-attr">      paths:</span>
<span class="hljs-attr">      - path:</span> /
<span class="hljs-attr">        backend:</span>
<span class="hljs-attr">          serviceName:</span> rolling-update-test
<span class="hljs-attr">          servicePort:</span> <span class="hljs-number">9090</span>
</code></pre>
<p>修改本地的 host 配置，增加一条配置：</p>
<pre><code>172.20.0.119 rolling-update-test.traefik.io
</code></pre><p>注意：172.20.0.119 是我们之前使用 keepalived 创建的 VIP。</p>
<p>打开浏览器访问 <code>http://rolling-update-test.traefik.io</code> 将会看到以下输出：</p>
<pre><code>This is version 1.
</code></pre><p><strong> 滚动升级 </strong></p>
<p>只需要将 <code>rolling-update-test.yaml</code> 文件中的 <code>image</code> 改成新版本的镜像名，然后执行：</p>
<pre><code class="lang-shell">kubectl apply -f rolling-update-test.yaml
</code></pre>
<p>也可以参考 <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/" target="_blank">Kubernetes Deployment Concept</a> 中的方法，直接设置新的镜像。</p>
<pre><code>kubectl set image deployment/rolling-update-test rolling-update-test=sz-pg-oam-docker-hub-001.tendcloud.com/library/hello:v2
</code></pre><p>或者使用 <code>kubectl edit deployment/rolling-update-test</code> 修改镜像名称后保存。</p>
<p>使用以下命令查看升级进度：</p>
<pre><code>kubectl rollout status deployment/rolling-update-test
</code></pre><p>升级完成后在浏览器中刷新 <code>http://rolling-update-test.traefik.io</code> 将会看到以下输出：</p>
<pre><code>This is version 2.
</code></pre><p>说明滚动升级成功。</p>
<h2 id="使用-replicationcontroller-创建的-pod-如何-rollingupdate">使用 ReplicationController 创建的 Pod 如何 RollingUpdate</h2>
<p>以上讲解使用 <strong>Deployment</strong> 创建的 Pod 的 RollingUpdate 方式，那么如果使用传统的 <strong>ReplicationController</strong> 创建的 Pod 如何 Update 呢？</p>
<p>举个例子：</p>
<pre><code class="lang-bash">$ kubectl -n spark-cluster rolling-update zeppelin-controller --image sz-pg-oam-docker-hub-001.tendcloud.com/library/zeppelin:0.7.1
Created zeppelin-controller-99be89dbbe5<span class="hljs-built_in">cd</span>5b8d6feab8f57a04a8b
Scaling up zeppelin-controller-99be89dbbe5<span class="hljs-built_in">cd</span>5b8d6feab8f57a04a8b from 0 to 1, scaling down zeppelin-controller from 1 to 0 (keep 1 pods available, don<span class="hljs-string">'t exceed 2 pods)
Scaling zeppelin-controller-99be89dbbe5cd5b8d6feab8f57a04a8b up to 1
Scaling zeppelin-controller down to 0
Update succeeded. Deleting old controller: zeppelin-controller
Renaming zeppelin-controller-99be89dbbe5cd5b8d6feab8f57a04a8b to zeppelin-controller
replicationcontroller "zeppelin-controller" rolling updated
</span></code></pre>
<p>只需要指定新的镜像即可，当然你可以配置 RollingUpdate 的策略。</p>
<h2 id="参考">参考</h2>
<ul>
<li><a href="http://dockone.io/article/328" target="_blank">Rolling update 机制解析</a></li>
<li><a href="https://kubernetes.io/docs/tasks/run-application/run-stateless-application-deployment/" target="_blank">Running a Stateless Application Using a Deployment</a></li>
<li><a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/cli/simple-rolling-update.md" target="_blank">Simple Rolling Update</a></li>
<li><a href="https://segmentfault.com/a/1190000008232770" target="_blank">使用 kubernetes 的 deployment 进行 RollingUpdate</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="helm" class="level3">Helm</h1>
<p><a href="https://github.com/kubernetes/helm" target="_blank">Helm</a> 是一个类似于 yum/apt/<a href="https://brew.sh/" target="_blank">homebrew</a> 的 Kubernetes 应用管理工具。Helm 使用 <a href="https://github.com/kubernetes/charts" target="_blank">Chart</a> 来管理 Kubernetes manifest 文件。</p>
<h2 id="helm-基本使用">Helm 基本使用</h2>
<p>安装 <code>helm</code> 客户端</p>
<pre><code class="lang-sh">brew install kubernetes-helm
</code></pre>
<p>初始化 Helm 并安装 <code>Tiller</code> 服务（需要事先配置好 kubectl）</p>
<pre><code class="lang-sh">helm init
</code></pre>
<p>更新 charts 列表</p>
<pre><code class="lang-sh">helm repo update
</code></pre>
<p>部署服务，比如 mysql</p>
<pre><code class="lang-sh">➜  ~ helm install stable/mysql
NAME:   quieting-warthog
LAST DEPLOYED: Tue Feb 21 16:13:02 2017
NAMESPACE: default
STATUS: DEPLOYED

RESOURCES:
==> v1/Secret
NAME                    TYPE    DATA  AGE
quieting-warthog-mysql  Opaque  2     1s

==> v1/PersistentVolumeClaim
NAME                    STATUS   VOLUME  CAPACITY  ACCESSMODES  AGE
quieting-warthog-mysql  Pending  1s

==> v1/Service
NAME                    CLUSTER-IP    EXTERNAL-IP  PORT(S)   AGE
quieting-warthog-mysql  10.3.253.105  <none>       3306/TCP  1s

==> extensions/v1beta1/Deployment
NAME                    DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE
quieting-warthog-mysql  1        1        1           0          1s


NOTES:
MySQL can be accessed via port 3306 on the following DNS name from within your cluster:
quieting-warthog-mysql.default.svc.cluster.local

To get your root password run:

    kubectl get secret --namespace default quieting-warthog-mysql -o jsonpath=<span class="hljs-string">"{.data.mysql-root-password}"</span> | base64 --decode; <span class="hljs-built_in">echo</span>

To connect to your database:

1. Run an Ubuntu pod that you can use as a client:

    kubectl run -i --tty ubuntu --image=ubuntu:16.04 --restart=Never -- bash -il

2. Install the mysql client:

    $ apt-get update && apt-get install mysql-client -y

3. Connect using the mysql cli, <span class="hljs-keyword">then</span> provide your password:
    $ mysql -h quieting-warthog-mysql -p
</code></pre>
<p>更多命令的使用方法可以参考下面的 "Helm 命令参考" 部分。</p>
<h2 id="helm-工作原理">Helm 工作原理</h2>
<h3 id="基本概念">基本概念</h3>
<p>Helm 的三个基本概念</p>
<ul>
<li>Chart：Helm 应用（package），包括该应用的所有 Kubernetes manifest 模版，类似于 YUM RPM 或 Apt dpkg 文件</li>
<li>Repository：Helm package 存储仓库</li>
<li>Release：chart 的部署实例，每个 chart 可以部署一个或多个 release</li>
</ul>
<h3 id="helm-工作原理">Helm 工作原理</h3>
<p>Helm 包括两个部分，<code>helm</code> 客户端和 <code>tiller</code> 服务端。</p>
<blockquote>
<p>the client is responsible for managing charts, and the server is responsible for managing releases.</p>
</blockquote>
<h4 id="helm-客户端">helm 客户端</h4>
<p>helm 客户端是一个命令行工具，负责管理 charts、repository 和 release。它通过 gPRC API（使用 <code>kubectl port-forward</code> 将 tiller 的端口映射到本地，然后再通过映射后的端口跟 tiller 通信）向 tiller 发送请求，并由 tiller 来管理对应的 Kubernetes 资源。</p>
<p><code>helm</code> 命令的使用方法可以参考下面的 "Helm 命令参考" 部分。</p>
<h4 id="tiller-服务端">tiller 服务端</h4>
<p>tiller 接收来自 helm 客户端的请求，并把相关资源的操作发送到 Kubernetes，负责管理（安装、查询、升级或删除等）和跟踪 Kubernetes 资源。为了方便管理，tiller 把 release 的相关信息保存在 kubernetes 的 ConfigMap 中。</p>
<p>tiller 对外暴露 gRPC API，供 helm 客户端调用。</p>
<h2 id="helm-charts">Helm Charts</h2>
<p>Helm 使用 <a href="https://github.com/kubernetes/charts" target="_blank">Chart</a> 来管理 Kubernetes manifest 文件。每个 chart 都至少包括</p>
<ul>
<li>应用的基本信息 <code>Chart.yaml</code></li>
<li>一个或多个 Kubernetes manifest 文件模版（放置于 templates / 目录中），可以包括 Pod、Deployment、Service 等各种 Kubernetes 资源</li>
</ul>
<h3 id="chartyaml-示例">Chart.yaml 示例</h3>
<pre><code class="lang-yaml"><span class="hljs-attr">name:</span> The name of the chart (required)
<span class="hljs-attr">version:</span> A SemVer <span class="hljs-number">2</span> version (required)
<span class="hljs-attr">description:</span> A single-sentence description of this project (optional)
<span class="hljs-attr">keywords:</span>
<span class="hljs-bullet">  -</span> A list of keywords about this project (optional)
<span class="hljs-attr">home:</span> The URL of this project<span class="hljs-string">'s home page (optional)
sources:
  - A list of URLs to source code for this project (optional)
maintainers: # (optional)
  - name: The maintainer'</span>s name (required for each maintainer)
<span class="hljs-attr">    email:</span> The maintainer<span class="hljs-string">'s email (optional for each maintainer)
engine: gotpl # The name of the template engine (optional, defaults to gotpl)
icon: A URL to an SVG or PNG image to be used as an icon (optional).
</span></code></pre>
<h3 id="依赖管理">依赖管理</h3>
<p>Helm 支持两种方式管理依赖的方式：</p>
<ul>
<li>直接把依赖的 package 放在 <code>charts/</code> 目录中</li>
<li>使用 <code>requirements.yaml</code> 并用 <code>helm dep up foochart</code> 来自动下载依赖的 packages</li>
</ul>
<pre><code class="lang-yaml"><span class="hljs-attr">dependencies:</span>
<span class="hljs-attr">  - name:</span> apache
<span class="hljs-attr">    version:</span> <span class="hljs-number">1.2</span><span class="hljs-number">.3</span>
<span class="hljs-attr">    repository:</span> http://example.com/charts
<span class="hljs-attr">  - name:</span> mysql
<span class="hljs-attr">    version:</span> <span class="hljs-number">3.2</span><span class="hljs-number">.1</span>
<span class="hljs-attr">    repository:</span> http://another.example.com/charts
</code></pre>
<h3 id="chart-模版">Chart 模版</h3>
<p>Chart 模板基于 Go template 和 <a href="https://github.com/Masterminds/sprig" target="_blank">Sprig</a>，比如</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> ReplicationController
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> deis-database
<span class="hljs-attr">  namespace:</span> deis
<span class="hljs-attr">  labels:</span>
<span class="hljs-attr">    heritage:</span> deis
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  replicas:</span> <span class="hljs-number">1</span>
<span class="hljs-attr">  selector:</span>
<span class="hljs-attr">    app:</span> deis-database
<span class="hljs-attr">  template:</span>
<span class="hljs-attr">    metadata:</span>
<span class="hljs-attr">      labels:</span>
<span class="hljs-attr">        app:</span> deis-database
<span class="hljs-attr">    spec:</span>
<span class="hljs-attr">      serviceAccount:</span> deis-database
<span class="hljs-attr">      containers:</span>
<span class="hljs-attr">        - name:</span> deis-database
<span class="hljs-attr">          image:</span> {{.Values.imageRegistry}}/postgres:{{.Values.dockerTag}}
<span class="hljs-attr">          imagePullPolicy:</span> {{.Values.pullPolicy}}
<span class="hljs-attr">          ports:</span>
<span class="hljs-attr">            - containerPort:</span> <span class="hljs-number">5432</span>
<span class="hljs-attr">          env:</span>
<span class="hljs-attr">            - name:</span> DATABASE_STORAGE
<span class="hljs-attr">              value:</span> {{default <span class="hljs-string">"minio"</span> .Values.storage}}
</code></pre>
<p>模版参数的默认值必须放到 <code>values.yaml</code> 文件中，其格式为</p>
<pre><code class="lang-yaml"><span class="hljs-attr">imageRegistry:</span> <span class="hljs-string">"quay.io/deis"</span>
<span class="hljs-attr">dockerTag:</span> <span class="hljs-string">"latest"</span>
<span class="hljs-attr">pullPolicy:</span> <span class="hljs-string">"alwaysPull"</span>
<span class="hljs-attr">storage:</span> <span class="hljs-string">"s3"</span>

<span class="hljs-comment"># 依赖的 mysql chart 的默认参数</span>
<span class="hljs-attr">mysql:</span>
<span class="hljs-attr">  max_connections:</span> <span class="hljs-number">100</span>
<span class="hljs-attr">  password:</span> <span class="hljs-string">"secret"</span>
</code></pre>
<h3 id="helm-插件">Helm 插件</h3>
<p>插件提供了扩展 Helm 核心功能的方法，它在客户端执行，并放在 <code>$(helm home)/plugins</code> 目录中。</p>
<p>一个典型的 helm 插件格式为</p>
<pre><code class="lang-sh">$(helm home)/plugins/
  |- keybase/
      |
      |- plugin.yaml
      |- keybase.sh
</code></pre>
<p>而 plugin.yaml 格式为</p>
<pre><code class="lang-yaml"><span class="hljs-attr">name:</span> <span class="hljs-string">"keybase"</span>
<span class="hljs-attr">version:</span> <span class="hljs-string">"0.1.0"</span>
<span class="hljs-attr">usage:</span> <span class="hljs-string">"Integreate Keybase.io tools with Helm"</span>
<span class="hljs-attr">description:</span> |-
  This plugin provides Keybase services to Helm.
<span class="hljs-attr">ignoreFlags:</span> <span class="hljs-literal">false</span>
<span class="hljs-attr">useTunnel:</span> <span class="hljs-literal">false</span>
<span class="hljs-attr">command:</span> <span class="hljs-string">"$HELM_PLUGIN_DIR/keybase.sh"</span>
</code></pre>
<p>这样，就可以用 <code>helm keybase</code> 命令来使用这个插件。</p>
<h2 id="helm-命令参考">Helm 命令参考</h2>
<h3 id="查询-charts">查询 charts</h3>
<pre><code class="lang-sh">helm search
helm search mysql
</code></pre>
<h3 id="查询-package-详细信息">查询 package 详细信息</h3>
<pre><code class="lang-sh">helm inspect stable/mariadb
</code></pre>
<h3 id="部署-package">部署 package</h3>
<pre><code class="lang-sh">helm install stable/mysql
</code></pre>
<p>部署之前可以自定义 package 的选项：</p>
<pre><code class="lang-sh"><span class="hljs-comment"># 查询支持的选项</span>
helm inspect values stable/mysql

<span class="hljs-comment"># 自定义 password</span>
<span class="hljs-built_in">echo</span> <span class="hljs-string">"mysqlRootPassword: passwd"</span> > config.yaml
helm install <span class="hljs-_">-f</span> config.yaml stable/mysql
</code></pre>
<p>另外，还可以通过打包文件（.tgz）或者本地 package 路径（如 path/foo）来部署应用。</p>
<h3 id="查询服务-release-列表">查询服务 (Release) 列表</h3>
<pre><code class="lang-sh">➜  ~ helm ls
NAME                REVISION    UPDATED                     STATUS      CHART          NAMESPACE
quieting-warthog    1           Tue Feb 21 20:13:02 2017    DEPLOYED    mysql-0.2.5    default
</code></pre>
<h3 id="查询服务-release-状态">查询服务 (Release) 状态</h3>
<pre><code class="lang-sh">➜  ~ helm status quieting-warthog
LAST DEPLOYED: Tue Feb 21 16:13:02 2017
NAMESPACE: default
STATUS: DEPLOYED

RESOURCES:
==> v1/Secret
NAME                    TYPE    DATA  AGE
quieting-warthog-mysql  Opaque  2     9m

==> v1/PersistentVolumeClaim
NAME                    STATUS  VOLUME                                    CAPACITY  ACCESSMODES  AGE
quieting-warthog-mysql  Bound   pvc-90af9bf9<span class="hljs-_">-f</span>80d-11e6-930a-42010af00102  8Gi       RWO          9m

==> v1/Service
NAME                    CLUSTER-IP    EXTERNAL-IP  PORT(S)   AGE
quieting-warthog-mysql  10.3.253.105  <none>       3306/TCP  9m

==> extensions/v1beta1/Deployment
NAME                    DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE
quieting-warthog-mysql  1        1        1           1          9m


NOTES:
MySQL can be accessed via port 3306 on the following DNS name from within your cluster:
quieting-warthog-mysql.default.svc.cluster.local

To get your root password run:

    kubectl get secret --namespace default quieting-warthog-mysql -o jsonpath=<span class="hljs-string">"{.data.mysql-root-password}"</span> | base64 --decode; <span class="hljs-built_in">echo</span>

To connect to your database:

1. Run an Ubuntu pod that you can use as a client:

    kubectl run -i --tty ubuntu --image=ubuntu:16.04 --restart=Never -- bash -il

2. Install the mysql client:

    $ apt-get update && apt-get install mysql-client -y

3. Connect using the mysql cli, <span class="hljs-keyword">then</span> provide your password:
    $ mysql -h quieting-warthog-mysql -p
</code></pre>
<h3 id="升级和回滚-release">升级和回滚 Release</h3>
<pre><code class="lang-sh"><span class="hljs-comment"># 升级</span>
cat <span class="hljs-string">"mariadbUser: user1"</span> >panda.yaml
helm upgrade <span class="hljs-_">-f</span> panda.yaml happy-panda stable/mariadb

<span class="hljs-comment"># 回滚</span>
helm rollback happy-panda 1
</code></pre>
<h3 id="删除-release">删除 Release</h3>
<pre><code class="lang-sh">helm delete quieting-warthog
</code></pre>
<h3 id="repo-管理">repo 管理</h3>
<pre><code class="lang-sh"><span class="hljs-comment"># 添加 incubator repo</span>
helm repo add incubator https://kubernetes-charts-incubator.storage.googleapis.com/

<span class="hljs-comment"># 查询 repo 列表</span>
helm repo list

<span class="hljs-comment"># 生成 repo 索引（用于搭建 helm repository）</span>
helm repo index
</code></pre>
<h3 id="chart-管理">chart 管理</h3>
<pre><code class="lang-sh"><span class="hljs-comment"># 创建一个新的 chart</span>
helm create deis-workflow

<span class="hljs-comment"># validate chart</span>
helm lint

<span class="hljs-comment"># 打包 chart 到 tgz</span>
helm package deis-workflow
</code></pre>
<h2 id="helm-ui">Helm UI</h2>
<p><a href="https://github.com/kubeapps/kubeapps" target="_blank">Kubeapps</a> 提供了一个开源的 Helm UI 界面，方便以图形界面的形式管理 Helm 应用。</p>
<pre><code class="lang-sh">curl <span class="hljs-_">-s</span> https://api.github.com/repos/kubeapps/kubeapps/releases/latest | grep -i $(uname <span class="hljs-_">-s</span>) | grep browser_download_url | cut <span class="hljs-_">-d</span> <span class="hljs-string">'"'</span> <span class="hljs-_">-f</span> 4 | wget -i -
sudo mv kubeapps-$(uname <span class="hljs-_">-s</span>| tr <span class="hljs-string">'[:upper:]'</span> <span class="hljs-string">'[:lower:]'</span>)-amd64 /usr/<span class="hljs-built_in">local</span>/bin/kubeapps
sudo chmod +x /usr/<span class="hljs-built_in">local</span>/bin/kubeapps

kubeapps up
kubeapps dashboard
</code></pre>
<p>更多使用方法请参考 <a href="https://kubeapps.com/" target="_blank">Kubeapps 官方网站</a>。</p>
<h2 id="helm-repository">Helm Repository</h2>
<p>官方 repository:</p>
<ul>
<li><a href="https://hub.helm.sh/" target="_blank">https://hub.helm.sh/</a></li>
<li><a href="https://github.com/kubernetes/charts" target="_blank">https://github.com/kubernetes/charts</a></li>
</ul>
<p>第三方 repository:</p>
<ul>
<li><a href="https://github.com/coreos/prometheus-operator/tree/master/helm" target="_blank">https://github.com/coreos/prometheus-operator/tree/master/helm</a></li>
<li><a href="https://github.com/deis/charts" target="_blank">https://github.com/deis/charts</a></li>
<li><a href="https://github.com/bitnami/charts" target="_blank">https://github.com/bitnami/charts</a></li>
<li><a href="https://github.com/att-comdev/openstack-helm" target="_blank">https://github.com/att-comdev/openstack-helm</a></li>
<li><a href="https://github.com/sapcc/openstack-helm" target="_blank">https://github.com/sapcc/openstack-helm</a></li>
<li><a href="https://github.com/helm/charts" target="_blank">https://github.com/helm/charts</a></li>
<li><a href="https://github.com/jackzampolin/tick-charts" target="_blank">https://github.com/jackzampolin/tick-charts</a></li>
</ul>
<h2 id="常用-helm-插件">常用 Helm 插件</h2>
<ol>
<li><a href="https://github.com/adamreese/helm-tiller" target="_blank">helm-tiller</a> - Additional commands to work with Tiller</li>
<li><a href="https://github.com/technosophos/helm-plugins" target="_blank">Technosophos's Helm Plugins</a> - Plugins for GitHub, Keybase, and GPG</li>
<li><a href="https://github.com/technosophos/helm-template" target="_blank">helm-template</a> - Debug/render templates client-side</li>
<li><a href="https://github.com/skuid/helm-value-store" target="_blank">Helm Value Store</a> - Plugin for working with Helm deployment values</li>
<li><a href="http://plugins.drone.io/ipedrazas/drone-helm/" target="_blank">Drone.io Helm Plugin</a> - Run Helm inside of the Drone CI/CD system</li>
</ol>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="operator" class="level3">Operator</h1>
<p>Operator 是 CoreOS 推出的旨在简化复杂有状态应用管理的框架，它是一个感知应用状态的控制器，通过扩展 Kubernetes API 来自动创建、管理和配置应用实例。</p>
<h2 id="operator-原理">Operator 原理</h2>
<p>Operator 基于 Third Party Resources 扩展了新的应用资源，并通过控制器来保证应用处于预期状态。比如 etcd operator 通过下面的三个步骤模拟了管理 etcd 集群的行为：</p>
<ol>
<li>通过 Kubernetes API 观察集群的当前状态；</li>
<li>分析当前状态与期望状态的差别；</li>
<li>调用 etcd 集群管理 API 或 Kubernetes API 消除这些差别。</li>
</ol>
<p><img src="images/etcd.png" alt="etcd"/></p>
<h2 id="如何创建-operator">如何创建 Operator</h2>
<p>Operator 是一个感知应用状态的控制器，所以实现一个 Operator 最关键的就是把管理应用状态的所有操作封装到配置资源和控制器中。通常来说 Operator 需要包括以下功能：</p>
<ul>
<li>Operator 自身以 deployment 的方式部署</li>
<li>Operator 自动创建一个 Third Party Resources 资源类型，用户可以用该类型创建应用实例</li>
<li>Operator 应该利用 Kubernetes 内置的 Serivce/ReplicaSet 等管理应用</li>
<li>Operator 应该向后兼容，并且在 Operator 自身退出或删除时不影响应用的状态</li>
<li>Operator 应该支持应用版本更新</li>
<li>Operator 应该测试 Pod 失效、配置错误、网络错误等异常情况</li>
</ul>
<h2 id="如何使用-operator">如何使用 Operator</h2>
<p>为了方便描述，以 Etcd Operator 为例，具体的链接可以参考 -<a href="https://coreos.com/operators/etcd/docs/latest" target="_blank">Etcd Operator</a>。</p>
<p>在 Kubernetes 部署 Operator：
通过在 Kubernetes 集群中创建一个 deploymet 实例，来部署对应的 Operator。具体的 Yaml 示例如下：</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> ServiceAccount
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> admin
<span class="hljs-attr">  namespace:</span> default

<span class="hljs-meta">---</span>
<span class="hljs-attr">kind:</span> ClusterRoleBinding
<span class="hljs-attr">apiVersion:</span> rbac.authorization.k8s.io/v1alpha1
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> admin
<span class="hljs-attr">subjects:</span>
<span class="hljs-attr">  - kind:</span> ServiceAccount
<span class="hljs-attr">    name:</span> admin
<span class="hljs-attr">    namespace:</span> default
<span class="hljs-attr">roleRef:</span>
<span class="hljs-attr">  kind:</span> ClusterRole
<span class="hljs-attr">  name:</span> cluster-admin
<span class="hljs-attr">  apiGroup:</span> rbac.authorization.k8s.io
<span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> extensions/v1beta1
<span class="hljs-attr">kind:</span> Deployment
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> etcd-operator
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  replicas:</span> <span class="hljs-number">1</span>
<span class="hljs-attr">  template:</span>
<span class="hljs-attr">    metadata:</span>
<span class="hljs-attr">      labels:</span>
<span class="hljs-attr">        name:</span> etcd-operator
<span class="hljs-attr">    spec:</span>
<span class="hljs-attr">      serviceAccountName:</span> admin
<span class="hljs-attr">      containers:</span>
<span class="hljs-attr">      - name:</span> etcd-operator
<span class="hljs-attr">        image:</span> quay.io/coreos/etcd-operator:v0<span class="hljs-number">.4</span><span class="hljs-number">.2</span>
<span class="hljs-attr">        env:</span>
<span class="hljs-attr">        - name:</span> MY_POD_NAMESPACE
<span class="hljs-attr">          valueFrom:</span>
<span class="hljs-attr">            fieldRef:</span>
<span class="hljs-attr">              fieldPath:</span> metadata.namespace
<span class="hljs-attr">        - name:</span> MY_POD_NAME
<span class="hljs-attr">          valueFrom:</span>
<span class="hljs-attr">            fieldRef:</span>
<span class="hljs-attr">              fieldPath:</span> metadata.name
</code></pre>
<pre><code class="lang-sh"><span class="hljs-comment"># kubectl create -f deployment.yaml</span>
serviceaccount <span class="hljs-string">"admin"</span> created
clusterrolebinding <span class="hljs-string">"admin"</span> created
deployment <span class="hljs-string">"etcd-operator"</span> created

<span class="hljs-comment"># kubectl  get pod</span>
NAME                            READY     STATUS    RESTARTS   AGE
etcd-operator-334633986-3nzk1   1/1       Running   0          31s
</code></pre>
<p>查看 operator 是否部署成功：</p>
<pre><code class="lang-sh"><span class="hljs-comment"># kubectl get thirdpartyresources</span>
NAME                      DESCRIPTION             VERSION(S)
cluster.etcd.coreos.com   Managed etcd clusters   v1beta1
</code></pre>
<p>对应的有状态服务 yaml 文件示例如下：</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">"etcd.coreos.com/v1beta1"</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">"Cluster"</span>
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> <span class="hljs-string">"example-etcd-cluster"</span>
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  size:</span> <span class="hljs-number">3</span>
<span class="hljs-attr">  version:</span> <span class="hljs-string">"3.1.8"</span>
</code></pre>
<p>部署对应的有状态服务：</p>
<pre><code class="lang-sh"><span class="hljs-comment"># kubectl create -f example-etcd-cluster.yaml</span>
Cluster <span class="hljs-string">"example-etcd-cluster"</span> created

<span class="hljs-comment"># kubectl get  cluster</span>
NAME                                        KIND
example-etcd-cluster   Cluster.v1beta1.etcd.coreos.com

<span class="hljs-comment"># kubectl get  service</span>
NAME                          CLUSTER-IP      EXTERNAL-IP   PORT(S)
example-etcd-cluster          None            <none>        2379/TCP,2380/TCP
example-etcd-cluster-client   10.105.90.190   <none>        2379/TCP

<span class="hljs-comment"># kubectl get pod</span>
NAME                            READY     STATUS    RESTARTS   AGE
example-etcd-cluster-0002       1/1       Running   0          5h
example-etcd-cluster-0003       1/1       Running   0          4h
example-etcd-cluster-0004       1/1       Running   0          4h
</code></pre>
<h2 id="其他示例">其他示例</h2>
<ul>
<li><a href="https://coreos.com/operators/prometheus/docs/latest" target="_blank">Prometheus Operator</a></li>
<li><a href="https://github.com/rook/rook" target="_blank">Rook Operator</a></li>
<li><a href="https://coreos.com/tectonic" target="_blank">Tectonic Operators</a></li>
<li><a href="https://github.com/sapcc/kubernetes-operators" target="_blank">https://github.com/sapcc/kubernetes-operators</a></li>
<li><a href="https://github.com/kbst/memcached" target="_blank">https://github.com/kbst/memcached</a></li>
<li><a href="https://github.com/Yolean/kubernetes-kafka" target="_blank">https://github.com/Yolean/kubernetes-kafka</a></li>
<li><a href="https://github.com/krallistic/kafka-operator" target="_blank">https://github.com/krallistic/kafka-operator</a></li>
<li><a href="https://github.com/huawei-cloudfederation/redis-operator" target="_blank">https://github.com/huawei-cloudfederation/redis-operator</a></li>
<li><a href="https://github.com/upmc-enterprises/elasticsearch-operator" target="_blank">https://github.com/upmc-enterprises/elasticsearch-operator</a></li>
<li><a href="https://github.com/pires/nats-operator" target="_blank">https://github.com/pires/nats-operator</a></li>
<li><a href="https://github.com/rosskukulinski/rethinkdb-operator" target="_blank">https://github.com/rosskukulinski/rethinkdb-operator</a></li>
<li><a href="https://istio.io/" target="_blank">https://istio.io/</a></li>
</ul>
<h2 id="与其他工具的关系">与其他工具的关系</h2>
<ul>
<li>StatefulSets：StatefulSets 为有状态服务提供了 DNS、持久化存储等，而 Operator 可以自动处理服务失效、备份、重配置等复杂的场景。</li>
<li>Puppet：Puppet 是一个静态配置工具，而 Operator 则可以实时、动态地保证应用处于预期状态</li>
<li>Helm：Helm 是一个打包工具，可以将多个应用打包到一起部署，而 Operator 则可以认为是 Helm 的补充，用来动态保证这些应用的正常运行</li>
</ul>
<h2 id="参考资料">参考资料</h2>
<ul>
<li><a href="https://coreos.com/operators" target="_blank">Kubernetes Operators</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="service-mesh" class="level3">ServiceMesh</h1>
<p>Service Mesh（服务网格）是一个用于保证服务间安全、快速、可靠通信的网络代理组件，是随着微服务和云原生应用兴起而诞生的基础设施层。它通常以轻量级网络代理的方式同应用部署在一起（比如sidecar方式，如下图所示）。Serivce Mesh可以看作是一个位于TCP/IP之上的网络模型，抽象了服务间可靠通信的机制。但与TCP不同，它是面向应用的，为应用提供了统一的可视化和控制。</p>
<p><img src="images/pattern-service-mesh.png" alt=""/></p>
<p>为了保证服务间通信的可靠性，Service Mesh需要支持熔断机制、延迟感知的负载均衡、服务发现、重试等一些列的特性。比如Linkerd处理一个请求的流程包括</p>
<ul>
<li>查找动态路由确定请求的服务</li>
<li>查找该服务的实例</li>
<li>Linkerd跟响应延迟等因素选择最优的实例</li>
<li>将请求转发给最优实例，记录延迟和响应情况</li>
<li>如果请求失败或实例实效，则转发给其他实例重试（需要是幂等请求）</li>
<li>如果请求超时，则直接失败，避免给后端增加更多的负载</li>
<li>记录请求的度量和分布式跟踪情况</li>
</ul>
<p>为什么Service Mesh是必要的</p>
<ul>
<li>将服务治理与实际服务解耦，避免微服务化过程中对应用的侵入</li>
<li>加速传统应用转型微服务或云原生应用</li>
</ul>
<p>Service Mesh并非一个全新的功能，而是将已存在于众多应用之中的相关功能分离出来，放到统一的组件来管理。特别是在微服务应用中，服务数量庞大，并且可能是基于不同的框架和语言构建，分离出来的Service Mesh组件更容易管理和协调它们。</p>
<p>常见的 Service Mesh 框架包括</p>
<ul>
<li><a href="istio.html">Istio</a></li>
<li><a href="conduit.md">Conduit</a></li>
<li><a href="linkerd.html">Linkerd</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="linkerd" class="level3">Linkerd</h1>
<p>Linkerd 是一个面向云原生应用的 Service Mesh 组件，也是 CNCF 项目之一。它为服务间通信提供了一个统一的管理和控制平面，并且解耦了应用程序代码和通信机制，从而无需更改应用程序就可以可视化控制服务间的通信。linkerd 实例是无状态的，可以以每个应用一个实例 (sidecar) 或者每台 Node 一个实例的方式部署。</p>
<p><img src="images/linkerd.png" alt=""/></p>
<p>Linkerd 的主要特性包括</p>
<ul>
<li>服务发现</li>
<li>动态请求路由</li>
<li>HTTP 代理集成，支持 HTTP、TLS、gRPC、HTTP/2 等</li>
<li>感知时延的负载均衡，支持多种负载均衡算法，如 Power of Two Choices (P2C) Least Loaded、Power of Two Choices (P2C) peak ewma、Aperture: least loaded、Heap: least loaded、Round robin 等</li>
<li>熔断机制，自动移除不健康的后端实例，包括 fail fast（只要连接失败就移除实例）和 failure accrual（超过 5 个请求处理失败时才将其标记为失效，并保留一定的恢复时间 ）两种</li>
<li>分布式跟踪和度量</li>
</ul>
<p><img src="images/linkerd-features.png" alt=""/></p>
<h2 id="linkerd-原理">Linkerd 原理</h2>
<p>Linkerd 路由将请求处理分解为多个步骤</p>
<ul>
<li>(1) IDENTIFICATION：为实际请求设置逻辑名字（即请求的目的服务），如默认将 HTTP 请求 <code>GET http://example/hello</code> 赋值名字 <code>/svc/example</code></li>
<li>(2) BINDING：dtabs 负责将逻辑名与客户端名字绑定起来，客户端名字总是以 <code>/#</code> 或 <code>/$</code> 开头，比如</li>
</ul>
<pre><code class="lang-sh"><span class="hljs-comment"># 假设 dtab 为</span>
/env => /<span class="hljs-comment">#/io.l5d.serversets/discovery</span>
/svc => /env/prod

<span class="hljs-comment"># 那么服务名 / svc/users 将会绑定为</span>
/svc/users
/env/prod/users
/<span class="hljs-comment">#/io.l5d.serversets/discovery/prod/users</span>
</code></pre>
<ul>
<li>(3) RESOLUTION：namer 负责解析客户端名，并得到真实的服务地址（IP + 端口）</li>
<li>(4) LOAD BALANCING：根据负载均衡算法选择如何发送请求</li>
</ul>
<p><img src="images/linkerd-routing.png" alt=""/></p>
<h2 id="linkerd-部署">Linkerd 部署</h2>
<p>Linkerd 以 DaemonSet 的方式部署在每个 Node 节点上：</p>
<pre><code class="lang-sh"><span class="hljs-comment"># Deploy linkerd.</span>
<span class="hljs-comment"># For CNI, deploy linkerd-cni.yml instead.</span>
<span class="hljs-comment"># kubectl apply -f https://github.com/linkerd/linkerd-examples/raw/master/k8s-daemonset/k8s/linkerd-cni.yml</span>
kubectl create ns linkerd
kubectl apply <span class="hljs-_">-f</span> https://raw.githubusercontent.com/linkerd/linkerd-examples/master/k8s-daemonset/k8s/servicemesh.yml

$ kubectl -n linkerd get pod
NAME        READY     STATUS    RESTARTS   AGE
l5d-6v67t   2/2       Running   0          2m
l5d-rn6v4   2/2       Running   0          2m
$ kubectl -n linkerd get svc
NAME      TYPE           CLUSTER-IP   EXTERNAL-IP     POR    AGE
l5d       LoadBalancer   10.0.71.9    <pending>       4140:32728/TCP,4141:31804/TCP,4240:31418/TCP,4241:30611/TCP,4340:31768/TCP,4341:30845/TCP,80:31144/TCP,8080:31115/TCP   3m
</code></pre>
<p>默认情况下，Linkerd 的 Dashboard 监听在每个容器实例的 9990 端口（注意未在 l5d 服务中对外暴露），可以通过服务的相应端口来访问。</p>
<pre><code class="lang-sh">kubectl -n linkerd port-forward $(kubectl -n linkerd get pod <span class="hljs-_">-l</span> app=l5d -o jsonpath=<span class="hljs-string">'{.items[0].metadata.name}'</span>) 9990 &
<span class="hljs-built_in">echo</span> <span class="hljs-string">"open http://localhost:9990 in browser"</span>
</code></pre>
<h3 id="grafana-和-prometheus">Grafana 和 Prometheus</h3>
<pre><code class="lang-sh">$ kubectl -n linkerd apply <span class="hljs-_">-f</span> https://github.com/linkerd/linkerd-viz/raw/master/k8s/linkerd-viz.yml
$ kubectl -n linkerd get svc linkerd-viz
NAME          TYPE           CLUSTER-IP    EXTERNAL-IP   PORT(S)                       AGE
linkerd-viz   LoadBalancer   10.0.235.21   <pending>     80:30895/TCP,9191:31145/TCP   24s
</code></pre>
<h3 id="tls">TLS</h3>
<pre><code class="lang-sh">kubectl -n linkerd apply <span class="hljs-_">-f</span> https://github.com/linkerd/linkerd-examples/raw/master/k8s-daemonset/k8s/certificates.yml
kubectl -n linkerd delete ds/l5d configmap/l5d-config
kubectl -n linkerd apply <span class="hljs-_">-f</span> https://github.com/linkerd/linkerd-examples/raw/master/k8s-daemonset/k8s/linkerd-tls.yml
</code></pre>
<h3 id="zipkin">Zipkin</h3>
<pre><code class="lang-sh"><span class="hljs-comment"># Deploy zipkin.</span>
kubectl -n linkerd apply <span class="hljs-_">-f</span> https://github.com/linkerd/linkerd-examples/raw/master/k8s-daemonset/k8s/zipkin.yml

<span class="hljs-comment"># Deploy linkerd for zipkin.</span>
kubectl -n linkerd apply <span class="hljs-_">-f</span> https://github.com/linkerd/linkerd-examples/raw/master/k8s-daemonset/k8s/linkerd-zipkin.yml

<span class="hljs-comment"># Get zipkin endpoint.</span>
ZIPKIN_LB=$(kubectl get svc zipkin -o jsonpath=<span class="hljs-string">"{.status.loadBalancer.ingress[0].*}"</span>)
<span class="hljs-built_in">echo</span> <span class="hljs-string">"open http://<span class="hljs-variable">$ZIPKIN_LB</span> in browser"</span>
</code></pre>
<h3 id="namerd">NAMERD</h3>
<pre><code class="lang-sh">$ kubectl apply <span class="hljs-_">-f</span> https://raw.githubusercontent.com/linkerd/linkerd-examples/master/k8s-daemonset/k8s/namerd.yml
$ kubectl apply <span class="hljs-_">-f</span> https://raw.githubusercontent.com/linkerd/linkerd-examples/master/k8s-daemonset/k8s/linkerd-namerd.yml

$ go get -u github.com/linkerd/namerctl
$ go install github.com/linkerd/namerctl
$ NAMERD_INGRESS_LB=$(kubectl get svc namerd -o jsonpath=<span class="hljs-string">"{.status.loadBalancer.ingress[0].*}"</span>)
$ <span class="hljs-built_in">export</span> NAMERCTL_BASE_URL=http://<span class="hljs-variable">$NAMERD_INGRESS_LB</span>:4180
$ $ namerctl dtab get internal
<span class="hljs-comment"># version MjgzNjk5NzI=</span>
/srv         => /<span class="hljs-comment">#/io.l5d.k8s/default/http ;</span>
/host        => /srv ;
/tmp         => /srv ;
/svc         => /host ;
/host/world  => /srv/world-v1 ;
</code></pre>
<h3 id="ingress-controller">Ingress Controller</h3>
<p>Linkerd 也可以作为 Kubernetes Ingress Controller 使用，注意下面的步骤将 Linkerd 部署到了 l5d-system namespace。</p>
<pre><code class="lang-sh">$ kubectl create ns l5d-system
$ kubectl apply <span class="hljs-_">-f</span> https://raw.githubusercontent.com/linkerd/linkerd-examples/master/k8s-daemonset/k8s/linkerd-ingress-controller.yml -n l5d-system

<span class="hljs-comment"># If load balancer is supported in kubernetes cluster</span>
$ L5D_SVC_IP=$(kubectl get svc l5d -n l5d-system -o jsonpath=<span class="hljs-string">"{.status.loadBalancer.ingress[0].*}"</span>)
$ <span class="hljs-built_in">echo</span> open http://<span class="hljs-variable">$L5D_SVC_IP</span>:9990

<span class="hljs-comment"># Or else</span>
$ HOST_IP=$(kubectl get po <span class="hljs-_">-l</span> app=l5d -n l5d-system -o jsonpath=<span class="hljs-string">"{.items[0].status.hostIP}"</span>)
$ L5D_SVC_IP=<span class="hljs-variable">$HOST_IP</span>:$(kubectl get svc l5d -n l5d-system -o <span class="hljs-string">'jsonpath={.spec.ports[0].nodePort}'</span>)
$ <span class="hljs-built_in">echo</span> open http://<span class="hljs-variable">$HOST_IP</span>:$(kubectl get svc l5d -n l5d-system -o <span class="hljs-string">'jsonpath={.spec.ports[1].nodePort}'</span>)
</code></pre>
<p>然后通过 <code>kubernetes.io/ingress.class: "linkerd"</code> annotation 使用 linkerd ingress 控制器：</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> extensions/v1beta1
<span class="hljs-attr">kind:</span> Ingress
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> hello-world
<span class="hljs-attr">  annotations:</span>
    kubernetes.io/ingress.class: <span class="hljs-string">"linkerd"</span>
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  backend:</span>
<span class="hljs-attr">    serviceName:</span> world-v1
<span class="hljs-attr">    servicePort:</span> http
<span class="hljs-attr">  rules:</span>
<span class="hljs-attr">  - host:</span> world.v2
<span class="hljs-attr">    http:</span>
<span class="hljs-attr">      paths:</span>
<span class="hljs-attr">      - backend:</span>
<span class="hljs-attr">          serviceName:</span> world-v2
<span class="hljs-attr">          servicePort:</span> http
</code></pre>
<p>更多使用方法见<a href="https://buoyant.io/2017/04/06/a-service-mesh-for-kubernetes-part-viii-linkerd-as-an-ingress-controller/" target="_blank">这里</a>。</p>
<h2 id="应用示例">应用示例</h2>
<p>可以通过 HTTP 代理和 linkerd-inject 等两种方式来使用 Linkerd。</p>
<h3 id="http-代理">HTTP 代理</h3>
<p>应用程序在使用 Linkerd 时需要为应用设置 HTTP 代理，其中</p>
<ul>
<li>HTTP 使用 <code>$(NODE_NAME):4140</code></li>
<li>HTTP/2 使用 <code>$(NODE_NAME):4240</code></li>
<li>gRPC 使用 <code>$(NODE_NAME):4340</code></li>
</ul>
<p>在 Kubernetes 中，可以使用 Downward API 来获取 <code>NODE_NAME</code>，比如</p>
<pre><code class="lang-yaml"><span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> ReplicationController
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> hello
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  replicas:</span> <span class="hljs-number">3</span>
<span class="hljs-attr">  selector:</span>
<span class="hljs-attr">    app:</span> hello
<span class="hljs-attr">  template:</span>
<span class="hljs-attr">    metadata:</span>
<span class="hljs-attr">      labels:</span>
<span class="hljs-attr">        app:</span> hello
<span class="hljs-attr">    spec:</span>
<span class="hljs-attr">      dnsPolicy:</span> ClusterFirst
<span class="hljs-attr">      containers:</span>
<span class="hljs-attr">      - name:</span> service
<span class="hljs-attr">        image:</span> buoyantio/helloworld:<span class="hljs-number">0.1</span><span class="hljs-number">.6</span>
<span class="hljs-attr">        env:</span>
<span class="hljs-attr">        - name:</span> NODE_NAME
<span class="hljs-attr">          valueFrom:</span>
<span class="hljs-attr">            fieldRef:</span>
<span class="hljs-attr">              fieldPath:</span> spec.nodeName
<span class="hljs-attr">        - name:</span> POD_IP
<span class="hljs-attr">          valueFrom:</span>
<span class="hljs-attr">            fieldRef:</span>
<span class="hljs-attr">              fieldPath:</span> status.podIP
<span class="hljs-attr">        - name:</span> http_proxy
<span class="hljs-attr">          value:</span> $(NODE_NAME):<span class="hljs-number">4140</span>
<span class="hljs-attr">        args:</span>
<span class="hljs-bullet">        -</span> <span class="hljs-string">"-addr=:7777"</span>
<span class="hljs-bullet">        -</span> <span class="hljs-string">"-text=Hello"</span>
<span class="hljs-bullet">        -</span> <span class="hljs-string">"-target=world"</span>
<span class="hljs-attr">        ports:</span>
<span class="hljs-attr">        - name:</span> service
<span class="hljs-attr">          containerPort:</span> <span class="hljs-number">7777</span>
<span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Service
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> hello
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  selector:</span>
<span class="hljs-attr">    app:</span> hello
<span class="hljs-attr">  clusterIP:</span> None
<span class="hljs-attr">  ports:</span>
<span class="hljs-attr">  - name:</span> http
<span class="hljs-attr">    port:</span> <span class="hljs-number">7777</span>
<span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> ReplicationController
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> world-v1
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  replicas:</span> <span class="hljs-number">3</span>
<span class="hljs-attr">  selector:</span>
<span class="hljs-attr">    app:</span> world-v1
<span class="hljs-attr">  template:</span>
<span class="hljs-attr">    metadata:</span>
<span class="hljs-attr">      labels:</span>
<span class="hljs-attr">        app:</span> world-v1
<span class="hljs-attr">    spec:</span>
<span class="hljs-attr">      dnsPolicy:</span> ClusterFirst
<span class="hljs-attr">      containers:</span>
<span class="hljs-attr">      - name:</span> service
<span class="hljs-attr">        image:</span> buoyantio/helloworld:<span class="hljs-number">0.1</span><span class="hljs-number">.6</span>
<span class="hljs-attr">        env:</span>
<span class="hljs-attr">        - name:</span> POD_IP
<span class="hljs-attr">          valueFrom:</span>
<span class="hljs-attr">            fieldRef:</span>
<span class="hljs-attr">              fieldPath:</span> status.podIP
<span class="hljs-attr">        - name:</span> TARGET_WORLD
<span class="hljs-attr">          value:</span> world
<span class="hljs-attr">        args:</span>
<span class="hljs-bullet">        -</span> <span class="hljs-string">"-addr=:7778"</span>
<span class="hljs-attr">        ports:</span>
<span class="hljs-attr">        - name:</span> service
<span class="hljs-attr">          containerPort:</span> <span class="hljs-number">7778</span>
<span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Service
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> world-v1
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  selector:</span>
<span class="hljs-attr">    app:</span> world-v1
<span class="hljs-attr">  clusterIP:</span> None
<span class="hljs-attr">  ports:</span>
<span class="hljs-attr">  - name:</span> http
<span class="hljs-attr">    port:</span> <span class="hljs-number">7778</span>
</code></pre>
<h3 id="linkerd-inject">linkerd-inject</h3>
<pre><code class="lang-sh"><span class="hljs-comment"># install linkerd-inject</span>
$ go get github.com/linkerd/linkerd-inject

<span class="hljs-comment"># inject init container and deploy this config</span>
$ kubectl apply <span class="hljs-_">-f</span> <(linkerd-inject <span class="hljs-_">-f</span> <your k8s config>.yml -linkerdPort 4140)
</code></pre>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://buoyant.io/2017/04/25/whats-a-service-mesh-and-why-do-i-need-one/" target="_blank">WHAT’S A SERVICE MESH? AND WHY DO I NEED ONE?</a></li>
<li><a href="https://linkerd.io/documentation/" target="_blank">Linkerd 官方文档</a></li>
<li><a href="https://buoyant.io/2016/10/04/a-service-mesh-for-kubernetes-part-i-top-line-service-metrics/" target="_blank">A SERVICE MESH FOR KUBERNETES</a></li>
<li><a href="https://github.com/linkerd/linkerd-examples" target="_blank">Linkerd examples</a></li>
<li><a href="http://philcalcado.com/2017/08/03/pattern_service_mesh.html" target="_blank">Service Mesh Pattern</a></li>
<li><a href="https://conduit.io" target="_blank">https://conduit.io</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="linkerd2-conduit" class="level3">Linkerd2</h1>
<p>Linkerd2 （曾命名为 <a href="https://conduit.io" target="_blank">Conduit</a>） 是 Buoyant 公司推出的下一代轻量级服务网格框架，开源在 <a href="https://github.com/linkerd/linkerd2" target="_blank">https://github.com/linkerd/linkerd2</a>。与 linkerd 不同的是，它专用于 Kubernetes 集群中，并且比 linkerd 更轻量级（基于 Rust 和 Go，没有了 JVM 等大内存的开销），可以以 sidecar 的方式把代理服务跟实际服务的 Pod 运行在一起（这点跟 Istio 类似）。Linkerd2 的主要特性包括：</p>
<ul>
<li>轻量级，速度快，每个代理容器仅占用 10mb RSS，并且额外延迟只有亚毫妙级</li>
<li>安全，基于 Rust，默认开启 TLS</li>
<li>端到端可视化</li>
<li>增强 Kubernetes 的可靠性、可视性以及安全性</li>
</ul>
<h2 id="部署">部署</h2>
<pre><code class="lang-sh">$ linkerd install | kubectl apply <span class="hljs-_">-f</span> -
namespace/linkerd configured
serviceaccount/linkerd-controller configured
clusterrole.rbac.authorization.k8s.io/linkerd-linkerd-controller configured
clusterrolebinding.rbac.authorization.k8s.io/linkerd-linkerd-controller configured
serviceaccount/linkerd-prometheus configured
clusterrole.rbac.authorization.k8s.io/linkerd-linkerd-prometheus configured
clusterrolebinding.rbac.authorization.k8s.io/linkerd-linkerd-prometheus configured
service/api configured
service/proxy-api configured
deployment.extensions/controller configured
service/web configured
deployment.extensions/web configured
service/prometheus configured
deployment.extensions/prometheus configured
configmap/prometheus-config configured
service/grafana configured
deployment.extensions/grafana configured
configmap/grafana-config configured

$ kubectl -n linkerd get svc
NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)             AGE
api          ClusterIP   10.0.173.27    <none>        8085/TCP            163m
grafana      ClusterIP   10.0.49.44     <none>        3000/TCP            163m
prometheus   ClusterIP   10.0.205.82    <none>        9090/TCP            163m
proxy-api    ClusterIP   10.0.170.201   <none>        8086/TCP            163m
web          ClusterIP   10.0.88.136    <none>        8084/TCP,9994/TCP   163m

$ kubectl -n linkerd get pod          
NAME                          READY     STATUS    RESTARTS   AGE
controller-67489d768d-75wjz   5/5       Running   0          163m
grafana-5df745d8b8-pv6tf      2/2       Running   0          163m
prometheus<span class="hljs-_">-d</span>96f9bf89-2s6jg    2/2       Running   0          163m
web-5<span class="hljs-built_in">cd</span>59f97b6-wf8nk          2/2       Running   0          57s
</code></pre>
<h2 id="dashboard">Dashboard</h2>
<pre><code class="lang-sh">$ linkerd dashboard
Linkerd dashboard available at:
http://127.0.0.1:37737/api/v1/namespaces/linkerd/services/web:http/proxy/
Grafana dashboard available at:
http://127.0.0.1:37737/api/v1/namespaces/linkerd/services/grafana:http/proxy/
Opening Linkerd dashboard <span class="hljs-keyword">in</span> the default browser
</code></pre>
<p><img src="images/linkerd2.png" alt=""/></p>
<h2 id="示例应用">示例应用</h2>
<pre><code class="lang-sh">curl https://run.linkerd.io/emojivoto.yml \
  | linkerd inject - \
  | kubectl apply <span class="hljs-_">-f</span> -
</code></pre>
<p>查看服务的网络流量统计情况：</p>
<pre><code class="lang-sh">linkerd -n emojivoto <span class="hljs-built_in">stat</span> deployment
NAME       MESHED   SUCCESS      RPS   LATENCY_P50   LATENCY_P95   LATENCY_P99   TLS
emoji         1/1   100.00%   8.1rps           1ms           1ms           1ms    0%
vote-bot      1/1         -        -             -             -             -     -
voting        1/1    87.88%   1.1rps           1ms           1ms           1ms    0%
web           1/1    93.65%   2.1rps           1ms           9ms          88ms    0%
</code></pre>
<p>跟踪服务的网络流量</p>
<pre><code class="lang-sh">$ linkerd -n emojivoto tap deploy voting
req id=0:809 src=10.244.6.239:57202 dst=10.244.1.237:8080 :method=POST :authority=voting-svc.emojivoto:8080 :path=/emojivoto.v1.VotingService/VoteDoughnut
rsp id=0:809 src=10.244.6.239:57202 dst=10.244.1.237:8080 :status=200 latency=478µs
end id=0:809 src=10.244.6.239:57202 dst=10.244.1.237:8080 grpc-status=OK duration=7µs response-length=5B
req id=0:810 src=10.244.6.239:57202 dst=10.244.1.237:8080 :method=POST :authority=voting-svc.emojivoto:8080 :path=/emojivoto.v1.VotingService/VoteDoughnut
rsp id=0:810 src=10.244.6.239:57202 dst=10.244.1.237:8080 :status=200 latency=419µs
end id=0:810 src=10.244.6.239:57202 dst=10.244.1.237:8080 grpc-status=OK duration=8µs response-length=5B
</code></pre>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://buoyant.io/2016/10/04/a-service-mesh-for-kubernetes-part-i-top-line-service-metrics/" target="_blank">A SERVICE MESH FOR KUBERNETES</a></li>
<li><a href="http://philcalcado.com/2017/08/03/pattern_service_mesh.html" target="_blank">Service Mesh Pattern</a></li>
<li><a href="https://linkerd.io/2/overview/" target="_blank">https://linkerd.io/2/overview/</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="istio-和-service-mesh" class="level2">Istio</h1>
<p>Istio 是 Google、IBM 和 Lyft 联合开源的服务网格（Service Mesh）框架，旨在解决大量微服务的发现、连接、管理、监控以及安全等问题。Istio 对应用是透明的，不需要改动任何服务代码就可以实现透明的服务治理。</p>
<p>Istio 的主要特性包括：</p>
<ul>
<li>HTTP、gRPC、WebSocket 和 TCP 网络流量的自动负载均衡</li>
<li>细粒度的网络流量行为控制， 包括丰富的路由规则、重试、故障转移和故障注入等</li>
<li>可选策略层和配置 API 支持访问控制、速率限制以及配额管理</li>
<li>自动度量、日志记录和跟踪所有进出的流量</li>
<li>强大的身份认证和授权机制实现服务间的安全通信</li>
</ul>
<h2 id="istio-原理">Istio 原理</h2>
<p>Istio 从逻辑上可以分为数据平面和控制平面：</p>
<ul>
<li><strong>数据平面</strong>主要由一系列的智能代理（默认为 Envoy）组成，管理微服务之间的网络通信</li>
<li><strong>控制平面</strong>负责管理和配置代理来路由流量，并配置 Mixer 以进行策略部署和遥测数据收集</li>
</ul>
<p>Istio 架构可以如下图所示</p>
<p><img src="images/istio-arch.png" alt=""/></p>
<p>它主要由以下组件构成</p>
<ul>
<li><a href="https://lyft.github.io/envoy/" target="_blank">Envoy</a>：Lyft 开源的高性能代理，用于调解服务网格中所有服务的入站和出站流量。它支持动态服务发现、负载均衡、TLS 终止、HTTP/2 和 gPRC 代理、熔断、健康检查、故障注入和性能测量等丰富的功能。Envoy 以 sidecar 的方式部署在相关的服务的 Pod 中，从而无需重新构建或重写代码。</li>
<li>Mixer：负责访问控制、执行策略并从 Envoy 代理中收集遥测数据。Mixer 支持灵活的插件模型，方便扩展（支持 GCP、AWS、Prometheus、Heapster 等多种后端）。</li>
<li>Pilot：动态管理 Envoy 实例的生命周期，提供服务发现、智能路由和弹性流量管理（如超时、重试）等功能。它将流量管理策略转化为 Envoy 数据平面配置，并传播到 sidecar 中。</li>
<li><a href="https://istio.io/zh/docs/concepts/traffic-management/#pilot-%E5%92%8C-envoy" target="_blank">Pilot</a> 为 Envoy sidecar 提供服务发现功能，为智能路由（例如 A/B 测试、金丝雀部署等）和弹性（超时、重试、熔断器等）提供流量管理功能。它将控制流量行为的高级路由规则转换为特定于 Envoy 的配置，并在运行时将它们传播到 sidecar。Pilot 将服务发现机制抽象为符合 <a href="https://github.com/envoyproxy/data-plane-api" target="_blank">Envoy 数据平面 API</a> 的标准格式，以便支持在多种环境下运行并保持流量管理的相同操作接口。</li>
<li>Citadel 通过内置身份和凭证管理提供服务间和最终用户的身份认证。支持基于角色的访问控制、基于服务标识的策略执行等。</li>
</ul>
<p><img src="images/istio-service.png" alt=""/></p>
<p>在数据平面上，除了 <a href="https://www.envoyproxy.io" target="_blank">Envoy</a>，还可以选择使用 <a href="https://github.com/nginmesh/nginmesh" target="_blank">nginxmesh</a>、<a href="https://linkerd.io/getting-started" target="_blank">linkerd</a> 等作为网络代理。比如，使用 nginxmesh 时，Istio 的控制平面（Pilot、Mixer、Auth）保持不变，但用 Nginx Sidecar 取代 Envoy：</p>
<p><img src="images/nginx_sidecar.png" alt=""/></p>
<h2 id="安装">安装</h2>
<p>Istio 的安装部署步骤见 <a href="istio-deploy.html">这里</a>。</p>
<h2 id="注入-sidecar-容器前对-pod-的要求">注入 Sidecar 容器前对 Pod 的要求</h2>
<p>为 Pod 注入 Sidecar 容器后才能成为服务网格的一部分。Istio 要求 Pod 必须满足以下条件：</p>
<ul>
<li>Pod 要关联服务并且必须属于单一的服务，不支持属于多个服务的 Pod</li>
<li>端口必须要命名，格式为 <code><协议>[-<后缀>]</code>，其中协议包括 <code>http</code>、<code>http2</code>、<code>grpc</code>、<code>mongo</code> 以及 <code>redis</code>。否则会被视为 TCP 流量</li>
<li>推荐所有 Deployment 中增加 <code>app</code> 标签，用来在分布式跟踪中添加上下文信息</li>
</ul>
<h2 id="示例应用">示例应用</h2>
<blockquote>
<p>以下步骤假设命令行终端在 <a href="istio-deploy.html">安装部署</a> 时下载的 <code>istio-${ISTIO_VERSION}</code> 目录中。</p>
</blockquote>
<h3 id="手动注入-sidecar-容器">手动注入 sidecar 容器</h3>
<p>在部署应用时，可以通过 <code>istioctl kube-inject</code> 给 Pod 手动插入 Envoy sidecar 容器，即</p>
<pre><code class="lang-sh">$  kubectl apply <span class="hljs-_">-f</span> <(istioctl kube-inject --debug <span class="hljs-_">-f</span> samples/bookinfo/platform/kube/bookinfo.yaml)
service <span class="hljs-string">"details"</span> configured
deployment.extensions <span class="hljs-string">"details-v1"</span> configured
service <span class="hljs-string">"ratings"</span> configured
deployment.extensions <span class="hljs-string">"ratings-v1"</span> configured
service <span class="hljs-string">"reviews"</span> configured
deployment.extensions <span class="hljs-string">"reviews-v1"</span> configured
deployment.extensions <span class="hljs-string">"reviews-v2"</span> configured
deployment.extensions <span class="hljs-string">"reviews-v3"</span> configured
service <span class="hljs-string">"productpage"</span> configured
deployment.extensions <span class="hljs-string">"productpage-v1"</span> configured
ingress.extensions <span class="hljs-string">"gateway"</span> configured

$ kubectl apply <span class="hljs-_">-f</span> samples/bookinfo/networking/bookinfo-gateway.yaml
</code></pre>
<p>原始应用如下图所示</p>
<p><img src="images/bookinfo.png" alt=""/></p>
<p><code>istioctl kube-inject</code> 在原始应用的每个 Pod 中插入了一个 Envoy 容器</p>
<p><img src="images/bookinfo2.png" alt=""/></p>
<p>服务启动后，可以通过 Gateway 地址 <code>http://<gateway-address>/productpage</code> 来访问 BookInfo 应用：</p>
<pre><code class="lang-sh">$ kubectl get svc istio-ingressgateway -n istio-system
kubectl get svc istio-ingressgateway -n istio-system
NAME                   TYPE           CLUSTER-IP    EXTERNAL-IP    PORT(S)                                                                                                     AGE
istio-ingressgateway   LoadBalancer   10.0.203.82   x.x.x.x        80:31380/TCP,443:31390/TCP,31400:31400/TCP,15011:31720/TCP,8060:31948/TCP,15030:32340/TCP,15031:31958/TCP   2h
</code></pre>
<p><img src="images/productpage.png" alt=""/></p>
<p>默认情况下，三个版本的 reviews 服务以负载均衡的方式轮询。</p>
<h3 id="自动注入-sidecar-容器">自动注入 sidecar 容器</h3>
<p>首先确认 <code>admissionregistration</code> API 已经开启：</p>
<pre><code class="lang-sh">$ kubectl api-versions | grep admissionregistration
admissionregistration.k8s.io/v1beta1
</code></pre>
<p>然后确认 istio-sidecar-injector 正常运行</p>
<pre><code class="lang-sh"><span class="hljs-comment"># Conform istio-sidecar-injector is working</span>
$ kubectl -n istio-system get deploy istio-sidecar-injector
NAME                     DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
istio-sidecar-injector   1         1         1            1           4m
</code></pre>
<p>为需要自动注入 sidecar 的 namespace 加上标签 <code>istio-injection=enabled</code>：</p>
<pre><code class="lang-sh"><span class="hljs-comment"># default namespace 没有 istio-injection 标签</span>
$ kubectl get namespace -L istio-injection
NAME           STATUS        AGE       ISTIO-INJECTION
default        Active        1h
istio-system   Active        1h
kube-public    Active        1h
kube-system    Active        1h

<span class="hljs-comment"># 打上 istio-injection=enabled 标签</span>
$ kubectl label namespace default istio-injection=enabled
</code></pre>
<p>这样，在 default namespace 中创建 Pod 后自动添加 istio sidecar 容器。</p>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://istio.io/" target="_blank">https://istio.io/</a></li>
<li><a href="https://istio.io/talks/istio_talk_gluecon_2017.pdf" target="_blank">Istio - A modern service mesh</a></li>
<li><a href="https://lyft.github.io/envoy/" target="_blank">https://lyft.github.io/envoy/</a></li>
<li><a href="https://github.com/nginmesh/nginmesh" target="_blank">https://github.com/nginmesh/nginmesh</a></li>
<li><a href="https://buoyant.io/2017/04/25/whats-a-service-mesh-and-why-do-i-need-one/" target="_blank">WHAT’S A SERVICE MESH? AND WHY DO I NEED ONE?</a></li>
<li><a href="https://buoyant.io/2016/10/04/a-service-mesh-for-kubernetes-part-i-top-line-service-metrics/" target="_blank">A SERVICE MESH FOR KUBERNETES</a></li>
<li><a href="http://philcalcado.com/2017/08/03/pattern_service_mesh.html" target="_blank">Service Mesh Pattern</a></li>
<li><a href="http://blog.kubernetes.io/2017/10/request-routing-and-policy-management.html" target="_blank">Request Routing and Policy Management with the Istio Service Mesh</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="istio-安装部署" class="level3">安装</h1>
<p>在安装 Istio 之前要确保 Kubernetes 集群（仅支持 v1.9.0 及以后版本）已部署并配置好本地的 kubectl 客户端。比如，使用 minikube：</p>
<pre><code class="lang-sh">minikube start --memory=4096 --kubernetes-version=v1.11.1 --vm-driver=hyperkit
</code></pre>
<h2 id="下载-istio">下载 Istio</h2>
<pre><code class="lang-sh">curl -L https://git.io/getLatestIstio | sh -
sudo apt-get install -y jq
ISTIO_VERSION=$(curl -L <span class="hljs-_">-s</span> https://api.github.com/repos/istio/istio/releases/latest | jq -r .tag_name)
<span class="hljs-built_in">cd</span> istio-<span class="hljs-variable">${ISTIO_VERSION}</span>
cp bin/istioctl /usr/<span class="hljs-built_in">local</span>/bin
</code></pre>
<h2 id="部署-istio-服务">部署 Istio 服务</h2>
<p>初始化 Helm Tiller：</p>
<pre><code class="lang-sh">kubectl create <span class="hljs-_">-f</span> install/kubernetes/helm/helm-service-account.yaml
helm init --service-account tiller
</code></pre>
<p>然后使用 Helm 部署：</p>
<pre><code class="lang-sh">kubectl apply <span class="hljs-_">-f</span> install/kubernetes/helm/istio/templates/crds.yaml
helm install install/kubernetes/helm/istio --name istio --namespace istio-system \
  --set ingress.enabled=<span class="hljs-literal">true</span> \
  --set gateways.enabled=<span class="hljs-literal">true</span> \
  --set galley.enabled=<span class="hljs-literal">true</span> \
  --set sidecarInjectorWebhook.enabled=<span class="hljs-literal">true</span> \
  --set mixer.enabled=<span class="hljs-literal">true</span> \
  --set prometheus.enabled=<span class="hljs-literal">true</span> \
  --set grafana.enabled=<span class="hljs-literal">true</span> \
  --set servicegraph.enabled=<span class="hljs-literal">true</span> \
  --set tracing.enabled=<span class="hljs-literal">true</span> \
  --set kiali.enabled=<span class="hljs-literal">false</span>
</code></pre>
<p>部署完成后，可以检查 isotio-system namespace 中的服务是否正常运行：</p>
<pre><code class="lang-sh">$  kubectl -n istio-system get pod
NAME                                        READY     STATUS    RESTARTS   AGE
grafana-5fb774bcc9-2rkng                    1/1       Running   0          6m
istio-citadel-5b956fdf54-5nb25              1/1       Running   0          6m
istio-egressgateway-6cff45b4db<span class="hljs-_">-gt</span>8tr        1/1       Running   0          6m
istio-galley-699888c459-sgz7z               1/1       Running   0          6m
istio-ingress-fc79cc885-dvjqh               1/1       Running   0          6m
istio-ingressgateway-fc648887c-q5s5h        1/1       Running   0          6m
istio-pilot-6<span class="hljs-built_in">cd</span>95f9cc4-fjdb5                2/2       Running   0          6m
istio-policy-75f75cc6fd-4mlhn               2/2       Running   0          6m
istio-sidecar-injector-6d59d46ff4-m79tl     1/1       Running   0          6m
istio-statsd-prom-bridge-7f44bb5ddb-phkh6   1/1       Running   0          6m
istio-telemetry-544b8d7dcf-mk5kw            2/2       Running   0          6m
istio-tracing-ff94688bb-7hmfb               1/1       Running   0          6m
prometheus-84bd4b9796-hcjwc                 1/1       Running   0          6m
servicegraph-6c6dbbf599-q4rxd               1/1       Running   0          6m

$ kubectl -n istio-system get service
NAME                       TYPE           CLUSTER-IP     EXTERNAL-IP    PORT(S)                                                                                                     AGE
grafana                    ClusterIP      10.0.150.84    <none>         3000/TCP                                                                                                    6m
istio-citadel              ClusterIP      10.0.9.108     <none>         8060/TCP,9093/TCP                                                                                           6m
istio-egressgateway        ClusterIP      10.0.168.237   <none>         80/TCP,443/TCP                                                                                              6m
istio-galley               ClusterIP      10.0.160.216   <none>         443/TCP,9093/TCP                                                                                            6m
istio-ingress              LoadBalancer   10.0.55.174    x.x.x.x        80:32000/TCP,443:32728/TCP                                                                                  6m
istio-ingressgateway       LoadBalancer   10.0.203.82    x.x.x.x        80:31380/TCP,443:31390/TCP,31400:31400/TCP,15011:31720/TCP,8060:31948/TCP,15030:32340/TCP,15031:31958/TCP   6m
istio-pilot                ClusterIP      10.0.195.162   <none>         15010/TCP,15011/TCP,8080/TCP,9093/TCP                                                                       6m
istio-policy               ClusterIP      10.0.14.130    <none>         9091/TCP,15004/TCP,9093/TCP                                                                                 6m
istio-sidecar-injector     ClusterIP      10.0.160.50    <none>         443/TCP                                                                                                     6m
istio-statsd-prom-bridge   ClusterIP      10.0.133.84    <none>         9102/TCP,9125/UDP                                                                                           6m
istio-telemetry            ClusterIP      10.0.247.30    <none>         9091/TCP,15004/TCP,9093/TCP,42422/TCP                                                                       6m
jaeger-agent               ClusterIP      None           <none>         5775/UDP,6831/UDP,6832/UDP                                                                                  6m
jaeger-collector           ClusterIP      10.0.29.72     <none>         14267/TCP,14268/TCP                                                                                         6m
jaeger-query               ClusterIP      10.0.19.250    <none>         16686/TCP                                                                                                   6m
prometheus                 ClusterIP      10.0.19.53     <none>         9090/TCP                                                                                                    6m
servicegraph               ClusterIP      10.0.251.76    <none>         8088/TCP                                                                                                    6m
tracing                    ClusterIP      10.0.62.176    <none>         80/TCP                                                                                                      6m
zipkin                     ClusterIP      10.0.158.231   <none>         9411/TCP                                                                                                    6m
</code></pre>
<h2 id="网格扩展">网格扩展</h2>
<p>Istio 还支持管理非 Kubernetes 应用。此时需要在应用所在的 VM 或者物理中部署 Istio，具体步骤请参考 <a href="https://istio.io/docs/setup/kubernetes/mesh-expansion.html" target="_blank">https://istio.io/docs/setup/kubernetes/mesh-expansion.html</a>。注意，在部署前需要满足以下条件</p>
<ul>
<li>待接入服务器必须能够通过 IP 接入网格中的服务端点。通常这需要 VPN 或者 VPC 的支持，或者容器网络为服务端点提供直接路由（非 NAT 或者防火墙屏蔽）。该服务器无需访问 Kubernetes 指派的集群 IP 地址。</li>
<li>Istio 控制平面服务（Pilot、Mixer、Citadel）以及 Kubernetes 的 DNS 服务器必须能够从虚拟机进行访问，通常会使用<a href="https://kubernetes.io/docs/concepts/services-networking/service/#internal-load-balancer" target="_blank">内部负载均衡器</a>（也可以使用 NodePort）来满足这一要求，在虚拟机上运行 Istio 组件，或者使用自定义网络配置。</li>
</ul>
<p>部署好后，就可以向 Istio 注册应用，如</p>
<pre><code class="lang-sh"><span class="hljs-comment"># istioctl register servicename machine-ip portname:port</span>
$ istioctl -n onprem register mysql 1.2.3.4 3306
$ istioctl -n onprem register svc1 1.2.3.4 http:7000
</code></pre>
<h2 id="prometheus、grafana-和-zipkin">Prometheus、Grafana 和 Zipkin</h2>
<p>等所有 Pod 启动后，可以通过 NodePort、负载均衡服务的外网 IP 或者 <code>kubectl proxy</code> 来访问这些服务。比如通过 <code>kubectl proxy</code> 方式，先启动 kubectl proxy</p>
<pre><code class="lang-sh">$ kubectl proxy
Starting to serve on 127.0.0.1:8001
</code></pre>
<p>通过 <code>http://localhost:8001/api/v1/namespaces/istio-system/services/grafana:3000/proxy/</code> 访问 Grafana 服务</p>
<p><img src="images/grafana.png" alt=""/></p>
<p>通过 <code>http://localhost:8001/api/v1/namespaces/istio-system/services/servicegraph:8088/proxy/</code> 访问 ServiceGraph 服务，展示服务之间调用关系图</p>
<p><img src="images/servicegraph.png" alt=""/></p>
<ul>
<li><code>/force/forcegraph.html</code> As explored above, this is an interactive <a href="https://d3js.org/" target="_blank">D3.js</a> visualization.</li>
<li><code>/dotviz</code> is a static <a href="https://www.graphviz.org/" target="_blank">Graphviz</a> visualization.</li>
<li><code>/dotgraph</code> provides a <a href="https://en.wikipedia.org/wiki/DOT_\(graph_description_language\" target="_blank">DOT</a>) serialization.</li>
<li><code>/d3graph</code> provides a JSON serialization for D3 visualization.</li>
<li><code>/graph</code> provides a generic JSON serialization.</li>
</ul>
<p>通过 <code>http://localhost:8001/api/v1/namespaces/istio-system/services/zipkin:9411/proxy/</code> 访问 Zipkin 跟踪页面</p>
<p><img src="images/zipkin.png" alt=""/></p>
<p>通过 <code>http://localhost:8001/api/v1/namespaces/istio-system/services/prometheus:9090/proxy/</code> 访问 Prometheus 页面</p>
<p><img src="images/prometheus.png" alt=""/></p>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="istio-流量管理" class="level3">流量管理</h1>
<p>Istio 提供了强大的流量管理功能，如智能路由、服务发现与负载均衡、故障恢复、故障注入等。</p>
<p><img src="images/istio-traffic.png" alt="istio-traffic-management"/></p>
<p>流量管理的功能由 Pilot 配合 Envoy 负责，并接管进入和离开容器的所有流量：</p>
<ul>
<li>流量管理的核心组件是 Pilot，负责管理和配置服务网格中的所有 Envoy 实例</li>
<li>而 Envoy 实例则负责维护负载均衡以及健康检查信息，从而允许其在目标实例之间智能分配流量，同时遵循其指定的路由规则</li>
</ul>
<p><img src="images/pilot-arch.png" alt="pilot"/></p>
<p><img src="images/istio-request-flow.png" alt="request-flow"/></p>
<h2 id="api-版本">API 版本</h2>
<p>Istio 0.7.X 及以前版本仅支持 <code>config.istio.io/v1alpha2</code>，0.8.0 将其升级为 <code>networking.istio.io/v1alpha3</code>，并且重命名了流量管理的几个资源对象：</p>
<ul>
<li>RouteRule -> <code>VirtualService</code>：定义服务网格内对服务的请求如何进行路由控制 ，支持根据 host、sourceLabels 、http headers 等不同的路由方式，也支持百分比、超时、重试、错误注入等功能。</li>
<li>DestinationPolicy -> <code>DestinationRule</code>：定义 <code>VirtualService</code> 之后的路由策略，包括断路器、负载均衡以及 TLS 等。</li>
<li>EgressRule -> <code>ServiceEntry</code>：定义了服务网格之外的服务，支持两种类型：网格内部和网格外部。网格内的条目和其他的内部服务类似，用于显式的将服务加入网格。可以用来把服务作为服务网格扩展的一部分加入不受管理的基础设置（例如加入到基于 Kubernetes 的服务网格中的虚拟机）中。网格外的条目用于表达网格外的服务。对这种条目来说，双向 TLS 认证是禁止的，策略实现需要在客户端执行，而不像内部服务请求中的服务端执行。 </li>
<li>Ingress -> <code>Gateway</code>：定义边缘网络流量的负载均衡。</li>
</ul>
<h2 id="服务发现和负载均衡">服务发现和负载均衡</h2>
<p>为了接管流量，Istio 假设所有容器在启动时自动将自己注册到 Istio 中（通过自动或手动给 Pod 注入 Envoy sidecar 容器）。Envoy 收到外部请求后，会对请求作负载均衡，并支持轮询、随机和加权最少请求等负载均衡算法。除此之外，Envoy 还会以熔断机制定期检查服务后端容器的健康状态，自动移除不健康的容器和加回恢复正常的容器。容器内也可以返回 HTTP 503 显示将自己从负载均衡中移除。</p>
<p><img src="images/istio-service-discovery.png" alt=""/></p>
<h3 id="流量接管">流量接管</h3>
<p>Istio 假定进入和离开服务网络的所有流量都会通过 Envoy 代理进行传输。Envoy sidecar 使用 iptables 把进入 Pod 和从 Pod 发出的流量转发到 Envoy 进程监听的端口（即 15001 端口）上：</p>
<pre><code class="lang-sh">*nat
:PREROUTING ACCEPT [0:0]
:INPUT ACCEPT [1:60]
:OUTPUT ACCEPT [482:44962]
:POSTROUTING ACCEPT [482:44962]
:ISTIO_INBOUND - [0:0]
:ISTIO_IN_REDIRECT - [0:0]
:ISTIO_OUTPUT - [0:0]
:ISTIO_REDIRECT - [0:0]
-A PREROUTING -p tcp -j ISTIO_INBOUND
-A OUTPUT -p tcp -j ISTIO_OUTPUT
-A ISTIO_INBOUND -p tcp -m tcp --dport 9080 -j ISTIO_IN_REDIRECT
-A ISTIO_IN_REDIRECT -p tcp -j REDIRECT --to-ports 15001
-A ISTIO_OUTPUT ! <span class="hljs-_">-d</span> 127.0.0.1/32 -o lo -j ISTIO_REDIRECT
-A ISTIO_OUTPUT -m owner --uid-owner 1337 -j RETURN
-A ISTIO_OUTPUT -m owner --gid-owner 1337 -j RETURN
-A ISTIO_OUTPUT <span class="hljs-_">-d</span> 127.0.0.1/32 -j RETURN
-A ISTIO_OUTPUT -j ISTIO_REDIRECT
-A ISTIO_REDIRECT -p tcp -j REDIRECT --to-ports 15001
</code></pre>
<h2 id="故障恢复">故障恢复</h2>
<p>Istio 提供了一系列开箱即用的故障恢复功能，如</p>
<ul>
<li>超时处理</li>
<li>重试处理，如限制最大重试时间以及可变重试间隔</li>
<li>健康检查，如自动移除不健康的容器</li>
<li>请求限制，如并发请求数和并发连接数</li>
<li>熔断</li>
</ul>
<p>这些功能均可以使用 VirtualService 动态配置。比如以下为用户 jason 的请求返回 500 （而其他用户均可正常访问）：</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> networking.istio.io/v1alpha3
<span class="hljs-attr">kind:</span> VirtualService
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> ratings
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  hosts:</span>
<span class="hljs-bullet">  -</span> ratings
<span class="hljs-attr">  http:</span>
<span class="hljs-attr">  - match:</span>
<span class="hljs-attr">    - headers:</span>
<span class="hljs-attr">        cookie:</span>
<span class="hljs-attr">          regex:</span> <span class="hljs-string">"^(.*?;)?(user=jason)(;.*)?$"</span>
<span class="hljs-attr">    fault:</span>
<span class="hljs-attr">      abort:</span>
<span class="hljs-attr">        percent:</span> <span class="hljs-number">100</span>
<span class="hljs-attr">        httpStatus:</span> <span class="hljs-number">500</span>
<span class="hljs-attr">    route:</span>
<span class="hljs-attr">    - destination:</span>
<span class="hljs-attr">        host:</span> ratings
<span class="hljs-attr">        subset:</span> v1
<span class="hljs-attr">  - route:</span>
<span class="hljs-attr">    - destination:</span>
<span class="hljs-attr">        host:</span> ratings
<span class="hljs-attr">        subset:</span> v1
</code></pre>
<p>熔断示例：</p>
<pre><code class="lang-sh">cat <<EOF | istioctl create <span class="hljs-_">-f</span> -
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: httpbin
spec:
  host: httpbin
  trafficPolicy:
    connectionPool:
      tcp:
        maxConnections: 1
      http:
        http1MaxPendingRequests: 1
        maxRequestsPerConnection: 1
    outlierDetection:
      http:
        consecutiveErrors: 1
        interval: 1s
        baseEjectionTime: 3m
        maxEjectionPercent: 100
EOF
</code></pre>
<h2 id="故障注入">故障注入</h2>
<p>Istio 支持为应用注入故障，以模拟实际生产中碰到的各种问题，包括</p>
<ul>
<li>注入延迟（模拟网络延迟和服务过载）</li>
<li>注入失败（模拟应用失效）</li>
</ul>
<p>这些故障均可以使用 VirtualService 动态配置。如以下配置 2 秒的延迟：</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> networking.istio.io/v1alpha3
<span class="hljs-attr">kind:</span> VirtualService
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> ratings
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  hosts:</span>
<span class="hljs-bullet">  -</span> ratings
<span class="hljs-attr">  http:</span>
<span class="hljs-attr">  - fault:</span>
<span class="hljs-attr">      delay:</span>
<span class="hljs-attr">        percent:</span> <span class="hljs-number">100</span>
<span class="hljs-attr">        fixedDelay:</span> <span class="hljs-number">2</span>s
<span class="hljs-attr">    route:</span>
<span class="hljs-attr">    - destination:</span>
<span class="hljs-attr">        host:</span> ratings
<span class="hljs-attr">        subset:</span> v1
</code></pre>
<h2 id="金丝雀部署">金丝雀部署</h2>
<p><img src="images/istio-service-versions.png" alt="service-versions"/></p>
<p>首先部署 bookinfo，并配置默认路由为 v1 版本：</p>
<pre><code class="lang-sh"><span class="hljs-comment"># 以下命令假设 bookinfo 示例程序已部署，如未部署，可以执行下面的命令</span>
$ kubectl apply <span class="hljs-_">-f</span> <(istioctl kube-inject <span class="hljs-_">-f</span> samples/bookinfo/platform/kube/bookinfo.yaml)
<span class="hljs-comment"># 此时，三个版本的 reviews 服务以负载均衡的方式轮询。</span>

<span class="hljs-comment"># 创建默认路由，全部请求转发到 v1</span>
$ istioctl create <span class="hljs-_">-f</span> samples/bookinfo/routing/route-rule-all-v1.yaml

$ kubectl get virtualservice reviews -o yaml
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: reviews
spec:
  hosts:
  - reviews
  http:
  - route:
    - destination:
        host: reviews
        subset: v1
</code></pre>
<h3 id="示例一：将-10-请求发送到-v2-版本而其余-90-发送到-v1-版本">示例一：将 10% 请求发送到 v2 版本而其余 90% 发送到 v1 版本</h3>
<pre><code class="lang-sh">cat <<EOF | istioctl create <span class="hljs-_">-f</span> -
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: reviews
spec:
  hosts:
    - reviews
  http:
  - route:
    - destination:
        host: reviews
        subset: v1
      weight: 75
    - destination:
        host: reviews
        subset: v2
      weight: 25
EOF
</code></pre>
<h3 id="示例二：将-jason-用户的请求全部发到-v2-版本">示例二：将 jason 用户的请求全部发到 v2 版本</h3>
<pre><code class="lang-sh">cat <<EOF | istioctl replace <span class="hljs-_">-f</span> -
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: ratings
spec:
  hosts:
  - ratings
  http:
  - match:
    - <span class="hljs-built_in">source</span>Labels:
        app: reviews
        version: v2
      headers:
        end-user:
          exact: jason
EOF
</code></pre>
<h3 id="示例三：全部切换到-v2-版本">示例三：全部切换到 v2 版本</h3>
<pre><code class="lang-sh">cat <<EOF | istioctl replace <span class="hljs-_">-f</span> -
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: reviews
spec:
  hosts:
  - reviews
  http:
  - route:
    - destination:
        host: reviews
        subset: v2
EOF
</code></pre>
<h3 id="示例四：限制并发访问">示例四：限制并发访问</h3>
<pre><code class="lang-sh">cat <<EOF | istioctl create <span class="hljs-_">-f</span> -
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: reviews
spec:
  host: reviews
  subsets:
  - name: v1
    labels:
      version: v1
    trafficPolicy:
      connectionPool:
        tcp:
          maxConnections: 100
EOF
</code></pre>
<p>为了查看访问次数限制的效果，可以使用 <a href="https://github.com/wg/wrk" target="_blank">wrk</a> 给应用加一些压力：</p>
<pre><code class="lang-sh"><span class="hljs-built_in">export</span> BOOKINFO_URL=$(kubectl get po -n istio-system <span class="hljs-_">-l</span> istio=ingress -o jsonpath={.items[0].status.hostIP}):$(kubectl get svc -n istio-system istio-ingress -o jsonpath={.spec.ports[0].nodePort})
wrk -t1 -c1 <span class="hljs-_">-d</span>20s http://<span class="hljs-variable">$BOOKINFO_URL</span>/productpage
</code></pre>
<h2 id="gateway">Gateway</h2>
<p>Istio 在部署时会自动创建一个 <a href="https://istio.io/docs/reference/config/istio.networking.v1alpha3/#Gateway" target="_blank">Istio Gateway</a>，用来控制 Ingress 访问。</p>
<pre><code class="lang-sh"><span class="hljs-comment"># prepare</span>
kubectl apply <span class="hljs-_">-f</span> <(istioctl kube-inject <span class="hljs-_">-f</span> samples/httpbin/httpbin.yaml)
openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /tmp/tls.key -out /tmp/tls.crt -subj <span class="hljs-string">"/CN=httpbin.example.com"</span>

<span class="hljs-comment"># get ingress external IP (suppose load balancer service)</span>
kubectl get svc istio-ingressgateway -n istio-system
<span class="hljs-built_in">export</span> INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath=<span class="hljs-string">'{.status.loadBalancer.ingress[0].ip}'</span>)
<span class="hljs-built_in">export</span> INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath=<span class="hljs-string">'{.spec.ports[?(@.name=="http")].port}'</span>)
<span class="hljs-built_in">export</span> SECURE_INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath=<span class="hljs-string">'{.spec.ports[?(@.name=="https")].port}'</span>)

<span class="hljs-comment"># create gateway</span>
cat <<EOF | istioctl create <span class="hljs-_">-f</span> -
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: httpbin-gateway
spec:
  selector:
    istio: ingressgateway <span class="hljs-comment"># use Istio default gateway implementation</span>
  servers:
  - port:
      number: 80
      name: http
      protocol: HTTP
    hosts:
    - <span class="hljs-string">"httpbin.example.com"</span>
EOF

<span class="hljs-comment"># configure routes for the gateway</span>
cat <<EOF | istioctl create <span class="hljs-_">-f</span> -
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: httpbin
spec:
  hosts:
  - <span class="hljs-string">"httpbin.example.com"</span>
  gateways:
  - httpbin-gateway
  http:
  - match:
    - uri:
        prefix: /status
    - uri:
        prefix: /delay
    route:
    - destination:
        port:
          number: 8000
        host: httpbin
EOF

<span class="hljs-comment"># validate 200</span>
curl --resolve httpbin.example.com:<span class="hljs-variable">$INGRESS_PORT</span>:<span class="hljs-variable">$INGRESS_HOST</span> -HHost:httpbin.example.com -I http://httpbin.example.com:<span class="hljs-variable">$INGRESS_PORT</span>/status/200

<span class="hljs-comment"># invalidate 404</span>
curl --resolve httpbin.example.com:<span class="hljs-variable">$INGRESS_PORT</span>:<span class="hljs-variable">$INGRESS_HOST</span> -HHost:httpbin.example.com -I http://httpbin.example.com:<span class="hljs-variable">$INGRESS_PORT</span>/headers
</code></pre>
<p>使用 TLS：</p>
<pre><code class="lang-sh">kubectl create -n istio-system secret tls istio-ingressgateway-certs --key /tmp/tls.key --cert /tmp/tls.crt

cat <<EOF | istioctl replace <span class="hljs-_">-f</span> -
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: httpbin-gateway
spec:
  selector:
    istio: ingressgateway <span class="hljs-comment"># use istio default ingress gateway</span>
  servers:
  - port:
      number: 80
      name: http
      protocol: HTTP
    hosts:
    - <span class="hljs-string">"httpbin.example.com"</span>
  - port:
      number: 443
      name: https
      protocol: HTTPS
    tls:
      mode: SIMPLE
      serverCertificate: /etc/istio/ingressgateway-certs/tls.crt
      privateKey: /etc/istio/ingressgateway-certs/tls.key
    hosts:
    - <span class="hljs-string">"httpbin.example.com"</span>
EOF


<span class="hljs-comment"># validate 200</span>
curl --resolve httpbin.example.com:<span class="hljs-variable">$SECURE_INGRESS_PORT</span>:<span class="hljs-variable">$INGRESS_HOST</span> -HHost:httpbin.example.com -I -k https://httpbin.example.com:<span class="hljs-variable">$SECURE_INGRESS_PORT</span>/status/200
</code></pre>
<h2 id="egress-流量">Egress 流量</h2>
<p>默认情况下，Istio 接管了容器的内外网流量，从容器内部无法访问 Kubernetes 集群外的服务。可以通过 ServiceEntry 为需要的容器开放 Egress 访问，如</p>
<pre><code class="lang-yaml">$ cat <<EOF | istioctl create -f -
<span class="hljs-attr">apiVersion:</span> networking.istio.io/v1alpha3
<span class="hljs-attr">kind:</span> ServiceEntry
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> httpbin-ext
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  hosts:</span>
<span class="hljs-bullet">  -</span> httpbin.org
<span class="hljs-attr">  ports:</span>
<span class="hljs-attr">  - number:</span> <span class="hljs-number">80</span>
<span class="hljs-attr">    name:</span> http
<span class="hljs-attr">    protocol:</span> HTTP
EOF

$ cat <<EOF | istioctl create -f -
<span class="hljs-attr">apiVersion:</span> networking.istio.io/v1alpha3
<span class="hljs-attr">kind:</span> VirtualService
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> httpbin-ext
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  hosts:</span>
<span class="hljs-bullet">    -</span> httpbin.org
<span class="hljs-attr">  http:</span>
<span class="hljs-attr">  - timeout:</span> <span class="hljs-number">3</span>s
<span class="hljs-attr">    route:</span>
<span class="hljs-attr">      - destination:</span>
<span class="hljs-attr">          host:</span> httpbin.org
<span class="hljs-attr">        weight:</span> <span class="hljs-number">100</span>
EOF
</code></pre>
<p>需要注意的是 ServiceEntry 仅支持 HTTP、TCP 和 HTTPS，对于其他协议需要通过 <code>--includeIPRanges</code> 的方式设置 IP 地址范围，如</p>
<pre><code class="lang-sh">helm template @install/kubernetes/helm/istio@ --name istio --namespace istio-system --set global.proxy.includeIPRanges=<span class="hljs-string">"10.0.0.1/24"</span> -x @templates/sidecar-injector-configmap.yaml@ | kubectl apply <span class="hljs-_">-f</span> -
</code></pre>
<h2 id="流量镜像">流量镜像</h2>
<pre><code class="lang-sh">cat <<EOF | istioctl replace <span class="hljs-_">-f</span> -
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: httpbin
spec:
  hosts:
    - httpbin
  http:
  - route:
    - destination:
        host: httpbin
        subset: v1
      weight: 100
    mirror:
      host: httpbin
      subset: v2
EOF
</code></pre>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://istio.io/docs/concepts/traffic-management/overview.html" target="_blank">Istio traffic management overview</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="istio-安全管理" class="level3">安全管理</h1>
<p>Istio 提供了 RBAC 访问控制、双向 TLS 认证以及密钥管理等安全管理功能。</p>
<h2 id="rbac">RBAC</h2>
<p>Istio Role-Based Access Control (RBAC) 提供了 namespace、service 以及 method 级别的访问控制。其特性包括</p>
<ul>
<li>简单易用：提供基于角色的语意</li>
<li>支持认证：提供服务 - 服务和用户 - 服务的认证</li>
<li>灵活：提供角色和角色绑定的自定义属性</li>
</ul>
<p><img src="images/istio-auth.png" alt="image-20180423202459184"/></p>
<h3 id="开启-rbac">开启 RBAC</h3>
<p>通过 <code>RbacConfig</code> 来启用 RBAC，其中 mode 支持如下选项：</p>
<ul>
<li><strong>OFF</strong>: 停用 RBAC。</li>
<li><strong>ON</strong>: 为网格中的所有服务启用 RBAC。</li>
<li><strong>ON_WITH_INCLUSION</strong>: 只对 <code>inclusion</code> 字段中包含的命名空间和服务启用 RBAC。</li>
<li><strong>ON_WITH_EXCLUSION</strong>: 对网格内的所有服务启用 RBAC，除 <code>exclusion</code> 字段中包含的命名空间和服务之外。</li>
</ul>
<p>下面的例子为 <code>default</code> 命名空间开启 RBAC：</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">"config.istio.io/v1alpha2"</span>
<span class="hljs-attr">kind:</span> RbacConfig
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> default
<span class="hljs-attr">  namespace:</span> istio-system
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  mode:</span> ON_WITH_INCLUSION
<span class="hljs-attr">  inclusion:</span>
<span class="hljs-attr">    namespaces:</span> [<span class="hljs-string">"default"</span>]
</code></pre>
<h3 id="访问控制">访问控制</h3>
<p>Istio RBAC 提供了 ServiceRole 和 ServiceRoleBinding 两种资源对象，并以 CustomResourceDefinition (CRD) 的方式管理。</p>
<ul>
<li>ServiceRole 定义了一个可访问特定资源（namespace 之内）的服务角色，并支持以前缀通配符和后缀通配符的形式匹配一组服务</li>
<li>ServiceRoleBinding 定义了赋予指定角色的绑定，即可以指定的角色和动作访问服务</li>
</ul>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">"rbac.istio.io/v1alpha1"</span>
<span class="hljs-attr">kind:</span> ServiceRole
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> products-viewer
<span class="hljs-attr">  namespace:</span> default
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  rules:</span>
<span class="hljs-attr">  - services:</span> [<span class="hljs-string">"products.default.svc.cluster.local"</span>]
<span class="hljs-attr">    methods:</span> [<span class="hljs-string">"GET"</span>, <span class="hljs-string">"HEAD"</span>]

<span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">"rbac.istio.io/v1alpha1"</span>
<span class="hljs-attr">kind:</span> ServiceRoleBinding
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> test-binding-products
<span class="hljs-attr">  namespace:</span> default
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  subjects:</span>
<span class="hljs-attr">  - user:</span> <span class="hljs-string">"service-account-a"</span>
<span class="hljs-attr">  - user:</span> <span class="hljs-string">"istio-ingress-service-account"</span>
<span class="hljs-attr">    properties:</span>
<span class="hljs-bullet">    -</span> request.auth.claims[email]: <span class="hljs-string">"a@foo.com"</span>
<span class="hljs-attr">    roleRef:</span>
<span class="hljs-attr">    kind:</span> ServiceRole
<span class="hljs-attr">    name:</span> <span class="hljs-string">"products-viewer"</span>
</code></pre>
<h2 id="双向-tls">双向 TLS</h2>
<p>双向 TLS 为服务间通信提供了 TLS 认证，并提供管理系统自动管理密钥和证书的生成、分发、替换以及撤销。</p>
<p><img src="images/istio-tls.png" alt=""/></p>
<h3 id="实现原理">实现原理</h3>
<p>Istio Auth 由三部分组成：</p>
<ul>
<li>身份（Identity）：Istio 使用 Kubernetes service account 来识别服务的身份，格式为 <code>spiffe://<*domain*>/ns/<*namespace*>/sa/<*serviceaccount*></code></li>
<li>通信安全：端到端 TLS 通信通过服务器端和客户端的 Envoy 容器完成</li>
<li>证书管理：Istio CA (Certificate Authority) 负责为每个 service account 生成 SPIFEE 密钥和证书、分发到 Pod（通过 Secret Volume Mount 的形式）、定期轮转（Rotate）以及必要时撤销。对于 Kuberentes 之外的服务，CA 配合 Istio node agent 共同完成整个过程。</li>
</ul>
<p>这样，一个容器使用证书的流程为</p>
<ul>
<li>首先，Istio CA 监听 Kubernetes API，并为 service account 生成 SPIFFE 密钥及证书，再以 secret 形式存储到 Kubernetes 中</li>
<li>然后，Pod 创建时，Kubernetes API Server 将 secret 挂载到容器中</li>
<li>最后，Pilot 生成一个访问控制的配置，定义哪些 service account 可以访问服务，并分发给 Envoy</li>
<li>而当容器间通信时，Pod 双方的 Envoy 就会基于访问控制配置来作认证</li>
</ul>
<h3 id="最佳实践">最佳实践</h3>
<ul>
<li>为不同团队创建不同 namespace 分别管理</li>
<li>将 Istio CA 运行在单独的 namespace 中，并且仅授予管理员权限</li>
</ul>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://istio.io/docs/concepts/security/" target="_blank">Istio Security 文档</a></li>
<li><a href="https://istio.io/docs/concepts/security/rbac.html" target="_blank">Istio Role-Based Access Control (RBAC)</a></li>
<li><a href="https://istio.io/docs/concepts/security/mutual-tls.html" target="_blank">Istio 双向 TLS 文档</a> </li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="istio-策略管理" class="level3">策略管理</h1>
<p>Mixer 为应用程序和基础架构后端之间提供了一个通用的策略控制层，负责先决条件检查（如认证授权）、配额管理并从 Envoy 代理中收集遥测数据等。</p>
<p><img src="images/istio-mixer.png" alt=""/></p>
<p>Mixer 是高度模块化和可扩展的组件。他的一个关键功能就是把不同后端的策略和遥测收集系统的细节抽象出来，使得 Istio 的其余部分对这些后端不知情。Mixer 处理不同基础设施后端的灵活性是通过使用通用插件模型实现的。每个插件都被称为 <strong>Adapter</strong>，Mixer通过它们与不同的基础设施后端连接，这些后端可提供核心功能，例如日志、监控、配额、ACL 检查等。通过配置能够决定在运行时使用的确切的适配器套件，并且可以轻松扩展到新的或定制的基础设施后端。</p>
<p><img src="images/istio-adapters.png" alt=""/></p>
<h2 id="实现原理">实现原理</h2>
<p>本质上，Mixer 是一个 <a href="https://istio.io/docs/concepts/policy-and-control/attributes.html" target="_blank">属性</a> 处理机，进入 Mixer 的请求带有一系列的属性，Mixer 按照不同的处理阶段处理：</p>
<ul>
<li>通过全局 Adapters 为请求引入新的属性</li>
<li>通过解析（Resolution）识别要用于处理请求的配置资源</li>
<li>处理属性，生成 Adapter 参数</li>
<li>分发请求到各个 Adapters 后端处理</li>
</ul>
<p><img src="images/istio-phase.png" alt=""/></p>
<h2 id="流量限制示例">流量限制示例</h2>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">"config.istio.io/v1alpha2"</span>
<span class="hljs-attr">kind:</span> memquota
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> handler
<span class="hljs-attr">  namespace:</span> istio-system
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  quotas:</span>
<span class="hljs-attr">  - name:</span> requestcount.quota.istio-system
<span class="hljs-attr">    maxAmount:</span> <span class="hljs-number">5000</span>
<span class="hljs-attr">    validDuration:</span> <span class="hljs-number">1</span>s
    <span class="hljs-comment"># The first matching override is applied.</span>
    <span class="hljs-comment"># A requestcount instance is checked against override dimensions.</span>
<span class="hljs-attr">    overrides:</span>
    <span class="hljs-comment"># The following override applies to 'ratings' when</span>
    <span class="hljs-comment"># the source is 'reviews'.</span>
<span class="hljs-attr">    - dimensions:</span>
<span class="hljs-attr">        destination:</span> ratings
<span class="hljs-attr">        source:</span> reviews
<span class="hljs-attr">      maxAmount:</span> <span class="hljs-number">1</span>
<span class="hljs-attr">      validDuration:</span> <span class="hljs-number">1</span>s
    <span class="hljs-comment"># The following override applies to 'ratings' regardless</span>
    <span class="hljs-comment"># of the source.</span>
<span class="hljs-attr">    - dimensions:</span>
<span class="hljs-attr">        destination:</span> ratings
<span class="hljs-attr">      maxAmount:</span> <span class="hljs-number">100</span>
<span class="hljs-attr">      validDuration:</span> <span class="hljs-number">1</span>s

<span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">"config.istio.io/v1alpha2"</span>
<span class="hljs-attr">kind:</span> quota
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> requestcount
<span class="hljs-attr">  namespace:</span> istio-system
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  dimensions:</span>
<span class="hljs-attr">    source:</span> source.labels[<span class="hljs-string">"app"</span>] | source.service | <span class="hljs-string">"unknown"</span>
<span class="hljs-attr">    sourceVersion:</span> source.labels[<span class="hljs-string">"version"</span>] | <span class="hljs-string">"unknown"</span>
<span class="hljs-attr">    destination:</span> destination.labels[<span class="hljs-string">"app"</span>] | destination.service | <span class="hljs-string">"unknown"</span>
<span class="hljs-attr">    destinationVersion:</span> destination.labels[<span class="hljs-string">"version"</span>] | <span class="hljs-string">"unknown"</span>

<span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">"config.istio.io/v1alpha2"</span>
<span class="hljs-attr">kind:</span> rule
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> quota
<span class="hljs-attr">  namespace:</span> istio-system
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  actions:</span>
<span class="hljs-attr">  - handler:</span> handler.memquota
<span class="hljs-attr">    instances:</span>
<span class="hljs-bullet">    -</span> requestcount.quota
<span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> config.istio.io/v1alpha2
<span class="hljs-attr">kind:</span> QuotaSpec
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> request-count
<span class="hljs-attr">  namespace:</span> istio-system
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  rules:</span>
<span class="hljs-attr">  - quotas:</span>
<span class="hljs-attr">    - charge:</span> <span class="hljs-number">1</span>
<span class="hljs-attr">      quota:</span> requestcount
<span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> config.istio.io/v1alpha2
<span class="hljs-attr">kind:</span> QuotaSpecBinding
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> request-count
<span class="hljs-attr">  namespace:</span> istio-system
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  quotaSpecs:</span>
<span class="hljs-attr">  - name:</span> request-count
<span class="hljs-attr">    namespace:</span> istio-system
<span class="hljs-attr">  services:</span>
<span class="hljs-attr">  - name:</span> ratings
<span class="hljs-attr">  - name:</span> reviews
<span class="hljs-attr">  - name:</span> details
<span class="hljs-attr">  - name:</span> productpage
</code></pre>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://istio.io/docs/concepts/policy-and-control/mixer.html" target="_blank">Istio Mixer</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="istio-度量" class="level3">度量管理</h1>
<h2 id="新增指标">新增指标</h2>
<p>Istio 支持 <a href="https://istio.io/docs/tasks/telemetry/metrics-logs/" target="_blank">自定义指标、日志</a> 以及 <a href="https://istio.io/docs/tasks/telemetry/tcp-metrics/" target="_blank">TCP 指标</a>。可以通过指标配置来新增这些度量，每个配置包括三方面的内容：</p>
<ol>
<li>从 Istio 属性中生成度量实例，如logentry 、metrics 等。</li>
<li>创建处理器（适配 Mixer），用来处理生成的度量实例，如 prometheus。</li>
<li><p>根据一系列的股则，把度量实例传递给处理器，即创建 rule。</p>
<p>```yaml</p>
<h1 id="指标-instance-的配置">指标 instance 的配置</h1>
<p>apiVersion: "config.istio.io/v1alpha2"
kind: metric
metadata:
name: doublerequestcount
namespace: istio-system
spec:
value: "2" # 每个请求计数两次
dimensions:
 source: source.service | "unknown"
 destination: destination.service | "unknown"
 message: '"twice the fun!"'
monitored_resource_type: '"UNSPECIFIED"'</p>
</li>
</ol>
<hr/>
<h1 id="prometheus-handler-的配置">prometheus handler 的配置</h1>
<p>apiVersion: "config.istio.io/v1alpha2"
kind: prometheus
metadata:
  name: doublehandler
  namespace: istio-system
spec:
  metrics:</p>
<ul>
<li>name: double_request_count # Prometheus 指标名称
instance_name: doublerequestcount.metric.istio-system # Mixer Instance 名称（全限定名称）
kind: COUNTER
label_names:<ul>
<li>source</li>
<li>destination</li>
<li>message</li>
</ul>
</li>
</ul>
<hr/>
<h1 id="将指标-instance-发送给-prometheus-handler-的-rule-对象">将指标 Instance 发送给 prometheus handler 的 rule 对象</h1>
<p>apiVersion: "config.istio.io/v1alpha2"
kind: rule
metadata:
  name: doubleprom
  namespace: istio-system
spec:
  actions:</p>
<ul>
<li>handler: doublehandler.prometheus
instances:<ul>
<li>doublerequestcount.metric
```</li>
</ul>
</li>
</ul>
<h2 id="prometheus">Prometheus</h2>
<p>在命令行中执行以下命令：</p>
<pre><code>$ kubectl -n istio-system port-forward service/prometheus 9090:9090 &
</code></pre><p>在 Web 浏览器中访问 <code>http://localhost:9090</code> 即可以访问 Prometheus UI，查询度量指标。</p>
<h2 id="jaeger-分布式跟踪">Jaeger 分布式跟踪</h2>
<p>在命令行中执行以下命令：</p>
<pre><code>$ kubectl -n istio-system port-forward service/jaeger-query 16686:16686 &
</code></pre><p>在 Web 浏览器中访问 <code>http://localhost:16686</code> 即可以访问 Jaeger UI。</p>
<h2 id="grafana-可视化">Grafana 可视化</h2>
<p>在命令行中执行以下命令：</p>
<pre><code>$ kubectl -n istio-system port-forward service/grafana 3000:3000 &
</code></pre><p>在 Web 浏览器中访问 <code>http://localhost:3000</code> 即可以访问 Grafana 界面。</p>
<h2 id="服务图">服务图</h2>
<p>在命令行中执行以下命令：</p>
<pre><code>$ kubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=servicegraph -o jsonpath='{.items[0].metadata.name}') 8088:8088 &
</code></pre><p>在 Web 浏览器中访问 <code>http://localhost:8088/force/forcegraph.html</code> 即可以访问生成的服务图。 </p>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="istio-排错" class="level3">排错</h1>
<p>请见 <a href="https://istio.io/help/" target="_blank">https://istio.io/help/</a>。 </p>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="istio-社区" class="level3">社区<a href="https://istio.io/community.html" target="_blank">社区</a></h1>
<ul>
<li><a href="https://github.com/istio/community" target="_blank">Github</a></li>
<li><a href="https://drive.google.com/corp/drive/u/0/folders/0AIS5p3eW9BCtUk9PVA" target="_blank">设计文档</a></li>
<li><a href="https://github.com/istio/community/blob/master/WORKING-GROUPS.md" target="_blank">工作组（Working Groups）</a></li>
<li>邮件列表<ul>
<li><a href="https://groups.google.com/forum/#!forum/istio-users" target="_blank">istio-users@</a></li>
<li><a href="https://groups.google.com/forum/#!forum/istio-dev" target="_blank">istio-dev@</a></li>
<li><a href="https://groups.google.com/forum/#!forum/istio-announce" target="_blank">istio-announce@</a></li>
</ul>
</li>
<li><a href="https://twitter.com/IstioMesh" target="_blank">Twitter</a></li>
<li><a href="https://istio.rocket.chat/home" target="_blank">Rocket.Chat</a></li>
<li><a href="https://istio.io/help/faq/general.html" target="_blank">FAQ</a></li>
<li><a href="https://istio.io/help/glossary.html" target="_blank">术语表</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="devops" class="level2">Devops</h1>
<p>Kubernetes 生态中的 Devops 工具实践。</p>
<h2 id="源码部署（source-to-deployment）">源码部署（Source to Deployment）</h2>
<ul>
<li><a href="draft.html">Draft</a>： 提供了一个用于简化容器构建和部署（基于 Helm）的工具，使用方法见<a href="draft.html">这里</a></li>
<li><a href="skaffold.html">Skaffold</a>：同 Draft 类似，但不支持 Helm，使用方法见<a href="skaffold.html">这里</a></li>
<li>Metaparticle：提供了一套用于开发云原生应用的标准库，使用方法见 <a href="https://metaparticle.io" target="_blank">https://metaparticle.io</a></li>
</ul>
<h2 id="cicd">CI/CD</h2>
<ul>
<li><a href="jenkinsx.html">Jenkins X</a></li>
<li><a href="spinnaker.html">Spinnaker</a></li>
<li><a href="argo.html">Argo</a></li>
<li><a href="flux.html">Flux GitOps</a></li>
</ul>
<h2 id="其他">其他</h2>
<ul>
<li><a href="kompose.html">Kompose</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="draft" class="level3">Draft</h1>
<p>Draft 是微软 Deis 团队开源（见 <a href="https://github.com/azure/draft" target="_blank">https://github.com/azure/draft</a>）的容器应用开发辅助工具，它可以帮助开发人员简化容器应用程序的开发流程。</p>
<p>Draft 主要由三个命令组成</p>
<ul>
<li><code>draft init</code>：初始化 docker registry 账号，并在 Kubernetes 集群中部署 draftd（负责镜像构建、将镜像推送到 docker registry 以及部署应用等）</li>
<li><code>draft create</code>：draft 根据 packs 检测应用的开发语言，并自动生成 Dockerfile 和 Kubernetes Helm Charts</li>
<li><code>draft up</code>：根据 Dockfile 构建镜像，并使用 Helm 将应用部署到 Kubernetes 集群（支持本地或远端集群）。同时，还会在本地启动一个 draft client，监控代码变化，并将更新过的代码推送给 draftd。</li>
</ul>
<h2 id="draft-安装">Draft 安装</h2>
<p>由于 Draft 需要构建镜像并部署应用到 Kubernetes 集群，因而在安装 Draft 之前需要</p>
<ul>
<li>部署一个 Kubernetes 集群，部署方法可以参考 <a href="../deploy/">kubernetes 部署方法</a></li>
<li>安装并初始化 helm（需要 v2.4.x 版本，并且不要忘记运行 <code>helm init</code>），具体步骤可以参考 <a href="helm-app.md">helm 使用方法</a></li>
<li>注册 docker registry 账号，比如 <a href="https://hub.docker.com/" target="_blank">Docker Hub</a> 或<a href="https://quay.io/" target="_blank">Quay.io</a></li>
<li>配置 Ingress Controller 并在 DNS 中设置通配符域 <code>*</code> 的 A 记录（如 <code>*.draft.example.com</code>）到 Ingress IP 地址。最简单的 Ingress Controller 创建方式是使用 helm：</li>
</ul>
<pre><code class="lang-sh"><span class="hljs-comment"># 部署 nginx ingress controller</span>
$ helm install stable/nginx-ingress --namespace=kube-system --name=nginx-ingress
<span class="hljs-comment"># 等待 ingress controller 配置完成，并记下外网 IP</span>
$ kubectl --namespace kube-system get services -w nginx-ingress-nginx-ingress-controller
</code></pre>
<blockquote>
<p><strong>minikube Ingress Controller</strong></p>
<p>minikube 中配置和使用 Ingress Controller 的方法可以参考 <a href="../practice/minikube-ingress.html">这里</a>。</p>
</blockquote>
<p>初始化好 Kubernetes 集群和 Helm 后，可以在 <a href="https://github.com/Azure/draft/releases/latest" target="_blank">这里</a> 下载 draft 二进制文件，并配置 draft</p>
<pre><code class="lang-sh"><span class="hljs-comment"># 注意修改用户名、密码和邮件</span>
$ token=$(<span class="hljs-built_in">echo</span> <span class="hljs-string">'{"username":"feisky","password":"secret","email":"feisky@email.com"}'</span> | base64)
<span class="hljs-comment"># 注意修改 registry.org 和 basedomain</span>
$ draft init --set registry.url=docker.io,registry.org=feisky,registry.authtoken=<span class="hljs-variable">${token}</span>,basedomain=app.feisky.xyz
</code></pre>
<h2 id="draft-入门">Draft 入门</h2>
<p>draft 源码中提供了很多应用的 <a href="https://github.com/Azure/draft/blob/master/examples" target="_blank">示例</a>，我们来看一下怎么用 draft 来简化 python 应用的开发流程。</p>
<pre><code class="lang-sh">$ git <span class="hljs-built_in">clone</span> https://github.com/Azure/draft.git
$ <span class="hljs-built_in">cd</span> draft/examples/python
$ ls
app.py           requirements.txt

$ cat requirements.txt
flask
$ cat app.py
from flask import Flask
app = Flask(__name__)

@app.route(<span class="hljs-string">'/'</span>)
def hello_world():
    <span class="hljs-built_in">return</span> <span class="hljs-string">"Hello, World!\n"</span>

<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:
    app.run(host=<span class="hljs-string">'0.0.0.0'</span>, port=8080)
</code></pre>
<p>Draft create 生成 Dockerfile 和 chart</p>
<pre><code class="lang-sh">$ draft create
--> Python app detected
--> Ready to sail
$ ls
Dockerfile       app.py           chart            draft.toml       requirements.txt
$ cat Dockerfile
FROM python:onbuild
EXPOSE 8080
ENTRYPOINT [<span class="hljs-string">"python"</span>]
CMD [<span class="hljs-string">"app.py"</span>]
$ cat draft.toml
[environments]
  [environments.development]
    name = <span class="hljs-string">"virulent-sheep"</span>
    namespace = <span class="hljs-string">"default"</span>
    watch = <span class="hljs-literal">true</span>
    watch_delay = 2
</code></pre>
<p>Draft Up 构建镜像并部署应用</p>
<pre><code class="lang-sh">$ draft up
--> Building Dockerfile
Step 1 : FROM python:onbuild
onbuild: Pulling from library/python
10a267c67f42: Pulling fs layer
....
Digest: sha256:5178d22192c2b8b4e1140a3bae9021ee0e808d754b4310014745c11f03fcc61b
Status: Downloaded newer image <span class="hljs-keyword">for</span> python:onbuild
<span class="hljs-comment"># Executing 3 build triggers...</span>
Step 1 : COPY requirements.txt /usr/src/app/
Step 1 : RUN pip install --no-cache-dir -r requirements.txt
....
Successfully built f742caba47ed
--> Pushing docker.io/feisky/virulent-sheep:de7e97d0d889b4cdb81ae4b972097d759c59e06e
....
de7e97d0d889b4cdb81ae4b972097d759c59e06e: digest: sha256:7ee10c1a56ced4f854e7934c9d4a1722d331d7e9bf8130c1a01d6adf7aed6238 size: 2840
--> Deploying to Kubernetes
    Release <span class="hljs-string">"virulent-sheep"</span> does not exist. Installing it now.
--> Status: DEPLOYED
--> Notes:

  http://virulent-sheep.app.feisky.xyzto access your application

Watching <span class="hljs-built_in">local</span> files <span class="hljs-keyword">for</span> changes...
</code></pre>
<p>打开一个新的 shell，就可以通过子域名来访问应用了</p>
<pre><code class="lang-sh">$ curl virulent-sheep.app.feisky.xyz
Hello, World!
</code></pre>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="jenkins-x" class="level3">JenkinsX</h1>
<p><a href="http://jenkins-x.io/" target="_blank">Jenkins X</a> 是一个基于 Jenkins 和 Kubernetes 的 CI/CD 平台，旨在解决微服务架构下云原生应用的持续集成和持续交付问题。它使用 Jenkins、Helm、Draft、GitOps 以及 Github 等工具链构造了一个从集群安装、环境管理、持续集成、持续部署一直到应用发布等支持整个流程的平台。</p>
<h2 id="安装部署">安装部署</h2>
<h3 id="安装-jx-命令行工具">安装 jx 命令行工具</h3>
<pre><code class="lang-sh"><span class="hljs-comment"># MacOS</span>
brew tap jenkins-x/jx
brew install jx 

<span class="hljs-comment"># Linux</span>
curl -L https://github.com/jenkins-x/jx/releases/download/v1.1.10/jx-linux-amd64.tar.gz | tar xzv 
sudo mv jx /usr/<span class="hljs-built_in">local</span>/bin
</code></pre>
<h3 id="部署-kubernetes-集群">部署 Kubernetes 集群</h3>
<p>如果 Kubernetes 集群已经部署好了，那么该步可以忽略。</p>
<p><code>jx</code> 命令提供了在公有云中直接部署 Kubernetes 的功能，比如</p>
<pre><code class="lang-sh">create cluster aks      <span class="hljs-comment"># Create a new kubernetes cluster on AKS: Runs on Azure</span>
create cluster aws      <span class="hljs-comment"># Create a new kubernetes cluster on AWS with kops</span>
create cluster gke      <span class="hljs-comment"># Create a new kubernetes cluster on GKE: Runs on Google Cloud</span>
create cluster minikube <span class="hljs-comment"># Create a new kubernetes cluster with minikube: Runs locally</span>
</code></pre>
<h3 id="部署-jenkins-x-服务">部署 Jenkins X 服务</h3>
<p>注意在安装 Jenkins X 服务之前，Kubernetes 集群需要开启 RBAC 并开启 insecure docker registries（<code>dockerd --insecure-registry=10.0.0.0/16</code> ）。</p>
<p>运行下面的命令按照提示操作，该过程会配置</p>
<ul>
<li>Ingress Controller （如果没有安装的话）</li>
<li>Ingress 公网 IP 的 DNS（默认使用 <code>ip.xip.io</code>）</li>
<li>Github API token（用于创建 github repo 和 webhook）</li>
<li>Jenkins-X 服务</li>
<li>创建 staging 和 production 等示例项目，包括 github repo 以及 Jenkins 配置等</li>
</ul>
<pre><code class="lang-sh">jx install --provider=kubernetes
</code></pre>
<p>安装完成后，会输出 Jenkins 的访问入口以及管理员的用户名和密码，用于登录 Jenkins。</p>
<h2 id="创建应用">创建应用</h2>
<p>Jenkins X 支持快速创建新的应用</p>
<pre><code class="lang-sh"><span class="hljs-comment"># 创建 Spring Boot 应用</span>
jx create spring <span class="hljs-_">-d</span> web <span class="hljs-_">-d</span> actuator

<span class="hljs-comment"># 创建快速启动项目</span>
jx create quickstart  <span class="hljs-_">-l</span> go
</code></pre>
<p>也支持导入已有的应用，只是需要注意导入前要保证</p>
<ul>
<li>使用 Github 等 git 系统管理源码并设置好 Jenkins webhook</li>
<li>添加 Dockerfile、Jenkinsfile 以及运行应用所需要的 Helm Chart</li>
</ul>
<pre><code class="lang-sh"><span class="hljs-comment"># 从本地导入</span>
$ <span class="hljs-built_in">cd</span> my-cool-app
$ jx import

<span class="hljs-comment"># 从 Github 导入</span>
jx import --github --org myname

<span class="hljs-comment"># 从 URL 导入</span>
jx import --url https://github.com/jenkins-x/spring-boot-web-example.git
</code></pre>
<h2 id="发布应用">发布应用</h2>
<pre><code class="lang-sh"><span class="hljs-comment"># 发布新版本到生产环境中</span>
jx promote myapp --version 1.2.3 --env production
</code></pre>
<p><img src="images/jenkinsx.png" alt=""/></p>
<h2 id="常用命令">常用命令</h2>
<pre><code class="lang-sh"><span class="hljs-comment"># Get pipelines</span>
jx get pipelines

<span class="hljs-comment"># Get pipeline activities</span>
jx get activities

<span class="hljs-comment"># Get build logs</span>
jx get build logs <span class="hljs-_">-f</span> myapp

<span class="hljs-comment"># Open Jenkins in brower</span>
jx console

<span class="hljs-comment"># Get applications</span>
jx get applications

<span class="hljs-comment"># Get environments</span>
jx get environments
</code></pre>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="spinnaker" class="level3">Spinnaker</h1>
<p><a href="https://www.spinnaker.io/" target="_blank">Spinnaker</a> 是 Google 与 Netflix 发布的企业级持续交付平台，具有多云部署、自动发布、权限控制以及应用最佳实践等诸多优点。</p>
<h2 id="部署">部署</h2>
<pre><code class="lang-sh">helm install --name spinnaker stable/spinnaker
</code></pre>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="kompose" class="level3">Kompose</h1>
<p>Kompose是一个将docker-compose配置转换成Kubernetes manifests的工具，官方网站为<a href="http://kompose.io/" target="_blank">http://kompose.io/</a>。</p>
<h2 id="kompose安装">Kompose安装</h2>
<pre><code class="lang-sh"><span class="hljs-comment"># Linux</span>
$ curl -L https://github.com/kubernetes-incubator/kompose/releases/download/v0.5.0/kompose-linux-amd64 -o kompose

<span class="hljs-comment"># macOS</span>
$ curl -L https://github.com/kubernetes-incubator/kompose/releases/download/v0.5.0/kompose-darwin-amd64 -o kompose

<span class="hljs-comment"># Windows</span>
$ curl -L https://github.com/kubernetes-incubator/kompose/releases/download/v0.5.0/kompose-windows-amd64.exe -o kompose.exe

<span class="hljs-comment"># 放到PATH中</span>
$ chmod +x kompose
$ sudo mv ./kompose /usr/<span class="hljs-built_in">local</span>/bin/kompose
</code></pre>
<h2 id="kompose使用">Kompose使用</h2>
<p>docker-compose.yaml</p>
<pre><code class="lang-yaml"><span class="hljs-attr">version:</span> <span class="hljs-string">"2"</span>

<span class="hljs-attr">services:</span>

<span class="hljs-attr">  redis-master:</span>
<span class="hljs-attr">    image:</span> gcr.io/google_containers/redis:e2e 
<span class="hljs-attr">    ports:</span>
<span class="hljs-bullet">      -</span> <span class="hljs-string">"6379"</span>

<span class="hljs-attr">  redis-slave:</span>
<span class="hljs-attr">    image:</span> gcr.io/google_samples/gb-redisslave:v1
<span class="hljs-attr">    ports:</span>
<span class="hljs-bullet">      -</span> <span class="hljs-string">"6379"</span>
<span class="hljs-attr">    environment:</span>
<span class="hljs-bullet">      -</span> GET_HOSTS_FROM=dns

<span class="hljs-attr">  frontend:</span>
<span class="hljs-attr">    image:</span> gcr.io/google-samples/gb-frontend:v4
<span class="hljs-attr">    ports:</span>
<span class="hljs-bullet">      -</span> <span class="hljs-string">"80:80"</span>
<span class="hljs-attr">    environment:</span>
<span class="hljs-bullet">      -</span> GET_HOSTS_FROM=dns
<span class="hljs-attr">    labels:</span>
      kompose.service.type: LoadBalancer
</code></pre>
<h2 id="kompose-up">kompose up</h2>
<pre><code class="lang-sh">$ kompose up
We are going to create Kubernetes Deployments, Services and PersistentVolumeClaims <span class="hljs-keyword">for</span> your Dockerized application. 
If you need different kind of resources, use the <span class="hljs-string">'kompose convert'</span> and <span class="hljs-string">'kubectl create -f'</span> commands instead. 

INFO Successfully created Service: redis
INFO Successfully created Service: web
INFO Successfully created Deployment: redis
INFO Successfully created Deployment: web

Your application has been deployed to Kubernetes. You can run <span class="hljs-string">'kubectl get deployment,svc,pods,pvc'</span> <span class="hljs-keyword">for</span> details.
</code></pre>
<h2 id="kompose-convert">kompose convert</h2>
<pre><code class="lang-sh">$ kompose convert
INFO file <span class="hljs-string">"frontend-service.yaml"</span> created
INFO file <span class="hljs-string">"redis-master-service.yaml"</span> created
INFO file <span class="hljs-string">"redis-slave-service.yaml"</span> created
INFO file <span class="hljs-string">"frontend-deployment.yaml"</span> created
INFO file <span class="hljs-string">"redis-master-deployment.yaml"</span> created
INFO file <span class="hljs-string">"redis-slave-deployment.yaml"</span> created
</code></pre>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="skaffold" class="level3">Skaffold</h1>
<p><a href="https://github.com/GoogleCloudPlatform/skaffold" target="_blank">Skaffold</a> 是谷歌开源的简化本地 Kubernetes 应用开发的工具。它将构建镜像、推送镜像以及部署 Kubernetes 服务等流程自动化，可以方便地对 Kubernetes 应用进行持续开发。其功能特点包括</p>
<ul>
<li>没有服务器组件</li>
<li>自动检测代码更改并自动构建、推送和部署服务</li>
<li>自动管理镜像标签</li>
<li>支持已有工作流</li>
<li>保存文件即部署</li>
</ul>
<p><img src="images/skaffold1.png" alt=""/></p>
<h2 id="安装">安装</h2>
<pre><code class="lang-sh"><span class="hljs-comment"># Linux</span>
curl -Lo skaffold https://storage.googleapis.com/skaffold/releases/latest/skaffold-linux-amd64 && chmod +x skaffold && sudo mv skaffold /usr/<span class="hljs-built_in">local</span>/bin

<span class="hljs-comment"># MacOS</span>
curl -Lo skaffold https://storage.googleapis.com/skaffold/releases/latest/skaffold-darwin-amd64 && chmod +x skaffold && sudo mv skaffold /usr/<span class="hljs-built_in">local</span>/bin
</code></pre>
<h2 id="使用">使用</h2>
<p>在使用 skaffold 之前需要确保</p>
<ul>
<li>Kubernetes 集群已部署并配置好本地 kubectl 命令行</li>
<li>本地 Docker 处于运行状态并登录 DockerHub 或其他的 Docker Registry</li>
<li>skaffold 命令行已下载并放到系统 PATH 路径中</li>
</ul>
<p>skaffold 代码库提供了一些列的<a href="https://github.com/GoogleCloudPlatform/skaffold/tree/master/examples" target="_blank">示例</a>，我们来看一个最简单的。</p>
<p>下载示例应用：</p>
<pre><code class="lang-sh">$ git <span class="hljs-built_in">clone</span> https://github.com/GoogleCloudPlatform/skaffold
$ <span class="hljs-built_in">cd</span> skaffold/examples/getting-started
</code></pre>
<p>修改 <code>k8s-pod.yaml</code> 和 <code>skaffold.yaml</code> 文件中的镜像，将 <code>gcr.io/k8s-skaffold</code> 替换为已登录的 Docker Registry。然后运行 skaffold</p>
<pre><code class="lang-sh">$ skaffold dev
Starting build...
Found [minikube] context, using <span class="hljs-built_in">local</span> docker daemon.
Sending build context to Docker daemon  6.144kB
Step 1/5 : FROM golang:1.9.4-alpine3.7
 ---> fb6e10bf973b
Step 2/5 : WORKDIR /go/src/github.com/GoogleCloudPlatform/skaffold/examples/getting-started
 ---> Using cache
 ---> e9d19a54595b
Step 3/5 : CMD ./app
 ---> Using cache
 ---> 154b6512c4d9
Step 4/5 : COPY main.go .
 ---> Using cache
 ---> e097086e73a7
Step 5/5 : RUN go build -o app main.go
 ---> Using cache
 ---> 9c4622e8f0e7
Successfully built 9c4622e8f0e7
Successfully tagged 930080f0965230e824a79b9e7eccffbd:latest
Successfully tagged gcr.io/k8s-skaffold/skaffold-example:9c4622e8f0e7b5549a61a503bf73366a9cf7f7512aa8e9d64f3327a3c7fded1b
Build complete <span class="hljs-keyword">in</span> 657.426821ms
Starting deploy...
Deploying k8s-pod.yaml...
Deploy complete <span class="hljs-keyword">in</span> 173.770268ms
[getting-started] Hello world!
</code></pre>
<p>此时，打开另外一个终端，修改 <code>main.go</code> 的内容后 skaffold 会自动执行</p>
<ul>
<li>构建一个新的镜像（带有不同的 sha256 TAG）</li>
<li>修改 <code>k8s-pod.yaml</code> 文件中的镜像为新的 TAG</li>
<li>重新部署 <code>k8s-pod.yaml</code> </li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="argo" class="level3">Argo</h1>
<p>Argo 是一个基于 Kubernetes 的工作流引擎，同时也支持 CI、CD 等丰富的功能。Argo 开源在 <a href="https://github.com/argoproj" target="_blank">https://github.com/argoproj</a>。</p>
<h2 id="安装-argo">安装 Argo</h2>
<h3 id="使用-argo-install">使用 argo install</h3>
<pre><code class="lang-sh"><span class="hljs-comment"># Downlaod Argo.</span>
curl <span class="hljs-_">-s</span>SL -o argo https://github.com/argoproj/argo/releases/download/v2.1.0/argo-linux-amd64
chmod +x argo
sudo mv argo /usr/<span class="hljs-built_in">local</span>/bin/argo

<span class="hljs-comment"># Deploy to kubernetes</span>
kubectl create namespace argo
argo install -n argo
</code></pre>
<pre><code class="lang-sh">ACCESS_KEY=AKIAIOSFODNN7EXAMPLE
ACCESS_SECRET_KEY=wJalrXUtnFEMI/K7MDENG/bPxR<span class="hljs-keyword">fi</span>CYEXAMPLEKEY

helm install --namespace argo --name argo-artifacts --set accessKey=<span class="hljs-variable">$ACCESS_KEY</span>,secretKey=<span class="hljs-variable">$ACCESS_SECRET_KEY</span>,service.type=LoadBalancer stable/minio
</code></pre>
<p>创建名为 <code>argo-bucket</code> 的 Bucket（可以通过 <code>kubectl port-forward service/argo-artifacts-minio :9000</code> 访问 Minio UI 来操作）：</p>
<pre><code class="lang-sh"><span class="hljs-comment"># download mc client</span>
sudo wget https://dl.minio.io/client/mc/release/linux-amd64/mc -O /usr/<span class="hljs-built_in">local</span>/bin/mc
sudo chmod +x /usr/<span class="hljs-built_in">local</span>/bin/mc

<span class="hljs-comment"># create argo-bucket</span>
EXTERNAL_IP=$(kubectl -n argo get service argo-artifacts-minio -o jsonpath=<span class="hljs-string">'{.status.loadBalancer.ingress[0].ip}'</span>)
mc config host add argo-artifacts-minio-local http://<span class="hljs-variable">$EXTERNAL_IP</span>:9000 <span class="hljs-variable">$ACCESS_KEY</span> <span class="hljs-variable">$ACCESS_SECRET_KEY</span> --api=s3v4
mc mb argo-artifacts-minio-local/argo-bucket
</code></pre>
<p>然后修改 Argo 工作流控制器使用 Minio：</p>
<pre><code class="lang-sh">$ kubectl -n argo create secret generic argo-artifacts-minio --from-literal=accesskey=<span class="hljs-variable">$ACCESS_KEY</span> --from-literal=secretkey=<span class="hljs-variable">$ACCESS_SECRET_KEY</span>
$ kubectl edit configmap workflow-controller-configmap -n argo
...
    executorImage: argoproj/argoexec:v2.0.0
    artifactRepository:
      s3:
        bucket: argo-bucket
        endpoint: argo-artifacts-minio.argo:9000
        insecure: <span class="hljs-literal">true</span>
        <span class="hljs-comment"># accessKeySecret and secretKeySecret are secret selectors.</span>
        <span class="hljs-comment"># It references the k8s secret named 'argo-artifacts-minio'</span>
        <span class="hljs-comment"># which was created during the minio helm install. The keys,</span>
        <span class="hljs-comment"># 'accesskey' and 'secretkey', inside that secret are where the</span>
        <span class="hljs-comment"># actual minio credentials are stored.</span>
        accessKeySecret:
          name: argo-artifacts-minio
          key: accesskey
        secretKeySecret:
          name: argo-artifacts-minio
          key: secretkey
</code></pre>
<h3 id="使用-helm">使用 Helm</h3>
<blockquote>
<p>注意：当前 Helm Charts 使用的 Minio 版本较老，部署有可能会失败。</p>
</blockquote>
<pre><code class="lang-sh"><span class="hljs-comment"># Downlaod Argo.</span>
curl <span class="hljs-_">-s</span>SL -o /usr/<span class="hljs-built_in">local</span>/bin/argo https://github.com/argoproj/argo/releases/download/v2.0.0/argo-linux-amd64
chmod +x /usr/<span class="hljs-built_in">local</span>/bin/argo

<span class="hljs-comment"># Deploy to kubernetes</span>
helm repo add argo https://argoproj.github.io/argo-helm/
kubectl create clusterrolebinding default-admin --clusterrole=cluster-admin --serviceaccount=kube-system:default
helm install argo/argo-ci --name argo-ci --namespace=kube-system
</code></pre>
<h2 id="访问-argo-ui">访问 Argo UI</h2>
<pre><code class="lang-sh">$ kubectl -n argo port-forward service/argo-ui :80
Forwarding from 127.0.0.1:52592 -> 8001
Forwarding from [::1]:52592 -> 8001

<span class="hljs-comment"># 使用浏览器打开 127.0.0.1:52592</span>
</code></pre>
<h2 id="工作流">工作流</h2>
<p>首先，给默认的 ServiceAccount 授予集群管理权限</p>
<pre><code class="lang-sh"><span class="hljs-comment"># Authz yourself if you are not admin.</span>
kubectl create clusterrolebinding default-admin --clusterrole=cluster-admin --serviceaccount=argo:default
</code></pre>
<p>示例1： 最简单的工作流</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> argoproj.io/v1alpha1
<span class="hljs-attr">kind:</span> Workflow
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  generateName:</span> hello-world-
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  entrypoint:</span> whalesay
<span class="hljs-attr">  templates:</span>
<span class="hljs-attr">  - name:</span> whalesay
<span class="hljs-attr">    container:</span>
<span class="hljs-attr">      image:</span> docker/whalesay:latest
<span class="hljs-attr">      command:</span> [cowsay]
<span class="hljs-attr">      args:</span> [<span class="hljs-string">"hello world"</span>]
</code></pre>
<pre><code class="lang-sh">argo -n argo submit https://raw.githubusercontent.com/argoproj/argo/master/examples/hello-world.yaml
</code></pre>
<p>示例2：包含多个容器的工作流</p>
<pre><code class="lang-yaml"><span class="hljs-comment"># This example demonstrates the ability to pass artifacts</span>
<span class="hljs-comment"># from one step to the next.</span>
<span class="hljs-attr">apiVersion:</span> argoproj.io/v1alpha1
<span class="hljs-attr">kind:</span> Workflow
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  generateName:</span> artifact-passing-
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  entrypoint:</span> artifact-example
<span class="hljs-attr">  templates:</span>
<span class="hljs-attr">  - name:</span> artifact-example
<span class="hljs-attr">    steps:</span>
<span class="hljs-attr">    - - name:</span> generate-artifact
<span class="hljs-attr">        template:</span> whalesay
<span class="hljs-attr">    - - name:</span> consume-artifact
<span class="hljs-attr">        template:</span> print-message
<span class="hljs-attr">        arguments:</span>
<span class="hljs-attr">          artifacts:</span>
<span class="hljs-attr">          - name:</span> message
<span class="hljs-attr">            from:</span> <span class="hljs-string">"<span class="hljs-template-variable">{{steps.generate-artifact.outputs.artifacts.hello-art}}</span>"</span>

<span class="hljs-attr">  - name:</span> whalesay
<span class="hljs-attr">    container:</span>
<span class="hljs-attr">      image:</span> docker/whalesay:latest
<span class="hljs-attr">      command:</span> [sh, -c]
<span class="hljs-attr">      args:</span> [<span class="hljs-string">"cowsay hello world | tee /tmp/hello_world.txt"</span>]
<span class="hljs-attr">    outputs:</span>
<span class="hljs-attr">      artifacts:</span>
<span class="hljs-attr">      - name:</span> hello-art
<span class="hljs-attr">        path:</span> /tmp/hello_world.txt

<span class="hljs-attr">  - name:</span> print-message
<span class="hljs-attr">    inputs:</span>
<span class="hljs-attr">      artifacts:</span>
<span class="hljs-attr">      - name:</span> message
<span class="hljs-attr">        path:</span> /tmp/message
<span class="hljs-attr">    container:</span>
<span class="hljs-attr">      image:</span> alpine:latest
<span class="hljs-attr">      command:</span> [sh, -c]
<span class="hljs-attr">      args:</span> [<span class="hljs-string">"cat /tmp/message"</span>]
</code></pre>
<pre><code class="lang-sh">argo -n argo submit https://raw.githubusercontent.com/argoproj/argo/master/examples/artifact-passing.yaml
</code></pre>
<p>工作流创建完成后，可以查询它们的状态和日志，并在不需要时删除：</p>
<pre><code class="lang-sh">$ argo list
NAME                     STATUS    AGE   DURATION
artifact-passing-65p6g   Running   6s    4s
hello-world-cdnpq        Running   8s    6s

$ argo -n argo logs hello-world-4dhg8
 _____________
< hello world >
 -------------
    \
     \
      \
                    <span class="hljs-comment">##        .</span>
              <span class="hljs-comment">## ## ##       ==</span>
           <span class="hljs-comment">## ## ## ##      ===</span>
       /<span class="hljs-string">""</span><span class="hljs-string">""</span><span class="hljs-string">""</span><span class="hljs-string">""</span><span class="hljs-string">""</span><span class="hljs-string">""</span><span class="hljs-string">""</span><span class="hljs-string">""</span>___/ ===
  ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ /  ===- ~~~
       \______ o          __/
        \    \        __/
          \____\______/

$ argo -n argo delete hello-world-4dhg8
Workflow <span class="hljs-string">'hello-world-4dhg8'</span> deleted
</code></pre>
<p>更多工作流 YAML 的格式见<a href="https://applatix.com/open-source/argo/docs/argo_v2_yaml.html" target="_blank">官方文档</a>和<a href="https://github.com/argoproj/argo/tree/master/examples" target="_blank">工作流示例</a>。</p>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="weave-flux" class="level3">FluxGitOps</h1>
<p>TODO: <a href="https://github.com/weaveworks/flux" target="_blank">https://github.com/weaveworks/flux</a>.</p>
</section>
                            
    <h1 class='level1'>实践案例</h1><section class="normal markdown-section">
                                
                                <h1 id="kubernetes-实践" class="level2">实践案例</h1>
<p>Kubernetes 实践及常用技巧，包括</p>
<ul>
<li><a href="../addons/monitor.html">监控</a></li>
<li><a href="../addons/logging.html">日志</a></li>
<li><a href="ha.html">高可用</a></li>
<li><a href="debugging.html">调试</a></li>
<li><a href="portmap.html">端口映射</a></li>
<li><a href="portforward.html">端口转发</a></li>
<li><a href="gpu.html">GPU</a></li>
<li><a href="security.html">安全</a></li>
<li><a href="audit.html">审计</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="kubernetes-ha" class="level3">高可用</h1>
<p>Kubernetes 从 1.5 开始，通过 <code>kops</code> 或者 <code>kube-up.sh</code> 部署的集群会自动部署一个高可用的系统，包括</p>
<ul>
<li>Etcd 集群模式</li>
<li>kube-apiserver 负载均衡</li>
<li>kube-controller-manager、kube-scheduler 和 cluster-autoscaler 自动选主（有且仅有一个运行实例）</li>
</ul>
<p>如下图所示</p>
<p><img src="images/ha.png" alt=""/></p>
<p>注意：以下步骤假设每台机器上 Kubelet 和 Docker 已配置并处于正常运行状态。</p>
<h2 id="etcd-集群">Etcd 集群</h2>
<p>安装 cfssl</p>
<pre><code class="lang-sh"><span class="hljs-comment"># On all etcd nodes</span>
curl -o /usr/<span class="hljs-built_in">local</span>/bin/cfssl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64
curl -o /usr/<span class="hljs-built_in">local</span>/bin/cfssljson https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64
chmod +x /usr/<span class="hljs-built_in">local</span>/bin/cfssl*
</code></pre>
<p>生成 CA certs:</p>
<pre><code class="lang-sh"><span class="hljs-comment"># SSH etcd0</span>
mkdir -p /etc/kubernetes/pki/etcd
<span class="hljs-built_in">cd</span> /etc/kubernetes/pki/etcd
cat >ca-config.json <<EOF
{
    <span class="hljs-string">"signing"</span>: {
        <span class="hljs-string">"default"</span>: {
            <span class="hljs-string">"expiry"</span>: <span class="hljs-string">"43800h"</span>
        },
        <span class="hljs-string">"profiles"</span>: {
            <span class="hljs-string">"server"</span>: {
                <span class="hljs-string">"expiry"</span>: <span class="hljs-string">"43800h"</span>,
                <span class="hljs-string">"usages"</span>: [
                    <span class="hljs-string">"signing"</span>,
                    <span class="hljs-string">"key encipherment"</span>,
                    <span class="hljs-string">"server auth"</span>,
                    <span class="hljs-string">"client auth"</span>
                ]
            },
            <span class="hljs-string">"client"</span>: {
                <span class="hljs-string">"expiry"</span>: <span class="hljs-string">"43800h"</span>,
                <span class="hljs-string">"usages"</span>: [
                    <span class="hljs-string">"signing"</span>,
                    <span class="hljs-string">"key encipherment"</span>,
                    <span class="hljs-string">"client auth"</span>
                ]
            },
            <span class="hljs-string">"peer"</span>: {
                <span class="hljs-string">"expiry"</span>: <span class="hljs-string">"43800h"</span>,
                <span class="hljs-string">"usages"</span>: [
                    <span class="hljs-string">"signing"</span>,
                    <span class="hljs-string">"key encipherment"</span>,
                    <span class="hljs-string">"server auth"</span>,
                    <span class="hljs-string">"client auth"</span>
                ]
            }
        }
    }
}
EOF
cat >ca-csr.json <<EOF
{
    <span class="hljs-string">"CN"</span>: <span class="hljs-string">"etcd"</span>,
    <span class="hljs-string">"key"</span>: {
        <span class="hljs-string">"algo"</span>: <span class="hljs-string">"rsa"</span>,
        <span class="hljs-string">"size"</span>: 2048
    }
}
EOF
cfssl gencert -initca ca-csr.json | cfssljson -bare ca -

<span class="hljs-comment"># generate client certs</span>
cat >client.json <<EOF
{
    <span class="hljs-string">"CN"</span>: <span class="hljs-string">"client"</span>,
    <span class="hljs-string">"key"</span>: {
        <span class="hljs-string">"algo"</span>: <span class="hljs-string">"ecdsa"</span>,
        <span class="hljs-string">"size"</span>: 256
    }
}
EOF
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=client client.json | cfssljson -bare client
</code></pre>
<p>生成 etcd server/peer certs</p>
<pre><code class="lang-sh"><span class="hljs-comment"># Copy files to other etcd nodes</span>
mkdir -p /etc/kubernetes/pki/etcd
<span class="hljs-built_in">cd</span> /etc/kubernetes/pki/etcd
scp root@<etcd0-ip-address>:/etc/kubernetes/pki/etcd/ca.pem .
scp root@<etcd0-ip-address>:/etc/kubernetes/pki/etcd/ca-key.pem .
scp root@<etcd0-ip-address>:/etc/kubernetes/pki/etcd/client.pem .
scp root@<etcd0-ip-address>:/etc/kubernetes/pki/etcd/client-key.pem .
scp root@<etcd0-ip-address>:/etc/kubernetes/pki/etcd/ca-config.json .

<span class="hljs-comment"># Run on all etcd nodes</span>
cfssl <span class="hljs-built_in">print</span>-defaults csr > config.json
sed -i <span class="hljs-string">'0,/CN/{s/example\.net/'</span><span class="hljs-string">"<span class="hljs-variable">$PEER_NAME</span>"</span><span class="hljs-string">'/}'</span> config.json
sed -i <span class="hljs-string">'s/www\.example\.net/'</span><span class="hljs-string">"<span class="hljs-variable">$PRIVATE_IP</span>"</span><span class="hljs-string">'/'</span> config.json
sed -i <span class="hljs-string">'s/example\.net/'</span><span class="hljs-string">"<span class="hljs-variable">$PUBLIC_IP</span>"</span><span class="hljs-string">'/'</span> config.json
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=server config.json | cfssljson -bare server
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=peer config.json | cfssljson -bare peer
</code></pre>
<p>最后运行 etcd，将如下的 yaml 配置写入每台 etcd 节点的 <code>/etc/kubernetes/manifests/etcd.yaml</code> 文件中，注意替换</p>
<ul>
<li><code><podname></code> 为 etcd 节点名称 （比如<code>etcd0</code>, <code>etcd1</code> 和 <code>etcd2</code>）</li>
<li><code><etcd0-ip-address></code>, <code><etcd1-ip-address></code> and <code><etcd2-ip-address></code> 为 etcd 节点的内网 IP 地址</li>
</ul>
<pre><code class="lang-sh">cat >/etc/kubernetes/manifests/etcd.yaml <<EOF
apiVersion: v1
kind: Pod
metadata:
labels:
    component: etcd
    tier: control-plane
name: <podname>
namespace: kube-system
spec:
containers:
- <span class="hljs-built_in">command</span>:
    - etcd --name <span class="hljs-variable">${PEER_NAME}</span> \
    - --data-dir /var/lib/etcd \
    - --listen-client-urls https://<span class="hljs-variable">${PRIVATE_IP}</span>:2379 \
    - --advertise-client-urls https://<span class="hljs-variable">${PRIVATE_IP}</span>:2379 \
    - --listen-peer-urls https://<span class="hljs-variable">${PRIVATE_IP}</span>:2380 \
    - --initial-advertise-peer-urls https://<span class="hljs-variable">${PRIVATE_IP}</span>:2380 \
    - --cert-file=/certs/server.pem \
    - --key-file=/certs/server-key.pem \
    - --client-cert-auth \
    - --trusted-ca-file=/certs/ca.pem \
    - --peer-cert-file=/certs/peer.pem \
    - --peer-key-file=/certs/peer-key.pem \
    - --peer-client-cert-auth \
    - --peer-trusted-ca-file=/certs/ca.pem \
    - --initial-cluster etcd0=https://<etcd0-ip-address>:2380,etcd1=https://<etcd1-ip-address>:2380,etcd1=https://<etcd2-ip-address>:2380 \
    - --initial-cluster-token my-etcd-token \
    - --initial-cluster-state new
    image: gcr.io/google_containers/etcd-amd64:3.1.0
    livenessProbe:
    httpGet:
        path: /health
        port: 2379
        scheme: HTTP
    initialDelaySeconds: 15
    timeoutSeconds: 15
    name: etcd
    env:
    - name: PUBLIC_IP
    valueFrom:
        fieldRef:
        fieldPath: status.hostIP
    - name: PRIVATE_IP
    valueFrom:
        fieldRef:
        fieldPath: status.podIP
    - name: PEER_NAME
    valueFrom:
        fieldRef:
        fieldPath: metadata.name
    volumeMounts:
    - mountPath: /var/lib/etcd
    name: etcd
    - mountPath: /certs
    name: certs
hostNetwork: <span class="hljs-literal">true</span>
volumes:
- hostPath:
    path: /var/lib/etcd
    <span class="hljs-built_in">type</span>: DirectoryOrCreate
    name: etcd
- hostPath:
    path: /etc/kubernetes/pki/etcd
    name: certs
EOF
</code></pre>
<blockquote>
<p>注意：以上方法需要每个 etcd 节点都运行 kubelet。如果不想使用 kubelet，还可以通过 systemd 的方式来启动 etcd：</p>
<pre><code class="lang-sh"><span class="hljs-built_in">export</span> ETCD_VERSION=v3.1.10
curl <span class="hljs-_">-s</span>SL https://github.com/coreos/etcd/releases/download/<span class="hljs-variable">${ETCD_VERSION}</span>/etcd-<span class="hljs-variable">${ETCD_VERSION}</span>-linux-amd64.tar.gz | tar -xzv --strip-components=1 -C /usr/<span class="hljs-built_in">local</span>/bin/
rm -rf etcd-<span class="hljs-variable">$ETCD_VERSION</span>-linux-amd64*

touch /etc/etcd.env
<span class="hljs-built_in">echo</span> <span class="hljs-string">"PEER_NAME=<span class="hljs-variable">$PEER_NAME</span>"</span> >> /etc/etcd.env
<span class="hljs-built_in">echo</span> <span class="hljs-string">"PRIVATE_IP=<span class="hljs-variable">$PRIVATE_IP</span>"</span> >> /etc/etcd.env

cat >/etc/systemd/system/etcd.service <<EOF
[Unit]
Description=etcd
Documentation=https://github.com/coreos/etcd
Conflicts=etcd.service
Conflicts=etcd2.service

[Service]
EnvironmentFile=/etc/etcd.env
Type=notify
Restart=always
RestartSec=5s
LimitNOFILE=40000
TimeoutStartSec=0

ExecStart=/usr/<span class="hljs-built_in">local</span>/bin/etcd --name <span class="hljs-variable">${PEER_NAME}</span> \
    --data-dir /var/lib/etcd \
    --listen-client-urls https://<span class="hljs-variable">${PRIVATE_IP}</span>:2379 \
    --advertise-client-urls https://<span class="hljs-variable">${PRIVATE_IP}</span>:2379 \
    --listen-peer-urls https://<span class="hljs-variable">${PRIVATE_IP}</span>:2380 \
    --initial-advertise-peer-urls https://<span class="hljs-variable">${PRIVATE_IP}</span>:2380 \
    --cert-file=/etc/kubernetes/pki/etcd/server.pem \
    --key-file=/etc/kubernetes/pki/etcd/server-key.pem \
    --client-cert-auth \
    --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.pem \
    --peer-cert-file=/etc/kubernetes/pki/etcd/peer.pem \
    --peer-key-file=/etc/kubernetes/pki/etcd/peer-key.pem \
    --peer-client-cert-auth \
    --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.pem \
    --initial-cluster etcd0=https://<etcd0-ip-address>:2380,etcd1=https://<etcd1-ip-address>:2380,etcd2=https://<etcd2-ip-address>:2380 \
    --initial-cluster-token my-etcd-token \
    --initial-cluster-state new

[Install]
WantedBy=multi-user.target
EOF

systemctl daemon-reload
systemctl start etcd
</code></pre>
</blockquote>
<h2 id="kube-apiserver">kube-apiserver</h2>
<p>把 <a href="https://kubernetes.io/docs/admin/high-availability/kube-apiserver.yaml" target="_blank">kube-apiserver.yaml</a> 放到每台 Master 节点的 <code>/etc/kubernetes/manifests/</code>，并把相关的配置放到 <code>/srv/kubernetes/</code>，即可由 kubelet 自动创建并启动 apiserver:</p>
<ul>
<li>basic_auth.csv - basic auth user and password</li>
<li>ca.crt - Certificate Authority cert</li>
<li>known_tokens.csv - tokens that entities (e.g. the kubelet) can use to talk to the apiserver</li>
<li>kubecfg.crt - Client certificate, public key</li>
<li>kubecfg.key - Client certificate, private key</li>
<li>server.cert - Server certificate, public key</li>
<li>server.key - Server certificate, private key</li>
</ul>
<blockquote>
<p>注意：确保 kube-apiserver 配置 --etcd-quorum-read=true（v1.9 之后默认为 true）。</p>
</blockquote>
<h3 id="kubeadm">kubeadm</h3>
<p>如果使用 kubeadm 来部署集群的话，可以按照如下步骤配置：</p>
<pre><code class="lang-sh"><span class="hljs-comment"># on master0</span>
<span class="hljs-comment"># deploy master0</span>
cat >config.yaml <<EOF
apiVersion: kubeadm.k8s.io/v1alpha2
kind: MasterConfiguration
kubernetesVersion: v1.11.0
apiServerCertSANs:
- <span class="hljs-string">"LOAD_BALANCER_DNS"</span>
api:
    controlPlaneEndpoint: <span class="hljs-string">"LOAD_BALANCER_DNS:LOAD_BALANCER_PORT"</span>
etcd:
  <span class="hljs-built_in">local</span>:
    extraArgs:
      listen-client-urls: <span class="hljs-string">"https://127.0.0.1:2379,https://CP0_IP:2379"</span>
      advertise-client-urls: <span class="hljs-string">"https://CP0_IP:2379"</span>
      listen-peer-urls: <span class="hljs-string">"https://CP0_IP:2380"</span>
      initial-advertise-peer-urls: <span class="hljs-string">"https://CP0_IP:2380"</span>
      initial-cluster: <span class="hljs-string">"CP0_HOSTNAME=https://CP0_IP:2380"</span>
    serverCertSANs:
      - CP0_HOSTNAME
      - CP0_IP
    peerCertSANs:
      - CP0_HOSTNAME
      - CP0_IP
networking:
    <span class="hljs-comment"># This CIDR is a Calico default. Substitute or remove for your CNI provider.</span>
    podSubnet: <span class="hljs-string">"192.168.0.0/16"</span>
EOF
kubeadm init --config=config.yaml

<span class="hljs-comment"># copy TLS certs to other master nodes</span>
CONTROL_PLANE_IPS=<span class="hljs-string">"10.0.0.7 10.0.0.8"</span>
<span class="hljs-keyword">for</span> host <span class="hljs-keyword">in</span> <span class="hljs-variable">${CONTROL_PLANE_IPS}</span>; <span class="hljs-keyword">do</span>
    scp /etc/kubernetes/pki/ca.crt <span class="hljs-string">"<span class="hljs-variable">${USER}</span>"</span>@<span class="hljs-variable">$host</span>:
    scp /etc/kubernetes/pki/ca.key <span class="hljs-string">"<span class="hljs-variable">${USER}</span>"</span>@<span class="hljs-variable">$host</span>:
    scp /etc/kubernetes/pki/sa.key <span class="hljs-string">"<span class="hljs-variable">${USER}</span>"</span>@<span class="hljs-variable">$host</span>:
    scp /etc/kubernetes/pki/sa.pub <span class="hljs-string">"<span class="hljs-variable">${USER}</span>"</span>@<span class="hljs-variable">$host</span>:
    scp /etc/kubernetes/pki/front-proxy-ca.crt <span class="hljs-string">"<span class="hljs-variable">${USER}</span>"</span>@<span class="hljs-variable">$host</span>:
    scp /etc/kubernetes/pki/front-proxy-ca.key <span class="hljs-string">"<span class="hljs-variable">${USER}</span>"</span>@<span class="hljs-variable">$host</span>:
    scp /etc/kubernetes/pki/etcd/ca.crt <span class="hljs-string">"<span class="hljs-variable">${USER}</span>"</span>@<span class="hljs-variable">$host</span>:etcd-ca.crt
    scp /etc/kubernetes/pki/etcd/ca.key <span class="hljs-string">"<span class="hljs-variable">${USER}</span>"</span>@<span class="hljs-variable">$host</span>:etcd-ca.key
    scp /etc/kubernetes/admin.conf <span class="hljs-string">"<span class="hljs-variable">${USER}</span>"</span>@<span class="hljs-variable">$host</span>:
<span class="hljs-keyword">done</span>


<span class="hljs-comment"># on other master nodes</span>
cat > kubeadm-config.yaml <<EOF
apiVersion: kubeadm.k8s.io/v1alpha2
kind: MasterConfiguration
kubernetesVersion: v1.11.0
apiServerCertSANs:
- <span class="hljs-string">"LOAD_BALANCER_DNS"</span>
api:
    controlPlaneEndpoint: <span class="hljs-string">"LOAD_BALANCER_DNS:LOAD_BALANCER_PORT"</span>
etcd:
  <span class="hljs-built_in">local</span>:
    extraArgs:
      listen-client-urls: <span class="hljs-string">"https://127.0.0.1:2379,https://CP1_IP:2379"</span>
      advertise-client-urls: <span class="hljs-string">"https://CP1_IP:2379"</span>
      listen-peer-urls: <span class="hljs-string">"https://CP1_IP:2380"</span>
      initial-advertise-peer-urls: <span class="hljs-string">"https://CP1_IP:2380"</span>
      initial-cluster: <span class="hljs-string">"CP0_HOSTNAME=https://CP0_IP:2380,CP1_HOSTNAME=https://CP1_IP:2380"</span>
      initial-cluster-state: existing
    serverCertSANs:
      - CP1_HOSTNAME
      - CP1_IP
    peerCertSANs:
      - CP1_HOSTNAME
      - CP1_IP
networking:
    <span class="hljs-comment"># This CIDR is a calico default. Substitute or remove for your CNI provider.</span>
    podSubnet: <span class="hljs-string">"192.168.0.0/16"</span>
EOF
<span class="hljs-comment"># move files</span>
mkdir -p /etc/kubernetes/pki/etcd
mv /home/<span class="hljs-variable">${USER}</span>/ca.crt /etc/kubernetes/pki/
mv /home/<span class="hljs-variable">${USER}</span>/ca.key /etc/kubernetes/pki/
mv /home/<span class="hljs-variable">${USER}</span>/sa.pub /etc/kubernetes/pki/
mv /home/<span class="hljs-variable">${USER}</span>/sa.key /etc/kubernetes/pki/
mv /home/<span class="hljs-variable">${USER}</span>/front-proxy-ca.crt /etc/kubernetes/pki/
mv /home/<span class="hljs-variable">${USER}</span>/front-proxy-ca.key /etc/kubernetes/pki/
mv /home/<span class="hljs-variable">${USER}</span>/etcd-ca.crt /etc/kubernetes/pki/etcd/ca.crt
mv /home/<span class="hljs-variable">${USER}</span>/etcd-ca.key /etc/kubernetes/pki/etcd/ca.key
mv /home/<span class="hljs-variable">${USER}</span>/admin.conf /etc/kubernetes/admin.conf
<span class="hljs-comment"># Run the kubeadm phase commands to bootstrap the kubelet:</span>
kubeadm alpha phase certs all --config kubeadm-config.yaml
kubeadm alpha phase kubelet config write-to-disk --config kubeadm-config.yaml
kubeadm alpha phase kubelet write-env-file --config kubeadm-config.yaml
kubeadm alpha phase kubeconfig kubelet --config kubeadm-config.yaml
systemctl start kubelet
<span class="hljs-comment"># Add the node to etcd cluster</span>
CP0_IP=10.0.0.7
CP0_HOSTNAME=cp0
CP1_IP=10.0.0.8
CP1_HOSTNAME=cp1
KUBECONFIG=/etc/kubernetes/admin.conf kubectl <span class="hljs-built_in">exec</span> -n kube-system etcd-<span class="hljs-variable">${CP0_HOSTNAME}</span> -- etcdctl --ca-file /etc/kubernetes/pki/etcd/ca.crt --cert-file /etc/kubernetes/pki/etcd/peer.crt --key-file /etc/kubernetes/pki/etcd/peer.key --endpoints=https://<span class="hljs-variable">${CP0_IP}</span>:2379 member add <span class="hljs-variable">${CP1_HOSTNAME}</span> https://<span class="hljs-variable">${CP1_IP}</span>:2380
kubeadm alpha phase etcd <span class="hljs-built_in">local</span> --config kubeadm-config.yaml
<span class="hljs-comment"># Deploy the master components</span>
kubeadm alpha phase kubeconfig all --config kubeadm-config.yaml
kubeadm alpha phase controlplane all --config kubeadm-config.yaml
kubeadm alpha phase mark-master --config kubeadm-config.yaml
</code></pre>
<p>kube-apiserver 启动后，还需要为它们做负载均衡，可以使用云平台的弹性负载均衡服务或者使用 haproxy/lvs 等为 master 节点配置负载均衡。</p>
<h2 id="kube-controller-manager-和-kube-scheduler">kube-controller-manager 和 kube-scheduler</h2>
<p>kube-controller manager 和 kube-scheduler 需要保证任何时刻都只有一个实例运行，需要一个选主的过程，所以在启动时要设置 <code>--leader-elect=true</code>，比如</p>
<pre><code>kube-scheduler --master=127.0.0.1:8080 --v=2 --leader-elect=true
kube-controller-manager --master=127.0.0.1:8080 --cluster-cidr=10.245.0.0/16 --allocate-node-cidrs=true --service-account-private-key-file=/srv/kubernetes/server.key --v=2 --leader-elect=true
</code></pre><p>把  <a href="https://kubernetes.io/docs/admin/high-availability/kube-scheduler.yaml" target="_blank">kube-scheduler.yaml</a> 和 <a href="https://kubernetes.io/docs/admin/high-availability/kube-controller-manager.yaml" target="_blank">kube-controller-manager.yaml</a>  放到每台 master 节点的 <code>/etc/kubernetes/manifests/</code> 即可。</p>
<h2 id="kube-dns">kube-dns</h2>
<p>kube-dns 可以通过 Deployment 的方式来部署，默认 kubeadm 会自动创建。但在大规模集群的时候，需要放宽资源限制，比如</p>
<pre><code>dns_replicas: 6
dns_cpu_limit: 100m
dns_memory_limit: 512Mi
dns_cpu_requests 70m
dns_memory_requests: 70Mi
</code></pre><p>另外，也需要给 dnsmasq 增加资源，比如增加缓存大小到 10000，增加并发处理数量 <code>--dns-forward-max=1000</code> 等。</p>
<h2 id="kube-proxy">kube-proxy</h2>
<p>默认 kube-proxy 使用 iptables 来为 Service 作负载均衡，这在大规模时会产生很大的 Latency，可以考虑使用 <a href="https://docs.google.com/presentation/d/1BaIAywY2qqeHtyGZtlyAp89JIZs59MZLKcFLxKE6LyM/edit#slide=id.p3" target="_blank">IPVS</a> 的替代方式（注意 IPVS 在 v1.9 中还是 beta 状态）。</p>
<p>另外，需要注意配置 kube-proxy 使用 kube-apiserver 负载均衡的 IP 地址：</p>
<pre><code class="lang-sh">kubectl get configmap -n kube-system kube-proxy -o yaml > kube-proxy-сm.yaml
sed -i <span class="hljs-string">'s#server:.*#server: https://<masterLoadBalancerFQDN>:6443#g'</span> kube-proxy-cm.yaml
kubectl apply <span class="hljs-_">-f</span> kube-proxy-cm.yaml --force
<span class="hljs-comment"># restart all kube-proxy pods to ensure that they load the new configmap</span>
kubectl delete pod -n kube-system <span class="hljs-_">-l</span> k8s-app=kube-proxy
</code></pre>
<h2 id="kubelet">kubelet</h2>
<p>kubelet 需要配置 kube-apiserver 负载均衡的 IP 地址</p>
<pre><code class="lang-sh">sudo sed -i <span class="hljs-string">'s#server:.*#server: https://<masterLoadBalancerFQDN>:6443#g'</span> /etc/kubernetes/kubelet.conf
sudo systemctl restart kubelet
</code></pre>
<h2 id="数据持久化">数据持久化</h2>
<p>除了上面提到的这些配置，持久化存储也是高可用 Kubernetes 集群所必须的。</p>
<ul>
<li>对于公有云上部署的集群，可以考虑使用云平台提供的持久化存储，比如 aws ebs 或者 gce persistent disk</li>
<li>对于物理机部署的集群，可以考虑使用 iSCSI、NFS、Gluster 或者 Ceph 等网络存储，也可以使用 RAID</li>
</ul>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://kubernetes.io/docs/setup/independent/high-availability/" target="_blank">Creating Highly Available Clusters with kubeadm</a></li>
<li><a href="http://kubecloud.io/setup-ha-k8s-kops/" target="_blank">http://kubecloud.io/setup-ha-k8s-kops/</a></li>
<li><a href="https://github.com/coreos/etcd/blob/master/Documentation/op-guide/clustering.md" target="_blank">https://github.com/coreos/etcd/blob/master/Documentation/op-guide/clustering.md</a></li>
<li><a href="http://fuel-ccp.readthedocs.io/en/latest/design/k8s_1000_nodes_architecture.html" target="_blank">Kubernetes Master Tier For 1000 Nodes Scale</a></li>
<li><a href="https://docs.google.com/presentation/d/1BaIAywY2qqeHtyGZtlyAp89JIZs59MZLKcFLxKE6LyM/edit#slide=id.p3" target="_blank">Scaling Kubernetes to Support 50000 Services</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="调试运行中的容器" class="level3">调试</h1>
<p>对于普通的服务器进程，我们可以很方便的使用宿主机上的各种工具来调试；但容器经常是仅包含必要的应用程序，一般不包含常用的调试工具，那如何在线调试容器中的进程呢？最简单的方法是再起一个新的包含了调试工具的容器。</p>
<p>来看一个最简单的 web 容器如何调试。</p>
<h3 id="webserver-容器">webserver 容器</h3>
<p>用 Go 编写一个最简单的 webserver：</p>
<pre><code class="lang-go"><span class="hljs-comment">// go-examples/basic/webserver</span>
<span class="hljs-keyword">package</span> main

<span class="hljs-keyword">import</span> <span class="hljs-string">"net/http"</span>
<span class="hljs-keyword">import</span> <span class="hljs-string">"fmt"</span>
<span class="hljs-keyword">import</span> <span class="hljs-string">"log"</span>

<span class="hljs-keyword">func</span> index(w http.ResponseWriter, r *http.Request) {
    fmt.Fprintln(w, <span class="hljs-string">"Hello World"</span>)
}

<span class="hljs-keyword">func</span> main() {
    http.HandleFunc(<span class="hljs-string">"/"</span>, index)
    err := http.ListenAndServe(<span class="hljs-string">":80"</span>, <span class="hljs-literal">nil</span>)
    <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> {

        log.Println(err)
    }
}
</code></pre>
<p>以 linux 平台方式编译</p>
<pre><code class="lang-sh">GOOS=linux go build -o webserver
</code></pre>
<p>然后用下面的 Docker build 一个 docker 镜像：</p>
<pre><code>FROM scratch

COPY ./webserver /
CMD ["/webserver"]
</code></pre><pre><code class="lang-sh"><span class="hljs-comment"># docker build -t feisky/hello-world .</span>
Sending build context to Docker daemon 5.655 MB
Step 1/3 : FROM scratch
 --->
Step 2/3 : COPY ./webserver /
 ---> 184eb7c074b5
Removing intermediate container abf107844295
Step 3/3 : CMD /webserver
 ---> Running <span class="hljs-keyword">in</span> fe9fa4841e70
 ---> dca5ec00b3e7
Removing intermediate container fe9fa4841e70
Successfully built dca5ec00b3e7
</code></pre>
<p>最后启动 webserver 容器</p>
<pre><code class="lang-sh">docker run -itd --name webserver -p 80:80 feisky/hello-world
</code></pre>
<p>访问映射后的 80 端口，webserver 容器正常返回 "Hello World"</p>
<pre><code class="lang-sh"><span class="hljs-comment"># curl http://$(hostname):80</span>
Hello World
</code></pre>
<h3 id="新建一个容器调试-webserver">新建一个容器调试 webserver</h3>
<p>用一个包含调试工具或者方便安装调试工具的镜像（如 alpine）创建一个新的 container，为了便于获取 webserver 进程的状态，新的容器共享 webserver 容器的 pid namespace 和 net namespace，并增加必要的 capability：</p>
<pre><code class="lang-sh">docker run -it --rm --pid=container:webserver --net=container:webserver --cap-add sys_admin --cap-add sys_ptrace alpine sh
/ <span class="hljs-comment"># ps -ef</span>
PID   USER     TIME   COMMAND
    1 root       0:00 /webserver
   13 root       0:00 sh
   18 root       0:00 ps -ef
</code></pre>
<p>这样，新的容器可以直接 attach 到 webserver 进程上来在线调试，比如 strace 到 webserver 进程</p>
<pre><code class="lang-sh"><span class="hljs-comment"># 继续在刚创建的新容器 sh 中执行</span>
/ <span class="hljs-comment"># apk update && apk add strace</span>
fetch http://dl-cdn.alpinelinux.org/alpine/v3.5/main/x86_64/APKINDEX.tar.gz
fetch http://dl-cdn.alpinelinux.org/alpine/v3.5/community/x86_64/APKINDEX.tar.gz
v3.5.1-34-g1d3b13bd53 [http://dl-cdn.alpinelinux.org/alpine/v3.5/main]
v3.5.1-29-ga981b1f149 [http://dl-cdn.alpinelinux.org/alpine/v3.5/community]
OK: 7958 distinct packages available
(1/1) Installing strace (4.14-r0)
Executing busybox-1.25.1-r0.trigger
OK: 5 MiB <span class="hljs-keyword">in</span> 12 packages
/ <span class="hljs-comment"># strace -p 1</span>
strace: Process 1 attached
epoll_<span class="hljs-built_in">wait</span>(4,
^Cstrace: Process 1 detached
 <detached ...>
</code></pre>
<p>也可以获取 webserver 容器的网络状态</p>
<pre><code class="lang-sh"><span class="hljs-comment"># 继续在刚创建的新容器 sh 中执行</span>
/ <span class="hljs-comment"># apk add lsof</span>
(1/1) Installing lsof (4.89-r0)
Executing busybox-1.25.1-r0.trigger
OK: 5 MiB <span class="hljs-keyword">in</span> 13 packages
/ <span class="hljs-comment"># lsof -i TCP</span>
COMMAND   PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME
webserver   1 root    3u  IPv6  14233      0t0  TCP *:http (LISTEN)
</code></pre>
<p>当然，也可以访问 webserver 容器的文件系统</p>
<pre><code class="lang-sh">/ <span class="hljs-comment"># ls -l /proc/1/root/</span>
total 5524
drwxr-xr-x    5 root     root           360 Feb 14 13:16 dev
drwxr-xr-x    2 root     root          4096 Feb 14 13:16 etc
dr-xr-xr-x  128 root     root             0 Feb 14 13:16 proc
dr-xr-xr-x   13 root     root             0 Feb 14 13:16 sys
-rwxr-xr-x    1 root     root       5651357 Feb 14 13:15 webserver
</code></pre>
<p>Kubernetes 社区也在提议增加一个 <code>kubectl debug</code> 命令，用类似的方式在 Pod 中启动一个新容器来调试运行中的进程，可以参见 <a href="https://github.com/kubernetes/community/pull/649" target="_blank">https://github.com/kubernetes/community/pull/649</a>。</p>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="端口映射" class="level3">端口映射</h1>
<p>在创建 Pod 时，可以指定容器的 hostPort 和 containerPort 来创建端口映射，这样可以通过 Pod 所在 Node 的 IP:hostPort 来访问服务。比如</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> nginx
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">  - image:</span> nginx
<span class="hljs-attr">    name:</span> nginx
<span class="hljs-attr">    ports:</span>
<span class="hljs-attr">    - containerPort:</span> <span class="hljs-number">80</span>
<span class="hljs-attr">      hostPort:</span> <span class="hljs-number">80</span>
<span class="hljs-attr">  restartPolicy:</span> Always
</code></pre>
<h2 id="注意事项">注意事项</h2>
<p>使用了 hostPort 的容器只能调度到端口不冲突的 Node 上，除非有必要（比如运行一些系统级的 daemon 服务），不建议使用端口映射功能。如果需要对外暴露服务，建议使用 <a href="../concepts/service.html#Service">NodePort Service</a>。</p>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="端口转发" class="level3">端口转发</h1>
<p>端口转发是 kubectl 的一个子命令，通过 <code>kubectl port-forward</code> 可以将本地端口转发到指定的 Pod。</p>
<h2 id="pod-端口转发">Pod 端口转发</h2>
<p>可以将本地端口转发到指定 Pod 的端口。</p>
<pre><code class="lang-sh"><span class="hljs-comment"># Listen on ports 5000 and 6000 locally, forwarding data to/from ports 5000 and 6000 in the pod</span>
kubectl port-forward mypod 5000 6000

<span class="hljs-comment"># Listen on port 8888 locally, forwarding to 5000 in the pod</span>
kubectl port-forward mypod 8888:5000

<span class="hljs-comment"># Listen on a random port locally, forwarding to 5000 in the pod</span>
kubectl port-forward mypod :5000

<span class="hljs-comment"># Listen on a random port locally, forwarding to 5000 in the pod</span>
kubectl port-forward mypod 0:5000
</code></pre>
<h2 id="服务端口转发">服务端口转发</h2>
<p>也可以将本地端口转发到服务、复制控制器或者部署的端口。</p>
<pre><code class="lang-sh"><span class="hljs-comment"># Forward to deployment</span>
kubectl port-forward deployment/redis-master 6379:6379

<span class="hljs-comment"># Forward to replicaSet</span>
kubectl port-forward rs/redis-master 6379:6379

<span class="hljs-comment"># Forward to service</span>
kubectl port-forward svc/redis-master 6379:6379
</code></pre>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="hugepage" class="level3">HugePage</h1>
<p>HugePage 是 v1.9 中引入的新特性（v1.9 Alpha，v1.10 Beta），允许在容器中直接使用 Node 上的 HugePage。</p>
<h2 id="配置">配置</h2>
<ul>
<li><code>--feature-gates=HugePages=true</code></li>
<li>Node 节点上预先分配好 HugePage，如</li>
</ul>
<pre><code class="lang-sh">mount -t hugetlbfs \
    -o uid=<value>,gid=<value>,mode=<value>,pagesize=<value>,size=<value>,\
    min_size=<value>,nr_inodes=<value> none /mnt/huge
</code></pre>
<h2 id="使用">使用</h2>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  generateName:</span> hugepages-volume-
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">  - image:</span> fedora:latest
<span class="hljs-attr">    command:</span>
<span class="hljs-bullet">    -</span> sleep
<span class="hljs-bullet">    -</span> inf
<span class="hljs-attr">    name:</span> example
<span class="hljs-attr">    volumeMounts:</span>
<span class="hljs-attr">    - mountPath:</span> /hugepages
<span class="hljs-attr">      name:</span> hugepage
<span class="hljs-attr">    resources:</span>
<span class="hljs-attr">      limits:</span>
<span class="hljs-attr">        hugepages-2Mi:</span> <span class="hljs-number">100</span>Mi
<span class="hljs-attr">  volumes:</span>
<span class="hljs-attr">  - name:</span> hugepage
<span class="hljs-attr">    emptyDir:</span>
<span class="hljs-attr">      medium:</span> HugePages
</code></pre>
<p>注意</p>
<ul>
<li>HugePage 请求和限制必须相等</li>
<li>HugePage 提供 Pod 级别的隔离，暂不支持容器级别的隔离</li>
<li>基于 HugePage 的 EmptyDir 存储卷仅可使用请求的 HugePage 内存</li>
<li>可以通过 ResourceQuota 限制 HugePage 的用量</li>
<li>容器应用内使用 <code>shmget(SHM_HUGETLB)</code> 获取 HugePage 时，必需配置与 <code>proc/sys/vm/hugetlb_shm_group</code> 中一致的用户组（<code>securityContext.SupplementalGroups</code>）</li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="容器安全" class="level3">安全</h1>
<p>Kubernetes 提供了多种机制来限制容器的行为，减少容器攻击面，保证系统安全性。</p>
<ul>
<li>Security Context：限制容器的行为，包括 Capabilities、ReadOnlyRootFilesystem、Privileged、RunAsNonRoot、RunAsUser 以及 SELinuxOptions 等</li>
<li>Pod Security Policy：集群级的 Pod 安全策略，自动为集群内的 Pod 和 Volume 设置 Security Context</li>
<li>Sysctls：允许容器设置内核参数，分为安全 Sysctls 和非安全 Sysctls</li>
<li>AppArmor：限制应用的访问权限</li>
<li>Seccomp：Secure computing mode 的缩写，限制容器应用可执行的系统调用</li>
</ul>
<h2 id="security-context-和-pod-security-policy">Security Context 和 Pod Security Policy</h2>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> extensions/v1beta1
<span class="hljs-attr">kind:</span> PodSecurityPolicy
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> restricted
<span class="hljs-attr">  annotations:</span>
    <span class="hljs-comment"># Seccomp v1.11 使用 'runtime/default'，而 v1.10 及更早版本使用 'docker/default'</span>
    seccomp.security.alpha.kubernetes.io/allowedProfileNames: <span class="hljs-string">'runtime/default'</span>
    seccomp.security.alpha.kubernetes.io/defaultProfileName:  <span class="hljs-string">'runtime/default'</span>
    apparmor.security.beta.kubernetes.io/allowedProfileNames: <span class="hljs-string">'runtime/default'</span>
    apparmor.security.beta.kubernetes.io/defaultProfileName:  <span class="hljs-string">'runtime/default'</span>
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  privileged:</span> <span class="hljs-literal">false</span>
  <span class="hljs-comment"># Required to prevent escalations to root.</span>
<span class="hljs-attr">  allowPrivilegeEscalation:</span> <span class="hljs-literal">false</span>
  <span class="hljs-comment"># This is redundant with non-root + disallow privilege escalation,</span>
  <span class="hljs-comment"># but we can provide it for defense in depth.</span>
<span class="hljs-attr">  requiredDropCapabilities:</span>
<span class="hljs-bullet">    -</span> ALL
  <span class="hljs-comment"># Allow core volume types.</span>
<span class="hljs-attr">  volumes:</span>
<span class="hljs-bullet">    -</span> <span class="hljs-string">'configMap'</span>
<span class="hljs-bullet">    -</span> <span class="hljs-string">'emptyDir'</span>
<span class="hljs-bullet">    -</span> <span class="hljs-string">'projected'</span>
<span class="hljs-bullet">    -</span> <span class="hljs-string">'secret'</span>
<span class="hljs-bullet">    -</span> <span class="hljs-string">'downwardAPI'</span>
    <span class="hljs-comment"># Assume that persistentVolumes set up by the cluster admin are safe to use.</span>
<span class="hljs-bullet">    -</span> <span class="hljs-string">'persistentVolumeClaim'</span>
<span class="hljs-attr">  hostNetwork:</span> <span class="hljs-literal">false</span>
<span class="hljs-attr">  hostIPC:</span> <span class="hljs-literal">false</span>
<span class="hljs-attr">  hostPID:</span> <span class="hljs-literal">false</span>
<span class="hljs-attr">  runAsUser:</span>
    <span class="hljs-comment"># Require the container to run without root privileges.</span>
<span class="hljs-attr">    rule:</span> <span class="hljs-string">'MustRunAsNonRoot'</span>
<span class="hljs-attr">  seLinux:</span>
    <span class="hljs-comment"># This policy assumes the nodes are using AppArmor rather than SELinux.</span>
<span class="hljs-attr">    rule:</span> <span class="hljs-string">'RunAsAny'</span>
<span class="hljs-attr">  supplementalGroups:</span>
<span class="hljs-attr">    rule:</span> <span class="hljs-string">'MustRunAs'</span>
<span class="hljs-attr">    ranges:</span>
      <span class="hljs-comment"># Forbid adding the root group.</span>
<span class="hljs-attr">      - min:</span> <span class="hljs-number">1</span>
<span class="hljs-attr">        max:</span> <span class="hljs-number">65535</span>
<span class="hljs-attr">  fsGroup:</span>
<span class="hljs-attr">    rule:</span> <span class="hljs-string">'MustRunAs'</span>
<span class="hljs-attr">    ranges:</span>
      <span class="hljs-comment"># Forbid adding the root group.</span>
<span class="hljs-attr">      - min:</span> <span class="hljs-number">1</span>
<span class="hljs-attr">        max:</span> <span class="hljs-number">65535</span>
<span class="hljs-attr">  readOnlyRootFilesystem:</span> <span class="hljs-literal">false</span>
</code></pre>
<p>完整参考见<a href="../concepts/security-context.html">这里</a>。</p>
<h2 id="sysctls">Sysctls</h2>
<p>Sysctls 允许容器设置内核参数，分为安全 Sysctls 和非安全 Sysctls</p>
<ul>
<li>安全 Sysctls：即设置后不影响其他 Pod 的内核选项，只作用在容器 namespace 中，默认开启。包括以下几种<ul>
<li><code>kernel.shm_rmid_forced</code></li>
<li><code>net.ipv4.ip_local_port_range</code></li>
<li><code>net.ipv4.tcp_syncookies</code></li>
</ul>
</li>
<li>非安全 Sysctls：即设置好有可能影响其他 Pod 和 Node 上其他服务的内核选项，默认禁止。如果使用，需要管理员在配置 kubelet 时开启，如 <code>kubelet --experimental-allowed-unsafe-sysctls 'kernel.msg*,net.ipv4.route.min_pmtu'</code></li>
</ul>
<p>Sysctls 在 v1.11 升级为 Beta 版，可以通过 PSP spec 直接设置，如</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> policy/v1beta1
<span class="hljs-attr">kind:</span> PodSecurityPolicy
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> sysctl-psp
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  allowedUnsafeSysctls:</span>
<span class="hljs-bullet">  -</span> kernel.msg*
<span class="hljs-attr">  forbiddenSysctls:</span>
<span class="hljs-bullet">  -</span> kernel.shm_rmid_forced
</code></pre>
<p>而 v1.10 及更早版本则为 Alpha 阶段，需要通过 Pod annotation 设置，如：</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> sysctl-example
<span class="hljs-attr">  annotations:</span>
    security.alpha.kubernetes.io/sysctls: kernel.shm_rmid_forced=<span class="hljs-number">1</span>
    security.alpha.kubernetes.io/unsafe-sysctls: net.ipv4.route.min_pmtu=<span class="hljs-number">1000</span>,kernel.msgmax=<span class="hljs-number">1</span> <span class="hljs-number">2</span> <span class="hljs-number">3</span>
<span class="hljs-attr">spec:</span>
  ...
</code></pre>
<h2 id="apparmor">AppArmor</h2>
<p><a href="http://wiki.apparmor.net/index.php/AppArmor_Core_Policy_Reference" target="_blank">AppArmor(Application Armor)</a> 是 Linux 内核的一个安全模块，允许系统管理员将每个程序与一个安全配置文件关联，从而限制程序的功能。通过它你可以指定程序可以读、写或运行哪些文件，是否可以打开网络端口等。作为对传统 Unix 的自主访问控制模块的补充，AppArmor 提供了强制访问控制机制。</p>
<p>在使用 AppArmor 之前需要注意</p>
<ul>
<li>Kubernetes 版本 >=v1.4</li>
<li>apiserver 和 kubelet 已开启 AppArmor 特性，<code>--feature-gates=AppArmor=true</code></li>
<li>已开启 apparmor 内核模块，通过 <code>cat /sys/module/apparmor/parameters/enabled</code> 查看</li>
<li>仅支持 docker container runtime</li>
<li>AppArmor profile 已经加载到内核，通过 <code>cat /sys/kernel/security/apparmor/profiles</code> 查看</li>
</ul>
<p>AppArmor 还在 alpha 阶段，需要通过 Pod annotation <code>container.apparmor.security.beta.kubernetes.io/<container_name></code> 来设置。可选的值包括</p>
<ul>
<li><code>runtime/default</code>: 使用 Container Runtime 的默认配置</li>
<li><code>localhost/<profile_name></code>: 使用已加载到内核的 AppArmor profile</li>
</ul>
<pre><code class="lang-sh">$ sudo apparmor_parser -q <<EOF
<span class="hljs-comment">#include <tunables/global></span>

profile k8s-apparmor-example-deny-write flags=(attach_disconnected) {
  <span class="hljs-comment">#include <abstractions/base></span>

  file,

  <span class="hljs-comment"># Deny all file writes.</span>
  deny /** w,
}
EOF<span class="hljs-string">'

$ kubectl create -f /dev/stdin <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: hello-apparmor
  annotations:
    container.apparmor.security.beta.kubernetes.io/hello: localhost/k8s-apparmor-example-deny-write
spec:
  containers:
  - name: hello
    image: busybox
    command: ["sh", "-c", "echo'</span>Hello AppArmor!<span class="hljs-string">'&& sleep 1h"]
EOF
pod "hello-apparmor" created

$ kubectl exec hello-apparmor cat /proc/1/attr/current
k8s-apparmor-example-deny-write (enforce)

$ kubectl exec hello-apparmor touch /tmp/test
touch: /tmp/test: Permission denied
error: error executing remote command: command terminated with non-zero exit code: Error executing in Docker Container: 1
</span></code></pre>
<h2 id="seccomp">Seccomp</h2>
<p><a href="https://www.kernel.org/doc/Documentation/prctl/seccomp_filter.txt" target="_blank">Seccomp</a> 是 Secure computing mode 的缩写，它是 Linux 内核提供的一个操作，用于限制一个进程可以执行的系统调用．Seccomp 需要有一个配置文件来指明容器进程允许和禁止执行的系统调用。</p>
<p>在 Kubernetes 中，需要将 seccomp 配置文件放到 <code>/var/lib/kubelet/seccomp</code> 目录中（可以通过 kubelet 选项 <code>--seccomp-profile-root</code> 修改）。比如禁止 chmod 的格式为</p>
<pre><code class="lang-sh">$ cat /var/lib/kubelet/seccomp/chmod.json
{
    <span class="hljs-string">"defaultAction"</span>: <span class="hljs-string">"SCMP_ACT_ALLOW"</span>,
    <span class="hljs-string">"syscalls"</span>: [
        {
            <span class="hljs-string">"name"</span>: <span class="hljs-string">"chmod"</span>,
            <span class="hljs-string">"action"</span>: <span class="hljs-string">"SCMP_ACT_ERRNO"</span>
        }
    ]
}
</code></pre>
<p>Seccomp 还在 alpha 阶段，需要通过 Pod annotation 设置，包括</p>
<ul>
<li><code>security.alpha.kubernetes.io/seccomp/pod</code>：应用到该 Pod 的所有容器</li>
<li><code>security.alpha.kubernetes.io/seccomp/container/<container name></code>：应用到指定容器</li>
</ul>
<p>而 value 有三个选项</p>
<ul>
<li><code>runtime/default</code>: 使用 Container Runtime 的默认配置</li>
<li><code>unconfined</code>: 允许所有系统调用</li>
<li><code>localhost/<profile-name></code>: 使用 Node 本地安装的 seccomp，需要放到 <code>/var/lib/kubelet/seccomp</code> 目录中</li>
</ul>
<p>比如使用刚才创建的 seccomp 配置：</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Pod
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> trustworthy-pod
<span class="hljs-attr">  annotations:</span>
    seccomp.security.alpha.kubernetes.io/pod: localhost/chmod
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">    - name:</span> trustworthy-container
<span class="hljs-attr">      image:</span> sotrustworthy:latest
</code></pre>
<h2 id="kube-bench">kube-bench</h2>
<p><a href="https://github.com/aquasecurity/kube-bench" target="_blank">kube-bench</a> 提供了一个简单的工具来检查 Kubernetes 的配置（包括 master 和 node）是否符合最佳的安全实践（基于 <a href="https://www.cisecurity.org/benchmark/kubernetes/" target="_blank">CIS Kubernetes Benchmark</a>）。</p>
<p><strong>推荐所有生产环境的 Kubernetes 集群定期运行 kube-bench，保证集群配置符合最佳的安全实践。</strong></p>
<p>安装 <code>kube-bench</code>：</p>
<pre><code class="lang-sh">$ docker run --rm -v `<span class="hljs-built_in">pwd</span>`:/host aquasec/kube-bench:latest install
$ ./kube-bench <master|node>
</code></pre>
<p>当然，kube-bench 也可以直接在容器内运行，比如通常对 Master 和 Node 的检查命令分别为：</p>
<pre><code class="lang-sh"><span class="hljs-comment"># Run master check</span>
$ kubectl run --rm -i -t kube-bench-master --image=aquasec/kube-bench:latest --restart=Never --overrides=<span class="hljs-string">"{ \"apiVersion\": \"v1\", \"spec\": { \"hostPID\": true, \"nodeSelector\": { \"kubernetes.io/role\": \"master\" }, \"tolerations\": [ { \"key\": \"node-role.kubernetes.io/master\", \"operator\": \"Exists\", \"effect\": \"NoSchedule\" } ] } }"</span> -- master --version 1.8

<span class="hljs-comment"># Run node check</span>
kubectl run --rm -i -t kube-bench-node --image=aquasec/kube-bench:latest --restart=Never --overrides=<span class="hljs-string">"{ \"apiVersion\": \"v1\", \"spec\": { \"hostPID\": true } }"</span> -- node --version 1.8
</code></pre>
<h2 id="镜像安全">镜像安全</h2>
<p><a href="https://github.com/coreos/clair/" target="_blank">Clair</a> 是 CoreOS 开源的容器安全工具，用来静态分析镜像中潜在的安全问题。推荐将 Clair 集成到 Devops 流程中，自动对所有镜像进行安全扫描。</p>
<p>安装 Clair 的方法为：</p>
<pre><code class="lang-sh">git <span class="hljs-built_in">clone</span> https://github.com/coreos/clair
<span class="hljs-built_in">cd</span> clair/contrib/helm
helm dependency update clair
helm install clair
</code></pre>
<p>Clair 项目本身只提供了 API，在实际使用中还需要一个<a href="https://github.com/coreos/clair/blob/master/Documentation/integrations.md" target="_blank">客户端（或集成Clair的服务）</a>配合使用。比如，使用 <a href="https://github.com/genuinetools/reg" target="_blank">reg</a> 的方法为</p>
<pre><code class="lang-sh"><span class="hljs-comment"># Install</span>
$ go get github.com/genuinetools/reg

<span class="hljs-comment"># Vulnerability Reports</span>
$ reg vulns --clair https://clair.j3ss.co r.j3ss.co/chrome

<span class="hljs-comment"># Generating Static Website for a Registry</span>
$ $ reg server --clair https://clair.j3ss.co
</code></pre>
<h2 id="其他安全工具">其他安全工具</h2>
<p>开源产品：</p>
<ul>
<li><a href="https://github.com/falcosecurity/falco" target="_blank">falco</a>：容器运行时安全行为监控工具。</li>
<li><a href="https://github.com/docker/docker-bench-security" target="_blank">docker-bench-security</a>：Docker 环境安全检查工具。</li>
<li><a href="https://github.com/aquasecurity/kube-hunter" target="_blank">kube-hunter</a>：Kubernetes 集群渗透测试工具。</li>
</ul>
<p>商业产品</p>
<ul>
<li><a href="https://www.twistlock.com/" target="_blank">Twistlock</a></li>
<li><a href="https://www.aquasec.com/" target="_blank">Aqua Container Security Platform</a></li>
<li><a href="https://sysdig.com/products/secure/" target="_blank">Sysdig Secure</a></li>
</ul>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/" target="_blank">Securing a Kubernetes cluster</a></li>
<li><a href="https://github.com/aquasecurity/kube-bench" target="_blank">kube-bench</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="kubernetes-审计" class="level3">审计</h1>
<p>Kubernetes 审计（Audit）提供了安全相关的时序操作记录，支持日志和 webhook 两种格式，并可以通过审计策略自定义事件类型。</p>
<h2 id="审计日志">审计日志</h2>
<p>通过配置 kube-apiserver 的下列参数开启审计日志</p>
<ul>
<li>audit-log-path：审计日志路径</li>
<li>audit-log-maxage：旧日志最长保留天数</li>
<li>audit-log-maxbackup：旧日志文件最多保留个数</li>
<li>audit-log-maxsize：日志文件最大大小（单位 MB），超过后自动做轮转（默认为 100MB）</li>
</ul>
<p>每条审计记录包括两行</p>
<ul>
<li>请求行包括：唯一 ID 和请求的元数据（如源 IP、用户名、请求资源等）</li>
<li>响应行包括：唯一 ID（与请求 ID 一致）和响应的元数据（如 HTTP 状态码）</li>
</ul>
<p>比如，admin 用户查询默认 namespace 的 Pod 列表的审计日志格式为</p>
<pre><code class="lang-sh">2017-03-21T03:57:09.106841886-04:00 AUDIT: id=<span class="hljs-string">"c939d2a7-1c37-4ef1-b2f7-4ba9b1e43b53"</span> ip=<span class="hljs-string">"127.0.0.1"</span> method=<span class="hljs-string">"GET"</span> user=<span class="hljs-string">"admin"</span> groups=<span class="hljs-string">"\"system:masters\",\"system:authenticated\""</span>as=<span class="hljs-string">"<self>"</span>asgroups=<span class="hljs-string">"<lookup>"</span>namespace=<span class="hljs-string">"default"</span>uri=<span class="hljs-string">"/api/v1/namespaces/default/pods"</span>
2017-03-21T03:57:09.108403639-04:00 AUDIT: id=<span class="hljs-string">"c939d2a7-1c37-4ef1-b2f7-4ba9b1e43b53"</span> response=<span class="hljs-string">"200"</span>
</code></pre>
<h2 id="审计策略">审计策略</h2>
<p>v1.7 + 支持实验性的高级审计特性，可以自定义审计策略（选择记录哪些事件）和审计存储后端（日志和 webhook）等。开启方法为</p>
<pre><code class="lang-sh">kube-apiserver ... --feature-gates=AdvancedAuditing=<span class="hljs-literal">true</span>
</code></pre>
<p>注意开启 AdvancedAuditing 后，日志的格式有一些修改，如新增了 stage 字段（包括 RequestReceived，ResponseStarted ，ResponseComplete，Panic 等）。</p>
<h2 id="审计策略">审计策略</h2>
<p>审计策略选择记录哪些事件，设置方法为</p>
<pre><code class="lang-sh">kube-apiserver ... --audit-policy-file=/etc/kubernetes/audit-policy.yaml
</code></pre>
<p>其中，设计策略的配置格式为</p>
<pre><code class="lang-yaml"><span class="hljs-attr">rules:</span>
  <span class="hljs-comment"># Don't log watch requests by the"system:kube-proxy" on endpoints or services</span>
<span class="hljs-attr">  - level:</span> None
<span class="hljs-attr">    users:</span> [<span class="hljs-string">"system:kube-proxy"</span>]
<span class="hljs-attr">    verbs:</span> [<span class="hljs-string">"watch"</span>]
<span class="hljs-attr">    resources:</span>
<span class="hljs-attr">    - group:</span> <span class="hljs-string">""</span> <span class="hljs-comment"># core API group</span>
<span class="hljs-attr">      resources:</span> [<span class="hljs-string">"endpoints"</span>, <span class="hljs-string">"services"</span>]

  <span class="hljs-comment"># Don't log authenticated requests to certain non-resource URL paths.</span>
<span class="hljs-attr">  - level:</span> None
<span class="hljs-attr">    userGroups:</span> [<span class="hljs-string">"system:authenticated"</span>]
<span class="hljs-attr">    nonResourceURLs:</span>
<span class="hljs-bullet">    -</span> <span class="hljs-string">"/api*"</span> <span class="hljs-comment"># Wildcard matching.</span>
<span class="hljs-bullet">    -</span> <span class="hljs-string">"/version"</span>

  <span class="hljs-comment"># Log the request body of configmap changes in kube-system.</span>
<span class="hljs-attr">  - level:</span> Request
<span class="hljs-attr">    resources:</span>
<span class="hljs-attr">    - group:</span> <span class="hljs-string">""</span> <span class="hljs-comment"># core API group</span>
<span class="hljs-attr">      resources:</span> [<span class="hljs-string">"configmaps"</span>]
    <span class="hljs-comment"># This rule only applies to resources in the "kube-system" namespace.</span>
    <span class="hljs-comment"># The empty string "" can be used to select non-namespaced resources.</span>
<span class="hljs-attr">    namespaces:</span> [<span class="hljs-string">"kube-system"</span>]

  <span class="hljs-comment"># Log configmap and secret changes in all other namespaces at the Metadata level.</span>
<span class="hljs-attr">  - level:</span> Metadata
<span class="hljs-attr">    resources:</span>
<span class="hljs-attr">    - group:</span> <span class="hljs-string">""</span> <span class="hljs-comment"># core API group</span>
<span class="hljs-attr">      resources:</span> [<span class="hljs-string">"secrets"</span>, <span class="hljs-string">"configmaps"</span>]

  <span class="hljs-comment"># Log all other resources in core and extensions at the Request level.</span>
<span class="hljs-attr">  - level:</span> Request
<span class="hljs-attr">    resources:</span>
<span class="hljs-attr">    - group:</span> <span class="hljs-string">""</span> <span class="hljs-comment"># core API group</span>
<span class="hljs-attr">    - group:</span> <span class="hljs-string">"extensions"</span> <span class="hljs-comment"># Version of group should NOT be included.</span>

  <span class="hljs-comment"># A catch-all rule to log all other requests at the Metadata level.</span>
<span class="hljs-attr">  - level:</span> Metadata
</code></pre>
<p>在生产环境中，推荐参考 <a href="https://github.com/kubernetes/kubernetes/blob/v1.7.0/cluster/gce/gci/configure-helper.sh#L490" target="_blank">GCE 审计策略</a> 配置。</p>
<h3 id="审计存储后端">审计存储后端</h3>
<p>审计存储后端支持两种方式</p>
<ul>
<li>日志，配置 <code>--audit-log-path</code> 开启，格式为</li>
</ul>
<pre><code>2017-06-15T21:50:50.259470834Z AUDIT: id="591e9fde-6a98-46f6-b7bc-ec8ef575696d" stage="RequestReceived" ip="10.2.1.3" method="update" user="system:serviceaccount:kube-system:default" groups="\"system:serviceaccounts\",\"system:serviceaccounts:kube-system\",\"system:authenticated\""as="<self>"asgroups="<lookup>"namespace="kube-system"uri="/api/v1/namespaces/kube-system/endpoints/kube-controller-manager"response="<deferred>"
2017-06-15T21:50:50.259470834Z AUDIT: id="591e9fde-6a98-46f6-b7bc-ec8ef575696d" stage="ResponseComplete" ip="10.2.1.3" method="update" user="system:serviceaccount:kube-system:default" groups="\"system:serviceaccounts\",\"system:serviceaccounts:kube-system\",\"system:authenticated\""as="<self>"asgroups="<lookup>"namespace="kube-system"uri="/api/v1/namespaces/kube-system/endpoints/kube-controller-manager"response="200"
</code></pre><ul>
<li>webhook，配置 <code>--audit-webhook-config-file=/etc/kubernetes/audit-webhook-kubeconfig --audit-webhook-mode=batch</code> 开启，其中 audit-webhook-mode 支持 batch 和 blocking 两种格式，而 webhook 配置文件格式为</li>
</ul>
<pre><code class="lang-yaml"><span class="hljs-comment"># clusters refers to the remote service.</span>
<span class="hljs-attr">clusters:</span>
<span class="hljs-attr">  - name:</span> name-of-remote-audit-service
<span class="hljs-attr">    cluster:</span>
<span class="hljs-attr">      certificate-authority:</span> /path/to/ca.pem  <span class="hljs-comment"># CA for verifying the remote service.</span>
<span class="hljs-attr">      server:</span> https://audit.example.com/audit <span class="hljs-comment"># URL of remote service to query. Must use 'https'.</span>

<span class="hljs-comment"># users refers to the API server's webhook configuration.</span>
<span class="hljs-attr">users:</span>
<span class="hljs-attr">  - name:</span> name-of-api-server
<span class="hljs-attr">    user:</span>
<span class="hljs-attr">      client-certificate:</span> /path/to/cert.pem <span class="hljs-comment"># cert for the webhook plugin to use</span>
<span class="hljs-attr">      client-key:</span> /path/to/key.pem          <span class="hljs-comment"># key matching the cert</span>

<span class="hljs-comment"># kubeconfig files require a context. Provide one for the API server.</span>
<span class="hljs-attr">current-context:</span> webhook
<span class="hljs-attr">contexts:</span>
<span class="hljs-attr">- context:</span>
<span class="hljs-attr">    cluster:</span> name-of-remote-audit-service
<span class="hljs-attr">    user:</span> name-of-api-sever
<span class="hljs-attr">  name:</span> webhook
</code></pre>
<p>所有的事件以 JSON 格式 POST 给 webhook server，如</p>
<pre><code class="lang-json">{
  <span class="hljs-string">"kind"</span>: <span class="hljs-string">"EventList"</span>,
  <span class="hljs-string">"apiVersion"</span>: <span class="hljs-string">"audit.k8s.io/v1alpha1"</span>,
  <span class="hljs-string">"items"</span>: [
    {
      <span class="hljs-string">"metadata"</span>: {
        <span class="hljs-string">"creationTimestamp"</span>: <span class="hljs-literal">null</span>
      },
      <span class="hljs-string">"level"</span>: <span class="hljs-string">"Metadata"</span>,
      <span class="hljs-string">"timestamp"</span>: <span class="hljs-string">"2017-06-15T23:07:40Z"</span>,
      <span class="hljs-string">"auditID"</span>: <span class="hljs-string">"4faf711a-9094-400f-a876-d9188ceda548"</span>,
      <span class="hljs-string">"stage"</span>: <span class="hljs-string">"ResponseComplete"</span>,
      <span class="hljs-string">"requestURI"</span>: <span class="hljs-string">"/apis/rbac.authorization.k8s.io/v1beta1/namespaces/kube-public/rolebindings/system:controller:bootstrap-signer"</span>,
      <span class="hljs-string">"verb"</span>: <span class="hljs-string">"get"</span>,
      <span class="hljs-string">"user"</span>: {
        <span class="hljs-string">"username"</span>: <span class="hljs-string">"system:apiserver"</span>,
        <span class="hljs-string">"uid"</span>: <span class="hljs-string">"97a62906-e4d7-4048-8eda-4f0fb6ff8f1e"</span>,
        <span class="hljs-string">"groups"</span>: [
          <span class="hljs-string">"system:masters"</span>
        ]
      },
      <span class="hljs-string">"sourceIPs"</span>: [
        <span class="hljs-string">"127.0.0.1"</span>
      ],
      <span class="hljs-string">"objectRef"</span>: {
        <span class="hljs-string">"resource"</span>: <span class="hljs-string">"rolebindings"</span>,
        <span class="hljs-string">"namespace"</span>: <span class="hljs-string">"kube-public"</span>,
        <span class="hljs-string">"name"</span>: <span class="hljs-string">"system:controller:bootstrap-signer"</span>,
        <span class="hljs-string">"apiVersion"</span>: <span class="hljs-string">"rbac.authorization.k8s.io/v1beta1"</span>
      },
      <span class="hljs-string">"responseStatus"</span>: {
        <span class="hljs-string">"metadata"</span>: {},
        <span class="hljs-string">"code"</span>: <span class="hljs-number">200</span>
      }
    }
  ]
}
</code></pre>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="kubernetes-大规模集群" class="level3">大规模集群</h1>
<p>Kubernetes v1.6-v1.11 单集群最大支持 5000 个节点，也就是说 Kubernetes 最新稳定版的单个集群支持</p>
<ul>
<li>不超过 5000 个节点</li>
<li>不超过 150000 个 Pod</li>
<li>不超过 300000 个容器</li>
<li>每台 Node 上不超过 100 个 Pod</li>
</ul>
<h2 id="公有云配额">公有云配额</h2>
<p>对于公有云上的 Kubernetes 集群，规模大了之后很容器碰到配额问题，需要提前在云平台上增大配额。这些需要增大的配额包括</p>
<ul>
<li>虚拟机个数</li>
<li>vCPU 个数</li>
<li>内网 IP 地址个数</li>
<li>公网 IP 地址个数</li>
<li>安全组条数</li>
<li>路由表条数</li>
<li>持久化存储大小</li>
</ul>
<h3 id="etcd-存储">Etcd 存储</h3>
<p>除了常规的 <a href="https://coreos.com/etcd/docs/3.2.15/op-guide/clustering.html" target="_blank">Etcd 高可用集群</a>配置、使用 SSD 存储等，还需要为 Events 配置单独的 Etcd 集群。即部署两套独立的 Etcd 集群，并配置 kube-apiserver</p>
<pre><code class="lang-sh">--etcd-servers=<span class="hljs-string">"http://etcd1:2379,http://etcd2:2379,http://etcd3:2379"</span> --etcd-servers-overrides=<span class="hljs-string">"/events#http://etcd4:2379,http://etcd5:2379,http://etcd6:2379"</span>
</code></pre>
<p>另外，Etcd 默认存储限制为 2GB，可以通过 <code>--quota-backend-bytes</code> 选项增大。</p>
<h2 id="master-节点大小">Master 节点大小</h2>
<p>可以参考 AWS 配置 Master 节点的大小：</p>
<ul>
<li>1-5 nodes: m3.medium</li>
<li>6-10 nodes: m3.large</li>
<li>11-100 nodes: m3.xlarge</li>
<li>101-250 nodes: m3.2xlarge</li>
<li>251-500 nodes: c4.4xlarge</li>
<li>more than 500 nodes: c4.8xlarge</li>
</ul>
<h2 id="为扩展分配更多资源">为扩展分配更多资源</h2>
<p>Kubernetes 集群内的扩展也需要分配更多的资源，包括为这些 Pod 分配更大的 CPU 和内存以及增大容器副本数量等。当 Node 本身的容量太小时，还需要增大 Node 本身的 CPU 和内存（特别是在公有云平台上）。</p>
<p>以下扩展服务需要增大 CPU 和内存：</p>
<ul>
<li><a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dns" target="_blank">DNS (kube-dns or CoreDNS)</a></li>
<li><a href="http://releases.k8s.io/master/cluster/addons/cluster-monitoring/influxdb/influxdb-grafana-controller.yaml" target="_blank">InfluxDB and Grafana</a></li>
<li><a href="http://releases.k8s.io/master/cluster/addons/fluentd-elasticsearch/kibana-deployment.yaml" target="_blank">Kibana</a></li>
<li><a href="http://releases.k8s.io/master/cluster/addons/fluentd-elasticsearch/fluentd-es-ds.yaml" target="_blank">FluentD with ElasticSearch Plugin</a></li>
<li><a href="http://releases.k8s.io/master/cluster/addons/fluentd-gcp/fluentd-gcp-ds.yaml" target="_blank">FluentD with GCP Plugin</a></li>
</ul>
<p>以下扩展服务需要增大副本数：</p>
<ul>
<li><a href="http://releases.k8s.io/master/cluster/addons/fluentd-elasticsearch/es-statefulset.yaml" target="_blank">elasticsearch</a></li>
<li><a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dns" target="_blank">DNS (kube-dns or CoreDNS)</a></li>
</ul>
<p>另外，为了保证多个副本分散调度到不同的 Node 上，需要为容器配置 <a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity" target="_blank">AntiAffinity</a>。比如，对 kube-dns，可以增加如下的配置：</p>
<pre><code class="lang-yaml"><span class="hljs-attr">affinity:</span>
<span class="hljs-attr"> podAntiAffinity:</span>
<span class="hljs-attr">   requiredDuringSchedulingIgnoredDuringExecution:</span>
<span class="hljs-attr">   - weight:</span> <span class="hljs-number">100</span>
<span class="hljs-attr">     labelSelector:</span>
<span class="hljs-attr">       matchExpressions:</span>
<span class="hljs-attr">       - key:</span> k8s-app
<span class="hljs-attr">         operator:</span> In
<span class="hljs-attr">         values:</span>
<span class="hljs-bullet">         -</span> kube-dns
<span class="hljs-attr">     topologyKey:</span> kubernetes.io/hostname
</code></pre>
<h2 id="kube-apiserver-配置">Kube-apiserver 配置</h2>
<ul>
<li>设置 <code>--max-requests-inflight=3000</code></li>
<li>设置 <code>--max-mutating-requests-inflight=1000</code></li>
</ul>
<h2 id="kube-scheduler-配置">Kube-scheduler 配置</h2>
<ul>
<li>设置 <code>--kube-api-qps=100</code></li>
</ul>
<h2 id="kube-controller-manager-配置">Kube-controller-manager 配置</h2>
<ul>
<li>设置 <code>--kube-api-qps=100</code></li>
<li>设置 <code>--kube-api-burst=100</code></li>
</ul>
<h2 id="kubelet-配置">Kubelet 配置</h2>
<ul>
<li>设置 <code>--image-pull-progress-deadline=30m</code></li>
<li>设置 <code>--serialize-image-pulls=false</code>（需要 Docker 使用 overlay2 ）</li>
<li>Kubelet 单节点允许运行的最大 Pod 数：<code>--max-pods=110</code>（默认是 110，可以根据实际需要设置）</li>
</ul>
<h2 id="docker-配置">Docker 配置</h2>
<ul>
<li>设置 <code>max-concurrent-downloads=10</code></li>
<li>使用 SSD 存储 <code>graph=/ssd-storage-path</code></li>
<li>预加载 pause 镜像，比如 <code>docker image save -o /opt/preloaded_docker_images.tar</code> 和 <code>docker image load -i /opt/preloaded_docker_images.tar</code></li>
</ul>
<h2 id="节点配置">节点配置</h2>
<p>增大内核选项配置 <code>/etc/sysctl.conf</code>：</p>
<pre><code class="lang-sh">fs.file-max=1000000

net.ipv4.ip_forward=1
net.netfilter.nf_conntrack_max=10485760
net.netfilter.nf_conntrack_tcp_timeout_established=300
net.netfilter.nf_conntrack_buckets=655360
net.core.netdev_max_backlog=10000

net.ipv4.neigh.default.gc_thresh1=1024
net.ipv4.neigh.default.gc_thresh2=4096
net.ipv4.neigh.default.gc_thresh3=8192

net.netfilter.nf_conntrack_max=10485760
net.netfilter.nf_conntrack_tcp_timeout_established=300
net.netfilter.nf_conntrack_buckets=655360
net.core.netdev_max_backlog=10000

fs.inotify.max_user_instances=524288
fs.inotify.max_user_watches=524288
</code></pre>
<h2 id="应用配置">应用配置</h2>
<p>在运行 Pod 的时候也需要注意遵循一些最佳实践，比如</p>
<ul>
<li>为容器设置资源请求和限制<ul>
<li><code>spec.containers[].resources.limits.cpu</code></li>
<li><code>spec.containers[].resources.limits.memory</code></li>
<li><code>spec.containers[].resources.requests.cpu</code></li>
<li><code>spec.containers[].resources.requests.memory</code></li>
<li><code>spec.containers[].resources.limits.ephemeral-storage</code></li>
<li><code>spec.containers[].resources.requests.ephemeral-storage</code></li>
</ul>
</li>
<li>对关键应用使用 PodDisruptionBudget、nodeAffinity、podAffinity 和 podAntiAffinity 等保护</li>
<li>尽量使用控制器来管理容器（如 Deployment、StatefulSet、DaemonSet、Job 等）</li>
<li>更多内容参考<a href="../deploy/kubernetes-configuration-best-practice.html">这里</a></li>
</ul>
<h2 id="必要的扩展">必要的扩展</h2>
<p>监控、告警以及可视化（如 Prometheus 和 Grafana）至关重要，推荐部署并开启。</p>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://kubernetes.io/docs/setup/cluster-large/" target="_blank">Building Large Clusters</a></li>
<li><a href="https://blog.openai.com/scaling-kubernetes-to-2500-nodes/" target="_blank">Scaling Kubernetes to 2,500 Nodes</a></li>
<li><a href="https://medium.com/@brendanrius/scaling-kubernetes-for-25m-users-a7937e3536a0" target="_blank">Scaling Kubernetes for 25M users</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="大数据与机器学习" class="level2">大数据与机器学习</h1>
<p>Kubernetes 在大数据与机器学习中的实践案例。 </p>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="spark-on-kubernetes" class="level3">Spark</h1>
<p><img src="https://i.imgur.com/6zYTLL8.png" alt=""/></p>
<p><strong>Kubernetes 从 v1.8 开始支持 <a href="https://apache-spark-on-k8s.github.io/userdocs/running-on-kubernetes.html" target="_blank">原生的 Apache Spark</a> 应用（需要 Spark 支持 Kubernetes，比如 v2.3）</strong>，可以通过 <code>spark-submit</code> 命令直接提交 Kubernetes 任务。比如计算圆周率</p>
<pre><code class="lang-sh">bin/spark-submit \
  --deploy-mode cluster \
  --class org.apache.spark.examples.SparkPi \
  --master k8s://https://<k8s-apiserver-host>:<k8s-apiserver-port> \
  --kubernetes-namespace default \
  --conf spark.executor.instances=5 \
  --conf spark.app.name=spark-pi \
  --conf spark.kubernetes.driver.docker.image=kubespark/spark-driver:v2.2.0-kubernetes-0.4.0 \
  --conf spark.kubernetes.executor.docker.image=kubespark/spark-executor:v2.2.0-kubernetes-0.4.0 \
  <span class="hljs-built_in">local</span>:///opt/spark/examples/jars/spark-examples_2.11-2.2.0-k8s-0.4.0.jar
</code></pre>
<p>或者使用 Python 版本</p>
<pre><code class="lang-sh">bin/spark-submit \
  --deploy-mode cluster \
  --master k8s://https://<k8s-apiserver-host>:<k8s-apiserver-port> \
  --kubernetes-namespace <k8s-namespace> \
  --conf spark.executor.instances=5 \
  --conf spark.app.name=spark-pi \
  --conf spark.kubernetes.driver.docker.image=kubespark/spark-driver-py:v2.2.0-kubernetes-0.4.0 \
  --conf spark.kubernetes.executor.docker.image=kubespark/spark-executor-py:v2.2.0-kubernetes-0.4.0 \
  --jars <span class="hljs-built_in">local</span>:///opt/spark/examples/jars/spark-examples_2.11-2.2.0-k8s-0.4.0.jar \
  --py-files <span class="hljs-built_in">local</span>:///opt/spark/examples/src/main/python/sort.py \
  <span class="hljs-built_in">local</span>:///opt/spark/examples/src/main/python/pi.py 10
</code></pre>
<h2 id="spark-on-kubernetes-部署">Spark on Kubernetes 部署</h2>
<p>Kubernetes 示例 <a href="https://github.com/kubernetes/examples/tree/master/staging/spark" target="_blank">github</a> 上提供了一个详细的 spark 部署方法，由于步骤复杂，这里简化一些部分让大家安装的时候不用去多设定一些东西。</p>
<h3 id="部署条件">部署条件</h3>
<ul>
<li>一个 kubernetes 群集, 可参考 <a href="../deploy/cluster.html">集群部署</a></li>
<li>kube-dns 正常运作</li>
</ul>
<h3 id="创建一个命名空间">创建一个命名空间</h3>
<p>namespace-spark-cluster.yaml</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Namespace
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> <span class="hljs-string">"spark-cluster"</span>
<span class="hljs-attr">  labels:</span>
<span class="hljs-attr">    name:</span> <span class="hljs-string">"spark-cluster"</span>
</code></pre>
<pre><code class="lang-sh">$ kubectl create <span class="hljs-_">-f</span> examples/staging/spark/namespace-spark-cluster.yaml
</code></pre>
<p>这边原文提到需要将 kubectl 的执行环境转到 spark-cluster, 这边为了方便我们不这样做, 而是将之后的佈署命名空间都加入 spark-cluster</p>
<h3 id="部署-master-service">部署 Master Service</h3>
<p>建立一个 replication controller, 来运行 Spark Master 服务</p>
<pre><code class="lang-yaml"><span class="hljs-attr">kind:</span> ReplicationController
<span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> spark-master-controller
<span class="hljs-attr">  namespace:</span> spark-cluster
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  replicas:</span> <span class="hljs-number">1</span>
<span class="hljs-attr">  selector:</span>
<span class="hljs-attr">    component:</span> spark-master
<span class="hljs-attr">  template:</span>
<span class="hljs-attr">    metadata:</span>
<span class="hljs-attr">      labels:</span>
<span class="hljs-attr">        component:</span> spark-master
<span class="hljs-attr">    spec:</span>
<span class="hljs-attr">      containers:</span>
<span class="hljs-attr">        - name:</span> spark-master
<span class="hljs-attr">          image:</span> gcr.io/google_containers/spark:<span class="hljs-number">1.5</span><span class="hljs-number">.2</span>_v1
<span class="hljs-attr">          command:</span> [<span class="hljs-string">"/start-master"</span>]
<span class="hljs-attr">          ports:</span>
<span class="hljs-attr">            - containerPort:</span> <span class="hljs-number">7077</span>
<span class="hljs-attr">            - containerPort:</span> <span class="hljs-number">8080</span>
<span class="hljs-attr">          resources:</span>
<span class="hljs-attr">            requests:</span>
<span class="hljs-attr">              cpu:</span> <span class="hljs-number">100</span>m
</code></pre>
<pre><code class="lang-sh">$ kubectl create <span class="hljs-_">-f</span> spark-master-controller.yaml
</code></pre>
<p>创建 master 服务</p>
<p>spark-master-service.yaml</p>
<pre><code class="lang-yaml"><span class="hljs-attr">kind:</span> Service
<span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> spark-master
<span class="hljs-attr">  namespace:</span> spark-cluster
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  ports:</span>
<span class="hljs-attr">    - port:</span> <span class="hljs-number">7077</span>
<span class="hljs-attr">      targetPort:</span> <span class="hljs-number">7077</span>
<span class="hljs-attr">      name:</span> spark
<span class="hljs-attr">    - port:</span> <span class="hljs-number">8080</span>
<span class="hljs-attr">      targetPort:</span> <span class="hljs-number">8080</span>
<span class="hljs-attr">      name:</span> http
<span class="hljs-attr">  selector:</span>
<span class="hljs-attr">    component:</span> spark-master
</code></pre>
<pre><code class="lang-sh">$ kubectl create <span class="hljs-_">-f</span> spark-master-service.yaml
</code></pre>
<p>检查 Master 是否正常运行</p>
<pre><code class="lang-sh">$ kubectl get pod -n spark-cluster
spark-master-controller-qtwm8     1/1       Running   0          6d
</code></pre>
<pre><code class="lang-sh">$ kubectl logs spark-master-controller-qtwm8 -n spark-cluster
17/08/07 02:34:54 INFO Master: Registered signal handlers <span class="hljs-keyword">for</span> [TERM, HUP, INT]
17/08/07 02:34:54 INFO SecurityManager: Changing view acls to: root
17/08/07 02:34:54 INFO SecurityManager: Changing modify acls to: root
17/08/07 02:34:54 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); users with modify permissions: Set(root)
17/08/07 02:34:55 INFO Slf4jLogger: Slf4jLogger started
17/08/07 02:34:55 INFO Remoting: Starting remoting
17/08/07 02:34:55 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkMaster@spark-master:7077]
17/08/07 02:34:55 INFO Utils: Successfully started service <span class="hljs-string">'sparkMaster'</span> on port 7077.
17/08/07 02:34:55 INFO Master: Starting Spark master at spark://spark-master:7077
17/08/07 02:34:55 INFO Master: Running Spark version 1.5.2
17/08/07 02:34:56 INFO Utils: Successfully started service <span class="hljs-string">'MasterUI'</span> on port 8080.
17/08/07 02:34:56 INFO MasterWebUI: Started MasterWebUI at http://10.2.6.12:8080
17/08/07 02:34:56 INFO Utils: Successfully started service on port 6066.
17/08/07 02:34:56 INFO StandaloneRestServer: Started REST server <span class="hljs-keyword">for</span> submitting applications on port 6066
17/08/07 02:34:56 INFO Master: I have been elected leader! New state: ALIVE
</code></pre>
<p>若 master 已经被建立与运行, 我们可以透过 Spark 开发的 webUI 来察看我们 spark 的群集状况, 我们将佈署 <a href="https://github.com/aseigneurin/spark-ui-proxy" target="_blank">specialized proxy</a></p>
<p>spark-ui-proxy-controller.yaml</p>
<pre><code class="lang-yaml"><span class="hljs-attr">kind:</span> ReplicationController
<span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> spark-ui-proxy-controller
<span class="hljs-attr">  namespace:</span> spark-cluster
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  replicas:</span> <span class="hljs-number">1</span>
<span class="hljs-attr">  selector:</span>
<span class="hljs-attr">    component:</span> spark-ui-proxy
<span class="hljs-attr">  template:</span>
<span class="hljs-attr">    metadata:</span>
<span class="hljs-attr">      labels:</span>
<span class="hljs-attr">        component:</span> spark-ui-proxy
<span class="hljs-attr">    spec:</span>
<span class="hljs-attr">      containers:</span>
<span class="hljs-attr">        - name:</span> spark-ui-proxy
<span class="hljs-attr">          image:</span> elsonrodriguez/spark-ui-proxy:<span class="hljs-number">1.0</span>
<span class="hljs-attr">          ports:</span>
<span class="hljs-attr">            - containerPort:</span> <span class="hljs-number">80</span>
<span class="hljs-attr">          resources:</span>
<span class="hljs-attr">            requests:</span>
<span class="hljs-attr">              cpu:</span> <span class="hljs-number">100</span>m
<span class="hljs-attr">          args:</span>
<span class="hljs-attr">            - spark-master:</span><span class="hljs-number">8080</span>
<span class="hljs-attr">          livenessProbe:</span>
<span class="hljs-attr">              httpGet:</span>
<span class="hljs-attr">                path:</span> /
<span class="hljs-attr">                port:</span> <span class="hljs-number">80</span>
<span class="hljs-attr">              initialDelaySeconds:</span> <span class="hljs-number">120</span>
<span class="hljs-attr">              timeoutSeconds:</span> <span class="hljs-number">5</span>
</code></pre>
<pre><code class="lang-sh">$ kubectl create <span class="hljs-_">-f</span> spark-ui-proxy-controller.yaml
</code></pre>
<p>提供一个 service 做存取, 这边原文是使用 LoadBalancer type, 这边我们改成 NodePort, 如果你的 kubernetes 运行环境是在 cloud provider, 也可以参考原文作法</p>
<p>spark-ui-proxy-service.yaml</p>
<pre><code class="lang-yaml"><span class="hljs-attr">kind:</span> Service
<span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> spark-ui-proxy
<span class="hljs-attr">  namespace:</span> spark-cluster
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  ports:</span>
<span class="hljs-attr">    - port:</span> <span class="hljs-number">80</span>
<span class="hljs-attr">      targetPort:</span> <span class="hljs-number">80</span>
<span class="hljs-attr">      nodePort:</span> <span class="hljs-number">30080</span>
<span class="hljs-attr">  selector:</span>
<span class="hljs-attr">    component:</span> spark-ui-proxy
<span class="hljs-attr">  type:</span> NodePort
</code></pre>
<pre><code class="lang-sh">$ kubectl create <span class="hljs-_">-f</span> spark-ui-proxy-service.yaml
</code></pre>
<p>部署完后你可以利用 <a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/http-proxy-access-api/" target="_blank">kubecrl proxy</a> 来察看你的 Spark 群集状态</p>
<pre><code class="lang-sh">$ kubectl proxy --port=8001
</code></pre>
<p>可以透过 <code>http://localhost:8001/api/v1/proxy/namespaces/spark-cluster/services/spark-master:8080/</code> 察看, 若 kubectl 中断就无法这样观察了, 但我们再先前有设定 nodeport 所以也可以透过任意台 node 的端口 30080 去察看（例如 <code>http://10.201.2.34:30080</code>）。</p>
<h3 id="部署-spark-workers">部署 Spark workers</h3>
<p>要先确定 Matser 是再运行的状态</p>
<p>spark-worker-controller.yaml</p>
<pre><code>kind: ReplicationController
apiVersion: v1
metadata:
  name: spark-worker-controller
  namespace: spark-cluster
spec:
  replicas: 2
  selector:
    component: spark-worker
  template:
    metadata:
      labels:
        component: spark-worker
    spec:
      containers:
        - name: spark-worker
          image: gcr.io/google_containers/spark:1.5.2_v1
          command: ["/start-worker"]
          ports:
            - containerPort: 8081
          resources:
            requests:
              cpu: 100m
</code></pre><pre><code class="lang-sh">$ kubectl create <span class="hljs-_">-f</span> spark-worker-controller.yaml
replicationcontroller <span class="hljs-string">"spark-worker-controller"</span> created
</code></pre>
<p>透过指令察看运行状况</p>
<pre><code class="lang-sh">$ kubectl get pod -n spark-cluster
spark-master-controller-qtwm8     1/1       Running   0          6d
spark-worker-controller-4rxrs     1/1       Running   0          6d
spark-worker-controller-z6f21     1/1       Running   0          6d
spark-ui-proxy-controller<span class="hljs-_">-d</span>4br2   1/1       Running   4          6d
</code></pre>
<p>也可以透过上面建立的 WebUI 服务去察看</p>
<p>基本上到这边 Spark 的群集已经建立完成了</p>
<h3 id="创建-zeppelin-ui">创建 Zeppelin UI</h3>
<p>我们可以利用 Zeppelin UI 经由 web notebook 直接去执行我们的任务,
详情可以看 <a href="http://zeppelin.apache.org/" target="_blank">Zeppelin UI</a> 与 <a href="https://spark.apache.org/docs/latest/cluster-overview.html" target="_blank"> Spark architecture</a></p>
<p>zeppelin-controller.yaml</p>
<pre><code class="lang-yaml"><span class="hljs-attr">kind:</span> ReplicationController
<span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> zeppelin-controller
<span class="hljs-attr">  namespace:</span> spark-cluster
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  replicas:</span> <span class="hljs-number">1</span>
<span class="hljs-attr">  selector:</span>
<span class="hljs-attr">    component:</span> zeppelin
<span class="hljs-attr">  template:</span>
<span class="hljs-attr">    metadata:</span>
<span class="hljs-attr">      labels:</span>
<span class="hljs-attr">        component:</span> zeppelin
<span class="hljs-attr">    spec:</span>
<span class="hljs-attr">      containers:</span>
<span class="hljs-attr">        - name:</span> zeppelin
<span class="hljs-attr">          image:</span> gcr.io/google_containers/zeppelin:v0<span class="hljs-number">.5</span><span class="hljs-number">.6</span>_v1
<span class="hljs-attr">          ports:</span>
<span class="hljs-attr">            - containerPort:</span> <span class="hljs-number">8080</span>
<span class="hljs-attr">          resources:</span>
<span class="hljs-attr">            requests:</span>
<span class="hljs-attr">              cpu:</span> <span class="hljs-number">100</span>m
</code></pre>
<pre><code class="lang-sh">$ kubectl create <span class="hljs-_">-f</span> zeppelin-controller.yaml
replicationcontroller <span class="hljs-string">"zeppelin-controller"</span> created
</code></pre>
<p>然后一样佈署 Service</p>
<p>zeppelin-service.yaml</p>
<pre><code class="lang-sh">kind: Service
apiVersion: v1
metadata:
  name: zeppelin
  namespace: spark-cluster
spec:
  ports:
    - port: 80
      targetPort: 8080
      nodePort: 30081
  selector:
    component: zeppelin
  <span class="hljs-built_in">type</span>: NodePort
</code></pre>
<pre><code class="lang-sh">$ kubectl create <span class="hljs-_">-f</span> zeppelin-service.yaml
</code></pre>
<p>可以看到我们把 NodePort 设再 30081, 一样可以透过任意台 node 的 30081 port 访问 zeppelin UI。</p>
<p>通过命令行访问 pyspark（记得把 pod 名字换成你自己的）：</p>
<pre><code>$ kubectl exec -it zeppelin-controller-8f14f -n spark-cluster pyspark
Python 2.7.9 (default, Mar  1 2015, 12:57:24)
[GCC 4.9.2] on linux2
Type "help", "copyright", "credits" or "license" for more information.
17/08/14 01:59:22 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 1.5.2
      /_/

Using Python version 2.7.9 (default, Mar  1 2015 12:57:24)
SparkContext available as sc, HiveContext available as sqlContext.
>>>
</code></pre><p>接着就能使用 Spark 的服务了, 如有错误欢迎更正。</p>
<h3 id="zeppelin-常见问题">zeppelin 常见问题</h3>
<ul>
<li>zeppelin 的镜像非常大, 所以再 pull 时会花上一些时间, 而 size 大小的问题现在也正在解决中, 详情可参考 issue #17231</li>
<li>在 GKE 的平台上, <code>kubectl post-forward</code> 可能有些不稳定, 如果你看现 zeppelin 的状态为 <code>Disconnected</code>,<code>port-forward</code> 可能已经失败你需要去重新启动它, 详情可参考 #12179</li>
</ul>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://apache-spark-on-k8s.github.io/userdocs/index.html" target="_blank">Apache Spark on Kubernetes</a></li>
<li><a href="https://github.com/kweisamx/spark-on-kubernetes" target="_blank">https://github.com/kweisamx/spark-on-kubernetes</a></li>
<li><a href="https://github.com/kubernetes/examples/tree/master/staging/spark" target="_blank">Spark examples</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="tensorflow" class="level3">Tensorflow</h1>
<p><a href="https://github.com/google/kubeflow" target="_blank">Kubeflow</a> 是 Google 发布的用于在 Kubernetes 集群中部署和管理 tensorflow 任务的框架。主要功能包括</p>
<ul>
<li>用于管理 Jupyter 的 JupyterHub 服务</li>
<li>用于管理训练任务的 Tensorflow Training Controller</li>
<li>用于模型服务的 TF Serving 容器</li>
</ul>
<h2 id="部署">部署</h2>
<p>部署之前需要确保</p>
<ul>
<li>一套部署好的 Kubernetes 集群或者 Minikube，并配置好 kubectl 命令行工具</li>
<li>安装 <a href="https://github.com/ksonnet/ksonnet" target="_blank">ksonnet</a> 0.8.0 以上版本</li>
</ul>
<p>对于开启 RBAC 的 Kubernetes 集群，首先要创建管理员角色绑定：</p>
<pre><code>kubectl create clusterrolebinding tf-admin --clusterrole=cluster-admin --serviceaccount=default:tf-job-operator
</code></pre><p>然后运行以下命令部署</p>
<pre><code class="lang-sh">ks init my-kubeflow
<span class="hljs-built_in">cd</span> my-kubeflow
ks registry add kubeflow github.com/google/kubeflow/tree/master/kubeflow
ks pkg install kubeflow/core
ks pkg install kubeflow/tf-serving
ks pkg install kubeflow/tf-job
ks generate core kubeflow-core --name=kubeflow-core
ks apply default -c kubeflow-core
</code></pre>
<p>如果有多个 Kubernetes 集群，也可以切换到其他其集群中部署，如</p>
<pre><code class="lang-sh">kubectl config use-context gke
ks env add gke
ks apply gke -c kubeflow-core
</code></pre>
<p>稍等一会，就可以看到 <code>tf-hub-lb</code> 服务的公网IP，也就是 JupyterHub 的访问地址</p>
<pre><code class="lang-sh">kubectl get svc tf-hub-lb
</code></pre>
<p>对于不支持 LoadBalancer Service 的集群，还可以通过端口转发（<code>http://127.0.0.1:8100</code>）的方式来访问：</p>
<pre><code class="lang-sh">kubectl port-forward tf-hub-0 8100:8000
</code></pre>
<p>JupyterHub 默认可以用任意用户名和密码登录。登陆后，可以使用自定义镜像来启动 Notebook Server，比如使用</p>
<ul>
<li><code>gcr.io/kubeflow/tensorflow-notebook-cpu</code></li>
<li><code>gcr.io/kubeflow/tensorflow-notebook-gpu</code></li>
</ul>
<h2 id="训练示例">训练示例</h2>
<p>使用 CPU：</p>
<pre><code class="lang-sh">ks generate tf-cnn cnn --name=cnn
ks apply gke -c cnn
</code></pre>
<p>使用 GPU：</p>
<pre><code class="lang-sh">ks param <span class="hljs-built_in">set</span> cnn num_gpus 1
ks param <span class="hljs-built_in">set</span>  cnn num_workers 1
ks apply default -c cnn
</code></pre>
<h2 id="模型部署">模型部署</h2>
<pre><code>MODEL_COMPONENT=serveInception
MODEL_NAME=inception
MODEL_PATH=gs://cloud-ml-dev_jlewi/tmp/inception
ks generate tf-serving ${MODEL_COMPONENT} --name=${MODEL_NAME} --namespace=default --model_path=${MODEL_PATH}

ks apply gke -c ${MODEL_COMPONENT}
</code></pre><h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="http://blog.kubernetes.io/2017/12/introducing-kubeflow-composable.html" target="_blank">Introducing Kubeflow - A Composable, Portable, Scalable ML Stack Built for Kubernetes</a></li>
<li><a href="https://github.com/google/kubeflow" target="_blank">https://github.com/google/kubeflow</a> </li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="serverless" class="level2">Serverless</h1>
<p>Serverless，即无服务器架构，将大家从服务器中解放了出来，只需要关注业务逻辑本身。用户只需要关注数据和业务逻辑，无需维护服务器，也不需要关心系统的容量和扩容。Serverless 本质上是一种更简单易用的 PaaS，包含两种含义：</p>
<p>仅依赖云端服务来管理业务逻辑和状态的应用或服务，一般称为 BaaS (Backend as a Service)
事件驱动且短时执行应用或服务，其主要逻辑由开发者完成，但由第三方管理（比如 AWS Lambda），一般称为 FaaS (Function as a Service)。目前大火的 Serverless 一般是指 FaaS。</p>
<p>引入 serverless 可以给应用开发者带来明显的好处</p>
<ul>
<li>用户无需配置和管理服务器</li>
<li>用户服务不需要基于特定框架或软件库</li>
<li>部署简单，只需要将代码上传到 serverless 平台即可</li>
<li>完全自动化的横向扩展</li>
<li>事件触发，比如 http 请求触发、文件更新触发、时间触发、消息触发等</li>
<li>低成本，比如 AWS Lambda 按执行时间和触发次数收费，在代码未运行时无需付费</li>
</ul>
<p>当然，serverless 也并非银弹，也有其特有的局限性</p>
<ul>
<li>无状态，服务的任何进程内或主机状态均无法被后续调用所使用，需要借助外部数据库或网络存储管理状态</li>
<li>每次函数调用的时间一般都有限制，比如 AWS Lambda 限制每个函数最长只能运行 5 分钟</li>
<li>启动延迟，特别是应用不活跃或者突发流量的情况下延迟尤为明显</li>
<li>平台依赖，比如服务发现、监控、调试、API Gateway 等都依赖于 serverless 平台提供的功能</li>
</ul>
<h2 id="开源框架">开源框架</h2>
<ul>
<li>OpenFaas: <a href="https://github.com/openfaas/faas" target="_blank">https://github.com/openfaas/faas</a></li>
<li>Fission: <a href="https://github.com/fission/fission" target="_blank">https://github.com/fission/fission</a></li>
<li>Kubeless: <a href="https://github.com/kubeless/kubeless" target="_blank">https://github.com/kubeless/kubeless</a></li>
<li>OpenWhisk: <a href="https://github.com/apache/incubator-openwhisk" target="_blank">https://github.com/apache/incubator-openwhisk</a></li>
<li>Fn: <a href="https://fnproject.io/" target="_blank">https://fnproject.io/</a></li>
</ul>
<h2 id="商业产品">商业产品</h2>
<ul>
<li>AWS Lambda: <a href="http://docs.aws.amazon.com/lambda/latest/dg/welcome.html" target="_blank">http://docs.aws.amazon.com/lambda/latest/dg/welcome.html</a></li>
<li>AWS Fargate: <a href="https://aws.amazon.com/cn/fargate/" target="_blank">https://aws.amazon.com/cn/fargate/</a></li>
<li>Azure Container Instance (ACI): <a href="https://azure.microsoft.com/zh-cn/services/container-instances/" target="_blank">https://azure.microsoft.com/zh-cn/services/container-instances/</a></li>
<li>Azure Functions: <a href="https://azure.microsoft.com/zh-cn/services/functions/" target="_blank">https://azure.microsoft.com/zh-cn/services/functions/</a></li>
<li>Google Cloud Functions: <a href="https://cloud.google.com/functions/" target="_blank">https://cloud.google.com/functions/</a></li>
<li>Hyper: <a href="https://hyper.sh/" target="_blank">https://hyper.sh/</a></li>
<li>Huawei CCI: <a href="https://www.huaweicloud.com/product/cci.html" target="_blank">https://www.huaweicloud.com/product/cci.html</a></li>
<li>Aliyun Serverless Kubernetes: <a href="https://help.aliyun.com/document_detail/71480.html" target="_blank">https://help.aliyun.com/document_detail/71480.html</a></li>
</ul>
<p>很多商业产品也可以与 Kubernetes 进行无缝集成，即通过 <a href="https://github.com/virtual-kubelet/virtual-kubelet" target="_blank">Virtual Kubelet</a> 将商业 Serverless 产品（如 ACI 和 Fargate等）作为 Kubernetes 集群的一个无限 Node 使用，这样就无需考虑 Node 数量的问题。</p>
<p><img src="images/virtual-kubelet.png" alt=""/></p>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://github.com/anaibol/awesome-serverless" target="_blank">Awesome Serverless</a></li>
<li><a href="http://docs.aws.amazon.com/lambda/latest/dg/welcome.html" target="_blank">AWS Lambda</a></li>
<li><a href="https://martinfowler.com/articles/serverless.html" target="_blank">Serverless Architectures</a></li>
<li><a href="http://thenewstack.io/tns-guide-serverless-technologies-best-frameworks-platforms-tools/" target="_blank">TNS Guide to Serverless Technologies</a></li>
<li><a href="https://github.com/JustServerless/awesome-serverless" target="_blank">Serverless blogs and posts</a></li>
</ul>
</section>
                            
    <h1 class='level1'>排错指南</h1><section class="normal markdown-section">
                                
                                <h1 id="排错概览" class="level2">排错概览</h1>
<p>Kubernetes 集群以及应用排错的一般方法，主要包括</p>
<ul>
<li><a href="cluster.html">集群状态异常排错</a></li>
<li><a href="pod.html">Pod运行异常排错</a></li>
<li><a href="network.html">网络异常排错</a></li>
<li><a href="pv.html">持久化存储异常排错</a><ul>
<li><a href="azuredisk.html">AzureDisk 排错</a></li>
<li><a href="azurefile.html">AzureFile 排错</a></li>
</ul>
</li>
<li><a href="windows.html">Windows容器排错</a></li>
<li><a href="cloud.html">云平台异常排错</a><ul>
<li><a href="azure.html">Azure 排错</a></li>
</ul>
</li>
<li><a href="tools.html">常用排错工具</a></li>
</ul>
<p>在排错过程中，<code>kubectl</code>  是最重要的工具，通常也是定位错误的起点。这里也列出一些常用的命令，在后续的各种排错过程中都会经常用到。</p>
<h4 id="查看-pod-状态以及运行节点">查看 Pod 状态以及运行节点</h4>
<pre><code class="lang-sh">kubectl get pods -o wide
kubectl -n kube-system get pods -o wide
</code></pre>
<h4 id="查看-pod-事件">查看 Pod 事件</h4>
<pre><code class="lang-sh">kubectl describe pod <pod-name>
</code></pre>
<h4 id="查看-node-状态">查看 Node 状态</h4>
<pre><code class="lang-sh">kubectl get nodes
kubectl describe node <node-name>
</code></pre>
<h4 id="kube-apiserver-日志">kube-apiserver 日志</h4>
<pre><code class="lang-sh">PODNAME=$(kubectl -n kube-system get pod <span class="hljs-_">-l</span> component=kube-apiserver -o jsonpath=<span class="hljs-string">'{.items[0].metadata.name}'</span>)
kubectl -n kube-system logs <span class="hljs-variable">$PODNAME</span> --tail 100
</code></pre>
<p>以上命令操作假设控制平面以 Kubernetes 静态 Pod 的形式来运行。如果 kube-apiserver 是用 systemd 管理的，则需要登录到 master 节点上，然后使用 journalctl -u kube-apiserver 查看其日志。</p>
<h4 id="kube-controller-manager-日志">kube-controller-manager 日志</h4>
<pre><code class="lang-sh">PODNAME=$(kubectl -n kube-system get pod <span class="hljs-_">-l</span> component=kube-controller-manager -o jsonpath=<span class="hljs-string">'{.items[0].metadata.name}'</span>)
kubectl -n kube-system logs <span class="hljs-variable">$PODNAME</span> --tail 100
</code></pre>
<p>以上命令操作假设控制平面以 Kubernetes 静态 Pod 的形式来运行。如果 kube-controller-manager 是用 systemd 管理的，则需要登录到 master 节点上，然后使用 journalctl -u kube-controller-manager 查看其日志。</p>
<h4 id="kube-scheduler-日志">kube-scheduler 日志</h4>
<pre><code class="lang-sh">PODNAME=$(kubectl -n kube-system get pod <span class="hljs-_">-l</span> component=kube-scheduler -o jsonpath=<span class="hljs-string">'{.items[0].metadata.name}'</span>)
kubectl -n kube-system logs <span class="hljs-variable">$PODNAME</span> --tail 100
</code></pre>
<p>以上命令操作假设控制平面以 Kubernetes 静态 Pod 的形式来运行。如果 kube-scheduler 是用 systemd 管理的，则需要登录到 master 节点上，然后使用 journalctl -u kube-scheduler 查看其日志。</p>
<h4 id="kube-dns-日志">kube-dns 日志</h4>
<p>kube-dns 通常以 Addon 的方式部署，每个 Pod 包含三个容器，最关键的是 kubedns 容器的日志：</p>
<pre><code class="lang-sh">PODNAME=$(kubectl -n kube-system get pod <span class="hljs-_">-l</span> k8s-app=kube-dns -o jsonpath=<span class="hljs-string">'{.items[0].metadata.name}'</span>)
kubectl -n kube-system logs <span class="hljs-variable">$PODNAME</span> -c kubedns
</code></pre>
<h4 id="kubelet-日志">Kubelet 日志</h4>
<p>Kubelet 通常以 systemd 管理。查看 Kubelet 日志需要首先 SSH 登录到 Node 上。</p>
<pre><code class="lang-sh">journalctl <span class="hljs-_">-l</span> -u kubelet
</code></pre>
<h4 id="kube-proxy-日志">Kube-proxy 日志</h4>
<p>Kube-proxy 通常以 DaemonSet 的方式部署，可以直接用 kubectl 查询其日志</p>
<pre><code class="lang-sh">$ kubectl -n kube-system get pod <span class="hljs-_">-l</span> component=kube-proxy
NAME               READY     STATUS    RESTARTS   AGE
kube-proxy-42zpn   1/1       Running   0          1d
kube-proxy-7gd4p   1/1       Running   0          3d
kube-proxy-87dbs   1/1       Running   0          4d
$ kubectl -n kube-system logs kube-proxy-42zpn
</code></pre>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="集群状态异常排错" class="level2">集群排错</h1>
<p>本章介绍集群状态异常的排错方法，包括 Kubernetes 主要组件以及必备扩展（如 kube-dns）等，而有关网络的异常排错请参考<a href="network.html">网络异常排错方法</a>。</p>
<h2 id="概述">概述</h2>
<p>排查集群状态异常问题通常从 Node 和 Kubernetes 服务 的状态出发，定位出具体的异常服务，再进而寻找解决方法。集群状态异常可能的原因比较多，常见的有</p>
<ul>
<li>虚拟机或物理机宕机</li>
<li>网络分区</li>
<li>Kubernetes 服务未正常启动</li>
<li>数据丢失或持久化存储不可用（一般在公有云或私有云平台中）</li>
<li>操作失误（如配置错误）</li>
</ul>
<p>按照不同的组件来说，具体的原因可能包括</p>
<ul>
<li>kube-apiserver 无法启动会导致<ul>
<li>集群不可访问</li>
<li>已有的 Pod 和服务正常运行（依赖于 Kubernetes API 的除外）</li>
</ul>
</li>
<li>etcd 集群异常会导致<ul>
<li>kube-apiserver 无法正常读写集群状态，进而导致 Kubernetes API 访问出错</li>
<li>kubelet 无法周期性更新状态</li>
</ul>
</li>
<li>kube-controller-manager/kube-scheduler 异常会导致<ul>
<li>复制控制器、节点控制器、云服务控制器等无法工作，从而导致 Deployment、Service 等无法工作，也无法注册新的 Node 到集群中来</li>
<li>新创建的 Pod 无法调度（总是 Pending 状态）</li>
</ul>
</li>
<li>Node 本身宕机或者 Kubelet 无法启动会导致<ul>
<li>Node 上面的 Pod 无法正常运行</li>
<li>已在运行的 Pod 无法正常终止</li>
</ul>
</li>
<li>网络分区会导致 Kubelet 等与控制平面通信异常以及 Pod 之间通信异常</li>
</ul>
<p>为了维持集群的健康状态，推荐在部署集群时就考虑以下</p>
<ul>
<li>在云平台上开启 VM 的自动重启功能</li>
<li>为 Etcd 配置多节点高可用集群，使用持久化存储（如 AWS EBS 等），定期备份数据</li>
<li>为控制平面配置高可用，比如多 kube-apiserver 负载均衡以及多节点运行 kube-controller-manager、kube-scheduler 以及 kube-dns 等</li>
<li>尽量使用复制控制器和 Service，而不是直接管理 Pod</li>
<li>跨地域的多 Kubernetes 集群</li>
</ul>
<h2 id="查看-node-状态">查看 Node 状态</h2>
<p>一般来说，可以首先查看 Node 的状态，确认 Node 本身是不是 Ready 状态</p>
<pre><code class="lang-sh">kubectl get nodes
kubectl describe node <node-name>
</code></pre>
<p>如果是 NotReady 状态，则可以执行 <code>kubectl describe node <node-name></code> 命令来查看当前 Node 的事件。这些事件通常都会有助于排查 Node 发生的问题。</p>
<h2 id="ssh-登录-node">SSH 登录 Node</h2>
<p>在排查 Kubernetes 问题时，通常需要 SSH 登录到具体的 Node 上面查看 kubelet、docker、iptables 等的状态和日志。在使用云平台时，可以给相应的 VM 绑定一个公网 IP；而在物理机部署时，可以通过路由器上的端口映射来访问。但更简单的方法是使用 SSH Pod （不要忘记替换成你自己的 nodeName）：</p>
<pre><code class="lang-yaml"><span class="hljs-comment"># cat ssh.yaml</span>
<span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Service
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> ssh
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  selector:</span>
<span class="hljs-attr">    app:</span> ssh
<span class="hljs-attr">  type:</span> LoadBalancer
<span class="hljs-attr">  ports:</span>
<span class="hljs-attr">  - protocol:</span> TCP
<span class="hljs-attr">    port:</span> <span class="hljs-number">22</span>
<span class="hljs-attr">    targetPort:</span> <span class="hljs-number">22</span>
<span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> extensions/v1beta1
<span class="hljs-attr">kind:</span> Deployment
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> ssh
<span class="hljs-attr">  labels:</span>
<span class="hljs-attr">    app:</span> ssh
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  replicas:</span> <span class="hljs-number">1</span>
<span class="hljs-attr">  selector:</span>
<span class="hljs-attr">    matchLabels:</span>
<span class="hljs-attr">      app:</span> ssh
<span class="hljs-attr">  template:</span>
<span class="hljs-attr">    metadata:</span>
<span class="hljs-attr">      labels:</span>
<span class="hljs-attr">        app:</span> ssh
<span class="hljs-attr">    spec:</span>
<span class="hljs-attr">      containers:</span>
<span class="hljs-attr">      - name:</span> alpine
<span class="hljs-attr">        image:</span> alpine
<span class="hljs-attr">        ports:</span>
<span class="hljs-attr">        - containerPort:</span> <span class="hljs-number">22</span>
<span class="hljs-attr">        stdin:</span> <span class="hljs-literal">true</span>
<span class="hljs-attr">        tty:</span> <span class="hljs-literal">true</span>
<span class="hljs-attr">      hostNetwork:</span> <span class="hljs-literal">true</span>
<span class="hljs-attr">      nodeName:</span> <node-name<span class="hljs-string">>
</span></code></pre>
<pre><code class="lang-sh">$ kubectl create <span class="hljs-_">-f</span> ssh.yaml
$ kubectl get svc ssh
NAME      TYPE           CLUSTER-IP    EXTERNAL-IP      PORT(S)        AGE
ssh       LoadBalancer   10.0.99.149   52.52.52.52   22:32008/TCP   5m
</code></pre>
<p>接着，就可以通过 ssh 服务的外网 IP 来登录 Node，如 <code>ssh user@52.52.52.52</code>。</p>
<p>在使用完后， 不要忘记删除 SSH 服务 <code>kubectl delete -f ssh.yaml</code>。</p>
<h2 id="查看日志">查看日志</h2>
<p>一般来说，Kubernetes 的主要组件有两种部署方法</p>
<ul>
<li>直接使用 systemd 等启动控制节点的各个服务</li>
<li>使用 Static Pod 来管理和启动控制节点的各个服务</li>
</ul>
<p>使用 systemd 等管理控制节点服务时，查看日志必须要首先 SSH 登录到机器上，然后查看具体的日志文件。如</p>
<pre><code class="lang-sh">journalctl <span class="hljs-_">-l</span> -u kube-apiserver
journalctl <span class="hljs-_">-l</span> -u kube-controller-manager
journalctl <span class="hljs-_">-l</span> -u kube-scheduler
journalctl <span class="hljs-_">-l</span> -u kubelet
journalctl <span class="hljs-_">-l</span> -u kube-proxy
</code></pre>
<p>或者直接查看日志文件</p>
<ul>
<li>/var/log/kube-apiserver.log</li>
<li>/var/log/kube-scheduler.log</li>
<li>/var/log/kube-controller-manager.log</li>
<li>/var/log/kubelet.log</li>
<li>/var/log/kube-proxy.log</li>
</ul>
<p>而对于使用 Static Pod 部署集群控制平面服务的场景，可以参考下面这些查看日志的方法。</p>
<h3 id="kube-apiserver-日志">kube-apiserver 日志</h3>
<pre><code class="lang-sh">PODNAME=$(kubectl -n kube-system get pod <span class="hljs-_">-l</span> component=kube-apiserver -o jsonpath=<span class="hljs-string">'{.items[0].metadata.name}'</span>)
kubectl -n kube-system logs <span class="hljs-variable">$PODNAME</span> --tail 100
</code></pre>
<h3 id="kube-controller-manager-日志">kube-controller-manager 日志</h3>
<pre><code class="lang-sh">PODNAME=$(kubectl -n kube-system get pod <span class="hljs-_">-l</span> component=kube-controller-manager -o jsonpath=<span class="hljs-string">'{.items[0].metadata.name}'</span>)
kubectl -n kube-system logs <span class="hljs-variable">$PODNAME</span> --tail 100
</code></pre>
<h3 id="kube-scheduler-日志">kube-scheduler 日志</h3>
<pre><code class="lang-sh">PODNAME=$(kubectl -n kube-system get pod <span class="hljs-_">-l</span> component=kube-scheduler -o jsonpath=<span class="hljs-string">'{.items[0].metadata.name}'</span>)
kubectl -n kube-system logs <span class="hljs-variable">$PODNAME</span> --tail 100
</code></pre>
<h3 id="kube-dns-日志">kube-dns 日志</h3>
<pre><code class="lang-sh">PODNAME=$(kubectl -n kube-system get pod <span class="hljs-_">-l</span> k8s-app=kube-dns -o jsonpath=<span class="hljs-string">'{.items[0].metadata.name}'</span>)
kubectl -n kube-system logs <span class="hljs-variable">$PODNAME</span> -c kubedns
</code></pre>
<h3 id="kubelet-日志">Kubelet 日志</h3>
<p>查看 Kubelet 日志需要首先 SSH 登录到 Node 上。</p>
<pre><code class="lang-sh">journalctl <span class="hljs-_">-l</span> -u kubelet
</code></pre>
<h3 id="kube-proxy-日志">Kube-proxy 日志</h3>
<p>Kube-proxy 通常以 DaemonSet 的方式部署</p>
<pre><code class="lang-sh">$ kubectl -n kube-system get pod <span class="hljs-_">-l</span> component=kube-proxy
NAME               READY     STATUS    RESTARTS   AGE
kube-proxy-42zpn   1/1       Running   0          1d
kube-proxy-7gd4p   1/1       Running   0          3d
kube-proxy-87dbs   1/1       Running   0          4d
$ kubectl -n kube-system logs kube-proxy-42zpn
</code></pre>
<h2 id="kube-dnsdashboard-crashloopbackoff">Kube-dns/Dashboard CrashLoopBackOff</h2>
<p>由于 Dashboard 依赖于 kube-dns，所以这个问题一般是由于 kube-dns 无法正常启动导致的。查看 kube-dns 的日志</p>
<pre><code class="lang-sh">$ kubectl logs --namespace=kube-system $(kubectl get pods --namespace=kube-system <span class="hljs-_">-l</span> k8s-app=kube-dns -o name) -c kubedns
$ kubectl logs --namespace=kube-system $(kubectl get pods --namespace=kube-system <span class="hljs-_">-l</span> k8s-app=kube-dns -o name) -c dnsmasq
$ kubectl logs --namespace=kube-system $(kubectl get pods --namespace=kube-system <span class="hljs-_">-l</span> k8s-app=kube-dns -o name) -c sidecar
</code></pre>
<p>可以发现如下的错误日志</p>
<pre><code class="lang-sh">Waiting <span class="hljs-keyword">for</span> services and endpoints to be initialized from apiserver...
skydns: failure to forward request <span class="hljs-string">"read udp 10.240.0.18:47848->168.63.129.16:53: i/o timeout"</span>
Timeout waiting <span class="hljs-keyword">for</span> initialization
</code></pre>
<p>这说明 kube-dns pod 无法转发 DNS 请求到上游 DNS 服务器。解决方法为</p>
<ul>
<li>如果使用的 Docker 版本大于 1.12，则在每个 Node 上面运行 <code>iptables -P FORWARD ACCEPT</code> 开启 Docker 容器的 IP 转发</li>
<li>等待一段时间，如果还未恢复，则检查 Node 网络是否正确配置，比如是否可以正常请求上游DNS服务器、是否开启了 IP 转发（包括 Node 内部和公有云上虚拟网卡等）、是否有安全组禁止了 DNS 请求等</li>
</ul>
<p>如果错误日志中不是转发 DNS 请求超时，而是访问 kube-apiserver 超时，比如</p>
<pre><code class="lang-sh">E0122 06:56:04.774977       1 reflector.go:199] k8s.io/dns/vendor/k8s.io/client-go/tools/cache/reflector.go:94: Failed to list *v1.Endpoints: Get https://10.0.0.1:443/api/v1/endpoints?resourceVersion=0: dial tcp 10.0.0.1:443: i/o timeout
I0122 06:56:04.775358       1 dns.go:174] Waiting <span class="hljs-keyword">for</span> services and endpoints to be initialized from apiserver...
E0122 06:56:04.775574       1 reflector.go:199] k8s.io/dns/vendor/k8s.io/client-go/tools/cache/reflector.go:94: Failed to list *v1.Service: Get https://10.0.0.1:443/api/v1/services?resourceVersion=0: dial tcp 10.0.0.1:443: i/o timeout
I0122 06:56:05.275295       1 dns.go:174] Waiting <span class="hljs-keyword">for</span> services and endpoints to be initialized from apiserver...
I0122 06:56:05.775182       1 dns.go:174] Waiting <span class="hljs-keyword">for</span> services and endpoints to be initialized from apiserver...
I0122 06:56:06.275288       1 dns.go:174] Waiting <span class="hljs-keyword">for</span> services and endpoints to be initialized from apiserver...
</code></pre>
<p>这说明 Pod 网络（一般是多主机之间）访问异常，包括 Pod->Node、Node->Pod 以及 Node-Node 等之间的往来通信异常。可能的原因比较多，具体的排错方法可以参考<a href="network.html">网络异常排错指南</a>。</p>
<h2 id="node-notready">Node NotReady</h2>
<p>Node 处于 NotReady 状态，社区 issue <a href="https://github.com/kubernetes/kubernetes/issues/45419" target="_blank">#45419</a>。</p>
<p>NotReady 的原因比较多，在排查时最重要的就是执行 <code>kubectl describe node <node name></code> 并查看 Kubelet 日志中的错误信息。常见问题的修复方法为：</p>
<ul>
<li>CNI 网络插件未部署：部署 CNI 插件。</li>
<li>Docker 僵死（API 不响应）：重启 Docker。</li>
<li>磁盘空间不足：清理磁盘空间，比如镜像、临时文件等。</li>
</ul>
<h2 id="kubelet-failed-to-initialize-top-level-qos-containers">Kubelet: failed to initialize top level QOS containers</h2>
<p>重启 kubelet 时报错 <code>Failed to start ContainerManager failed to initialise top level QOS containers</code>（参考 <a href="https://github.com/kubernetes/kubernetes/issues/43856" target="_blank">#43856</a>），临时解决方法是：</p>
<ol>
<li>在 docker.service 配置中增加 <code>--exec-opt native.cgroupdriver=systemd</code> 选项。</li>
<li>重启主机</li>
</ol>
<p>该问题已于2017年4月27日修复（v1.7.0+， <a href="https://github.com/kubernetes/kubernetes/pull/44940" target="_blank">#44940</a>）。更新集群到新版本即可解决这个问题。</p>
<h2 id="kubelet-一直报-failednodeallocatableenforcement-事件">Kubelet 一直报 FailedNodeAllocatableEnforcement 事件</h2>
<p>当 NodeAllocatable 特性未开启时（即 kubelet 设置了 <code>--cgroups-per-qos=false</code> ），查看 node 的事件会发现每分钟都会有 <code>Failed to update Node Allocatable Limits</code> 的警告信息：</p>
<pre><code class="lang-sh">$ kubectl describe node node1
Events:
  Type     Reason                            Age                  From                               Message
  ----     ------                            ----                 ----                               -------
  Warning  FailedNodeAllocatableEnforcement  2m (x1001 over 16h)  kubelet, aks-agentpool-22604214-0  Failed to update Node Allocatable Limits <span class="hljs-string">""</span>: failed to <span class="hljs-built_in">set</span> supported cgroup subsystems <span class="hljs-keyword">for</span> cgroup : Failed to <span class="hljs-built_in">set</span> config <span class="hljs-keyword">for</span> supported subsystems : failed to write 7285047296 to memory.limit_<span class="hljs-keyword">in</span>_bytes: write /var/lib/docker/overlay2/5650a1aadf9c758946073fefa1558446ab582148ddd3ee7e7cb9d269fab20f72/merged/sys/fs/cgroup/memory/memory.limit_<span class="hljs-keyword">in</span>_bytes: invalid argument
</code></pre>
<p>如果 NodeAllocatable 特性确实不需要，那么该警告事件可以忽略。但根据 Kubernetes 文档 <a href="https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/" target="_blank">Reserve Compute Resources for System Daemons</a>，最好开启该特性：</p>
<blockquote>
<p>Kubernetes nodes can be scheduled to <code>Capacity</code>. Pods can consume all the available capacity on a node by default. This is an issue because nodes typically run quite a few system daemons that power the OS and Kubernetes itself. Unless resources are set aside for these system daemons, pods and system daemons compete for resources and lead to resource starvation issues on the node.</p>
<p>The <code>kubelet</code> exposes a feature named <code>Node Allocatable</code> that helps to reserve compute resources for system daemons. Kubernetes recommends cluster administrators to configure <code>Node Allocatable</code> based on their workload density on each node.</p>
<pre><code class="lang-sh">      Node Capacity
---------------------------
|     kube-reserved       |
|-------------------------|
|     system-reserved     |
|-------------------------|
|    eviction-threshold   |
|-------------------------|
|                         |
|      allocatable        |
|   (available <span class="hljs-keyword">for</span> pods)  |
|                         |
|                         |
---------------------------
</code></pre>
</blockquote>
<p>开启方法为：</p>
<pre><code class="lang-sh">kubelet --cgroups-per-qos=<span class="hljs-literal">true</span> --enforce-node-allocatable=pods ...
</code></pre>
<h2 id="kube-proxy-error-looking-for-path-of-conntrack">Kube-proxy: error looking for path of conntrack</h2>
<p>kube-proxy 报错，并且 service 的 DNS 解析异常</p>
<pre><code class="lang-sh">kube-proxy[2241]: E0502 15:55:13.889842    2241 conntrack.go:42] conntrack returned error: error looking <span class="hljs-keyword">for</span> path of conntrack: <span class="hljs-built_in">exec</span>: <span class="hljs-string">"conntrack"</span>: executable file not found <span class="hljs-keyword">in</span> <span class="hljs-variable">$PATH</span>
</code></pre>
<p>解决方式是安装 <code>conntrack-tools</code> 包后重启 kube-proxy 即可。</p>
<h2 id="dashboard-中无资源使用图表">Dashboard 中无资源使用图表</h2>
<p>正常情况下，Dashboard 首页应该会显示资源使用情况的图表，如</p>
<p><img src="images/dashboard-ui.png" alt=""/></p>
<p>如果没有这些图表，则需要首先检查 Heapster 是否正在运行（因为Dashboard 需要访问 Heapster 来查询资源使用情况）：</p>
<pre><code class="lang-sh">kubectl -n kube-system get pods <span class="hljs-_">-l</span> k8s-app=heapster
NAME                        READY     STATUS    RESTARTS   AGE
heapster-86b59f68f6-h4vt6   2/2       Running   0          5d
</code></pre>
<p>如果查询结果为空，说明 Heapster 还未部署，可以参考 <a href="https://github.com/kubernetes/heapster" target="_blank">https://github.com/kubernetes/heapster</a> 来部署。</p>
<p>但如果 Heapster 处于正常状态，那么需要查看 dashboard 的日志，确认是否还有其他问题</p>
<pre><code class="lang-sh">$ kubectl -n kube-system get pods <span class="hljs-_">-l</span> k8s-app=kubernetes-dashboard
NAME                                   READY     STATUS    RESTARTS   AGE
kubernetes-dashboard-665b4f7df-dsjpn   1/1       Running   0          5d

$ kubectl -n kube-system logs kubernetes-dashboard-665b4f7df-dsjpn
</code></pre>
<h2 id="hpa-不自动扩展-pod">HPA 不自动扩展 Pod</h2>
<p>查看 HPA 的事件，发现</p>
<pre><code class="lang-sh">$ kubectl describe hpa php-apache
Name:                                                  php-apache
Namespace:                                             default
Labels:                                                <none>
Annotations:                                           <none>
CreationTimestamp:                                     Wed, 27 Dec 2017 14:36:38 +0800
Reference:                                             Deployment/php-apache
Metrics:                                               ( current / target )
  resource cpu on pods  (as a percentage of request):  <unknown> / 50%
M<span class="hljs-keyword">in</span> replicas:                                          1
Max replicas:                                          10
Conditions:
  Type           Status  Reason                   Message
  ----           ------  ------                   -------
  AbleToScale    True    SucceededGetScale        the HPA controller was able to get the target<span class="hljs-string">'s current scale
  ScalingActive  False   FailedGetResourceMetric  the HPA was unable to compute the replica count: unable to get metrics for resource cpu: unable to fetch metrics from API: the server could not find the requested resource (get pods.metrics.k8s.io)
Events:
  Type     Reason                   Age                  From                       Message
  ----     ------                   ----                 ----                       -------
  Warning  FailedGetResourceMetric  3m (x2231 over 18h)  horizontal-pod-autoscaler  unable to get metrics for resource cpu: unable to fetch metrics from API: the server could not find the requested resource (get pods.metrics.k8s.io)
</span></code></pre>
<p>这说明 <a href="../addons/metrics.html">metrics-server</a> 未部署，可以参考 <a href="../addons/metrics.html">这里</a> 部署。</p>
<h2 id="node-存储空间不足">Node 存储空间不足</h2>
<p>Node 存储空间不足一般是容器镜像未及时清理导致的，比如短时间内运行了很多使用较大镜像的容器等。Kubelet 会自动清理未使用的镜像，但如果想要立即清理，可以使用 <a href="https://github.com/spotify/docker-gc" target="_blank">spotify/docker-gc</a>：</p>
<pre><code class="lang-sh">sudo docker run --rm -v /var/run/docker.sock:/var/run/docker.sock -v /etc:/etc:ro spotify/docker-gc
</code></pre>
<h2 id="sysfscgroup-空间不足">/sys/fs/cgroup 空间不足</h2>
<p>很多发行版默认的 fs.inotify.max_user_watches 太小，只有 8192，可以通过增大该配置解决。比如</p>
<pre><code class="lang-sh">$ sudo sysctl fs.inotify.max_user_watches=524288
</code></pre>
<h2 id="大量-configmapsecret-导致kubernetes缓慢">大量 ConfigMap/Secret 导致Kubernetes缓慢</h2>
<p>这是从 Kubernetes 1.12 开始才有的问题，Kubernetes issue: <a href="https://github.com/kubernetes/kubernetes/issues/74412" target="_blank">#74412</a>。</p>
<blockquote>
<p>This worked well on version 1.11 of Kubernetes. After upgrading to 1.12 or 1.13, I've noticed that doing this will cause the cluster to significantly slow down; up to the point where nodes are being marked as NotReady and no new work is being scheduled.</p>
<p>For example, consider a scenario in which I schedule 400 jobs, each with its own ConfigMap, which print "Hello World" on a single-node cluster would.</p>
<ul>
<li><p>On v1.11, it takes about 10 minutes for the cluster to process all jobs. New jobs can be scheduled.</p>
</li>
<li><p>On v1.12 and v1.13, it takes about 60 minutes for the cluster to process all jobs. After this, no new jobs can be scheduled.</p>
</li>
</ul>
<p>This is related to max concurrent http2 streams and the change of configmap manager of kubelet. By default, max concurrent http2 stream of http2 server in kube-apiserver is 250, and every configmap will consume one stream to watch in kubelet at least from version 1.13.x. Kubelet will stuck to communicate to kube-apiserver and then become NotReady if too many pods with configmap scheduled to it. A work around is to change the config http2-max-streams-per-connection of kube-apiserver to a bigger value.</p>
</blockquote>
<p>临时解决方法：为 Kubelet 设置 <code>configMapAndSecretChangeDetectionStrategy: Cache</code> （参考 <a href="https://github.com/kubernetes/kubernetes/pull/74755" target="_blank">这里</a> ）。</p>
<p>修复方法：升级 Go 版本到 1.12 后重新构建 Kubernetes（社区正在进行中）。修复后，Kubelet 可以 watch 的 configmap 可以从之前的 236 提高到至少 10000。</p>
<h2 id="kubelet-内存泄漏">Kubelet 内存泄漏</h2>
<p>这是从 1.12 版本开始有的问题（只在使用 hyperkube 启动 kubelet 时才有问题），社区 issue 为 <a href="https://github.com/kubernetes/kubernetes/issues/73587" target="_blank">#73587</a>。</p>
<pre><code>(pprof) root@ip-172-31-10-50:~# go tool pprof  http://localhost:10248/debug/pprof/heap
Fetching profile from http://localhost:10248/debug/pprof/heap
Saved profile in /root/pprof/pprof.hyperkube.localhost:10248.alloc_objects.alloc_space.inuse_objects.inuse_space.002.pb.gz
Entering interactive mode (type "help" for commands)
(pprof) top
2406.93MB of 2451.55MB total (98.18%)
Dropped 2863 nodes (cum <= 12.26MB)
Showing top 10 nodes out of 34 (cum >= 2411.39MB)
      flat  flat%   sum%        cum   cum%
 2082.07MB 84.93% 84.93%  2082.07MB 84.93%  k8s.io/kubernetes/vendor/github.com/beorn7/perks/quantile.newStream (inline)
  311.65MB 12.71% 97.64%  2398.72MB 97.84%  k8s.io/kubernetes/vendor/github.com/prometheus/client_golang/prometheus.newSummary
   10.71MB  0.44% 98.08%  2414.43MB 98.49%  k8s.io/kubernetes/vendor/github.com/prometheus/client_golang/prometheus.(*MetricVec).getOrCreateMetricWithLabelValues
    2.50MB   0.1% 98.18%  2084.57MB 85.03%  k8s.io/kubernetes/vendor/github.com/beorn7/perks/quantile.NewTargeted
         0     0% 98.18%  2412.06MB 98.39%  k8s.io/kubernetes/cmd/kubelet/app.startKubelet.func1
         0     0% 98.18%  2412.06MB 98.39%  k8s.io/kubernetes/pkg/kubelet.(*Kubelet).HandlePodAdditions
         0     0% 98.18%  2412.06MB 98.39%  k8s.io/kubernetes/pkg/kubelet.(*Kubelet).Run
</code></pre><pre><code class="lang-sh">curl <span class="hljs-_">-s</span> localhost:10255/metrics | sed <span class="hljs-string">'s/{.*//'</span> | sort | uniq -c | sort -nr
  25749 reflector_watch_duration_seconds
  25749 reflector_list_duration_seconds
  25749 reflector_items_per_watch
  25749 reflector_items_per_list
   8583 reflector_watches_total
   8583 reflector_watch_duration_seconds_sum
   8583 reflector_watch_duration_seconds_count
   8583 reflector_short_watches_total
   8583 reflector_lists_total
   8583 reflector_list_duration_seconds_sum
   8583 reflector_list_duration_seconds_count
   8583 reflector_last_resource_version
   8583 reflector_items_per_watch_sum
   8583 reflector_items_per_watch_count
   8583 reflector_items_per_list_sum
   8583 reflector_items_per_list_count
    165 storage_operation_duration_seconds_bucket
     51 kubelet_runtime_operations_latency_microseconds
     44 rest_client_request_latency_seconds_bucket
     33 kubelet_docker_operations_latency_microseconds
     17 kubelet_runtime_operations_latency_microseconds_sum
     17 kubelet_runtime_operations_latency_microseconds_count
     17 kubelet_runtime_operations
</code></pre>
<p>修复方法：禁止 <a href="https://github.com/kubernetes/kubernetes/issues/73587" target="_blank">Reflector metrics</a>。</p>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-cluster/" target="_blank">Troubleshoot Clusters</a></li>
<li><a href="https://docs.microsoft.com/en-us/azure/aks/aks-ssh#configure-ssh-access" target="_blank">SSH into Azure Container Service (AKS) cluster nodes</a></li>
<li><a href="https://github.com/kubernetes/dashboard/wiki/FAQ" target="_blank">Kubernetes dashboard FAQ</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="pod-异常排错" class="level2">Pod排错</h1>
<p>本章介绍 Pod 运行异常的排错方法。</p>
<p>一般来说，无论 Pod 处于什么异常状态，都可以执行以下命令来查看 Pod 的状态</p>
<ul>
<li><code>kubectl get pod <pod-name> -o yaml</code> 查看 Pod 的配置是否正确</li>
<li><code>kubectl describe pod <pod-name></code> 查看 Pod 的事件</li>
<li><code>kubectl logs <pod-name> [-c <container-name>]</code> 查看容器日志</li>
</ul>
<p>这些事件和日志通常都会有助于排查 Pod 发生的问题。</p>
<h2 id="pod-一直处于-pending-状态">Pod 一直处于 Pending 状态</h2>
<p>Pending 说明 Pod 还没有调度到某个 Node 上面。可以通过 <code>kubectl describe pod <pod-name></code> 命令查看到当前 Pod 的事件，进而判断为什么没有调度。如</p>
<pre><code class="lang-sh">$ kubectl describe pod mypod
...
Events:
  Type     Reason            Age                From               Message
  ----     ------            ----               ----               -------
  Warning  FailedScheduling  12s (x6 over 27s)  default-scheduler  0/4 nodes are available: 2 Insufficient cpu.
</code></pre>
<p>可能的原因包括</p>
<ul>
<li>资源不足，集群内所有的 Node 都不满足该 Pod 请求的 CPU、内存、GPU 或者临时存储空间等资源。解决方法是删除集群内不用的 Pod 或者增加新的 Node。</li>
<li>HostPort 端口已被占用，通常推荐使用 Service 对外开放服务端口</li>
</ul>
<h2 id="pod-一直处于-waiting-或-containercreating-状态">Pod 一直处于 Waiting 或 ContainerCreating 状态</h2>
<p>首先还是通过 <code>kubectl describe pod <pod-name></code> 命令查看到当前 Pod 的事件</p>
<pre><code class="lang-sh">$ kubectl -n kube-system describe pod nginx-pod
Events:
  Type     Reason                 Age               From               Message
  ----     ------                 ----              ----               -------
  Normal   Scheduled              1m                default-scheduler  Successfully assigned nginx-pod to node1
  Normal   SuccessfulMountVolume  1m                kubelet, gpu13     MountVolume.SetUp succeeded <span class="hljs-keyword">for</span> volume <span class="hljs-string">"config-volume"</span>
  Normal   SuccessfulMountVolume  1m                kubelet, gpu13     MountVolume.SetUp succeeded <span class="hljs-keyword">for</span> volume <span class="hljs-string">"coredns-token-sxdmc"</span>
  Warning  FailedSync             2s (x4 over 46s)  kubelet, gpu13     Error syncing pod
  Normal   SandboxChanged         1s (x4 over 46s)  kubelet, gpu13     Pod sandbox changed, it will be killed and re-created.
</code></pre>
<p>可以发现，该 Pod 的 Sandbox 容器无法正常启动，具体原因需要查看 Kubelet 日志：</p>
<pre><code class="lang-sh">$ journalctl -u kubelet
...
Mar 14 04:22:04 node1 kubelet[29801]: E0314 04:22:04.649912   29801 cni.go:294] Error adding network: failed to <span class="hljs-built_in">set</span> bridge addr: <span class="hljs-string">"cni0"</span> already has an IP address different from 10.244.4.1/24
Mar 14 04:22:04 node1 kubelet[29801]: E0314 04:22:04.649941   29801 cni.go:243] Error <span class="hljs-keyword">while</span> adding to cni network: failed to <span class="hljs-built_in">set</span> bridge addr: <span class="hljs-string">"cni0"</span> already has an IP address different from 10.244.4.1/24
Mar 14 04:22:04 node1 kubelet[29801]: W0314 04:22:04.891337   29801 cni.go:258] CNI failed to retrieve network namespace path: Cannot find network namespace <span class="hljs-keyword">for</span> the terminated container <span class="hljs-string">"c4fd616cde0e7052c240173541b8543f746e75c17744872aa04fe06f52b5141c"</span>
Mar 14 04:22:05 node1 kubelet[29801]: E0314 04:22:05.965801   29801 remote_runtime.go:91] RunPodSandbox from runtime service failed: rpc error: code = 2 desc = NetworkPlugin cni failed to <span class="hljs-built_in">set</span> up pod <span class="hljs-string">"nginx-pod"</span> network: failed to <span class="hljs-built_in">set</span> bridge addr: <span class="hljs-string">"cni0"</span> already has an IP address different from 10.244.4.1/24
</code></pre>
<p>发现是 cni0 网桥配置了一个不同网段的 IP 地址导致，删除该网桥（网络插件会自动重新创建）即可修复</p>
<pre><code class="lang-sh">$ ip link <span class="hljs-built_in">set</span> cni0 down
$ brctl delbr cni0
</code></pre>
<p>除了以上错误，其他可能的原因还有</p>
<ul>
<li>镜像拉取失败，比如<ul>
<li>配置了错误的镜像</li>
<li>Kubelet 无法访问镜像（国内环境访问 <code>gcr.io</code> 需要特殊处理）</li>
<li>私有镜像的密钥配置错误</li>
<li>镜像太大，拉取超时（可以适当调整 kubelet 的 <code>--image-pull-progress-deadline</code> 和 <code>--runtime-request-timeout</code> 选项）</li>
</ul>
</li>
<li>CNI 网络错误，一般需要检查 CNI 网络插件的配置，比如<ul>
<li>无法配置 Pod 网络</li>
<li>无法分配 IP 地址</li>
</ul>
</li>
<li>容器无法启动，需要检查是否打包了正确的镜像或者是否配置了正确的容器参数</li>
</ul>
<h2 id="pod-处于-imagepullbackoff-状态">Pod 处于 ImagePullBackOff 状态</h2>
<p>这通常是镜像名称配置错误或者私有镜像的密钥配置错误导致。这种情况可以使用 <code>docker pull <image></code> 来验证镜像是否可以正常拉取。</p>
<pre><code class="lang-sh">$ kubectl describe pod mypod
...
Events:
  Type     Reason                 Age                From                                Message
  ----     ------                 ----               ----                                -------
  Normal   Scheduled              36s                default-scheduler                   Successfully assigned sh to k8s-agentpool1-38622806-0
  Normal   SuccessfulMountVolume  35s                kubelet, k8s-agentpool1-38622806-0  MountVolume.SetUp succeeded <span class="hljs-keyword">for</span> volume <span class="hljs-string">"default-token-n4pn6"</span>
  Normal   Pulling                17s (x2 over 33s)  kubelet, k8s-agentpool1-38622806-0  pulling image <span class="hljs-string">"a1pine"</span>
  Warning  Failed                 14s (x2 over 29s)  kubelet, k8s-agentpool1-38622806-0  Failed to pull image <span class="hljs-string">"a1pine"</span>: rpc error: code = Unknown desc = Error response from daemon: repository a1pine not found: does not exist or no pull access
  Warning  Failed                 14s (x2 over 29s)  kubelet, k8s-agentpool1-38622806-0  Error: ErrImagePull
  Normal   SandboxChanged         4s (x7 over 28s)   kubelet, k8s-agentpool1-38622806-0  Pod sandbox changed, it will be killed and re-created.
  Normal   BackOff                4s (x5 over 25s)   kubelet, k8s-agentpool1-38622806-0  Back-off pulling image <span class="hljs-string">"a1pine"</span>
  Warning  Failed                 1s (x6 over 25s)   kubelet, k8s-agentpool1-38622806-0  Error: ImagePullBackOff
</code></pre>
<p>如果是私有镜像，需要首先创建一个 docker-registry 类型的 Secret</p>
<pre><code class="lang-sh">kubectl create secret docker-registry my-secret --docker-server=DOCKER_REGISTRY_SERVER --docker-username=DOCKER_USER --docker-password=DOCKER_PASSWORD --docker-email=DOCKER_EMAIL
</code></pre>
<p>然后在容器中引用这个 Secret</p>
<pre><code class="lang-yaml"><span class="hljs-attr">spec:</span>
<span class="hljs-attr">  containers:</span>
<span class="hljs-attr">  - name:</span> private-reg-container
<span class="hljs-attr">    image:</span> <your-private-image<span class="hljs-string">>
</span><span class="hljs-attr">  imagePullSecrets:</span>
<span class="hljs-attr">  - name:</span> my-secret
</code></pre>
<h2 id="pod-一直处于-crashloopbackoff-状态">Pod 一直处于 CrashLoopBackOff 状态</h2>
<p>CrashLoopBackOff 状态说明容器曾经启动了，但又异常退出了。此时 Pod 的 RestartCounts 通常是大于 0 的，可以先查看一下容器的日志</p>
<pre><code class="lang-sh">kubectl describe pod <pod-name>
kubectl logs <pod-name>
kubectl logs --previous <pod-name>
</code></pre>
<p>这里可以发现一些容器退出的原因，比如</p>
<ul>
<li>容器进程退出</li>
<li>健康检查失败退出</li>
<li>OOMKilled</li>
</ul>
<pre><code class="lang-sh">$ kubectl describe pod mypod
...
Containers:
  sh:
    Container ID:  docker://3f7a2ee0e7e0e16c22090a25f9b6e42b5c06ec049405bc34d3aa183060eb4906
    Image:         alpine
    Image ID:      docker-pullable://alpine@sha256:7b848083f93822dd21b0a2f14a110bd99f6efb4b838d499df6d04a49d0debf8b
    Port:          <none>
    Host Port:     <none>
    State:          Terminated
      Reason:       OOMKilled
      Exit Code:    2
    Last State:     Terminated
      Reason:       OOMKilled
      Exit Code:    2
    Ready:          False
    Restart Count:  3
    Limits:
      cpu:     1
      memory:  1G
    Requests:
      cpu:        100m
      memory:     500M
...
</code></pre>
<p>如果此时如果还未发现线索，还可以到容器内执行命令来进一步查看退出原因</p>
<pre><code class="lang-sh">kubectl <span class="hljs-built_in">exec</span> cassandra -- cat /var/<span class="hljs-built_in">log</span>/cassandra/system.log
</code></pre>
<p>如果还是没有线索，那就需要 SSH 登录该 Pod 所在的 Node 上，查看 Kubelet 或者 Docker 的日志进一步排查了</p>
<pre><code class="lang-sh"><span class="hljs-comment"># Query Node</span>
kubectl get pod <pod-name> -o wide

<span class="hljs-comment"># SSH to Node</span>
ssh <username>@<node-name>
</code></pre>
<h2 id="pod-处于-error-状态">Pod 处于 Error 状态</h2>
<p>通常处于 Error 状态说明 Pod 启动过程中发生了错误。常见的原因包括</p>
<ul>
<li>依赖的 ConfigMap、Secret 或者 PV 等不存在</li>
<li>请求的资源超过了管理员设置的限制，比如超过了 LimitRange 等</li>
<li>违反集群的安全策略，比如违反了 PodSecurityPolicy 等</li>
<li>容器无权操作集群内的资源，比如开启 RBAC 后，需要为 ServiceAccount 配置角色绑定</li>
</ul>
<h2 id="pod-处于-terminating-或-unknown-状态">Pod 处于 Terminating 或 Unknown 状态</h2>
<p>从 v1.5 开始，Kubernetes 不会因为 Node 失联而删除其上正在运行的 Pod，而是将其标记为 Terminating 或 Unknown 状态。想要删除这些状态的 Pod 有三种方法：</p>
<ul>
<li>从集群中删除该 Node。使用公有云时，kube-controller-manager 会在 VM 删除后自动删除对应的 Node。而在物理机部署的集群中，需要管理员手动删除 Node（如 <code>kubectl delete node <node-name></code>。</li>
<li>Node 恢复正常。Kubelet 会重新跟 kube-apiserver 通信确认这些 Pod 的期待状态，进而再决定删除或者继续运行这些 Pod。</li>
<li>用户强制删除。用户可以执行 <code>kubectl delete pods <pod> --grace-period=0 --force</code> 强制删除 Pod。除非明确知道 Pod 的确处于停止状态（比如 Node 所在 VM 或物理机已经关机），否则不建议使用该方法。特别是 StatefulSet 管理的 Pod，强制删除容易导致脑裂或者数据丢失等问题。</li>
</ul>
<p>如果 Kubelet 是以 Docker 容器的形式运行的，此时 kubelet 日志中可能会发现<a href="https://github.com/kubernetes/kubernetes/issues/51835" target="_blank">如下的错误</a>：</p>
<pre><code class="lang-json">{<span class="hljs-string">"log"</span>:<span class="hljs-string">"I0926 19:59:07.162477   54420 kubelet.go:1894] SyncLoop (DELETE, \"api\"): \"billcenter-737844550-26z3w_meipu(30f3ffec-a29f-11e7-b693-246e9607517c)\"\n"</span>,<span class="hljs-string">"stream"</span>:<span class="hljs-string">"stderr"</span>,<span class="hljs-string">"time"</span>:<span class="hljs-string">"2017-09-26T11:59:07.162748656Z"</span>}
{<span class="hljs-string">"log"</span>:<span class="hljs-string">"I0926 19:59:39.977126   54420 reconciler.go:186] operationExecutor.UnmountVolume started for volume \"default-token-6tpnm\" (UniqueName: \"kubernetes.io/secret/30f3ffec-a29f-11e7-b693-246e9607517c-default-token-6tpnm\") pod \"30f3ffec-a29f-11e7-b693-246e9607517c\" (UID: \"30f3ffec-a29f-11e7-b693-246e9607517c\") \n"</span>,<span class="hljs-string">"stream"</span>:<span class="hljs-string">"stderr"</span>,<span class="hljs-string">"time"</span>:<span class="hljs-string">"2017-09-26T11:59:39.977438174Z"</span>}
{<span class="hljs-string">"log"</span>:<span class="hljs-string">"E0926 19:59:39.977461   54420 nestedpendingoperations.go:262] Operation for \"\\\"kubernetes.io/secret/30f3ffec-a29f-11e7-b693-246e9607517c-default-token-6tpnm\\\" (\\\"30f3ffec-a29f-11e7-b693-246e9607517c\\\")\" failed. No retries permitted until 2017-09-26 19:59:41.977419403 +0800 CST (durationBeforeRetry 2s). Error: UnmountVolume.TearDown failed for volume \"default-token-6tpnm\" (UniqueName: \"kubernetes.io/secret/30f3ffec-a29f-11e7-b693-246e9607517c-default-token-6tpnm\") pod \"30f3ffec-a29f-11e7-b693-246e9607517c\" (UID: \"30f3ffec-a29f-11e7-b693-246e9607517c\") : remove /var/lib/kubelet/pods/30f3ffec-a29f-11e7-b693-246e9607517c/volumes/kubernetes.io~secret/default-token-6tpnm: device or resource busy\n"</span>,<span class="hljs-string">"stream"</span>:<span class="hljs-string">"stderr"</span>,<span class="hljs-string">"time"</span>:<span class="hljs-string">"2017-09-26T11:59:39.977728079Z"</span>}
</code></pre>
<p>如果是这种情况，则需要给 kubelet 容器设置 <code>--containerized</code> 参数并传入以下的存储卷</p>
<pre><code class="lang-sh"><span class="hljs-comment"># 以使用 calico 网络插件为例</span>
      -v /:/rootfs:ro,shared \
      -v /sys:/sys:ro \
      -v /dev:/dev:rw \
      -v /var/<span class="hljs-built_in">log</span>:/var/<span class="hljs-built_in">log</span>:rw \
      -v /run/calico/:/run/calico/:rw \
      -v /run/docker/:/run/docker/:rw \
      -v /run/docker.sock:/run/docker.sock:rw \
      -v /usr/lib/os-release:/etc/os-release \
      -v /usr/share/ca-certificates/:/etc/ssl/certs \
      -v /var/lib/docker/:/var/lib/docker:rw,shared \
      -v /var/lib/kubelet/:/var/lib/kubelet:rw,shared \
      -v /etc/kubernetes/ssl/:/etc/kubernetes/ssl/ \
      -v /etc/kubernetes/config/:/etc/kubernetes/config/ \
      -v /etc/cni/net.d/:/etc/cni/net.d/ \
      -v /opt/cni/bin/:/opt/cni/bin/ \
</code></pre>
<p>处于 <code>Terminating</code> 状态的 Pod 在 Kubelet 恢复正常运行后一般会自动删除。但有时也会出现无法删除的情况，并且通过 <code>kubectl delete pods <pod> --grace-period=0 --force</code> 也无法强制删除。此时一般是由于 <code>finalizers</code> 导致的，通过 <code>kubectl edit</code> 将 finalizers 删除即可解决。</p>
<pre><code class="lang-yaml"><span class="hljs-attr">"finalizers":</span> [
  <span class="hljs-string">"foregroundDeletion"</span>
]
</code></pre>
<h2 id="pod-行为异常">Pod 行为异常</h2>
<p>这里所说的行为异常是指 Pod 没有按预期的行为执行，比如没有运行 podSpec 里面设置的命令行参数。这一般是 podSpec yaml 文件内容有误，可以尝试使用 <code>--validate</code> 参数重建容器，比如</p>
<pre><code class="lang-sh">kubectl delete pod mypod
kubectl create --validate <span class="hljs-_">-f</span> mypod.yaml
</code></pre>
<p>也可以查看创建后的 podSpec 是否是对的，比如</p>
<pre><code class="lang-sh">kubectl get pod mypod -o yaml
</code></pre>
<h2 id="修改静态-pod-的-manifest-后未自动重建">修改静态 Pod 的 Manifest 后未自动重建</h2>
<p>Kubelet 使用 inotify 机制检测 <code>/etc/kubernetes/manifests</code> 目录（可通过 Kubelet 的 <code>--pod-manifest-path</code> 选项指定）中静态 Pod 的变化，并在文件发生变化后重新创建相应的 Pod。但有时也会发生修改静态 Pod 的 Manifest 后未自动创建新 Pod 的情景，此时一个简单的修复方法是重启 Kubelet。</p>
<h3 id="参考文档">参考文档</h3>
<ul>
<li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application/" target="_blank">Troubleshoot Applications</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="网络异常排错" class="level2">网络排错</h1>
<p>本章主要介绍各种常见的网络问题以及排错方法，包括 Pod 访问异常、Service 访问异常以及网络安全策略异常等。</p>
<p>说到 Kubernetes 的网络，其实无非就是以下三种情况之一</p>
<ul>
<li>Pod 访问容器外部网络</li>
<li>从容器外部访问 Pod 网络</li>
<li>Pod 之间相互访问</li>
</ul>
<p>当然，以上每种情况还都分别包括本地访问和跨主机访问两种场景，并且一般情况下都是通过 Service 间接访问 Pod。</p>
<p>排查网络问题基本上也是从这几种情况出发，定位出具体的网络异常点，再进而寻找解决方法。网络异常可能的原因比较多，常见的有</p>
<ul>
<li>CNI 网络插件配置错误，导致多主机网络不通，比如<ul>
<li>IP 网段与现有网络冲突</li>
<li>插件使用了底层网络不支持的协议</li>
<li>忘记开启 IP 转发等<ul>
<li><code>sysctl net.ipv4.ip_forward</code></li>
<li><code>sysctl net.bridge.bridge-nf-call-iptables</code></li>
</ul>
</li>
</ul>
</li>
<li>Pod 网络路由丢失，比如<ul>
<li>kubenet 要求网络中有 podCIDR 到主机 IP 地址的路由，这些路由如果没有正确配置会导致 Pod 网络通信等问题</li>
<li>在公有云平台上，kube-controller-manager 会自动为所有 Node 配置路由，但如果配置不当（如认证授权失败、超出配额等），也有可能导致无法配置路由</li>
</ul>
</li>
<li>主机内或者云平台的安全组、防火墙或者安全策略等阻止了 Pod 网络，比如<ul>
<li>非 Kubernetes 管理的 iptables 规则禁止了 Pod 网络</li>
<li>公有云平台的安全组禁止了 Pod 网络（注意 Pod 网络有可能与 Node 网络不在同一个网段）</li>
<li>交换机或者路由器的 ACL 禁止了 Pod 网络</li>
</ul>
</li>
</ul>
<h2 id="flannel-pods-一直处于-initcrashloopbackoff-状态">Flannel Pods 一直处于 Init:CrashLoopBackOff 状态</h2>
<p>Flannel 网络插件非常容易部署，只要一条命令即可</p>
<pre><code class="lang-sh">kubectl apply <span class="hljs-_">-f</span> https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
</code></pre>
<p>然而，部署完成后，Flannel Pod 有可能会碰到初始化失败的错误</p>
<pre><code class="lang-sh">$ kubectl -n kube-system get pod
NAME                            READY     STATUS                  RESTARTS   AGE
kube-flannel-ds-ckfdc           0/1       Init:CrashLoopBackOff   4          2m
kube-flannel-ds-jpp96           0/1       Init:CrashLoopBackOff   4          2m
</code></pre>
<p>查看日志会发现</p>
<pre><code class="lang-sh">$ kubectl -n kube-system logs kube-flannel-ds-jpp96 -c install-cni
cp: can<span class="hljs-string">'t create '</span>/etc/cni/net.d/10-flannel.conflist<span class="hljs-string">': Permission denied
</span></code></pre>
<p>这一般是由于 SELinux 开启导致的，关闭 SELinux 既可解决。有两种方法：</p>
<ul>
<li>修改 <code>/etc/selinux/config</code> 文件方法：<code>SELINUX=disabled</code></li>
<li>通过命令临时修改（重启会丢失）：<code>setenforce 0</code></li>
</ul>
<h2 id="pod-无法分配-ip">Pod 无法分配 IP</h2>
<p>Pod 一直处于 ContainerCreating 状态，查看事件发现网络插件无法为其分配 IP：</p>
<pre><code class="lang-sh">  Normal   SandboxChanged          5m (x74 over 8m)    kubelet, k8s-agentpool-66825246-0  Pod sandbox changed, it will be killed and re-created.
  Warning  FailedCreatePodSandBox  21s (x204 over 8m)  kubelet, k8s-agentpool-66825246-0  Failed create pod sandbox: rpc error: code = Unknown desc = NetworkPlugin cni failed to <span class="hljs-built_in">set</span> up pod <span class="hljs-string">"deployment-azuredisk6-56d8dcb746-487td_default"</span> network: Failed to allocate address: Failed to delegate: Failed to allocate address: No available addresses
</code></pre>
<p>查看网络插件的 IP 分配情况，进一步发现 IP 地址确实已经全部分配完，但真正处于 Running 状态的 Pod 数却很少：</p>
<pre><code class="lang-sh"><span class="hljs-comment"># 详细路径取决于具体的网络插件，当使用 host-local IPAM 插件时，路径位于 /var/lib/cni/networks 下面</span>
$ <span class="hljs-built_in">cd</span> /var/lib/cni/networks/kubenet
$ ls -al|wc <span class="hljs-_">-l</span>
258

$ docker ps | grep POD | wc <span class="hljs-_">-l</span>
7
</code></pre>
<p>这有两种可能的原因</p>
<ul>
<li>网络插件本身的问题，Pod 停止后其 IP 未释放</li>
<li>Pod 重新创建的速度比 Kubelet 调用 CNI 插件回收网络（垃圾回收时删除已停止 Pod 前会先调用 CNI 清理网络）的速度快</li>
</ul>
<p>对第一个问题，最好联系插件开发者询问修复方法或者临时性的解决方法。当然，如果对网络插件的工作原理很熟悉的话，也可以考虑手动释放未使用的 IP 地址，比如：</p>
<ul>
<li>停止 Kubelet</li>
<li>找到 IPAM 插件保存已分配 IP 地址的文件，比如 <code>/var/lib/cni/networks/cbr0</code>（flannel）或者 <code>/var/run/azure-vnet-ipam.json</code>（Azure CNI）等</li>
<li>查询容器已用的 IP 地址，比如 <code>kubectl get pod -o wide --all-namespaces | grep <node-name></code></li>
<li>对比两个列表，从 IPAM 文件中删除未使用的 IP 地址，并手动删除相关的虚拟网卡和网络命名空间（如果有的话）</li>
<li>重启启动 Kubelet</li>
</ul>
<pre><code class="lang-sh"><span class="hljs-comment"># Take kubenet for example to delete the unused IPs</span>
$ <span class="hljs-keyword">for</span> <span class="hljs-built_in">hash</span> <span class="hljs-keyword">in</span> $(tail -n +1 * | grep <span class="hljs-string">'^[A-Za-z0-9]*$'</span> | cut -c 1-8); <span class="hljs-keyword">do</span> <span class="hljs-keyword">if</span> [ -z $(docker ps <span class="hljs-_">-a</span> | grep <span class="hljs-variable">$hash</span> | awk <span class="hljs-string">'{print $1}'</span>) ]; <span class="hljs-keyword">then</span> grep -ilr <span class="hljs-variable">$hash</span> ./; <span class="hljs-keyword">fi</span>; <span class="hljs-keyword">done</span> | xargs rm
</code></pre>
<p>而第二个问题则可以给 Kubelet 配置更快的垃圾回收，如</p>
<pre><code class="lang-sh">--minimum-container-ttl-duration=15s
--maximum-dead-containers-per-container=1
--maximum-dead-containers=100
</code></pre>
<h2 id="pod-无法解析-dns">Pod 无法解析 DNS</h2>
<p>如果 Node 上安装的 Docker 版本大于 1.12，那么 Docker 会把默认的 iptables FORWARD 策略改为 DROP。这会引发 Pod 网络访问的问题。解决方法则在每个 Node 上面运行 <code>iptables -P FORWARD ACCEPT</code>，比如</p>
<pre><code class="lang-sh"><span class="hljs-built_in">echo</span> <span class="hljs-string">"ExecStartPost=/sbin/iptables -P FORWARD ACCEPT"</span> >> /etc/systemd/system/docker.service.d/<span class="hljs-built_in">exec</span>_start.conf
systemctl daemon-reload
systemctl restart docker
</code></pre>
<p>如果使用了 flannel/weave 网络插件，更新为最新版本也可以解决这个问题。</p>
<p>DNS 无法解析也有可能是 kube-dns 服务异常导致的，可以通过下面的命令来检查 kube-dns 是否处于正常运行状态</p>
<pre><code class="lang-sh">$ kubectl get pods --namespace=kube-system <span class="hljs-_">-l</span> k8s-app=kube-dns
NAME                    READY     STATUS    RESTARTS   AGE
...
kube-dns-v19-ezo1y      3/3       Running   0           1h
...
</code></pre>
<p>如果 kube-dns 处于 CrashLoopBackOff 状态，那么可以参考 <a href="cluster.html">Kube-dns/Dashboard CrashLoopBackOff 排错</a> 来查看具体排错方法。</p>
<p>如果 kube-dns Pod 处于正常 Running 状态，则需要进一步检查是否正确配置了 kube-dns 服务：</p>
<pre><code class="lang-sh">$ kubectl get svc kube-dns --namespace=kube-system
NAME          CLUSTER-IP     EXTERNAL-IP   PORT(S)             AGE
kube-dns      10.0.0.10      <none>        53/UDP,53/TCP        1h

$ kubectl get ep kube-dns --namespace=kube-system
NAME       ENDPOINTS                       AGE
kube-dns   10.180.3.17:53,10.180.3.17:53    1h
</code></pre>
<p>如果 kube-dns service 不存在，或者 endpoints 列表为空，则说明 kube-dns service 配置错误，可以重新创建 <a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dns" target="_blank">kube-dns service</a>，比如</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Service
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> kube-dns
<span class="hljs-attr">  namespace:</span> kube-system
<span class="hljs-attr">  labels:</span>
<span class="hljs-attr">    k8s-app:</span> kube-dns
    kubernetes.io/cluster-service: <span class="hljs-string">"true"</span>
    kubernetes.io/name: <span class="hljs-string">"KubeDNS"</span>
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  selector:</span>
<span class="hljs-attr">    k8s-app:</span> kube-dns
<span class="hljs-attr">  clusterIP:</span> <span class="hljs-number">10.0</span><span class="hljs-number">.0</span><span class="hljs-number">.10</span>
<span class="hljs-attr">  ports:</span>
<span class="hljs-attr">  - name:</span> dns
<span class="hljs-attr">    port:</span> <span class="hljs-number">53</span>
<span class="hljs-attr">    protocol:</span> UDP
<span class="hljs-attr">  - name:</span> dns-tcp
<span class="hljs-attr">    port:</span> <span class="hljs-number">53</span>
<span class="hljs-attr">    protocol:</span> TCP
</code></pre>
<h2 id="dns解析缓慢">DNS解析缓慢</h2>
<p>由于内核的一个 <a href="https://www.weave.works/blog/racy-conntrack-and-dns-lookup-timeouts" target="_blank">BUG</a>，连接跟踪模块会发生竞争，导致　DNS　解析缓慢。</p>
<p>临时<a href="https://github.com/kubernetes/kubernetes/issues/56903" target="_blank">解决方法</a>：为容器配置 <code>options single-request-reopen</code></p>
<pre><code class="lang-yaml"><span class="hljs-attr">        lifecycle:</span>
<span class="hljs-attr">          postStart:</span>
<span class="hljs-attr">            exec:</span>
<span class="hljs-attr">              command:</span>
<span class="hljs-bullet">              -</span> /bin/sh
<span class="hljs-bullet">              -</span> -c 
<span class="hljs-bullet">              -</span> <span class="hljs-string">"/bin/echo 'options single-request-reopen' >> /etc/resolv.conf"</span>
</code></pre>
<p>修复方法：升级内核并保证包含以下两个补丁</p>
<ol>
<li><a href="http://patchwork.ozlabs.org/patch/937963/" target="_blank">"netfilter: nf_conntrack: resolve clash for matching conntracks"</a> fixes the 1st race (accepted).</li>
<li><a href="http://patchwork.ozlabs.org/patch/952939/" target="_blank">"netfilter: nf_nat: return the same reply tuple for matching CTs"</a> fixes the 2nd race (waiting for a review).</li>
</ol>
<p>其他可能的原因和修复方法还有：</p>
<ul>
<li>Kube-dns 和 CoreDNS 同时存在时也会有问题，只保留一个即可。</li>
<li>kube-dns 或者 CoreDNS 的资源限制太小时会导致 DNS 解析缓慢，这时候需要增大资源限制。</li>
</ul>
<p>更多 DNS 配置的方法可以参考 <a href="https://kubernetes.io/docs/tasks/administer-cluster/dns-custom-nameservers/" target="_blank">Customizing DNS Service</a>。</p>
<h2 id="service-无法访问">Service 无法访问</h2>
<p>访问 Service ClusterIP 失败时，可以首先确认是否有对应的 Endpoints</p>
<pre><code class="lang-sh">kubectl get endpoints <service-name>
</code></pre>
<p>如果该列表为空，则有可能是该 Service 的 LabelSelector 配置错误，可以用下面的方法确认一下</p>
<pre><code class="lang-sh"><span class="hljs-comment"># 查询 Service 的 LabelSelector</span>
kubectl get svc <service-name> -o jsonpath=<span class="hljs-string">'{.spec.selector}'</span>

<span class="hljs-comment"># 查询匹配 LabelSelector 的 Pod</span>
kubectl get pods <span class="hljs-_">-l</span> key1=value1,key2=value2
</code></pre>
<p>如果 Endpoints 正常，可以进一步检查</p>
<ul>
<li>Pod 的 containerPort 与 Service 的 containerPort 是否对应</li>
<li>直接访问 <code>podIP:containerPort</code> 是否正常</li>
</ul>
<p>再进一步，即使上述配置都正确无误，还有其他的原因会导致 Service 无法访问，比如</p>
<ul>
<li>Pod 内的容器有可能未正常运行或者没有监听在指定的 containerPort 上</li>
<li>CNI 网络或主机路由异常也会导致类似的问题</li>
<li>kube-proxy 服务有可能未启动或者未正确配置相应的 iptables 规则，比如正常情况下名为 <code>hostnames</code> 的服务会配置以下 iptables 规则</li>
</ul>
<pre><code class="lang-sh">$ iptables-save | grep hostnames
-A KUBE-SEP-57KPRZ3JQVENLNBR <span class="hljs-_">-s</span> 10.244.3.6/32 -m comment --comment <span class="hljs-string">"default/hostnames:"</span> -j MARK --set-xmark 0x00004000/0x00004000
-A KUBE-SEP-57KPRZ3JQVENLNBR -p tcp -m comment --comment <span class="hljs-string">"default/hostnames:"</span> -m tcp -j DNAT --to-destination 10.244.3.6:9376
-A KUBE-SEP-WNBA2IHDGP2BOBGZ <span class="hljs-_">-s</span> 10.244.1.7/32 -m comment --comment <span class="hljs-string">"default/hostnames:"</span> -j MARK --set-xmark 0x00004000/0x00004000
-A KUBE-SEP-WNBA2IHDGP2BOBGZ -p tcp -m comment --comment <span class="hljs-string">"default/hostnames:"</span> -m tcp -j DNAT --to-destination 10.244.1.7:9376
-A KUBE-SEP-X3P2623AGDH6CDF3 <span class="hljs-_">-s</span> 10.244.2.3/32 -m comment --comment <span class="hljs-string">"default/hostnames:"</span> -j MARK --set-xmark 0x00004000/0x00004000
-A KUBE-SEP-X3P2623AGDH6CDF3 -p tcp -m comment --comment <span class="hljs-string">"default/hostnames:"</span> -m tcp -j DNAT --to-destination 10.244.2.3:9376
-A KUBE-SERVICES <span class="hljs-_">-d</span> 10.0.1.175/32 -p tcp -m comment --comment <span class="hljs-string">"default/hostnames: cluster IP"</span> -m tcp --dport 80 -j KUBE-SVC-NWV5X2332I4OT4T3
-A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment <span class="hljs-string">"default/hostnames:"</span> -m statistic --mode random --probability 0.33332999982 -j KUBE-SEP-WNBA2IHDGP2BOBGZ
-A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment <span class="hljs-string">"default/hostnames:"</span> -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-X3P2623AGDH6CDF3
-A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment <span class="hljs-string">"default/hostnames:"</span> -j KUBE-SEP-57KPRZ3JQVENLNBR
</code></pre>
<h2 id="pod-无法通过-service-访问自己">Pod 无法通过 Service 访问自己</h2>
<p>这通常是 hairpin 配置错误导致的，可以通过 Kubelet 的 <code>--hairpin-mode</code> 选项配置，可选参数包括 "promiscuous-bridge"、"hairpin-veth" 和 "none"（默认为"promiscuous-bridge"）。</p>
<p>对于 hairpin-veth 模式，可以通过以下命令来确认是否生效</p>
<pre><code class="lang-sh">$ <span class="hljs-keyword">for</span> intf <span class="hljs-keyword">in</span> /sys/devices/virtual/net/cbr0/brif/*; <span class="hljs-keyword">do</span> cat <span class="hljs-variable">$intf</span>/hairpin_mode; <span class="hljs-keyword">done</span>
1
1
1
1
</code></pre>
<p>而对于 promiscuous-bridge 模式，可以通过以下命令来确认是否生效</p>
<pre><code class="lang-sh">$ ifconfig cbr0 |grep PROMISC
UP BROADCAST RUNNING PROMISC MULTICAST  MTU:1460  Metric:1
</code></pre>
<h2 id="无法访问-kubernetes-api">无法访问 Kubernetes API</h2>
<p>很多扩展服务需要访问 Kubernetes API 查询需要的数据（比如 kube-dns、Operator 等）。通常在 Kubernetes API 无法访问时，可以首先通过下面的命令验证 Kubernetes API 是正常的：</p>
<pre><code class="lang-sh">$ kubectl run curl  --image=appropriate/curl -i -t  --restart=Never --command -- sh
If you don<span class="hljs-string">'t see a command prompt, try pressing enter.
/ #
/ # KUBE_TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)
/ # curl -sSk -H "Authorization: Bearer $KUBE_TOKEN" https://$KUBERNETES_SERVICE_HOST:$KUBERNETES_SERVICE_PORT/api/v1/namespaces/default/pods
{
  "kind": "PodList",
  "apiVersion": "v1",
  "metadata": {
    "selfLink": "/api/v1/namespaces/default/pods",
    "resourceVersion": "2285"
  },
  "items": [
   ...
  ]
 }
</span></code></pre>
<p>如果出现超时错误，则需要进一步确认名为 <code>kubernetes</code> 的服务以及 endpoints 列表是正常的：</p>
<pre><code class="lang-sh">$ kubectl get service kubernetes
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   25m
$ kubectl get endpoints kubernetes
NAME         ENDPOINTS          AGE
kubernetes   172.17.0.62:6443   25m
</code></pre>
<p>然后可以直接访问 endpoints 查看 kube-apiserver 是否可以正常访问。无法访问时通常说明 kube-apiserver 未正常启动，或者有防火墙规则阻止了访问。</p>
<p>但如果出现了 <code>403 - Forbidden</code> 错误，则说明 Kubernetes 集群开启了访问授权控制（如 RBAC），此时就需要给 Pod 所用的 ServiceAccount 创建角色和角色绑定授权访问所需要的资源。比如 CoreDNS 就需要创建以下 ServiceAccount 以及角色绑定：</p>
<pre><code class="lang-yaml"><span class="hljs-comment"># 1. service account</span>
<span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> ServiceAccount
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> coredns
<span class="hljs-attr">  namespace:</span> kube-system
<span class="hljs-attr">  labels:</span>
      kubernetes.io/cluster-service: <span class="hljs-string">"true"</span>
      addonmanager.kubernetes.io/mode: Reconcile
<span class="hljs-meta">---</span>
<span class="hljs-comment"># 2. cluster role</span>
<span class="hljs-attr">apiVersion:</span> rbac.authorization.k8s.io/v1
<span class="hljs-attr">kind:</span> ClusterRole
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  labels:</span>
    kubernetes.io/bootstrapping: rbac-defaults
    addonmanager.kubernetes.io/mode: Reconcile
<span class="hljs-attr">  name:</span> system:coredns
<span class="hljs-attr">rules:</span>
<span class="hljs-attr">- apiGroups:</span>
<span class="hljs-bullet">  -</span> <span class="hljs-string">""</span>
<span class="hljs-attr">  resources:</span>
<span class="hljs-bullet">  -</span> endpoints
<span class="hljs-bullet">  -</span> services
<span class="hljs-bullet">  -</span> pods
<span class="hljs-bullet">  -</span> namespaces
<span class="hljs-attr">  verbs:</span>
<span class="hljs-bullet">  -</span> list
<span class="hljs-bullet">  -</span> watch
<span class="hljs-meta">---</span>
<span class="hljs-comment"># 3. cluster role binding</span>
<span class="hljs-attr">apiVersion:</span> rbac.authorization.k8s.io/v1
<span class="hljs-attr">kind:</span> ClusterRoleBinding
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  annotations:</span>
    rbac.authorization.kubernetes.io/autoupdate: <span class="hljs-string">"true"</span>
<span class="hljs-attr">  labels:</span>
    kubernetes.io/bootstrapping: rbac-defaults
    addonmanager.kubernetes.io/mode: EnsureExists
<span class="hljs-attr">  name:</span> system:coredns
<span class="hljs-attr">roleRef:</span>
<span class="hljs-attr">  apiGroup:</span> rbac.authorization.k8s.io
<span class="hljs-attr">  kind:</span> ClusterRole
<span class="hljs-attr">  name:</span> system:coredns
<span class="hljs-attr">subjects:</span>
<span class="hljs-attr">- kind:</span> ServiceAccount
<span class="hljs-attr">  name:</span> coredns
<span class="hljs-attr">  namespace:</span> kube-system
<span class="hljs-meta">---</span>
<span class="hljs-comment"># 4. use created service account</span>
<span class="hljs-attr">apiVersion:</span> extensions/v1beta1
<span class="hljs-attr">kind:</span> Deployment
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> coredns
<span class="hljs-attr">  namespace:</span> kube-system
<span class="hljs-attr">  labels:</span>
<span class="hljs-attr">    k8s-app:</span> coredns
    kubernetes.io/cluster-service: <span class="hljs-string">"true"</span>
    addonmanager.kubernetes.io/mode: Reconcile
    kubernetes.io/name: <span class="hljs-string">"CoreDNS"</span>
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  replicas:</span> <span class="hljs-number">2</span>
<span class="hljs-attr">  selector:</span>
<span class="hljs-attr">    matchLabels:</span>
<span class="hljs-attr">      k8s-app:</span> coredns
<span class="hljs-attr">  template:</span>
<span class="hljs-attr">    metadata:</span>
<span class="hljs-attr">      labels:</span>
<span class="hljs-attr">        k8s-app:</span> coredns
<span class="hljs-attr">    spec:</span>
<span class="hljs-attr">      serviceAccountName:</span> coredns
      ...
</code></pre>
<h2 id="内核导致的问题">内核导致的问题</h2>
<p>除了以上问题，还有可能碰到因内核问题导致的服务无法访问或者服务访问超时的错误，比如</p>
<ul>
<li><a href="https://tech.xing.com/a-reason-for-unexplained-connection-timeouts-on-kubernetes-docker-abd041cf7e02" target="_blank">未设置 <code>--random-fully</code> 导致无法为 SNAT 分配端口，进而会导致服务访问超时</a>。注意， Kubernetes 暂时没有为 SNAT 设置 <code>--random-fully</code> 选项，如果碰到这个问题可以参考<a href="https://gist.github.com/maxlaverse/1fb3bfdd2509e317194280f530158c98" target="_blank">这里</a> 配置。</li>
</ul>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application/" target="_blank">Troubleshoot Applications</a></li>
<li><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-service/" target="_blank">Debug Services</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="pv-异常排错" class="level2">PV排错</h1>
<p>本章介绍持久化存储异常（PV、PVC、StorageClass等）的排错方法。</p>
<p>一般来说，无论 PV 处于什么异常状态，都可以执行 <code>kubectl describe pv/pvc <pod-name></code> 命令来查看当前 PV 的事件。这些事件通常都会有助于排查 PV 或 PVC 发生的问题。</p>
<pre><code class="lang-sh">kubectl get pv
kubectl get pvc
kubectl get sc

kubectl describe pv <pv-name>
kubectl describe pvc <pvc-name>
kubectl describe sc <storage-class-name>
</code></pre>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="azuredisk-排错" class="level3">AzureDisk</h1>
<p><a href="https://docs.microsoft.com/zh-cn/azure/virtual-machines/windows/about-disks-and-vhds" target="_blank">AzureDisk</a> 为 Azure 上面运行的虚拟机提供了弹性块存储服务，它以 VHD 的形式挂载到虚拟机中，并可以在 Kubernetes 容器中使用。AzureDisk 有点是性能高，特别是 Premium Storage 提供了非常好的<a href="https://docs.microsoft.com/en-us/azure/virtual-machines/windows/premium-storage" target="_blank">性能</a>；其缺点是不支持共享，只可以用在单个 Pod 内。</p>
<p>根据配置的不同，Kubernetes 支持的 AzureDisk 可以分为以下几类</p>
<ul>
<li>Managed Disks: 由 Azure 自动管理磁盘和存储账户</li>
<li>Blob Disks:<ul>
<li>Dedicated (默认)：为每个 AzureDisk 创建单独的存储账户，当删除 PVC 的时候删除该存储账户</li>
<li>Shared：AzureDisk 共享 ResourceGroup 内的同一个存储账户，这时删除 PVC 不会删除该存储账户</li>
</ul>
</li>
</ul>
<blockquote>
<p>注意：</p>
<ul>
<li>AzureDisk 的类型必须跟 VM OS Disk 类型一致，即要么都是 Manged Disks，要么都是 Blob Disks。当两者不一致时，AzureDisk PV 会报无法挂载的错误。</li>
<li>由于 Managed Disks 需要创建和管理存储账户，其创建过程会比 Blob Disks 慢（3 分钟 vs 1-2 分钟）。</li>
<li>但节点最大支持同时挂载 16 个 AzureDisk。</li>
</ul>
</blockquote>
<p>使用 <a href="https://github.com/Azure/acs-engine" target="_blank">acs-engine</a> 部署的 Kubernetes 集群，会自动创建两个 StorageClass，默认为managed-standard（即HDD）：</p>
<pre><code class="lang-sh">kubectl get storageclass
NAME                PROVISIONER                AGE
default (default)   kubernetes.io/azure-disk   45d
managed-premium     kubernetes.io/azure-disk   53d
managed-standard    kubernetes.io/azure-disk   53d
</code></pre>
<h2 id="azuredisk-挂载失败">AzureDisk 挂载失败</h2>
<p>在 AzureDisk 从一个 Pod 迁移到另一 Node 上面的 Pod 时或者同一台 Node 上面使用了多块 AzureDisk 时有可能会碰到这个问题。这是由于 kube-controller-manager 未对 AttachDisk 和 DetachDisk 操作加锁从而引发了竞争问题（<a href="https://github.com/kubernetes/kubernetes/issues/60101" target="_blank">kubernetes#60101</a> <a href="https://github.com/Azure/acs-engine/issues/2002" target="_blank">acs-engine#2002</a> <a href="https://github.com/Azure/ACS/issues/12" target="_blank">ACS#12</a>）。</p>
<p>通过 kube-controller-manager 的日志，可以查看具体的错误原因。常见的错误日志为</p>
<pre><code class="lang-sh">Cannot attach data disk <span class="hljs-string">'cdb-dynamic-pvc-92972088-11b9-11e8-888f-000d3a018174'</span> to VM <span class="hljs-string">'kn-edge-0'</span> because the disk is currently being detached or the last detach operation failed. Please <span class="hljs-built_in">wait</span> until the disk is completely detached and <span class="hljs-keyword">then</span> try again or delete/detach the disk explicitly again.
</code></pre>
<p>临时性解决方法为</p>
<p>（1）更新所有受影响的虚拟机状态</p>
<pre><code class="lang-powershell"><span class="hljs-variable">$vm</span> = Get-AzureRMVM -ResourceGroupName <span class="hljs-variable">$rg</span> -Name <span class="hljs-variable">$vmname</span>
Update-AzureRmVM -ResourceGroupName <span class="hljs-variable">$rg</span> -VM <span class="hljs-variable">$vm</span> -verbose -debug
</code></pre>
<p>（2）重启虚拟机</p>
<ul>
<li><code>kubectl cordon NODE</code></li>
<li>如果 Node 上运行有 StatefulSet，需要手动删除相应的 Pod</li>
<li><code>kubectl drain NODE</code></li>
<li><code>Get-AzureRMVM -ResourceGroupName $rg -Name $vmname | Restart-AzureVM</code></li>
<li><code>kubectl uncordon NODE</code></li>
</ul>
<p>该问题的修复 <a href="https://github.com/kubernetes/kubernetes/pull/60183" target="_blank">#60183</a> 已包含在 v1.10 中。</p>
<h2 id="挂载新的-azuredisk-后，该-node-中其他-pod-已挂载的-azuredisk-不可用">挂载新的 AzureDisk 后，该 Node 中其他 Pod 已挂载的 AzureDisk 不可用</h2>
<p>在 Kubernetes v1.7 中，AzureDisk 默认的缓存策略修改为 <code>ReadWrite</code>，这会导致在同一个 Node 中挂载超过 5 块 AzureDisk 时，已有 AzureDisk 的盘符会随机改变（<a href="https://github.com/kubernetes/kubernetes/issues/60344" target="_blank">kubernetes#60344</a> <a href="https://github.com/kubernetes/kubernetes/issues/57444" target="_blank">kubernetes#57444</a> <a href="https://github.com/Azure/AKS/issues/201" target="_blank">AKS#201</a> <a href="https://github.com/Azure/acs-engine/issues/1918" target="_blank">acs-engine#1918</a>）。比如，当挂载第六块 AzureDisk 后，原来 lun0 磁盘的挂载盘符有可能从 <code>sdc</code> 变成 <code>sdk</code>：</p>
<pre><code class="lang-sh">$ tree /dev/disk/azure
...
â””â”€â”€ scsi1
    â”œâ”€â”€ lun0 -> ../../../sdk
    â”œâ”€â”€ lun1 -> ../../../sdj
    â”œâ”€â”€ lun2 -> ../../../sde
    â”œâ”€â”€ lun3 -> ../../../sdf
    â”œâ”€â”€ lun4 -> ../../../sdg
    â”œâ”€â”€ lun5 -> ../../../sdh
    â””â”€â”€ lun6 -> ../../../sdi
</code></pre>
<p>这样，原来使用 lun0 磁盘的 Pod 就无法访问 AzureDisk 了</p>
<pre><code class="lang-sh">[root@admin-0 /]<span class="hljs-comment"># ls /datadisk</span>
ls: reading directory .: Input/output error
</code></pre>
<p>临时性解决方法是设置 AzureDisk StorageClass 的 <code>cachingmode: None</code>，如</p>
<pre><code class="lang-yaml"><span class="hljs-attr">kind:</span> StorageClass
<span class="hljs-attr">apiVersion:</span> storage.k8s.io/v1
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> managed-standard
<span class="hljs-attr">provisioner:</span> kubernetes.io/azure-disk
<span class="hljs-attr">parameters:</span>
<span class="hljs-attr">  skuname:</span> Standard_LRS
<span class="hljs-attr">  kind:</span> Managed
<span class="hljs-attr">  cachingmode:</span> None
</code></pre>
<p>该问题的修复 <a href="https://github.com/kubernetes/kubernetes/pull/60346" target="_blank">#60346</a> 将包含在 v1.10 中。</p>
<h2 id="azuredisk-挂载慢">AzureDisk 挂载慢</h2>
<p>AzureDisk PVC 的挂载过程一般需要 1 分钟的时间，这些时间主要消耗在 Azure ARM API 的调用上（查询 VM 以及挂载 Disk）。<a href="https://github.com/kubernetes/kubernetes/pull/57432" target="_blank">#57432</a> 为 Azure VM 增加了一个缓存，消除了 VM 的查询时间，将整个挂载过程缩短到大约 30 秒。该修复包含在v1.9.2+ 和 v1.10 中。</p>
<p>另外，如果 Node 使用了 <code>Standard_B1s</code> 类型的虚拟机，那么 AzureDisk 的第一次挂载一般会超时，等再次重复时才会挂载成功。这是因为在 <code>Standard_B1s</code>  虚拟机中格式化 AzureDisk 就需要很长时间（如超过 70 秒）。</p>
<pre><code class="lang-sh">$ kubectl describe pod <pod-name>
...
Events:
  FirstSeen     LastSeen        Count   From                                    SubObjectPath                           Type            Reason                  Message
  ---------     --------        -----   ----                                    -------------                           --------        ------                  -------
  8m            8m              1       default-scheduler                                                               Normal          Scheduled               Successfully assigned nginx-azuredisk to aks-nodepool1-15012548-0
  7m            7m              1       kubelet, aks-nodepool1-15012548-0                                               Normal          SuccessfulMountVolume   MountVolume.SetUp succeeded <span class="hljs-keyword">for</span> volume <span class="hljs-string">"default-token-mrw8h"</span>
  5m            5m              1       kubelet, aks-nodepool1-15012548-0                                               Warning         FailedMount             Unable to mount volumes <span class="hljs-keyword">for</span> pod <span class="hljs-string">"nginx-azuredisk_default(4eb22bb2-0bb5-11e8-8
d9e-0a58ac1f0a2e)"</span>: timeout expired waiting <span class="hljs-keyword">for</span> volumes to attach/mount <span class="hljs-keyword">for</span> pod <span class="hljs-string">"default"</span>/<span class="hljs-string">"nginx-azuredisk"</span>. list of unattached/unmounted volumes=[disk01]
  5m            5m              1       kubelet, aks-nodepool1-15012548-0                                               Warning         FailedSync              Error syncing pod
  4m            4m              1       kubelet, aks-nodepool1-15012548-0                                               Normal          SuccessfulMountVolume   MountVolume.SetUp succeeded <span class="hljs-keyword">for</span> volume <span class="hljs-string">"pvc-20240841-0bb5-11e8-8d9e-0a58ac1f0
a2e"</span>
  4m            4m              1       kubelet, aks-nodepool1-15012548-0       spec.containers{nginx-azuredisk}        Normal          Pulling                 pulling image <span class="hljs-string">"nginx"</span>
  3m            3m              1       kubelet, aks-nodepool1-15012548-0       spec.containers{nginx-azuredisk}        Normal          Pulled                  Successfully pulled image <span class="hljs-string">"nginx"</span>
  3m            3m              1       kubelet, aks-nodepool1-15012548-0       spec.containers{nginx-azuredisk}        Normal          Created                 Created container
  2m            2m              1       kubelet, aks-nodepool1-15012548-0       spec.containers{nginx-azuredisk}        Normal          Started                 Started container
</code></pre>
<h2 id="azure-german-cloud-无法使用-azuredisk">Azure German Cloud 无法使用 AzureDisk</h2>
<p>Azure German Cloud 仅在 v1.7.9+、v1.8.3+ 以及更新版本中支持（<a href="https://github.com/kubernetes/kubernetes/pull/50673" target="_blank">#50673</a>），升级 Kubernetes 版本即可解决。</p>
<h2 id="mountvolumewaitforattach-failed">MountVolume.WaitForAttach failed</h2>
<pre><code class="lang-sh">MountVolume.WaitForAttach failed <span class="hljs-keyword">for</span> volume <span class="hljs-string">"pvc-f1562ecb-3e5f-11e8-ab6b-000d3af9f967"</span> : azureDisk - Wait <span class="hljs-keyword">for</span> attach expect device path as a lun number, instead got: /dev/disk/azure/scsi1/lun1 (strconv.Atoi: parsing <span class="hljs-string">"/dev/disk/azure/scsi1/lun1"</span>: invalid syntax)
</code></pre>
<p><a href="https://github.com/kubernetes/kubernetes/issues/62540" target="_blank">该问题</a> 仅在 Kubernetes v1.10.0 和 v1.10.1 中存在，将在 v1.10.2 中修复。</p>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://github.com/andyzhangx/demo/tree/master/issues" target="_blank">Known kubernetes issues on Azure</a></li>
<li><a href="https://docs.microsoft.com/zh-cn/azure/virtual-machines/windows/about-disks-and-vhds" target="_blank">Introduction of AzureDisk</a></li>
<li><a href="https://github.com/kubernetes/examples/tree/master/staging/volumes/azure_disk" target="_blank">AzureDisk volume examples</a></li>
<li><a href="https://docs.microsoft.com/en-us/azure/virtual-machines/windows/premium-storage" target="_blank">High-performance Premium Storage and managed disks for VMs</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="azurefile-排错" class="level3">AzureFile</h1>
<p><a href="https://docs.microsoft.com/zh-cn/azure/storage/files/storage-files-introduction" target="_blank">AzureFile</a> 提供了基于 SMB 协议（也称 CIFS）托管文件共享服务。它支持 Windows 和 Linux 容器，并支持跨主机的共享，可用于多个 Pod 之间的共享存储。AzureFile 的缺点是性能<a href="https://docs.microsoft.com/en-us/azure/storage/files/storage-files-scale-targets" target="_blank">较差</a>（<a href="https://github.com/Azure/AKS/issues/223" target="_blank">AKS#223</a>），并且不提供 Premium 存储。</p>
<p>推荐基于 StorageClass 来使用 AzureFile，即</p>
<pre><code class="lang-yaml"><span class="hljs-attr">kind:</span> StorageClass
<span class="hljs-attr">apiVersion:</span> storage.k8s.io/v1
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> azurefile
<span class="hljs-attr">provisioner:</span> kubernetes.io/azure-file
<span class="hljs-attr">mountOptions:</span>
<span class="hljs-bullet">  -</span> dir_mode=<span class="hljs-number">0777</span>
<span class="hljs-bullet">  -</span> file_mode=<span class="hljs-number">0777</span>
<span class="hljs-bullet">  -</span> uid=<span class="hljs-number">1000</span>
<span class="hljs-bullet">  -</span> gid=<span class="hljs-number">1000</span>
<span class="hljs-attr">parameters:</span>
<span class="hljs-attr">  skuName:</span> Standard_LRS
</code></pre>
<h2 id="访问权限">访问权限</h2>
<p>AzureFile 使用 <a href="https://linux.die.net/man/8/mount.cifs" target="_blank">mount.cifs</a> 将其远端存储挂载到 Node 上，而<code>fileMode</code> 和 <code>dirMode</code> 控制了挂载后文件和目录的访问权限。不同的 Kubernetes 版本，<code>fileMode</code> 和 <code>dirMode</code> 的默认选项是不同的</p>
<table>
<thead>
<tr>
<th>Kubernetes 版本</th>
<th>fileMode和dirMode</th>
</tr>
</thead>
<tbody>
<tr>
<td>v1.6.x, v1.7.x</td>
<td>0777</td>
</tr>
<tr>
<td>v1.8.0-v1.8.5</td>
<td>0700</td>
</tr>
<tr>
<td>v1.8.6 or above</td>
<td>0755</td>
</tr>
<tr>
<td>v1.9.0</td>
<td>0700</td>
</tr>
<tr>
<td>v1.9.1 or above</td>
<td>0755</td>
</tr>
</tbody>
</table>
<p>按照默认的权限会导致非跟用户无法在目录中创建新的文件，解决方法为</p>
<ul>
<li>v1.8.0-v1.8.5：设置容器以 root 用户运行，如设置 <code>spec.securityContext.runAsUser: 0</code></li>
<li>v1.8.6 以及更新版本：在 AzureFile StorageClass 通过 mountOptions 设置默认权限，比如设置为 <code>0777</code> 的方法为</li>
</ul>
<pre><code class="lang-yaml"><span class="hljs-attr">kind:</span> StorageClass
<span class="hljs-attr">apiVersion:</span> storage.k8s.io/v1
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> azurefile
<span class="hljs-attr">provisioner:</span> kubernetes.io/azure-file
<span class="hljs-attr">mountOptions:</span>
<span class="hljs-bullet">  -</span> dir_mode=<span class="hljs-number">0777</span>
<span class="hljs-bullet">  -</span> file_mode=<span class="hljs-number">0777</span>
<span class="hljs-bullet">  -</span> uid=<span class="hljs-number">1000</span>
<span class="hljs-bullet">  -</span> gid=<span class="hljs-number">1000</span>
<span class="hljs-attr">parameters:</span>
<span class="hljs-attr">  skuName:</span> Standard_LRS
</code></pre>
<h2 id="windows-node-重启后无法访问-azurefile">Windows Node 重启后无法访问 AzureFile</h2>
<p>Windows Node 重启后，挂载 AzureFile 的 Pod 可以看到如下错误（<a href="https://github.com/kubernetes/kubernetes/issues/60624" target="_blank">#60624</a>）：</p>
<pre><code class="lang-sh">Warning  Failed                 1m (x7 over 1m)  kubelet, 77890k8s9010  Error: Error response from daemon: invalid <span class="hljs-built_in">bind</span> mount spec <span class="hljs-string">"c:\\var\\lib\\kubelet\\pods\\07251c5c-1cfc-11e8-8f70-000d3afd4b43\\volumes\\kubernetes.io~azure-file\\pvc-fb6159f6-1cfb-11e8-8f70-000d3afd4b43:c:/mnt/azure"</span>: invalid volume specification: <span class="hljs-string">'c:\var\lib\kubelet\pods\07251c5c-1cfc-11e8-8f70-000d3afd4b43\volumes\kubernetes.io~azure-file\pvc-fb6159f6-1cfb-11e8-8f70-000d3afd4b43:c:/mnt/azure'</span>: invalid mount config <span class="hljs-keyword">for</span> <span class="hljs-built_in">type</span> <span class="hljs-string">"bind"</span>: <span class="hljs-built_in">bind</span> <span class="hljs-built_in">source</span> path does not exist
  Normal   SandboxChanged         1m (x8 over 1m)  kubelet, 77890k8s9010  Pod sandbox changed, it will be killed and re-created.
</code></pre>
<p>临时性解决方法为删除并重新创建使用了 AzureFile 的 Pod。当 Pod 使用控制器（如 Deployment、StatefulSet等）时，删除 Pod 后控制器会自动创建一个新的 Pod。</p>
<p>该问题的修复 <a href="https://github.com/kubernetes/kubernetes/pull/60625" target="_blank">#60625</a> 包含在 v1.10 中。</p>
<h2 id="azurefile-provisioningfailed">AzureFile ProvisioningFailed</h2>
<p>Azure 文件共享的名字最大只允许 63 个字节，因而在集群名字较长的集群（Kubernetes v1.7.10 或者更老的集群）里面有可能会碰到 AzureFile 名字长度超限的情况，导致 AzureFile ProvisioningFailed：</p>
<pre><code class="lang-sh">persistentvolume-controller    Warning    ProvisioningFailed Failed to provision volume with StorageClass <span class="hljs-string">"azurefile"</span>: failed to find a matching storage account
</code></pre>
<p>碰到该问题时可以通过升级集群解决，其修复 <a href="https://github.com/kubernetes/kubernetes/pull/48326" target="_blank">#48326</a> 已经包含在 v1.7.11、v1.8 以及更新版本中。</p>
<p>在开启 RBAC 的集群中，由于 AzureFile 需要访问 Secret，而 kube-controller-manager 中并未为 AzureFile 自动授权，从而也会导致 ProvisioningFailed：</p>
<pre><code class="lang-sh">Events:
  Type     Reason              Age   From                         Message
  ----     ------              ----  ----                         -------
  Warning  ProvisioningFailed  8s    persistentvolume-controller  Failed to provision volume with StorageClass <span class="hljs-string">"azurefile"</span>: Couldn<span class="hljs-string">'t create secret secrets is forbidden: User "system:serviceaccount:kube-syste
m:persistent-volume-binder" cannot create secrets in the namespace "default"
  Warning  ProvisioningFailed  8s    persistentvolume-controller  Failed to provision volume with StorageClass "azurefile": failed to find a matching storage account
</span></code></pre>
<p>解决方法是为 ServiceAccount <code>persistent-volume-binder</code> 授予 Secret 的访问权限：</p>
<pre><code class="lang-yaml"><span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> rbac.authorization.k8s.io/v1
<span class="hljs-attr">kind:</span> ClusterRole
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> system:azure-cloud-provider
<span class="hljs-attr">rules:</span>
<span class="hljs-attr">- apiGroups:</span> [<span class="hljs-string">''</span>]
<span class="hljs-attr">  resources:</span> [<span class="hljs-string">'secrets'</span>]
<span class="hljs-attr">  verbs:</span>     [<span class="hljs-string">'get'</span>,<span class="hljs-string">'create'</span>]
<span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> rbac.authorization.k8s.io/v1beta1
<span class="hljs-attr">kind:</span> ClusterRoleBinding
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> system:azure-cloud-provider
<span class="hljs-attr">roleRef:</span>
<span class="hljs-attr">  kind:</span> ClusterRole
<span class="hljs-attr">  apiGroup:</span> rbac.authorization.k8s.io
<span class="hljs-attr">  name:</span> system:azure-cloud-provider
<span class="hljs-attr">subjects:</span>
<span class="hljs-attr">- kind:</span> ServiceAccount
<span class="hljs-attr">  name:</span> persistent-volume-binder
<span class="hljs-attr">  namespace:</span> kube-system
</code></pre>
<h2 id="azure-german-cloud-无法使用-azurefile">Azure German Cloud 无法使用 AzureFile</h2>
<p>Azure German Cloud 仅在 v1.7.11+、v1.8+ 以及更新版本中支持（<a href="https://github.com/kubernetes/kubernetes/pull/48460" target="_blank">#48460</a>），升级 Kubernetes 版本即可解决。</p>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://github.com/andyzhangx/demo/tree/master/issues" target="_blank">Known kubernetes issues on Azure</a></li>
<li><a href="https://docs.microsoft.com/zh-cn/azure/storage/files/storage-files-introduction" target="_blank">Introduction of Azure File Storage</a></li>
<li><a href="https://github.com/kubernetes/examples/tree/master/staging/volumes/azure_file" target="_blank">AzureFile volume examples</a></li>
<li><a href="https://docs.microsoft.com/en-us/azure/aks/azure-files-dynamic-pv" target="_blank">Persistent volumes with Azure files</a></li>
<li><a href="https://docs.microsoft.com/en-us/azure/storage/files/storage-files-scale-targets" target="_blank">Azure Files scalability and performance targets</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="windows-容器异常排错" class="level2">Windows排错</h1>
<p>本章介绍 Windows 容器异常的排错方法。</p>
<h2 id="rdp-登录到-node">RDP 登录到 Node</h2>
<p>通常在排查 Windows 容器异常问题时需要通过 RDP 登录到 Windows Node上面查看 kubelet、docker、HNS 等的状态和日志。在使用云平台时，可以给相应的 VM 绑定一个公网 IP；而在物理机部署时，可以通过路由器上的端口映射来访问。</p>
<p>除此之外，还有一种更简单的方法，即通过 Kubernetes Service 对外暴露 Node 的 3389 端口（注意替换为你自己的 node-ip）：</p>
<pre><code class="lang-yaml"><span class="hljs-comment"># rdp.yaml</span>
<span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">kind:</span> Service
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> rdp
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  type:</span> LoadBalancer
<span class="hljs-attr">  ports:</span>
<span class="hljs-attr">  - protocol:</span> TCP
<span class="hljs-attr">    port:</span> <span class="hljs-number">3389</span>
<span class="hljs-attr">    targetPort:</span> <span class="hljs-number">3389</span>
<span class="hljs-meta">---</span>
<span class="hljs-attr">kind:</span> Endpoints
<span class="hljs-attr">apiVersion:</span> v1
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  name:</span> rdp
<span class="hljs-attr">subsets:</span>
<span class="hljs-attr">  - addresses:</span>
<span class="hljs-attr">      - ip:</span> <node-ip<span class="hljs-string">>
</span><span class="hljs-attr">    ports:</span>
<span class="hljs-attr">      - port:</span> <span class="hljs-number">3389</span>
</code></pre>
<pre><code class="lang-sh">$ kubectl create <span class="hljs-_">-f</span> rdp.yaml
$ kubectl get svc rdp
NAME      TYPE           CLUSTER-IP    EXTERNAL-IP      PORT(S)        AGE
rdp       LoadBalancer   10.0.99.149   52.52.52.52   3389:32008/TCP   5m
</code></pre>
<p>接着，就可以通过 rdp 服务的外网 IP 来登录 Node，如 <code>mstsc.exe -v 52.52.52.52</code>。</p>
<p>在使用完后， 不要忘记删除 RDP 服务 <code>kubectl delete -f rdp.yaml</code>。</p>
<h2 id="windows-pod-一直处于-containercreating-状态">Windows Pod 一直处于 ContainerCreating 状态</h2>
<p>一般有两种可能的原因</p>
<ul>
<li>Pause 镜像配置错误</li>
<li>容器<a href="https://docs.microsoft.com/en-us/virtualization/windowscontainers/deploy-containers/version-compatibility" target="_blank">镜像版本与 Windows 系统不兼容</a></li>
</ul>
<p>在 Windows Server 1709 上面需要使用 1709 标签的镜像，比如</p>
<pre><code>* `microsoft/aspnet:4.7.2-windowsservercore-1709`
* `microsoft/windowsservercore:1709`
* `microsoft/iis:windowsservercore-1709`
</code></pre><p>在 Windows Server 1803 上面需要使用 1803 标签的镜像，比如</p>
<pre><code>* `microsoft/aspnet:4.7.2-windowsservercore-1803`
* `microsoft/iis:windowsservercore-1803`
* `microsoft/windowsservercore:1803`
</code></pre><h2 id="windows-pod-内无法解析-dns">Windows Pod 内无法解析 DNS</h2>
<p>这是一个<a href="https://github.com/Azure/acs-engine/issues/2027" target="_blank">已知问题</a>，有以下三种临时解决方法：</p>
<p>（1）Windows 重启后，清空 HNS Policy 并重启 KubeProxy 服务：</p>
<pre><code class="lang-powershell">Start-BitsTransfer -Source https://raw.githubusercontent.com/Microsoft/SDN/master/Kubernetes/windows/hns.psm1
Import-Module .\hns.psm1

<span class="hljs-built_in">Stop-Service</span> kubeproxy
<span class="hljs-built_in">Stop-Service</span> kubelet
Get-HnsNetwork | ? Name <span class="hljs-nomarkup">-eq</span> l2Bridge | Remove-HnsNetwork
Get-HnsPolicyList | Remove-HnsPolicyList
<span class="hljs-built_in">Start-Service</span> kubelet
<span class="hljs-built_in">Start-Service</span> kubeproxy
</code></pre>
<p>（2）是为 Pod 直接配置 kube-dns Pod 的地址：</p>
<pre><code class="lang-powershell"><span class="hljs-variable">$adapter</span>=Get-NetAdapter
Set-DnsClientServerAddress -InterfaceIndex <span class="hljs-variable">$adapter</span>.ifIndex -ServerAddresses <span class="hljs-number">10.244</span>.<span class="hljs-number">0.4</span>,<span class="hljs-number">10.244</span>.<span class="hljs-number">0.6</span>
Set-DnsClient -InterfaceIndex <span class="hljs-variable">$adapter</span>.ifIndex -ConnectionSpecificSuffix <span class="hljs-string">"default.svc.cluster.local"</span>
</code></pre>
<p>（3）更简单的为每个 Windows Node <a href="https://github.com/Azure/acs-engine/issues/2027#issuecomment-373767442" target="_blank">多运行一个 Pod</a>，即保证每台 Node 上面至少有两个 Pod 在运行。此时，DNS 解析也是正常的。</p>
<p>如果 Windows Node 运行在 Azure 上面，并且部署 Kubernetes 时使用了<a href="https://github.com/Azure/acs-engine/blob/master/docs/kubernetes/features.md#feat-custom-vnet" target="_blank">自定义 VNET</a>，那么需要<a href="https://github.com/Azure/acs-engine/blob/master/docs/custom-vnet.md#post-deployment-attach-cluster-route-table-to-vnet" target="_blank">为该 VNET 添加路由表</a>：</p>
<pre><code class="lang-sh"><span class="hljs-meta">#!/bin/bash</span>
<span class="hljs-comment"># KubernetesSubnet is the name of the vnet subnet</span>
<span class="hljs-comment"># KubernetesCustomVNET is the name of the custom VNET itself</span>
rt=$(az network route-table list -g acs-custom-vnet -o json | jq -r <span class="hljs-string">'.[].id'</span>)
az network vnet subnet update -n KubernetesSubnet \
-g acs-custom-vnet \
--vnet-name KubernetesCustomVNET \
--route-table <span class="hljs-variable">$rt</span>
</code></pre>
<p>如果 VNET 在不同的 ResourceGroup 里面，那么</p>
<pre><code class="lang-sh">rt=$(az network route-table list -g RESOURCE_GROUP_NAME_KUBE -o json | jq -r <span class="hljs-string">'.[].id'</span>)
az network vnet subnet update \
-g RESOURCE_GROUP_NAME_VNET \
--route-table <span class="hljs-variable">$rt</span> \
--ids <span class="hljs-string">"/subscriptions/SUBSCRIPTION_ID/resourceGroups/RESOURCE_GROUP_NAME_VNET/providers/Microsoft.Network/VirtualNetworks/KUBERNETES_CUSTOM_VNET/subnets/KUBERNETES_SUBNET"</span>
</code></pre>
<h2 id="remote-endpoint-creation-failed-hns-failed-with-error-the-switch-port-was-not-found">Remote endpoint creation failed: HNS failed with error: The switch-port was not found</h2>
<p>这个错误发生在 kube-proxy 为服务配置负载均衡的时候，需要安装 <a href="https://support.microsoft.com/en-us/help/4089848/windows-10-update-kb4089848" target="_blank">KB4089848</a>：</p>
<pre><code class="lang-powershell">Start-BitsTransfer http://download.windowsupdate.com/d/msdownload/update/software/updt/<span class="hljs-number">2018</span>/<span class="hljs-number">03</span>/windows10.<span class="hljs-number">0</span>-kb4089848-x64_db7c5aad31c520c6983a937c3d53170e84372b11.msu
wusa.exe windows10.<span class="hljs-number">0</span>-kb4089848-x64_db7c5aad31c520c6983a937c3d53170e84372b11.msu
Restart-Computer
</code></pre>
<p>重启后确认更新安装成功：</p>
<pre><code class="lang-powershelgl">PS C:\k> Get-HotFix

Source        Description      HotFixID      InstalledBy          InstalledOn
------        -----------      --------      -----------          -----------
27171k8s9000  Update           KB4087256     NT AUTHORITY\SYSTEM  3/22/2018 12:00:00 AM
27171k8s9000  Update           KB4089848     NT AUTHORITY\SYSTEM  4/4/2018 12:00:00 AM
</code></pre>
<p>安装更新后，如果 DNS 解析还是有问题，可以按照上一节中的方法（1） 重启 kubelet 和 kube-proxy。</p>
<h2 id="windows-pod-内无法访问-serviceaccount-secret">Windows Pod 内无法访问 ServiceAccount Secret</h2>
<p>这是老版本 Windows 的<a href="https://github.com/moby/moby/issues/28401" target="_blank">已知问题</a>，升级 Windows 到 1803 即可解决，升级步骤见<a href="https://blogs.windows.com/windowsexperience/2018/04/30/how-to-get-the-windows-10-april-2018-update/" target="_blank">这里</a>。</p>
<h2 id="windows-pod-内无法访问-kubernetes-api">Windows Pod 内无法访问 Kubernetes API</h2>
<p>如果使用了 Hyper-V 隔离容器，需要开启 MAC spoofing 。</p>
<h2 id="windows-node-内无法访问-service-clusterip">Windows Node 内无法访问 Service ClusterIP</h2>
<p>这是个当前 Windows 网络协议栈的已知问题，只有在 Pod 内才可以访问 Service ClusterIP。</p>
<h2 id="kubelet-无法启动">Kubelet 无法启动</h2>
<p>使用 Docker 18.03 版本和 Kubelet v1.12.x 时，Kubelet 无法正常启动报错：</p>
<pre><code class="lang-sh">Error response from daemon: client version 1.38 is too new. Maximum supported API version is 1.37
</code></pre>
<p>解决方法是为 Windows 上面的 Docker 设置 API 版本的环境变量：</p>
<pre><code class="lang-powershell">[System.Environment]::SetEnvironmentVariable(<span class="hljs-string">'DOCKER_API_VERSION'</span>, <span class="hljs-string">'1.37'</span>, [System.EnvironmentVariableTarget]::Machine)
</code></pre>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://docs.microsoft.com/en-us/virtualization/windowscontainers/kubernetes/common-problems" target="_blank">Kubernetes On Windows - Troubleshooting Kubernetes</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="云平台排错" class="level2">云平台排错</h1>
<p>本章主要介绍在公有云中运行 Kubernetes 时可能会碰到的问题以及解决方法。</p>
<p>在公有云平台上运行 Kubernetes，一般可以使用云平台提供的托管 Kubernetes 服务（比如 Google 的 GKE、微软 Azure 的 AKS 或者 AWS 的 Amazon EKS 等）。当然，为了更自由的灵活性，也可以直接在这些公有云平台的虚拟机中部署 Kubernetes。无论哪种方法，一般都需要给 Kubernetes 配置 Cloud Provider 选项，以方便直接利用云平台提供的高级网络、持久化存储以及安全控制等功能。</p>
<p>而在云平台中运行 Kubernetes 的常见问题有</p>
<ul>
<li>认证授权问题：比如 Kubernetes Cloud Provider 中配置的认证方式无权操作虚拟机所在的网络或持久化存储。这一般从 kube-controller-manager 的日志中很容易发现。</li>
<li>网络路由配置失败：正常情况下，Cloud Provider 会为每个 Node 配置一条 PodCIDR 至 NodeIP 的路由规则，如果这些规则有问题就会导致多主机 Pod 相互访问的问题。</li>
<li>公网 IP 分配失败：比如 LoadBalancer 类型的 Service 无法分配公网 IP 或者指定的公网 IP 无法使用。这一版也是配置错误导致的。</li>
<li>安全组配置失败：比如无法为 Service 创建安全组（如超出配额等）或与已有的安全组冲突等。</li>
<li>持久化存储分配或者挂载问题：比如分配 PV 失败（如超出配额、配置错误等）或挂载到虚拟机失败（比如 PV 正被其他异常 Pod 引用而导致无法从旧的虚拟机中卸载）。</li>
<li>网络插件使用不当：比如网络插件使用了云平台不支持的网络协议等。</li>
</ul>
<h2 id="node-未注册到集群中">Node 未注册到集群中</h2>
<p>通常，在 Kubelet 启动时会自动将自己注册到 kubernetes API 中，然后通过 <code>kubectl get nodes</code> 就可以查询到该节点。 如果新的 Node 没有自动注册到 Kubernetes 集群中，那说明这个注册过程有错误发生，需要检查 kubelet 和 kube-controller-manager 的日志，进而再根据日志查找具体的错误原因。</p>
<h3 id="kubelet-日志">Kubelet 日志</h3>
<p>查看 Kubelet 日志需要首先 SSH 登录到 Node 上，然后运行 <code>journalctl</code> 命令查看 kubelet 的日志：</p>
<pre><code class="lang-sh">journalctl <span class="hljs-_">-l</span> -u kubelet
</code></pre>
<h3 id="kube-controller-manager-日志">kube-controller-manager 日志</h3>
<p>kube-controller-manager 会自动在云平台中给 Node 创建路由，如果路由创建创建失败也有可能导致 Node 注册失败。</p>
<pre><code class="lang-sh">PODNAME=$(kubectl -n kube-system get pod <span class="hljs-_">-l</span> component=kube-controller-manager -o jsonpath=<span class="hljs-string">'{.items[0].metadata.name}'</span>)
kubectl -n kube-system logs <span class="hljs-variable">$PODNAME</span> --tail 100
</code></pre>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="azure-云平台排错" class="level3">Azure</h1>
<h2 id="azure-负载均衡">Azure 负载均衡</h2>
<p>使用 Azure Cloud Provider 后，Kubernetes 会为 LoadBalancer 类型的 Service 创建 Azure 负载均衡器以及相关的 公网 IP、BackendPool 和 Network Security Group (NSG)。注意目前 Azure Cloud Provider 仅支持 <code>Basic</code> SKU 的负载均衡，并将在 v1.11 中支持 Standard SKU。<code>Basic</code> 与 <code>Standard</code> SKU 负载均衡相比有一定的<a href="https://docs.microsoft.com/en-us/azure/load-balancer/load-balancer-standard-overview" target="_blank">局限</a>：</p>
<table>
<thead>
<tr>
<th>Load Balancer</th>
<th>Basic</th>
<th>Standard</th>
</tr>
</thead>
<tbody>
<tr>
<td>Back-end pool size</td>
<td>up to 100</td>
<td>up to 1,000</td>
</tr>
<tr>
<td>Back-end pool boundary</td>
<td>Availability Set</td>
<td>virtual network, region</td>
</tr>
<tr>
<td>Back-end pool design</td>
<td>VMs in Availability Set, virtual machine scale set in Availability Set</td>
<td>Any VM instance in the virtual network</td>
</tr>
<tr>
<td>HA Ports</td>
<td>Not supported</td>
<td>Available</td>
</tr>
<tr>
<td>Diagnostics</td>
<td>Limited, public only</td>
<td>Available</td>
</tr>
<tr>
<td>VIP Availability</td>
<td>Not supported</td>
<td>Available</td>
</tr>
<tr>
<td>Fast IP Mobility</td>
<td>Not supported</td>
<td>Available</td>
</tr>
<tr>
<td>Availability Zones scenarios</td>
<td>Zonal only</td>
<td>Zonal, Zone-redundant, Cross-zone load-balancing</td>
</tr>
<tr>
<td>Outbound SNAT algorithm</td>
<td>On-demand</td>
<td>Preallocated</td>
</tr>
<tr>
<td>Outbound SNAT front-end selection</td>
<td>Not configurable, multiple candidates</td>
<td>Optional configuration to reduce candidates</td>
</tr>
<tr>
<td>Network Security Group</td>
<td>Optional on NIC/subnet</td>
<td>Required</td>
</tr>
</tbody>
</table>
<p>同样，对应的 Public IP 也是 Basic SKU，与 Standard SKU 相比也有一定的<a href="https://docs.microsoft.com/en-us/azure/load-balancer/load-balancer-standard-overview#sku-service-limits-and-abilities" target="_blank">局限</a>：</p>
<table>
<thead>
<tr>
<th>Public IP</th>
<th>Basic</th>
<th>Standard</th>
</tr>
</thead>
<tbody>
<tr>
<td>Availability Zones scenarios</td>
<td>Zonal only</td>
<td>Zone-redundant (default), zonal (optional)</td>
</tr>
<tr>
<td>Fast IP Mobility</td>
<td>Not supported</td>
<td>Available</td>
</tr>
<tr>
<td>VIP Availability</td>
<td>Not supported</td>
<td>Available</td>
</tr>
<tr>
<td>Counters</td>
<td>Not supported</td>
<td>Available</td>
</tr>
<tr>
<td>Network Security Group</td>
<td>Optional on NIC</td>
<td>Required</td>
</tr>
</tbody>
</table>
<p>在创建 Service 时，可以通过 <code>metadata.annotation</code> 来自定义 Azure 负载均衡的行为，可选的选项包括</p>
<table>
<thead>
<tr>
<th>Annotation</th>
<th>功能</th>
</tr>
</thead>
<tbody>
<tr>
<td>service.beta.kubernetes.io/azure-load-balancer-internal</td>
<td>如果设置，则创建内网负载均衡</td>
</tr>
<tr>
<td>service.beta.kubernetes.io/azure-load-balancer-internal-subnet</td>
<td>设置内网负载均衡 IP 使用的子网</td>
</tr>
<tr>
<td>service.beta.kubernetes.io/azure-load-balancer-mode</td>
<td>设置如何为负载均衡选择所属的 AvailabilitySet（之所以有该选项是因为在 Azure 的每个 AvailabilitySet 中只能创建最多一个外网负载均衡和一个内网负载均衡）。可选项为：（1）不设置或者设置为空，使用 <code>/etc/kubernetes/azure.json</code> 中设置的 <code>primaryAvailabilitySet</code>；（2）设置为 <code>auto</code>，选择负载均衡规则最少的 AvailabilitySet；（3）设置为<code>as1,as2</code>，指定 AvailabilitySet 列表</td>
</tr>
<tr>
<td>service.beta.kubernetes.io/azure-dns-label-name</td>
<td>设置后为公网 IP 创建 外网 DNS</td>
</tr>
<tr>
<td>service.beta.kubernetes.io/azure-shared-securityrule</td>
<td>如果设置，则为多个 Service 共享相同的 NSG 规则。注意该选项需要 <a href="https://docs.microsoft.com/en-us/azure/virtual-network/security-overview#augmented-security-rules" target="_blank">Augmented Security Rules</a></td>
</tr>
<tr>
<td>service.beta.kubernetes.io/azure-load-balancer-resource-group</td>
<td>当为 Service 指定公网 IP 并且该公网 IP 与 Kubernetes 集群不在同一个 Resource Group 时，需要使用该 Annotation 指定公网 IP 所在的 Resource Group</td>
</tr>
</tbody>
</table>
<p>在 Kubernetes 中，负载均衡的创建逻辑都在 kube-controller-manager 中，因而排查负载均衡相关的问题时，除了查看 Service 自身的状态，如</p>
<pre><code class="lang-sh">kubectl describe service <service-name>
</code></pre>
<p>还需要查看 kube-controller-manager 是否有异常发生：</p>
<pre><code>PODNAME=$(kubectl -n kube-system get pod -l component=kube-controller-manager -o jsonpath='{.items[0].metadata.name}')
kubectl -n kube-system logs $PODNAME --tail 100
</code></pre><h2 id="loadbalancer-service-一直处于-pending-状态">LoadBalancer Service 一直处于 pending 状态</h2>
<p>查看 Service <code>kubectl describe service <service-name></code> 没有错误信息，但 EXTERNAL-IP 一直是 <code><pending></code>，说明 Azure Cloud Provider 在创建 LB/NSG/PublicIP 过程中出错。一般按照前面的步骤查看 kube-controller-manager 可以查到具体失败的原因，可能的因素包括</p>
<ul>
<li>clientId、clientSecret、tenandId 或 subscriptionId 配置错误导致 Azure API 认证失败：更新所有节点的 <code>/etc/kubernetes/azure.json</code> ，修复错误的配置即可恢复服务</li>
<li>配置的客户端无权管理 LB/NSG/PublicIP/VM：可以为使用的 clientId 增加授权或创建新的 <code>az ad sp create-for-rbac --role="Contributor" --scopes="/subscriptions/<subscriptionID>/resourceGroups/<resourceGroupName>"</code></li>
<li>Kuberentes v1.8.X 中还有可能出现 <code>Security rule must specify SourceAddressPrefixes, SourceAddressPrefix, or SourceApplicationSecurityGroups</code> 的错误，这是由于 Azure Go SDK 的问题导致的，可以通过升级集群到 v1.9.X/v1.10.X 或者将 SourceAddressPrefixes 替换为多条 SourceAddressPrefix 规则来解决</li>
</ul>
<h2 id="负载均衡公网-ip-无法访问">负载均衡公网 IP 无法访问</h2>
<p>Azure Cloud Provider 会为负载均衡器创建探测器，只有探测正常的服务才可以响应用户的请求。负载均衡公网 IP 无法访问一般是探测失败导致的，可能原因有：</p>
<ul>
<li>后端 VM  本身不正常（可以重启 VM 恢复）</li>
<li>后端容器未监听在设置的端口上（可通过配置正确的端口解决）</li>
<li>防火墙或网络安全组阻止了要访问的端口（可通过增加安全规则解决）</li>
<li>当使用内网负载均衡时，从同一个 ILB 的后端 VM 上访问 ILB VIP 时也会失败，这是 Azure 的<a href="https://docs.microsoft.com/en-us/azure/load-balancer/load-balancer-troubleshoot#cause-4-accessing-the-internal-load-balancer-vip-from-the-participating-load-balancer-backend-pool-vm" target="_blank">预期行为</a>（此时可以访问 service 的 clusterIP）</li>
<li>后端容器不响应（部分或者全部）外部请求时也会导致负载均衡 IP 无法访问。注意这里包含<strong>部分容器不响应的场景</strong>，这是由于 Azure 探测器与 Kubernetes 服务发现机制共同导致的结果：<ul>
<li>（1）Azure 探测器定期去访问 service 的端口（即 NodeIP:NodePort）</li>
<li>（2）Kubernetes 将其负载均衡到后端容器中</li>
<li>（3）当负载均衡到异常容器时，访问失败会导致探测失败，进而 Azure 可能会将 VM 移出负载均衡</li>
<li>该问题的解决方法是使用<a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/" target="_blank">健康探针</a>，保证异常容器自动从服务的后端（endpoints）中删除。</li>
</ul>
</li>
</ul>
<h2 id="内网负载均衡-backendpool-为空">内网负载均衡 BackendPool 为空</h2>
<p>Kubernetes 1.9.0-1.9.3 中会有这个问题（<a href="https://github.com/kubernetes/kubernetes/issues/59746" target="_blank">kubernetes#59746</a> <a href="https://github.com/kubernetes/kubernetes/issues/60060" target="_blank">kubernetes#60060</a> <a href="https://github.com/Azure/acs-engine/issues/2151" target="_blank">acs-engine#2151</a>），这是由于一个查找负载均衡所属 AvaibilitySet 的缺陷导致的。</p>
<p>该问题的修复（<a href="https://github.com/kubernetes/kubernetes/pull/59747" target="_blank">kubernetes#59747</a> <a href="https://github.com/kubernetes/kubernetes/pull/59083" target="_blank">kubernetes#59083</a>）将包含到 v1.9.4 和 v1.10 中。</p>
<h2 id="外网负载均衡均衡-backendpool-为空">外网负载均衡均衡 BackendPool 为空</h2>
<p>在使用不支持 Cloud Provider 的工具（如 kubeadm）部署的集群中，如果未给 Kubelet 配置 <code>--cloud-provider=azure --cloud-config=/etc/kubernetes/cloud-config</code>，那么 Kubelet 会以 hostname 将其注册到集群中。此时，查看该 Node 的信息（kubectl get node <node-name> -o yaml），可以发现其 externalID 与 hostname 相同。此时，kube-controller-manager 也无法将其加入到负载均衡的后端中。</node-name></p>
<p>一个简单的确认方式是查看 Node 的 externalID 和 name 是否不同：</p>
<pre><code class="lang-sh">$ kubectl get node -o jsonpath=<span class="hljs-string">'{.items[*].metadata.name}'</span>
k8s-agentpool1-27347916-0
$ kubectl get node -o jsonpath=<span class="hljs-string">'{.items[*].spec.externalID}'</span>
/subscriptions/<subscription-id>/resourceGroups/<rg-name>/providers/Microsoft.Compute/virtualMachines/k8s-agentpool1-27347916-0
</code></pre>
<p>该问题的解决方法是先删除 Node <code>kubectl delete node <node-name></code>，为 Kubelet 配置 <code>--cloud-provider=azure --cloud-config=/etc/kubernetes/cloud-config</code>，最后再重启 Kubelet。</p>
<h2 id="service-删除后-azure-公网-ip-未自动删除">Service 删除后 Azure 公网 IP 未自动删除</h2>
<p>Kubernetes 1.9.0-1.9.3 中会有这个问题（<a href="https://github.com/kubernetes/kubernetes/issues/59255" target="_blank">kubernetes#59255</a>）：当创建超过 10 个 LoadBalancer Service 后有可能会碰到由于超过 FrontendIPConfiguations Quota（默认为 10）导致负载均衡无法创建的错误。此时虽然负载均衡无法创建，但公网 IP 已经创建成功了，由于 Cloud Provider 的缺陷导致删除 Service 后公网 IP 却未删除。</p>
<p>该问题的修复（<a href="https://github.com/kubernetes/kubernetes/pull/59340" target="_blank">kubernetes#59340</a>）将包含到 v1.9.4 和 v1.10 中。</p>
<p>另外，超过 FrontendIPConfiguations Quota 的问题可以参考 <a href="https://docs.microsoft.com/en-us/azure/azure-subscription-service-limits" target="_blank">Azure subscription and service limits, quotas, and constraints</a> 增加 Quota 来解决。</p>
<h2 id="msi-无法使用">MSI 无法使用</h2>
<p>配置 <code>"useManagedIdentityExtension": true</code> 后，可以使用 <a href="https://docs.microsoft.com/en-us/azure/active-directory/msi-overview" target="_blank">Managed Service Identity (MSI)</a> 来管理 Azure API 的认证授权。但由于 Cloud Provider 的缺陷（<a href="https://github.com/kubernetes/kubernetes/issues/60691" target="_blank">kubernetes #60691</a> 未定义 <code>useManagedIdentityExtension</code> yaml 标签导致无法解析该选项。</p>
<p>该问题的修复（<a href="https://github.com/kubernetes/kubernetes/pull/60775" target="_blank">kubernetes#60775</a>）将包含在 v1.10 中。</p>
<h2 id="azure-arm-api-调用请求过多">Azure ARM API 调用请求过多</h2>
<p>有时 kube-controller-manager 或者 kubelet 会因请求调用过多而导致 Azure ARM API 失败的情况，比如</p>
<pre><code class="lang-sh"><span class="hljs-string">"OperationNotAllowed"</span>,\r\n    <span class="hljs-string">"message"</span>: <span class="hljs-string">"The server rejected the request because too many requests have been received for this subscription.
</span></code></pre>
<p>特别是在 Kubernetes 集群创建或者批量增加 Nodes 的时候。从 <a href="https://github.com/kubernetes/kubernetes/issues/58770" target="_blank">v1.9.2 和 v1.10</a> 开始， Azure cloud provider 为一些列的 Azure 资源（如 VM、VMSS、安全组和路由表等）增加了缓存，大大缓解了这个问题。</p>
<p>一般来说，如果该问题重复出现可以考虑</p>
<ul>
<li>使用 Azure instance metadata，即为所有 Node 的 <code>/etc/kubernetes/azure.json</code> 设置 <code>"useInstanceMetadata": true</code> 并重启 kubelet</li>
<li>为 kube-controller-manager 增大 <code>--route-reconciliation-period</code>（默认为 10s），比如在 <code>/etc/kubernetes/manifests/kube-controller-manager.yaml</code> 中设置 <code>--route-reconciliation-period=1m</code> 后 kubelet 会自动重新创建 kube-controller-manager Pod。</li>
</ul>
<h2 id="aks-kubectl-logs-connection-timed-out">AKS kubectl logs connection timed out</h2>
<p><code>kubectl logs</code> 命令报 <code>getsockopt: connection timed out</code> 的错误（<a href="https://github.com/Azure/AKS/issues/232" target="_blank">AKS#232</a>）：</p>
<pre><code class="lang-sh">$ kubectl --v=8 logs x
I0308 10:32:21.539580   26486 round_trippers.go:417] curl -k -v -XGET  -H <span class="hljs-string">"Accept: application/json, */*"</span> -H <span class="hljs-string">"User-Agent: kubectl/v1.8.1 (linux/amd64) kubernetes/f38e43b"</span> -H <span class="hljs-string">"Authorization: Bearer x"</span> https://x:443/api/v1/namespaces/default/pods/x/<span class="hljs-built_in">log</span>?container=x
I0308 10:34:32.790295   26486 round_trippers.go:436] GET https://X:443/api/v1/namespaces/default/pods/x/<span class="hljs-built_in">log</span>?container=x 500 Internal Server Error <span class="hljs-keyword">in</span> 131250 milliseconds
I0308 10:34:32.790356   26486 round_trippers.go:442] Response Headers:
I0308 10:34:32.790376   26486 round_trippers.go:445]     Content-Type: application/json
I0308 10:34:32.790390   26486 round_trippers.go:445]     Content-Length: 275
I0308 10:34:32.790414   26486 round_trippers.go:445]     Date: Thu, 08 Mar 2018 09:34:32 GMT
I0308 10:34:32.790504   26486 request.go:836] Response Body: {<span class="hljs-string">"kind"</span>:<span class="hljs-string">"Status"</span>,<span class="hljs-string">"apiVersion"</span>:<span class="hljs-string">"v1"</span>,<span class="hljs-string">"metadata"</span>:{},<span class="hljs-string">"status"</span>:<span class="hljs-string">"Failure"</span>,<span class="hljs-string">"message"</span>:<span class="hljs-string">"Get https://aks-nodepool1-53392281-1:10250/containerLogs/default/x: dial tcp 10.240.0.6:10250: getsockopt: connection timed out"</span>,<span class="hljs-string">"code"</span>:500}
I0308 10:34:32.790999   26486 helpers.go:207] server response object: [{
  <span class="hljs-string">"metadata"</span>: {},
  <span class="hljs-string">"status"</span>: <span class="hljs-string">"Failure"</span>,
  <span class="hljs-string">"message"</span>: <span class="hljs-string">"Get https://aks-nodepool1-53392281-1:10250/containerLogs/default/x/x: dial tcp 10.240.0.6:10250: getsockopt: connection timed out"</span>,
  <span class="hljs-string">"code"</span>: 500
}]
F0308 10:34:32.791043   26486 helpers.go:120] Error from server: Get https://aks-nodepool1-53392281-1:10250/containerLogs/default/x/x: dial tcp 10.240.0.6:10250: getsockopt: connection timed out
</code></pre>
<p>在 AKS 中，kubectl logs, exec, and attach 等命令需要 Master 与 Nodes 节点之间建立隧道连接。在 <code>kube-system</code> namespace 中可以看到 <code>tunnelfront</code> 和 <code>kube-svc-redirect</code> Pod：</p>
<pre><code>$ kubectl -n kube-system get po -l component=tunnel
NAME                           READY     STATUS    RESTARTS   AGE
tunnelfront-7644cd56b7-l5jmc   1/1       Running   0          2d

$ kubectl -n kube-system get po -l component=kube-svc-redirect
NAME                      READY     STATUS    RESTARTS   AGE
kube-svc-redirect-pq6kf   1/1       Running   0          2d
kube-svc-redirect-x6sq5   1/1       Running   0          2d
kube-svc-redirect-zjl7x   1/1       Running   1          2d
</code></pre><p>如果它们不是处于 <code>Running</code> 状态或者 Exec/Logs/PortForward 等命令报 <code>net/http: TLS handshake timeout</code> 错误，删除 <code>tunnelfront</code> Pod，稍等一会就会自动创建新的出来，如：</p>
<pre><code>$ kubectl -n kube-system delete po -l component=tunnel
pod "tunnelfront-7644cd56b7-l5jmc" deleted
</code></pre><h2 id="使用-virtual-kubelet-后-loadbalancer-service-无法分配公网-ip">使用 Virtual Kubelet 后 LoadBalancer Service 无法分配公网 IP</h2>
<p>使用 Virtual Kubelet 后，LoadBalancer Service 可能会一直处于 pending 状态，无法分配 IP 地址。查看该服务的事件（如 <code>kubectl describe svc）</code>会发现错误 <code>CreatingLoadBalancerFailed  4m (x15 over 45m)  service-controller  Error creating load balancer (will retry): failed to ensure load balancer for service default/nginx: ensure(default/nginx): lb(kubernetes) - failed to ensure host in pool: "instance not found"</code>。这是由于 Virtual Kubelet 创建的虚拟 Node 并不存在于 Azure 云平台中，因而无法将其加入到 Azure Load Balancer 的后端中。</p>
<p>解决方法是开启 ServiceNodeExclusion 特性，即设置 <code>kube-controller-manager --feature-gates=ServiceNodeExclusion=true</code>。开启后，所有带有 <code>alpha.service-controller.kubernetes.io/exclude-balancer</code> 标签的 Node 都不会加入到云平台负载均衡的后端中。</p>
<p>注意该特性仅适用于 Kubernetes 1.9 及以上版本。</p>
<h2 id="node-的-gpu-数总是-0">Node 的 GPU 数总是 0</h2>
<p>当在 AKS 集群中运行 GPU 负载时，发现它们无法调度，这可能是由于 Node 容量中的 <code>nvidia.com/gpu</code> 总是0。</p>
<p>解决方法是重新部署 nvidia-gpu 设备插件扩展：</p>
<pre><code class="lang-yaml"><span class="hljs-attr">apiVersion:</span> extensions/v1beta1
<span class="hljs-attr">kind:</span> DaemonSet
<span class="hljs-attr">metadata:</span>
<span class="hljs-attr">  labels:</span>
    kubernetes.io/cluster-service: <span class="hljs-string">"true"</span>
<span class="hljs-attr">  name:</span> nvidia-device-plugin
<span class="hljs-attr">  namespace:</span> kube-system
<span class="hljs-attr">spec:</span>
<span class="hljs-attr">  template:</span>
<span class="hljs-attr">    metadata:</span>
      <span class="hljs-comment"># Mark this pod as a critical add-on; when enabled, the critical add-on scheduler</span>
      <span class="hljs-comment"># reserves resources for critical add-on pods so that they can be rescheduled after</span>
      <span class="hljs-comment"># a failure.  This annotation works in tandem with the toleration below.</span>
<span class="hljs-attr">      annotations:</span>
        scheduler.alpha.kubernetes.io/critical-pod: <span class="hljs-string">""</span>
<span class="hljs-attr">      labels:</span>
<span class="hljs-attr">        name:</span> nvidia-device-plugin-ds
<span class="hljs-attr">    spec:</span>
<span class="hljs-attr">      tolerations:</span>
      <span class="hljs-comment"># Allow this pod to be rescheduled while the node is in "critical add-ons only" mode.</span>
      <span class="hljs-comment"># This, along with the annotation above marks this pod as a critical add-on.</span>
<span class="hljs-attr">      - key:</span> CriticalAddonsOnly
<span class="hljs-attr">        operator:</span> Exists
<span class="hljs-attr">      containers:</span>
<span class="hljs-attr">      - image:</span> nvidia/k8s-device-plugin:<span class="hljs-number">1.10</span>
<span class="hljs-attr">        name:</span> nvidia-device-plugin-ctr
<span class="hljs-attr">        securityContext:</span>
<span class="hljs-attr">          allowPrivilegeEscalation:</span> <span class="hljs-literal">false</span>
<span class="hljs-attr">          capabilities:</span>
<span class="hljs-attr">            drop:</span> [<span class="hljs-string">"ALL"</span>]
<span class="hljs-attr">        volumeMounts:</span>
<span class="hljs-attr">          - name:</span> device-plugin
<span class="hljs-attr">            mountPath:</span> /var/lib/kubelet/device-plugins
<span class="hljs-attr">      volumes:</span>
<span class="hljs-attr">        - name:</span> device-plugin
<span class="hljs-attr">          hostPath:</span>
<span class="hljs-attr">            path:</span> /var/lib/kubelet/device-plugins
<span class="hljs-attr">      nodeSelector:</span>
        beta.kubernetes.io/os: linux
<span class="hljs-attr">        accelerator:</span> nvidia
</code></pre>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://docs.microsoft.com/en-us/azure/azure-subscription-service-limits" target="_blank">Azure subscription and service limits, quotas, and constraints</a></li>
<li><a href="https://github.com/virtual-kubelet/virtual-kubelet#missing-load-balancer-ip-addresses-for-services" target="_blank">Virtual Kubelet - Missing Load Balancer IP addresses for services</a></li>
<li><a href="https://docs.microsoft.com/en-us/azure/load-balancer/load-balancer-troubleshoot#cause-4-accessing-the-internal-load-balancer-vip-from-the-participating-load-balancer-backend-pool-vm" target="_blank">Troubleshoot Azure Load Balancer</a></li>
<li><a href="https://github.com/Azure/acs-engine/blob/master/docs/kubernetes/troubleshooting.md" target="_blank">Troubleshooting CustomScriptExtension (CSE) and acs-engine</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="排错工具" class="level2">排错工具</h1>
<p>本章主要介绍在 Kubernetes 排错中常用的工具。</p>
<h2 id="必备工具">必备工具</h2>
<ul>
<li><code>kubectl</code>：用于查看 Kubernetes 集群以及容器的状态，如 <code>kubectl describe pod <pod-name></code></li>
<li><code>journalctl</code>：用于查看 Kubernetes 组件日志，如 <code>journalctl -u kubelet -l</code></li>
<li><code>iptables</code>和<code>ebtables</code>：用于排查 Service 是否工作，如 <code>iptables -t nat -nL</code> 查看 kube-proxy 配置的 iptables 规则是否正常</li>
<li><code>tcpdump</code>：用于排查容器网络问题，如 <code>tcpdump -nn host 10.240.0.8</code></li>
<li><code>perf</code>：Linux 内核自带的性能分析工具，常用来排查性能问题，如 <a href="https://dzone.com/articles/container-isolation-gone-wrong" target="_blank">Container Isolation Gone Wrong</a> 问题的排查</li>
</ul>
<h2 id="sysdig">sysdig</h2>
<p>sysdig 是一个容器排错工具，提供了开源和商业版本。对于常规排错来说，使用开源版本即可。</p>
<p>除了 sysdig，还可以使用其他两个辅助工具</p>
<ul>
<li>csysdig：与 sysdig 一起自动安装，提供了一个命令行界面</li>
</ul>
<ul>
<li><a href="https://github.com/draios/sysdig-inspect" target="_blank">sysdig-inspect</a>：为 sysdig 保存的跟踪文件（如 <code>sudo sysdig -w filename.scap</code>）提供了一个图形界面（非实时）</li>
</ul>
<h3 id="安装">安装</h3>
<pre><code class="lang-sh"><span class="hljs-comment"># on Ubuntu</span>
curl <span class="hljs-_">-s</span> https://s3.amazonaws.com/download.draios.com/DRAIOS-GPG-KEY.public | apt-key add -
curl <span class="hljs-_">-s</span> -o /etc/apt/sources.list.d/draios.list http://download.draios.com/stable/deb/draios.list
apt-get update
apt-get -y install linux-headers-$(uname -r)
apt-get -y install sysdig

<span class="hljs-comment"># on REHL</span>
rpm --import https://s3.amazonaws.com/download.draios.com/DRAIOS-GPG-KEY.public
curl <span class="hljs-_">-s</span> -o /etc/yum.repos.d/draios.repo http://download.draios.com/stable/rpm/draios.repo
rpm -i http://mirror.us.leaseweb.net/epel/6/i386/epel-release-6-8.noarch.rpm
yum -y install kernel-devel-$(uname -r)
yum -y install sysdig

<span class="hljs-comment"># on MacOS</span>
brew install sysdig
</code></pre>
<h3 id="示例">示例</h3>
<pre><code class="lang-sh"><span class="hljs-comment"># Refer https://www.sysdig.org/wiki/sysdig-examples/.</span>
<span class="hljs-comment"># View the top network connections</span>
sudo sysdig -pc -c topconns
<span class="hljs-comment"># View the top network connections inside the wordpress1 container</span>
sudo sysdig -pc -c topconns container.name=wordpress1

<span class="hljs-comment"># Show the network data exchanged with the host 192.168.0.1</span>
sudo sysdig fd.ip=192.168.0.1
sudo sysdig <span class="hljs-_">-s</span>2000 -A -c <span class="hljs-built_in">echo</span>_fds fd.cip=192.168.0.1

<span class="hljs-comment"># List all the incoming connections that are not served by apache.</span>
sudo sysdig -p<span class="hljs-string">"%proc.name %fd.name"</span> <span class="hljs-string">"evt.type=accept and proc.name!=httpd"</span>

<span class="hljs-comment"># View the CPU/Network/IO usage of the processes running inside the container.</span>
sudo sysdig -pc -c topprocs_cpu container.id=2e854c4525b8
sudo sysdig -pc -c topprocs_net container.id=2e854c4525b8
sudo sysdig -pc -c topfiles_bytes container.id=2e854c4525b8

<span class="hljs-comment"># See the files where apache spends the most time doing I/O</span>
sudo sysdig -c topfiles_time proc.name=httpd

<span class="hljs-comment"># Show all the interactive commands executed inside a given container.</span>
sudo sysdig -pc -c spy_users 

<span class="hljs-comment"># Show every time a file is opened under /etc.</span>
sudo sysdig evt.type=open and fd.name

<span class="hljs-comment"># View the list of processes with container context</span>
sudo csysdig -pc
</code></pre>
<p>更多示例和使用方法可以参考 <a href="https://github.com/draios/sysdig/wiki/Sysdig-User-Guide" target="_blank">Sysdig User Guide</a>。</p>
<h2 id="weave-scope">Weave Scope</h2>
<p>Weave Scope 是另外一款可视化容器监控和排错工具。与 sysdig 相比，它没有强大的命令行工具，但提供了一个简单易用的交互界面，自动描绘了整个集群的拓扑，并可以通过插件扩展其功能。从其官网的介绍来看，其提供的功能包括</p>
<ul>
<li><a href="https://www.weave.works/docs/scope/latest/features/#topology-mapping" target="_blank">交互式拓扑界面</a></li>
<li><a href="https://www.weave.works/docs/scope/latest/features/#mode" target="_blank">图形模式和表格模式</a></li>
<li><a href="https://www.weave.works/docs/scope/latest/features/#flexible-filtering" target="_blank">过滤功能</a></li>
<li><a href="https://www.weave.works/docs/scope/latest/features/#powerful-search" target="_blank">搜索功能</a></li>
<li><a href="https://www.weave.works/docs/scope/latest/features/#real-time-app-and-container-metrics" target="_blank">实时度量</a></li>
<li><a href="https://www.weave.works/docs/scope/latest/features/#interact-with-and-manage-containers" target="_blank">容器排错</a></li>
<li><a href="https://www.weave.works/docs/scope/latest/features/#custom-plugins" target="_blank">插件扩展</a></li>
</ul>
<p>Weave Scope 由 <a href="https://www.weave.works/docs/scope/latest/how-it-works" target="_blank">App 和 Probe 两部分</a>组成，它们</p>
<ul>
<li>Probe 负责收集容器和宿主的信息，并发送给 App</li>
<li>App 负责处理这些信息，并生成相应的报告，并以交互界面的形式展示</li>
</ul>
<pre><code class="lang-sh">                    +--Docker host----------+      +--Docker host----------+
.---------------.   |  +--Container------+  |      |  +--Container------+  |
| Browser       |   |  |                 |  |      |  |                 |  |
|---------------|   |  |  +-----------+  |  |      |  |  +-----------+  |  |
|               |----->|  | scope-app |<-----.    .----->| scope-app |  |  |
|               |   |  |  +-----------+  |  | \  / |  |  +-----------+  |  |
|               |   |  |        ^        |  |  \/  |  |        ^        |  |
<span class="hljs-string">'---------------'</span>   |  |        |        |  |  /\  |  |        |        |  |
                    |  | +-------------+ |  | /  \ |  | +-------------+ |  |
                    |  | | scope-probe |-----<span class="hljs-string">'    '</span>-----| scope-probe | |  |
                    |  | +-------------+ |  |      |  | +-------------+ |  |
                    |  |                 |  |      |  |                 |  |
                    |  +-----------------+  |      |  +-----------------+  |
                    +-----------------------+      +-----------------------+
</code></pre>
<h3 id="安装">安装</h3>
<pre><code class="lang-sh">kubectl apply <span class="hljs-_">-f</span> <span class="hljs-string">"https://cloud.weave.works/k8s/scope.yaml?k8s-version=<span class="hljs-variable">$(kubectl version | base64 | tr -d '\n')</span>&k8s-service-type=LoadBalancer"</span>
</code></pre>
<h3 id="查看界面">查看界面</h3>
<p>安装完成后，可以通过 weave-scope-app 来访问交互界面</p>
<pre><code class="lang-sh">kubectl -n weave get service weave-scope-app
kubectl -n weave port-forward service/weave-scope-app :80
</code></pre>
<p><img src="images/weave-scope.png" alt=""/></p>
<p>点击 Pod，还可以查看该 Pod 所有容器的实时状态和度量数据：</p>
<p><img src="images/scope-pod.png" alt=""/></p>
<h3 id="已知问题">已知问题</h3>
<p>在 Ubuntu 内核 4.4.0 上面开启 <code>--probe.ebpf.connections</code> 时（默认开启），Node 有可能会因为<a href="https://github.com/weaveworks/scope/issues/3131" target="_blank">内核问题而不停重启</a>：</p>
<pre><code class="lang-sh">[ 263.736006] CPU: 0 PID: 6309 Comm: scope Not tainted 4.4.0-119-generic <span class="hljs-comment">#143-Ubuntu</span>
[ 263.736006] Hardware name: Microsoft Corporation Virtual Machine/Virtual Machine, BIOS 090007 06/02/2017
[ 263.736006] task: ffff88011cef5400 ti: ffff88000a0e4000 task.ti: ffff88000a0e4000
[ 263.736006] RIP: 0010:[] [] bpf_map_lookup_elem+0x6/0x20
[ 263.736006] RSP: 0018:ffff88000a0e7a70 EFLAGS: 00010082
[ 263.736006] RAX: ffffffff8117<span class="hljs-built_in">cd</span>70 RBX: ffffc90000762068 RCX: 0000000000000000
[ 263.736006] RDX: 0000000000000000 RSI: ffff88000a0e7<span class="hljs-built_in">cd</span>8 RDI: 000000001cdee380
[ 263.736006] RBP: ffff88000a0e7cf8 R08: 0000000005080021 R09: 0000000000000000
[ 263.736006] R10: 0000000000000020 R11: ffff880159e1c700 R12: 0000000000000000
[ 263.736006] R13: ffff88011cfaf400 R14: ffff88000a0e7e38 R15: ffff88000a0f8800
[ 263.736006] FS: 00007f5b0<span class="hljs-built_in">cd</span>79700(0000) GS:ffff88015b600000(0000) knlGS:0000000000000000
[ 263.736006] CS: 0010 DS: 0000 ES: 0000 CR0: 0000000080050033
[ 263.736006] CR2: 000000001cdee3a8 CR3: 000000011ce04000 CR4: 0000000000040670
[ 263.736006] Stack:
[ 263.736006] ffff88000a0e7cf8 ffffffff81177411 0000000000000000 00001887000018a5
[ 263.736006] 000000001cdee380 ffff88000a0e7<span class="hljs-built_in">cd</span>8 0000000000000000 0000000000000000
[ 263.736006] 0000000005080021 ffff88000a0e7e38 0000000000000000 0000000000000046
[ 263.736006] Call Trace:
[ 263.736006] [] ? __bpf_prog_run+0x7a1/0x1360
[ 263.736006] [] ? update_curr+0x79/0x170
[ 263.736006] [] ? update_cfs_shares+0xbc/0x100
[ 263.736006] [] ? update_curr+0x79/0x170
[ 263.736006] [] ? dput+0xb8/0x230
[ 263.736006] [] ? follow_managed+0x265/0x300
[ 263.736006] [] ? kmem_cache_alloc_trace+0x1d4/0x1f0
[ 263.736006] [] ? seq_open+0x5a/0xa0
[ 263.736006] [] ? probes_open+0x33/0x100
[ 263.736006] [] ? dput+0x34/0x230
[ 263.736006] [] ? mntput+0x24/0x40
[ 263.736006] [] trace_call_bpf+0x37/0x50
[ 263.736006] [] kretprobe_perf_func+0x3d/0x250
[ 263.736006] [] ? pre_handler_kretprobe+0x135/0x1b0
[ 263.736006] [] kretprobe_dispatcher+0x3d/0x60
[ 263.736006] [] ? <span class="hljs-keyword">do</span>_sys_open+0x1b2/0x2a0
[ 263.736006] [] ? kretprobe_trampoline_holder+0x9/0x9
[ 263.736006] [] trampoline_handler+0x133/0x210
[ 263.736006] [] ? <span class="hljs-keyword">do</span>_sys_open+0x1b2/0x2a0
[ 263.736006] [] kretprobe_trampoline+0x25/0x57
[ 263.736006] [] ? kretprobe_trampoline_holder+0x9/0x9
[ 263.736006] [] SyS_openat+0x14/0x20
[ 263.736006] [] entry_SYSCALL_64_fastpath+0x1c/0xbb
</code></pre>
<p>解决方法有两种</p>
<ul>
<li>禁止 eBPF 探测，如 <code>--probe.ebpf.connections=false</code></li>
<li>升级内核，如升级到 4.13.0</li>
</ul>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://kubernetes.io/docs/reference/kubectl/overview/" target="_blank">Overview of kubectl</a></li>
<li><a href="https://sysdig.com/blog/kubernetes-service-discovery-docker/" target="_blank">Monitoring Kuberietes with sysdig</a> </li>
</ul>
</section>
                            
    <h1 class='level1'>社区贡献</h1><section class="normal markdown-section">
                                
                                <h1 id="kubernetes-开发环境" class="level2">开发指南</h1>
<h2 id="配置开发环境">配置开发环境</h2>
<p>以 Ubuntu 为例，配置一个 Kubernetes 的开发环境</p>
<pre><code class="lang-sh">apt-get install -y gcc make socat git build-essential

<span class="hljs-comment"># 安装 Docker</span>
sh -c <span class="hljs-string">'echo"deb https://apt.dockerproject.org/repo ubuntu-$(lsb_release -cs) main"> /etc/apt/sources.list.d/docker.list'</span>
curl -fsSL https://apt.dockerproject.org/gpg | sudo apt-key add -
apt-key fingerprint 58118E89F3A912897C070ADBF76221572C52609D
apt-get update
apt-get -y install <span class="hljs-string">"docker-engine=1.13.1-0~ubuntu-<span class="hljs-variable">$(lsb_release -cs)</span>"</span>

<span class="hljs-comment"># 安装 etcd</span>
ETCD_VER=v3.2.18
DOWNLOAD_URL=<span class="hljs-string">"https://github.com/coreos/etcd/releases/download"</span>
curl -L <span class="hljs-variable">${DOWNLOAD_URL}</span>/<span class="hljs-variable">${ETCD_VER}</span>/etcd-<span class="hljs-variable">${ETCD_VER}</span>-linux-amd64.tar.gz -o /tmp/etcd-<span class="hljs-variable">${ETCD_VER}</span>-linux-amd64.tar.gz
tar xzvf /tmp/etcd-<span class="hljs-variable">${ETCD_VER}</span>-linux-amd64.tar.gz
sudo /bin/cp <span class="hljs-_">-f</span> etcd-<span class="hljs-variable">${ETCD_VER}</span>-linux-amd64/{etcd,etcdctl} /usr/bin
rm -rf /tmp/etcd-<span class="hljs-variable">${ETCD_VER}</span>-linux-amd64.tar.gz etcd-<span class="hljs-variable">${ETCD_VER}</span>-linux-amd64

<span class="hljs-comment"># 安装 Go</span>
curl <span class="hljs-_">-s</span>L https://storage.googleapis.com/golang/go1.10.2.linux-amd64.tar.gz | tar -C /usr/<span class="hljs-built_in">local</span> -zxf -
<span class="hljs-built_in">export</span> GOPATH=/gopath
<span class="hljs-built_in">export</span> PATH=<span class="hljs-variable">$PATH</span>:<span class="hljs-variable">$GOPATH</span>/bin:/usr/<span class="hljs-built_in">local</span>/bin:/usr/<span class="hljs-built_in">local</span>/go/bin/

<span class="hljs-comment"># 下载 Kubernetes 代码</span>
mkdir -p <span class="hljs-variable">$GOPATH</span>/src/k8s.io
git <span class="hljs-built_in">clone</span> https://github.com/kubernetes/kubernetes <span class="hljs-variable">$GOPATH</span>/src/k8s.io/kubernetes
<span class="hljs-built_in">cd</span> <span class="hljs-variable">$GOPATH</span>/src/k8s.io/kubernetes

<span class="hljs-comment"># 启动一个本地集群</span>
<span class="hljs-built_in">export</span> KUBERNETES_PROVIDER=<span class="hljs-built_in">local</span>
hack/<span class="hljs-built_in">local</span>-up-cluster.sh
</code></pre>
<p>打开另外一个终端，配置 kubectl 之后就可以开始使用了:</p>
<pre><code class="lang-sh"><span class="hljs-built_in">cd</span> <span class="hljs-variable">$GOPATH</span>/src/k8s.io/kubernetes
<span class="hljs-built_in">export</span> KUBECONFIG=/var/run/kubernetes/admin.kubeconfig
cluster/kubectl.sh
</code></pre>
<h2 id="单元测试">单元测试</h2>
<p>单元测试是 Kubernetes 开发中不可缺少的，一般在代码修改的同时还要更新或添加对应的单元测试。这些单元测试大都支持在不同的系统上直接运行，比如 OSX、Linux 等。</p>
<p>比如，加入修改了 <code>pkg/kubelet/kuberuntime</code> 的代码后，</p>
<pre><code class="lang-sh"><span class="hljs-comment"># 可以加上 Go package 的全路径来测试</span>
go <span class="hljs-built_in">test</span> -v k8s.io/kubernetes/pkg/kubelet/kuberuntime
<span class="hljs-comment"># 也可以用相对目录</span>
go <span class="hljs-built_in">test</span> -v ./pkg/kubelet/kuberuntime
</code></pre>
<h2 id="端到端测试">端到端测试</h2>
<p>端到端（e2e）测试需要启动一个 Kubernetes 集群，仅支持在 Linux 系统上运行。</p>
<p>本地运行方法示例：</p>
<pre><code class="lang-sh">make WHAT=<span class="hljs-string">'test/e2e/e2e.test'</span>
make ginkgo

<span class="hljs-built_in">export</span> KUBERNETES_PROVIDER=<span class="hljs-built_in">local</span>
go run hack/e2e.go -v -test --test_args=<span class="hljs-string">'--ginkgo.focus=Port\sforwarding'</span>
go run hack/e2e.go -v -test --test_args=<span class="hljs-string">'--ginkgo.focus=Feature:SecurityContext'</span>
</code></pre>
<blockquote>
<p>注：Kubernetes 的每个 PR 都会自动运行一系列的 e2e 测试。</p>
</blockquote>
<h2 id="node-e2e-测试">Node e2e 测试</h2>
<p>Node e2e 测试需要启动 Kubelet，目前仅支持在 Linux 系统上运行。</p>
<pre><code class="lang-sh"><span class="hljs-built_in">export</span> KUBERNETES_PROVIDER=<span class="hljs-built_in">local</span>
make <span class="hljs-built_in">test</span><span class="hljs-_">-e</span>2e-node FOCUS=<span class="hljs-string">"InitContainer"</span>
</code></pre>
<blockquote>
<p>注：Kubernetes 的每个 PR 都会自动运行 node e2e 测试。</p>
</blockquote>
<h2 id="有用的-git-命令">有用的 git 命令</h2>
<p>很多时候，我们需要把 Pull Request 拉取到本地来测试，比如拉取 Pull Request #365 的方法为</p>
<pre><code class="lang-sh">git fetch upstream pull/365/merge:branch-fix-1
git checkout branch-fix-1
</code></pre>
<p>当然，也可以配置 <code>.git/config</code> 并运行 <code>git fetch</code> 拉取所有的 Pull Requests（注意 Kubernetes 的 Pull Requests 非常多，这个过程可能会很慢）:</p>
<pre><code>fetch = +refs/pull/*:refs/remotes/origin/pull/*
</code></pre><h2 id="其他参考">其他参考</h2>
<ul>
<li>编译 release 版：<code>make quick-release</code></li>
<li>机器人命令：<a href="https://prow.k8s.io/command-help" target="_blank">命令列表</a> 和 <a href="https://prow.k8s.io/plugins" target="_blank">使用文档</a>。</li>
<li><a href="https://k8s-testgrid.appspot.com/" target="_blank">Kubernetes TestGrid</a>，包含所有的测试历史</li>
<li><a href="https://submit-queue.k8s.io/#/queue" target="_blank">Kuberentes Submit Queue Status</a>，包含所有的 Pull Request 状态以及合并队列</li>
<li><a href="http://node-perf-dash.k8s.io/#/builds" target="_blank">Node Performance Dashboard</a>，包含 Node 组性能测试报告</li>
<li><a href="http://perf-dash.k8s.io/" target="_blank">Kubernetes Performance Dashboard</a>，包含 Density 和 Load 测试报告</li>
<li><a href="https://k8s-gubernator.appspot.com/pr" target="_blank">Kubernetes PR Dashboard</a>，包含主要关注的 Pull Request 列表（需要以 Github 登录）</li>
<li><a href="https://k8s-gubernator.appspot.com/" target="_blank">Jenkins Logs</a> 和 <a href="http://prow.k8s.io/?type=presubmit" target="_blank">Prow Status</a>，包含所有 Pull Request 的 Jenkins 测试日志</li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="kubernetes-测试" class="level2">单元测试和集成测试</h1>
<ul>
<li><a href="https://prow.k8s.io/" target="_blank">Current Test Status</a></li>
<li><a href="https://storage.googleapis.com/k8s-gubernator/triage/index.html" target="_blank">Aggregated Failures</a></li>
<li><a href="https://k8s-testgrid.appspot.com/" target="_blank">Test Grid</a></li>
</ul>
<h2 id="单元测试">单元测试</h2>
<p>单元测试仅依赖于源代码，是测试代码逻辑是否符合预期的最简单方法。</p>
<h3 id="运行所有的单元测试">运行所有的单元测试</h3>
<pre><code class="lang-sh">make <span class="hljs-built_in">test</span>
</code></pre>
<h3 id="仅测试指定的-package">仅测试指定的 package</h3>
<pre><code class="lang-sh"><span class="hljs-comment"># 单个 package</span>
make <span class="hljs-built_in">test</span> WHAT=./pkg/api
<span class="hljs-comment"># 多个 packages</span>
make <span class="hljs-built_in">test</span> WHAT=./pkg/{api,kubelet}
</code></pre>
<p>或者，也可以直接用 <code>go test</code></p>
<pre><code class="lang-sh">go <span class="hljs-built_in">test</span> -v k8s.io/kubernetes/pkg/kubelet
</code></pre>
<h3 id="仅测试指定-package-的某个测试-case">仅测试指定 package 的某个测试 case</h3>
<pre><code class="lang-sh"><span class="hljs-comment"># Runs TestValidatePod in pkg/api/validation with the verbose flag set</span>
make <span class="hljs-built_in">test</span> WHAT=./pkg/api/validation KUBE_GOFLAGS=<span class="hljs-string">"-v"</span> KUBE_TEST_ARGS=<span class="hljs-string">'-run ^TestValidatePod$'</span>

<span class="hljs-comment"># Runs tests that match the regex ValidatePod|ValidateConfigMap in pkg/api/validation</span>
make <span class="hljs-built_in">test</span> WHAT=./pkg/api/validation KUBE_GOFLAGS=<span class="hljs-string">"-v"</span> KUBE_TEST_ARGS=<span class="hljs-string">"-run ValidatePod\|ValidateConfigMap$"</span>
</code></pre>
<p>或者直接用 <code>go test</code></p>
<pre><code class="lang-sh">go <span class="hljs-built_in">test</span> -v k8s.io/kubernetes/pkg/api/validation -run ^TestValidatePod$
</code></pre>
<h3 id="并行测试">并行测试</h3>
<p>并行测试是 root out flakes 的一种有效方法：</p>
<pre><code class="lang-sh"><span class="hljs-comment"># Have 2 workers run all tests 5 times each (10 total iterations).</span>
make <span class="hljs-built_in">test</span> PARALLEL=2 ITERATION=5
</code></pre>
<h3 id="生成测试报告">生成测试报告</h3>
<pre><code class="lang-sh">make <span class="hljs-built_in">test</span> KUBE_COVER=y
</code></pre>
<h2 id="benchmark-测试">Benchmark 测试</h2>
<pre><code class="lang-sh">go <span class="hljs-built_in">test</span> ./pkg/apiserver -benchmem -run=XXX -bench=BenchmarkWatch
</code></pre>
<h2 id="集成测试">集成测试</h2>
<p>Kubernetes 集成测试需要安装 etcd（只要按照即可，不需要启动），比如</p>
<pre><code class="lang-sh">hack/install-etcd.sh  <span class="hljs-comment"># Installs in ./third_party/etcd</span>
<span class="hljs-built_in">echo</span> <span class="hljs-built_in">export</span> PATH=<span class="hljs-string">"\$PATH:<span class="hljs-variable">$(pwd)</span>/third_party/etcd"</span> >> ~/.profile  <span class="hljs-comment"># Add to PATH</span>
</code></pre>
<p>集成测试会在需要的时候自动启动 etcd 和 kubernetes 服务，并运行 <a href="https://github.com/kubernetes/kubernetes/tree/master/test/integration" target="_blank">test/integration</a> 里面的测试。</p>
<h3 id="运行所有集成测试">运行所有集成测试</h3>
<pre><code class="lang-sh">make <span class="hljs-built_in">test</span>-integration  <span class="hljs-comment"># Run all integration tests.</span>
</code></pre>
<h3 id="指定集成测试用例">指定集成测试用例</h3>
<pre><code class="lang-sh"><span class="hljs-comment"># Run integration test TestPodUpdateActiveDeadlineSeconds with the verbose flag set.</span>
make <span class="hljs-built_in">test</span>-integration KUBE_GOFLAGS=<span class="hljs-string">"-v"</span> KUBE_TEST_ARGS=<span class="hljs-string">"-run ^TestPodUpdateActiveDeadlineSeconds$"</span>
</code></pre>
<h2 id="end-to-end-e2e-测试">End to end (e2e) 测试</h2>
<p>End to end (e2e) 测试模拟用户行为操作 Kubernetes，用来保证 Kubernetes 服务或集群的行为完全符合设计预期。</p>
<p>在开启 e2e 测试之前，需要先编译测试文件，并设置 KUBERNETES_PROVIDER（默认为 gce）：</p>
<pre><code>make WHAT='test/e2e/e2e.test'
make ginkgo
export KUBERNETES_PROVIDER=local
</code></pre><h3 id="启动-cluster，测试，最后停止-cluster">启动 cluster，测试，最后停止 cluster</h3>
<pre><code class="lang-sh"><span class="hljs-comment"># build Kubernetes, up a cluster, run tests, and tear everything down</span>
go run hack/e2e.go -- -v --build --up --test --down
</code></pre>
<h3 id="仅测试指定的用例">仅测试指定的用例</h3>
<pre><code class="lang-sh">go run hack/e2e.go -v -test --test_args=<span class="hljs-string">'--ginkgo.focus=Kubectl\sclient\s\[k8s\.io\]\sKubectl\srolling\-update\sshould\ssupport\srolling\-update\sto\ssame\simage\s\[Conformance\]$'</span>
</code></pre>
<h3 id="跳过测试用例">跳过测试用例</h3>
<pre><code class="lang-sh">go run hack/e2e.go -- -v --test --test_args=<span class="hljs-string">"--ginkgo.skip=Pods.*env
</span></code></pre>
<h3 id="并行测试">并行测试</h3>
<pre><code class="lang-sh"><span class="hljs-comment"># Run tests in parallel, skip any that must be run serially</span>
GINKGO_PARALLEL=y go run hack/e2e.go --v --test --test_args=<span class="hljs-string">"--ginkgo.skip=\[Serial\]"</span>

<span class="hljs-comment"># Run tests in parallel, skip any that must be run serially and keep the test namespace if test failed</span>
GINKGO_PARALLEL=y go run hack/e2e.go --v --test --test_args=<span class="hljs-string">"--ginkgo.skip=\[Serial\] --delete-namespace-on-failure=false"</span>
</code></pre>
<h3 id="清理测试资源">清理测试资源</h3>
<pre><code class="lang-sh">go run hack/e2e.go -- -v --down
</code></pre>
<h3 id="有用的--ctl">有用的 <code>-ctl</code></h3>
<pre><code class="lang-sh"><span class="hljs-comment"># -ctl can be used to quickly call kubectl against your e2e cluster. Useful for</span>
<span class="hljs-comment"># cleaning up after a failed test or viewing logs. Use -v to avoid suppressing</span>
<span class="hljs-comment"># kubectl output.</span>
go run hack/e2e.go -- -v -ctl=<span class="hljs-string">'get events'</span>
go run hack/e2e.go -- -v -ctl=<span class="hljs-string">'delete pod foobar'</span>
</code></pre>
<h2 id="fedaration-e2e-测试">Fedaration e2e 测试</h2>
<pre><code class="lang-sh"><span class="hljs-built_in">export</span> FEDERATION=<span class="hljs-literal">true</span>
<span class="hljs-built_in">export</span> E2E_ZONES=<span class="hljs-string">"us-central1-a us-central1-b us-central1-f"</span>
<span class="hljs-comment"># or export FEDERATION_PUSH_REPO_BASE="quay.io/colin_hom"</span>
<span class="hljs-built_in">export</span> FEDERATION_PUSH_REPO_BASE=<span class="hljs-string">"gcr.io/<span class="hljs-variable">${GCE_PROJECT_NAME}</span>"</span>

<span class="hljs-comment"># build container images</span>
KUBE_RELEASE_RUN_TESTS=n KUBE_FASTBUILD=<span class="hljs-literal">true</span> go run hack/e2e.go -- -v -build

<span class="hljs-comment"># push the federation container images</span>
build/push-federation-images.sh

<span class="hljs-comment"># Deploy federation control plane</span>
go run hack/e2e.go -- -v --up

<span class="hljs-comment"># Finally, run the tests</span>
go run hack/e2e.go -- -v --test --test_args=<span class="hljs-string">"--ginkgo.focus=\[Feature:Federation\]"</span>

<span class="hljs-comment"># Don't forget to teardown everything down</span>
go run hack/e2e.go -- -v --down
</code></pre>
<p>可以用 <code>cluster/log-dump.sh <directory></code> 方便的下载相关日志，帮助排查测试中碰到的问题。</p>
<h2 id="node-e2e-测试">Node e2e 测试</h2>
<p>Node e2e 仅测试 Kubelet 的相关功能，可以在本地或者集群中测试</p>
<pre><code class="lang-sh"><span class="hljs-built_in">export</span> KUBERNETES_PROVIDER=<span class="hljs-built_in">local</span>
make <span class="hljs-built_in">test</span><span class="hljs-_">-e</span>2e-node FOCUS=<span class="hljs-string">"InitContainer"</span>
make <span class="hljs-built_in">test</span>_e2e_node TEST_ARGS=<span class="hljs-string">"--experimental-cgroups-per-qos=true"</span>
</code></pre>
<h2 id="补充说明">补充说明</h2>
<p>借助 kubectl 的模版可以方便获取想要的数据，比如查询某个 container 的镜像的方法为</p>
<pre><code class="lang-sh">kubectl get pods nginx-4263166205-ggst4 -o template <span class="hljs-string">'--template={{if (exists ."status""containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "nginx"}}{{.image}}{{end}}{{end}}{{end}}'</span>
</code></pre>
<h2 id="参考文档">参考文档</h2>
<ul>
<li><a href="https://github.com/kubernetes/community/blob/master/contributors/devel/testing.md" target="_blank">Kubernetes testing</a></li>
<li><a href="https://github.com/kubernetes/community/blob/master/contributors/devel/e2e-tests.md" target="_blank">End-to-End Testing</a></li>
<li><a href="https://github.com/kubernetes/community/blob/master/contributors/devel/e2e-node-tests.md" target="_blank">Node e2e test</a></li>
<li><a href="https://github.com/kubernetes/community/blob/master/contributors/devel/writing-good-e2e-tests.md" target="_blank">How to write e2e test</a></li>
<li><a href="https://github.com/kubernetes/community/blob/master/contributors/guide/coding-conventions.md" target="_blank">Coding Conventions</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="kubernetes-社区贡献" class="level2">社区贡献</h1>
<p>Kubernetes 支持以许多种方式来贡献社区，包括汇报代码缺陷、提交问题修复和功能实现、添加或修复文档、协助用户解决问题等等。</p>
<h2 id="社区结构">社区结构</h2>
<p>Kubernetes 社区由三部分组成</p>
<ul>
<li><a href="http://blog.kubernetes.io/2017/10/kubernetes-community-steering-committee-election-results.html" target="_blank">Steering committe</a></li>
<li><a href="https://contributor.kubernetes.io/sigs/" target="_blank">Special Interest Groups (SIG)</a></li>
<li><a href="https://contributor.kubernetes.io/sigs/#master-working-group-list" target="_blank">Working Groups (WG)</a></li>
</ul>
<p><img src="images/community.png" alt=""/></p>
<h2 id="提交-pull-request-到主分支">提交 Pull Request 到主分支</h2>
<p>当需要修改 Kubernetes 代码时，可以给 Kubernetes 主分支提 Pull Request。这其实是一个标准的 Github 工作流：</p>
<p><img src="images/git_workflow.png" alt=""/></p>
<p>一些加快 PR 合并的方法：</p>
<ul>
<li>使用小的提交，将不同功能的代码分拆到不同的提交甚至是不同的 Pull Request 中</li>
<li>必要的逻辑添加注释说明变更的理由</li>
<li>遵循代码约定，如 <a href="https://github.com/kubernetes/community/blob/master/contributors/devel/coding-conventions.md" target="_blank">Coding Conventions</a>、<a href="https://github.com/kubernetes/community/blob/master/contributors/devel/api-conventions.md" target="_blank">API Conventions</a> 和 <a href="https://github.com/kubernetes/community/blob/master/contributors/devel/kubectl-conventions.md" target="_blank">kubectl Conventions</a></li>
<li>确保修改部分可以本地跑过单元测试和功能测试</li>
<li>使用 <a href="https://prow.k8s.io/command-help" target="_blank">Bot 命令</a> 设置正确的标签或重试失败的测试</li>
</ul>
<h2 id="提交-pull-request-到发布分支">提交 Pull Request 到发布分支</h2>
<p>发布分支的问题一般是首先在主分支里面修复（发送 Pull Request 到主分支并通过代码审核之后合并），然后通过 cherry-pick 的方式发送 Pull Request 到老的分支（如 <code>release-1.7</code> 等）。</p>
<p>对于主分支的 PR，待 Reviewer 添加 <code>cherrypick-candidate</code> 标签后就可以开始 cherry-pick 到老的分支了。但首先需要安装一个 Github 发布的 <a href="https://github.com/github/hub" target="_blank">hub</a> 工具，如</p>
<pre><code class="lang-sh"><span class="hljs-comment"># on macOS</span>
brew install hub

<span class="hljs-comment"># on others</span>
go get github.com/github/hub
</code></pre>
<p>然后执行下面的脚本自动 cherry-pick 并发送 PR 到需要的分支，其中 <code>upstream/release-1.7</code> 是要发布的分支，而 <code>51870</code> 则是发送到主分支的 PR 号：</p>
<pre><code class="lang-sh">hack/cherry_pick_pull.sh upstream/release-1.7 51870
</code></pre>
<p>然后安装输出中的提示操作即可。如果合并过程中发生错误，需要另开一个终端手动合并冲突，并执行 <code>git add . && git am --continue</code>，最后再回去继续，直到 PR 发送成功。</p>
<p>注意：提交到发布分支的每个 PR 除了需要正常的代码审核之外，还需要对应版本的 release manager 批准。当前所有版本的 release manager 可以在 <a href="https://github.com/kubernetes/sig-release/blob/master/release-managers.md" target="_blank">这里</a> 找到。</p>
<h2 id="参考文档">参考文档</h2>
<p>如果在社区贡献中碰到问题，可以参考以下指南</p>
<ul>
<li><strong><a href="https://contributor.kubernetes.io/" target="_blank">Kubernetes Contributor Community</a></strong></li>
<li><strong><a href="https://github.com/kubernetes/community/tree/master/contributors/guide" target="_blank">Kubernetes Contributor Guide</a></strong></li>
<li><strong><a href="https://github.com/kubernetes/community/tree/master/contributors/devel" target="_blank">Kubernetes Developer Guide</a></strong></li>
<li><a href="https://github.com/kubernetes/community" target="_blank">Special Interest Groups</a></li>
<li><a href="https://github.com/kubernetes/features" target="_blank">Feature Tracking and Backlog</a></li>
<li><a href="https://github.com/kubernetes/community/blob/master/contributors/guide/community-expectations.md" target="_blank">Community Expectations</a></li>
<li><a href="https://github.com/kubernetes/sig-release/blob/master/release-managers.md" target="_blank">Kubernetes release managers</a></li>
</ul>
</section>
                            
    <h1 class='level1'>附录</h1><section class="normal markdown-section">
                                
                                <h1 id="kubernetes-生态圈" class="level2">生态圈</h1>
<h2 id="云原生全景">云原生全景</h2>
<p><img src="CloudNativeLandscape.png" alt=""/></p>
<p>图片来源：<a href="https://landscape.cncf.io/" target="_blank">https://landscape.cncf.io/</a>。</p>
<h2 id="云原生地图">云原生地图</h2>
<p><img src="CNCF_TrailMap_latest.png" alt=""/></p>
<p>图片来源：<a href="https://github.com/cncf/landscape" target="_blank">https://github.com/cncf/landscape</a>。</p>
<h2 id="serverless">Serverless</h2>
<p><img src="CloudNativeLandscape_Serverless_latest.png" alt=""/></p>
<p>图片来源：<a href="https://s.cncf.io" target="_blank">https://s.cncf.io</a>。</p>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="kubernetes-学习资源" class="level2">学习资源</h1>
<h2 id="官方文档">官方文档</h2>
<ul>
<li><a href="https://kubernetes.io/" target="_blank">Kubernetes官方网站</a></li>
<li><a href="https://kubernetes.io/docs/" target="_blank">Kubernetes文档</a></li>
<li><a href="https://kubernetes.io/docs/tutorials/" target="_blank">Kubernetes tutorials</a></li>
</ul>
<h2 id="在线课程">在线课程</h2>
<ul>
<li><a href="http://www.edx.org/course/introduction-kubernetes-linuxfoundationx-lfs158x" target="_blank">edX: Introduction to Kubernetes</a></li>
<li><a href="http://in.udacity.com/course/scalable-microservices-with-kubernetes--ud615" target="_blank">Udacity: Scalable Microservices with Kubernetes</a></li>
<li><a href="https://www.edx.org/course/fundamentals-containers-kubernetes-red-hat-do081x" target="_blank">edX: Fundamentals of Containers, Kubernetes, and Red Hat OpenShift</a></li>
</ul>
<h2 id="在线指导">在线指导</h2>
<ul>
<li><a href="https://github.com/kelseyhightower/kubernetes-the-hard-way" target="_blank">Kubernetes the hard way</a></li>
<li><a href="https://github.com/aws-samples/aws-workshop-for-kubernetes" target="_blank">AWS Workshop for Kubernetes</a></li>
<li><a href="https://www.katacoda.com/courses/kubernetes" target="_blank">Learn Kubernetes using Interactive Browser-Based Scenarios</a></li>
<li><a href="https://kubernetesbootcamp.github.io/kubernetes-bootcamp/index.html" target="_blank">Kubernetes Bootcamp</a></li>
<li><a href="https://github.com/walidshaari/Kubernetes-Certified-Administrator" target="_blank">Kubernetes Certified Administration (CKA) online resources</a></li>
<li><a href="http://kubernetesbyexample.com" target="_blank">Kubernetes By Example</a></li>
<li><a href="https://docs.google.com/spreadsheets/d/10NltoF_6y3mBwUzQ4bcQLQfCE1BWSgUDcJXy-Qp2JEU/edit#gid=0" target="_blank">Kubernetes Learning Resources</a></li>
<li><a href="https://github.com/Azure/k8s-best-practices" target="_blank">Kubernetes Best Practices</a></li>
</ul>
<h2 id="电子书籍">电子书籍</h2>
<ul>
<li><a href="https://open.microsoft.com/2018/03/26/new-oreilly-e-book-on-designing-distributed-systems-available-for-free-download/" target="_blank">Designing Distributed Systems</a></li>
<li><a href="https://github.com/feiskyer/kubernetes-handbook" target="_blank">Kubernetes Handbook (Kubernetes 指南)</a></li>
</ul>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="国内镜像列表" class="level2">国内镜像</h1>
<h2 id="docker-hub-镜像">Docker Hub 镜像</h2>
<ul>
<li>Docker 中国镜像：<a href="https://registry.docker-cn.com" target="_blank">https://registry.docker-cn.com</a></li>
<li>开源社镜像：<a href="https://dockerhub.akscn.io" target="_blank">https://dockerhub.akscn.io</a></li>
</ul>
<p>示例</p>
<pre><code class="lang-sh">docker pull registry.docker-cn.com/library/nginx
</code></pre>
<h2 id="gcr（google-container-registry）镜像">GCR（Google Container Registry）镜像</h2>
<ul>
<li>开源社镜像：<a href="https://gcr.akscn.io/google_containers" target="_blank">https://gcr.akscn.io/google_containers</a></li>
</ul>
<p>示例</p>
<pre><code class="lang-sh">docker pull gcr.akscn.io/google_containers/hyperkube:v1.12.1
docker pull gcr.akscn.io/google_containers/pause-amd64:3.1
</code></pre>
<h2 id="kubernetes-rpmdeb镜像">Kubernetes RPM/DEB镜像</h2>
<ul>
<li><a href="http://mirror.azure.cn/kubernetes/packages/" target="_blank">开源社镜像</a></li>
</ul>
<p>示例：</p>
<pre><code class="lang-sh"><span class="hljs-comment"># Ubuntu</span>
cat <<EOF >/etc/apt/sources.list.d/kubernetes.list
deb http://mirror.azure.cn/kubernetes/packages/apt/ kubernetes-xenial main
EOF
</code></pre>
<h3 id="helm-charts-镜像">Helm Charts 镜像</h3>
<ul>
<li>Helm: <a href="http://mirror.azure.cn/kubernetes/helm/" target="_blank">http://mirror.azure.cn/kubernetes/helm/</a></li>
<li>Stable Charts: <a href="http://mirror.azure.cn/kubernetes/charts/" target="_blank">http://mirror.azure.cn/kubernetes/charts/</a></li>
<li>Incubator Charts: <a href="http://mirror.azure.cn/kubernetes/charts-incubator/" target="_blank">http://mirror.azure.cn/kubernetes/charts-incubator/</a></li>
</ul>
<p>示例</p>
<pre><code class="lang-sh">helm repo add stable http://mirror.azure.cn/kubernetes/charts/
helm repo add incubator http://mirror.azure.cn/kubernetes/charts-incubator/
</code></pre>
<h2 id="操作系统镜像">操作系统镜像</h2>
<ul>
<li><a href="http://mirror.azure.cn/" target="_blank">开源社开源镜像</a></li>
<li><a href="https://mirrors.163.com/" target="_blank">网易开源镜像</a></li>
</ul>
<p>以 Ubuntu 18.04（Bionic）为例，修改 /etc/apt/sources.list 文件的内容为</p>
<pre><code class="lang-sh">deb http://azure.archive.ubuntu.com/ubuntu/ bionic main restricted universe multiverse
deb http://azure.archive.ubuntu.com/ubuntu/ bionic-security main restricted universe multiverse
deb http://azure.archive.ubuntu.com/ubuntu/ bionic-updates main restricted universe multiverse
deb http://azure.archive.ubuntu.com/ubuntu/ bionic-proposed main restricted universe multiverse
deb http://azure.archive.ubuntu.com/ubuntu/ bionic-backports main restricted universe multiverse
deb-src http://azure.archive.ubuntu.com/ubuntu/ bionic main restricted universe multiverse
deb-src http://azure.archive.ubuntu.com/ubuntu/ bionic-security main restricted universe multiverse
deb-src http://azure.archive.ubuntu.com/ubuntu/ bionic-updates main restricted universe multiverse
deb-src http://azure.archive.ubuntu.com/ubuntu/ bionic-proposed main restricted universe multiverse
deb-src http://azure.archive.ubuntu.com/ubuntu/ bionic-backports main restricted universe multiverse
</code></pre>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="如何贡献" class="level2">如何贡献</h1>
<ol>
<li>在 Github 上 <a href="https://github.com/feiskyer/kubernetes-handbook/fork" target="_blank">Fork</a> 项目到自己的仓库。</li>
<li>将 fork 后的项目拉到本地: <code>git clone https://github.com/<user-name>/kubernetes-handbook</code>。</li>
<li>新建一个分支，并添加或编辑内容：<code>git checkout -b new-branch</code>。</li>
<li>提交并推送到 github：<code>git commit -am "comments"; git push</code>。</li>
<li>在 Github 上提交 Pull Request。</li>
</ol>
</section>
                            
    <section class="normal markdown-section">
                                
                                <h1 id="参考文档" class="level2">参考文档</h1>
<ul>
<li><a href="https://kubernetes.io/" target="_blank">Kubernetes官方网站</a></li>
<li><a href="https://kubernetes.io/docs/home/" target="_blank">Kubernetes Documentation</a></li>
<li><a href="https://discuss.kubernetes.io/" target="_blank">Discuss Kubernetes</a></li>
<li><a href="https://github.com/kubernetes/community" target="_blank">Kubernetes Contributor Community</a><ul>
<li><a href="https://github.com/kubernetes/community/tree/master/contributors/guide" target="_blank">Kubernetes Contributor Guide</a></li>
<li><a href="https://github.com/kubernetes/community/tree/master/contributors/devel" target="_blank">Kubernetes Developer Guide</a></li>
<li><a href="https://github.com/kubernetes/community" target="_blank">Special Interest Groups</a></li>
<li><a href="https://github.com/kubernetes/features" target="_blank">Kubernetes Features and KEPs</a></li>
<li><a href="https://github.com/kubernetes/sig-release/tree/master/releases" target="_blank">Kubernetes release managers</a></li>
<li><a href="http://submit-queue.k8s.io/#/e2e" target="_blank">Kubernetes submit queue</a></li>
<li><a href="http://perf-dash.k8s.io/" target="_blank">Kubernetes Performance Dashboard</a></li>
<li><a href="http://node-perf-dash.k8s.io/#/builds" target="_blank">Node Performance Dashboard</a></li>
</ul>
</li>
<li><a href="https://cncf.biterg.io" target="_blank">CNCF项目贡献统计</a></li>
<li><a href="https://devstats.k8s.io" target="_blank">Kubernetes项目贡献统计</a></li>
<li><a href="http://velodrome.k8s.io" target="_blank">Kubernetes github metrics</a></li>
<li><a href="https://cloud.google.com/bigquery/public-data/github" target="_blank">Github public data</a></li>
<li><a href="https://github.com/kelseyhightower/kubernetes-the-hard-way" target="_blank">Kubernetes the hard way</a></li>
<li><a href="https://kubernetesbootcamp.github.io/kubernetes-bootcamp/index.html" target="_blank">Kubernetes Bootcamp</a></li>
<li><a href="https://www.usenix.org/system/files/conference/hotcloud16/hotcloud16_burns.pdf" target="_blank">Design patterns for container-based distributed systems</a></li>
<li><a href="https://cloudblogs.microsoft.com/opensource/2018/03/26/new-oreilly-e-book-on-designing-distributed-systems-available-for-free-download/" target="_blank">Designing Distributed Systems</a></li>
<li><a href="https://github.com/ramitsurana/awesome-kubernetes" target="_blank">Awesome Kubernetes</a></li>
<li><a href="https://github.com/veggiemonk/awesome-docker" target="_blank">Awesome Docker</a></li>
</ul>
</section>
                            
    
</body>
</html>
